<?xml version='1.0' encoding='UTF-8'?>
<root>
  <system>lucene</system>
  <issues>
    <issue>
      <id>152</id>
      <title>[PATCH] KStem for Lucene</title>
      <description>September 10th 2003 contributionn from "Sergio Guzman-Lara" &lt;guzman@cs.umass.edu&gt; Original email: Hi all, I have ported the kstem stemmer to Java and incorporated it to Lucene. You can get the source code (Kstem.jar) from the following website: http://ciir.cs.umass.edu/downloads/ Just click on "KStem Java Implementation" (you will need to register your e-mail, for free of course, with the CIIR --Center for Intelligent Information Retrieval, UMass – and get an access code). Content of Kstem.jar: java/org/apache/lucene/analysis/KStemData1.java java/org/apache/lucene/analysis/KStemData2.java java/org/apache/lucene/analysis/KStemData3.java java/org/apache/lucene/analysis/KStemData4.java java/org/apache/lucene/analysis/KStemData5.java java/org/apache/lucene/analysis/KStemData6.java java/org/apache/lucene/analysis/KStemData7.java java/org/apache/lucene/analysis/KStemData8.java java/org/apache/lucene/analysis/KStemFilter.java java/org/apache/lucene/analysis/KStemmer.java KStemData1.java, ..., KStemData8.java Contain several lists of words used by Kstem KStemmer.java Implements the Kstem algorithm KStemFilter.java Extends TokenFilter applying Kstem To compile unjar the file Kstem.jar to Lucene's "src" directory, and compile it there. What is Kstem? A stemmer designed by Bob Krovetz (for more information see http://ciir.cs.umass.edu/pubfiles/ir-35.pdf). Copyright issues This is open source. The actual license agreement is included at the top of every source file. Any comments/questions/suggestions are welcome, Sergio Guzman-Lara Senior Research Fellow CIIR UMass</description>
      <attachments/>
    </issue>
    <issue>
      <id>436</id>
      <title>[PATCH] TermInfosReader, SegmentTermEnum Out Of Memory Exception</title>
      <description>We've been experiencing terrible memory problems on our production search server, running lucene (1.4.3). Our live app regularly opens new indexes and, in doing so, releases old IndexReaders for garbage collection. But...there appears to be a memory leak in org.apache.lucene.index.TermInfosReader.java. Under certain conditions (possibly related to JVM version, although I've personally observed it under both linux JVM 1.4.2_06, and 1.5.0_03, and SUNOS JVM 1.4.1) the ThreadLocal member variable, "enumerators" doesn't get garbage-collected when the TermInfosReader object is gc-ed. Looking at the code in TermInfosReader.java, there's no reason why it shouldn't be gc-ed, so I can only presume (and I've seen this suggested elsewhere) that there could be a bug in the garbage collector of some JVMs. I've seen this problem briefly discussed; in particular at the following URL: http://java2.5341.com/msg/85821.html The patch that Doug recommended, which is included in lucene-1.4.3 doesn't work in our particular circumstances. Doug's patch only clears the ThreadLocal variable for the thread running the finalizer (my knowledge of java breaks down here - I'm not sure which thread actually runs the finalizer). In our situation, the TermInfosReader is (potentially) used by more than one thread, meaning that Doug's patch doesn't allow the affected JVMs to correctly collect garbage. So...I've devised a simple patch which, from my observations on linux JVMs 1.4.2_06, and 1.5.0_03, fixes this problem. Kieran PS Thanks to daniel naber for pointing me to jira/lucene @@ -19,6 +19,7 @@ import java.io.IOException; import org.apache.lucene.store.Directory; +import java.util.Hashtable; /** This stores a monotonically increasing set of &lt;Term, TermInfo&gt; pairs in a Directory. Pairs are accessed either by Term or by ordinal position the @@ -29,7 +30,7 @@ private String segment; private FieldInfos fieldInfos; private ThreadLocal enumerators = new ThreadLocal(); + private final Hashtable enumeratorsByThread = new Hashtable(); private SegmentTermEnum origEnum; private long size; @@ -60,10 +61,10 @@ } private SegmentTermEnum getEnum() { SegmentTermEnum termEnum = (SegmentTermEnum)enumerators.get(); + SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread()); if (termEnum == null) { termEnum = terms(); - enumerators.set(termEnum); + enumeratorsByThread.put(Thread.currentThread(), termEnum); } return termEnum; } @@ -195,5 +196,15 @@ public SegmentTermEnum terms(Term term) throws IOException { get(term); return (SegmentTermEnum)getEnum().clone(); + } + + /* some jvms might have trouble gc-ing enumeratorsByThread */ + protected void finalize() throws Throwable Unknown macro: {+ try { + // make sure gc can clear up. + enumeratorsByThread.clear(); + } finally { + super.finalize(); + } } } TermInfosReader.java, full source: ====================================== package org.apache.lucene.index; /** Copyright 2004 The Apache Software Foundation * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at * http://www.apache.org/licenses/LICENSE-2.0 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import java.io.IOException; import org.apache.lucene.store.Directory; import java.util.Hashtable; /** This stores a monotonically increasing set of &lt;Term, TermInfo&gt; pairs in a Directory. Pairs are accessed either by Term or by ordinal position the set. */ final class TermInfosReader { private Directory directory; private String segment; private FieldInfos fieldInfos; private final Hashtable enumeratorsByThread = new Hashtable(); private SegmentTermEnum origEnum; private long size; TermInfosReader(Directory dir, String seg, FieldInfos fis) throws IOException { directory = dir; segment = seg; fieldInfos = fis; origEnum = new SegmentTermEnum(directory.openFile(segment + ".tis"), fieldInfos, false); size = origEnum.size; readIndex(); } public int getSkipInterval() { return origEnum.skipInterval; } final void close() throws IOException { if (origEnum != null) origEnum.close(); } /** Returns the number of term/value pairs in the set. */ final long size() { return size; } private SegmentTermEnum getEnum() { SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread()); if (termEnum == null) { termEnum = terms(); enumeratorsByThread.put(Thread.currentThread(), termEnum); } return termEnum; } Term[] indexTerms = null; TermInfo[] indexInfos; long[] indexPointers; private final void readIndex() throws IOException { SegmentTermEnum indexEnum = new SegmentTermEnum(directory.openFile(segment + ".tii"), fieldInfos, true); try { int indexSize = (int)indexEnum.size; indexTerms = new Term[indexSize]; indexInfos = new TermInfo[indexSize]; indexPointers = new long[indexSize]; for (int i = 0; indexEnum.next(); i++) { indexTerms[i] = indexEnum.term(); indexInfos[i] = indexEnum.termInfo(); indexPointers[i] = indexEnum.indexPointer; } } finally { indexEnum.close(); } } /** Returns the offset of the greatest index entry which is less than or equal to term.*/ private final int getIndexOffset(Term term) throws IOException { int lo = 0; // binary search indexTerms[] int hi = indexTerms.length - 1; while (hi &gt;= lo) { int mid = (lo + hi) &gt;&gt; 1; int delta = term.compareTo(indexTerms[mid]); if (delta &lt; 0) hi = mid - 1; else if (delta &gt; 0) lo = mid + 1; else return mid; } return hi; } private final void seekEnum(int indexOffset) throws IOException { getEnum().seek(indexPointers[indexOffset], (indexOffset * getEnum().indexInterval) - 1, indexTerms[indexOffset], indexInfos[indexOffset]); } /** Returns the TermInfo for a Term in the set, or null. */ TermInfo get(Term term) throws IOException { if (size == 0) return null; // optimize sequential access: first try scanning cached enum w/o seeking SegmentTermEnum enumerator = getEnum(); if (enumerator.term() != null // term is at or past current &amp;&amp; ((enumerator.prev != null &amp;&amp; term.compareTo(enumerator.prev) &gt; 0) term.compareTo(enumerator.term()) &gt;= 0)) { int enumOffset = (int)(enumerator.position/enumerator.indexInterval)+1; if (indexTerms.length == enumOffset // but before end of block || term.compareTo(indexTerms[enumOffset]) &lt; 0) return scanEnum(term); // no need to seek } // random-access: must seek seekEnum(getIndexOffset(term)); return scanEnum(term); } /** Scans within block for matching term. */ private final TermInfo scanEnum(Term term) throws IOException { SegmentTermEnum enumerator = getEnum(); while (term.compareTo(enumerator.term()) &gt; 0 &amp;&amp; enumerator.next()) {} if (enumerator.term() != null &amp;&amp; term.compareTo(enumerator.term()) == 0) return enumerator.termInfo(); else return null; } /** Returns the nth term in the set. */ final Term get(int position) throws IOException { if (size == 0) return null; SegmentTermEnum enumerator = getEnum(); if (enumerator != null &amp;&amp; enumerator.term() != null &amp;&amp; position &gt;= enumerator.position &amp;&amp; position &lt; (enumerator.position + enumerator.indexInterval)) return scanEnum(position); // can avoid seek seekEnum(position / enumerator.indexInterval); // must seek return scanEnum(position); } private final Term scanEnum(int position) throws IOException { SegmentTermEnum enumerator = getEnum(); while(enumerator.position &lt; position) if (!enumerator.next()) return null; return enumerator.term(); } /** Returns the position of a Term in the set or -1. */ final long getPosition(Term term) throws IOException { if (size == 0) return -1; int indexOffset = getIndexOffset(term); seekEnum(indexOffset); SegmentTermEnum enumerator = getEnum(); while(term.compareTo(enumerator.term()) &gt; 0 &amp;&amp; enumerator.next()) {} if (term.compareTo(enumerator.term()) == 0) return enumerator.position; else return -1; } /** Returns an enumeration of all the Terms and TermInfos in the set. */ public SegmentTermEnum terms() { return (SegmentTermEnum)origEnum.clone(); } /** Returns an enumeration of terms starting at or after the named term. */ public SegmentTermEnum terms(Term term) throws IOException { get(term); return (SegmentTermEnum)getEnum().clone(); } /* some jvms might have trouble gc-ing enumeratorsByThread */ protected void finalize() throws Throwable { try { // make sure gc can clear up. enumeratorsByThread.clear(); } finally { super.finalize(); } } }</description>
      <attachments/>
    </issue>
    <issue>
      <id>532</id>
      <title>[PATCH] Indexing on Hadoop distributed file system</title>
      <description>In my current project we needed a way to create very large Lucene indexes on Hadoop distributed file system. When we tried to do it directly on DFS using Nutch FsDirectory class - we immediately found that indexing fails because DfsIndexOutput.seek() method throws UnsupportedOperationException. The reason for this behavior is clear - DFS does not support random updates and so seek() method can't be supported (at least not easily). Well, if we can't support random updates - the question is: do we really need them? Search in the Lucene code revealed 2 places which call IndexOutput.seek() method: one is in TermInfosWriter and another one in CompoundFileWriter. As we weren't planning to use CompoundFileWriter - the only place that concerned us was in TermInfosWriter. TermInfosWriter uses IndexOutput.seek() in its close() method to write total number of terms in the file back into the beginning of the file. It was very simple to change file format a little bit and write number of terms into last 8 bytes of the file instead of writing them into beginning of file. The only other place that should be fixed in order for this to work is in SegmentTermEnum constructor - to read this piece of information at position = file length - 8. With this format hack - we were able to use FsDirectory to write index directly to DFS without any problems. Well - we still don't index directly to DFS for performance reasons, but at least we can build small local indexes and merge them into the main index on DFS without copying big main index back and forth.</description>
      <attachments/>
    </issue>
    <issue>
      <id>550</id>
      <title>InstantiatedIndex - faster but memory consuming index</title>
      <description>Represented as a coupled graph of class instances, this all-in-memory index store implementation delivers search results up to a 100 times faster than the file-centric RAMDirectory at the cost of greater RAM consumption. Performance seems to be a little bit better than log2n (binary search). No real data on that, just my eyes. Populated with a single document InstantiatedIndex is almost, but not quite, as fast as MemoryIndex. At 20,000 document 10-50 characters long InstantiatedIndex outperforms RAMDirectory some 30x, 15x at 100 documents of 2000 charachters length, and is linear to RAMDirectory at 10,000 documents of 2000 characters length. Mileage may vary depending on term saturation.</description>
      <attachments/>
    </issue>
    <issue>
      <id>584</id>
      <title>Decouple Filter from BitSet</title>
      <description>package org.apache.lucene.search; public abstract class Filter implements java.io.Serializable { public abstract AbstractBitSet bits(IndexReader reader) throws IOException; } public interface AbstractBitSet { public boolean get(int index); } It would be useful if the method =Filter.bits()= returned an abstract interface, instead of =java.util.BitSet=. Use case: there is a very large index, and, depending on the user's privileges, only a small portion of the index is actually visible. Sparsely populated =java.util.BitSet=s are not efficient and waste lots of memory. It would be desirable to have an alternative BitSet implementation with smaller memory footprint. Though it is possibly to derive classes from =java.util.BitSet=, it was obviously not designed for that purpose. That's why I propose to use an interface instead. The default implementation could still delegate to =java.util.BitSet=.</description>
      <attachments/>
    </issue>
    <issue>
      <id>675</id>
      <title>Lucene benchmark: objective performance test for Lucene</title>
      <description>We need an objective way to measure the performance of Lucene, both indexing and querying, on a known corpus. This issue is intended to collect comments and patches implementing a suite of such benchmarking tests. Regarding the corpus: one of the widely used and freely available corpora is the original Reuters collection, available from http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz or http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz. I propose to use this corpus as a base for benchmarks. The benchmarking suite could automatically retrieve it from known locations, and cache it locally.</description>
      <attachments/>
    </issue>
    <issue>
      <id>707</id>
      <title>Lucene Java Site docs</title>
      <description>It would be really nice if the Java site docs where consistent with the rest of the Lucene family (namely, with navigation tabs, etc.) so that one can easily go between Nutch, Hadoop, etc.</description>
      <attachments/>
    </issue>
    <issue>
      <id>753</id>
      <title>Use NIO positional read to avoid synchronization in FSIndexInput</title>
      <description>As suggested by Doug, we could use NIO pread to avoid synchronization on the underlying file. This could mitigate any MT performance drop caused by reducing the number of files in the index format.</description>
      <attachments/>
    </issue>
    <issue>
      <id>794</id>
      <title>Extend contrib Highlighter to properly support PhraseQuery, SpanQuery, ConstantScoreRangeQuery</title>
      <description>This patch adds a new Scorer class (SpanQueryScorer) to the Highlighter package that scores just like QueryScorer, but scores a 0 for Terms that did not cause the Query hit. This gives 'actual' hit highlighting for the range of SpanQuerys, PhraseQuery, and ConstantScoreRangeQuery. New Query types are easy to add. There is also a new Fragmenter that attempts to fragment without breaking up Spans. See http://issues.apache.org/jira/browse/LUCENE-403 for some background. There is a dependency on MemoryIndex.</description>
      <attachments/>
    </issue>
    <issue>
      <id>831</id>
      <title>Complete overhaul of FieldCache API/Implementation</title>
      <description>Motivation: 1) Complete overhaul the API/implementation of "FieldCache" type things... a) eliminate global static map keyed on IndexReader (thus eliminating synch block between completley independent IndexReaders) b) allow more customization of cache management (ie: use expiration/replacement strategies, disk backed caches, etc) c) allow people to define custom cache data logic (ie: custom parsers, complex datatypes, etc... anything tied to a reader) d) allow people to inspect what's in a cache (list of CacheKeys) for an IndexReader so a new IndexReader can be likewise warmed. e) Lend support for smarter cache management if/when IndexReader.reopen is added (merging of cached data from subReaders). 2) Provide backwards compatibility to support existing FieldCache API with the new implementation, so there is no redundent caching as client code migrades to new API.</description>
      <attachments/>
    </issue>
    <issue>
      <id>855</id>
      <title>MemoryCachedRangeFilter to boost performance of Range queries</title>
      <description>Currently RangeFilter uses TermEnum and TermDocs to find documents that fall within the specified range. This requires iterating through every single term in the index and can get rather slow for large document sets. MemoryCachedRangeFilter reads all &lt;docId, value&gt; pairs of a given field, sorts by value, and stores in a SortedFieldCache. During bits(), binary searches are used to find the start and end indices of the lower and upper bound values. The BitSet is populated by all the docId values that fall in between the start and end indices. TestMemoryCachedRangeFilterPerformance creates a 100K RAMDirectory-backed index with random date values within a 5 year range. Executing bits() 1000 times on standard RangeQuery using random date intervals took 63904ms. Using MemoryCachedRangeFilter, it took 876ms. Performance increase is less dramatic when you have less unique terms in a field or using less number of documents. Currently MemoryCachedRangeFilter only works with numeric values (values are stored in a long[] array) but it can be easily changed to support Strings. A side "benefit" of storing the values are stored as longs, is that there's no longer the need to make the values lexographically comparable, i.e. padding numeric values with zeros. The downside of using MemoryCachedRangeFilter is there's a fairly significant memory requirement. So it's designed to be used in situations where range filter performance is critical and memory consumption is not an issue. The memory requirements are: (sizeof(int) + sizeof(long)) * numDocs. MemoryCachedRangeFilter also requires a warmup step which can take a while to run in large datasets (it took 40s to run on a 3M document corpus). Warmup can be called explicitly or is automatically called the first time MemoryCachedRangeFilter is applied using a given field. So in summery, MemoryCachedRangeFilter can be useful when: Performance is critical Memory is not an issue Field contains many unique numeric values Index contains large amount of documents</description>
      <attachments/>
    </issue>
    <issue>
      <id>965</id>
      <title>Implement a state-of-the-art retrieval function in Lucene</title>
      <description>We implemented the axiomatic retrieval function, which is a state-of-the-art retrieval function, to replace the default similarity function in Lucene. We compared the performance of these two functions and reported the results at http://sifaka.cs.uiuc.edu/hfang/lucene/Lucene_exp.pdf. The report shows that the performance of the axiomatic retrieval function is much better than the default function. The axiomatic retrieval function is able to find more relevant documents and users can see more relevant documents in the top-ranked documents. Incorporating such a state-of-the-art retrieval function could improve the search performance of all the applications which were built upon Lucene. Most changes related to the implementation are made in AXSimilarity, TermScorer and TermQuery.java. However, many test cases are hand coded to test whether the implementation of the default function is correct. Thus, I also made the modification to many test files to make the new retrieval function pass those cases. In fact, we found that some old test cases are not reasonable. For example, in the testQueries02 of TestBoolean2.java, the query is "+w3 xx", and we have two documents "w1 xx w2 yy w3" and "w1 w3 xx w2 yy w3". The second document should be more relevant than the first one, because it has more occurrences of the query term "w3". But the original test case would require us to rank the first document higher than the second one, which is not reasonable.</description>
      <attachments/>
    </issue>
    <issue>
      <id>997</id>
      <title>Add search timeout support to Lucene</title>
      <description>This patch is based on Nutch-308. This patch adds support for a maximum search time limit. After this time is exceeded, the search thread is stopped, partial results (if any) are returned and the total number of results is estimated. This patch tries to minimize the overhead related to time-keeping by using a version of safe unsynchronized timer. This was also discussed in an e-mail thread. http://www.nabble.com/search-timeout-tf3410206.html#a9501029</description>
      <attachments/>
    </issue>
    <issue>
      <id>1257</id>
      <title>Port to Java5</title>
      <description>For my needs I've updated Lucene so that it uses Java 5 constructs. I know Java 5 migration had been planned for 2.1 someday in the past, but don't know when it is planned now. This patch against the trunk includes : most obvious generics usage (there are tons of usages of sets, ... Those which are commonly used have been generified) PriorityQueue generification replacement of indexed for loops with for each constructs removal of unnececessary unboxing The code is to my opinion much more readable with those features (you actually know what is stored in collections reading the code, without the need to lookup for field definitions everytime) and it simplifies many algorithms. Note that this patch also includes an interface for the Query class. This has been done for my company's needs for building custom Query classes which add some behaviour to the base Lucene queries. It prevents multiple unnnecessary casts. I know this introduction is not wanted by the team, but it really makes our developments easier to maintain. If you don't want to use this, replace all /Queriable/ calls with standard /Query/.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1343</id>
      <title>A replacement for AsciiFoldingFilter that does a more thorough job of removing diacritical marks or non-spacing modifiers.</title>
      <description>The ISOLatin1AccentFilter takes Unicode characters that have diacritical marks and replaces them with a version of that character with the diacritical mark removed. For example é becomes e. However another equally valid way of representing an accented character in Unicode is to have the unaccented character followed by a non-spacing modifier character (like this: é ) The ISOLatin1AccentFilter doesn't handle the accents in decomposed unicode characters at all. Additionally there are some instances where a word will contain what looks like an accented character, that is actually considered to be a separate unaccented character such as Ł but which to make searching easier you want to fold onto the latin1 lookalike version L . The UnicodeNormalizationFilter can filter out accents and diacritical marks whether they occur as composed characters or decomposed characters, it can also handle cases where as described above characters that look like they have diacritics (but don't) are to be folded onto the letter that they look like ( Ł -&gt; L )</description>
      <attachments/>
    </issue>
    <issue>
      <id>1344</id>
      <title>Make the Lucene jar an OSGi bundle</title>
      <description>In order to use Lucene in an OSGi environment, some additional headers are needed in the manifest of the jar. As Lucene has no dependency, it is pretty straight forward and it ill be easy to maintain I think.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1458</id>
      <title>Further steps towards flexible indexing</title>
      <description>I attached a very rough checkpoint of my current patch, to get early feedback. All tests pass, though back compat tests don't pass due to changes to package-private APIs plus certain bugs in tests that happened to work (eg call TermPostions.nextPosition() too many times, which the new API asserts against). [Aside: I think, when we commit changes to package-private APIs such that back-compat tests don't pass, we could go back, make a branch on the back-compat tag, commit changes to the tests to use the new package private APIs on that branch, then fix nightly build to use the tip of that branch?o] There's still plenty to do before this is committable! This is a rather large change: Switches to a new more efficient terms dict format. This still uses tii/tis files, but the tii only stores term &amp; long offset (not a TermInfo). At seek points, tis encodes term &amp; freq/prox offsets absolutely instead of with deltas delta. Also, tis/tii are structured by field, so we don't have to record field number in every term. . On first 1 M docs of Wikipedia, tii file is 36% smaller (0.99 MB -&gt; 0.64 MB) and tis file is 9% smaller (75.5 MB -&gt; 68.5 MB). . RAM usage when loading terms dict index is significantly less since we only load an array of offsets and an array of String (no more TermInfo array). It should be faster to init too. . This part is basically done. Introduces modular reader codec that strongly decouples terms dict from docs/positions readers. EG there is no more TermInfo used when reading the new format. . There's nice symmetry now between reading &amp; writing in the codec chain – the current docs/prox format is captured in: FormatPostingsTermsDictWriter/Reader FormatPostingsDocsWriter/Reader (.frq file) and FormatPostingsPositionsWriter/Reader (.prx file). This part is basically done. Introduces a new "flex" API for iterating through the fields, terms, docs and positions: FieldProducer -&gt; TermsEnum -&gt; DocsEnum -&gt; PostingsEnum This replaces TermEnum/Docs/Positions. SegmentReader emulates the old API on top of the new API to keep back-compat. Next steps: Plug in new codecs (pulsing, pfor) to exercise the modularity / fix any hidden assumptions. Expose new API out of IndexReader, deprecate old API but emulate old API on top of new one, switch all core/contrib users to the new API. Maybe switch to AttributeSources as the base class for TermsEnum, DocsEnum, PostingsEnum – this would give readers API flexibility (not just index-file-format flexibility). EG if someone wanted to store payload at the term-doc level instead of term-doc-position level, you could just add a new attribute. Test performance &amp; iterate.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1483</id>
      <title>Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector</title>
      <description>This issue changes how an IndexSearcher searches over multiple segments. The current method of searching multiple segments is to use a MultiSegmentReader and treat all of the segments as one. This causes filters and FieldCaches to be keyed to the MultiReader and makes reopen expensive. If only a few segments change, the FieldCache is still loaded for all of them. This patch changes things by searching each individual segment one at a time, but sharing the HitCollector used across each segment. This allows FieldCaches and Filters to be keyed on individual SegmentReaders, making reopen much cheaper. FieldCache loading over multiple segments can be much faster as well - with the old method, all unique terms for every segment is enumerated against each segment - because of the likely logarithmic change in terms per segment, this can be very wasteful. Searching individual segments avoids this cost. The term/document statistics from the multireader are used to score results for each segment. When sorting, its more difficult to use a single HitCollector for each sub searcher. Ordinals are not comparable across segments. To account for this, a new field sort enabled HitCollector is introduced that is able to collect and sort across segments (because of its ability to compare ordinals across segments). This TopFieldCollector class will collect the values/ordinals for a given segment, and upon moving to the next segment, translate any ordinals/values so that they can be compared against the values for the new segment. This is done lazily. All and all, the switch seems to provide numerous performance benefits, in both sorted and non sorted search. We were seeing a good loss on indices with lots of segments (1000?) and certain queue sizes / queries, but the latest results seem to show thats been mostly taken care of (you shouldnt be using such a large queue on such a segmented index anyway). Introduces MultiReaderHitCollector - a HitCollector that can collect across multiple IndexReaders. Old HitCollectors are wrapped to support multiple IndexReaders. TopFieldCollector - a HitCollector that can compare values/ordinals across IndexReaders and sort on fields. FieldValueHitQueue - a Priority queue that is part of the TopFieldCollector implementation. FieldComparator - a new Comparator class that works across IndexReaders. Part of the TopFieldCollector implementation. FieldComparatorSource - new class to allow for custom Comparators. Alters IndexSearcher uses a single HitCollector to collect hits against each individual SegmentReader. All the other changes stem from this Deprecates TopFieldDocCollector FieldSortedHitQueue</description>
      <attachments/>
    </issue>
    <issue>
      <id>1486</id>
      <title>Wildcards, ORs etc inside Phrase queries</title>
      <description>An extension to the default QueryParser that overrides the parsing of PhraseQueries to allow more complex syntax e.g. wildcards in phrase queries. The implementation feels a little hacky - this is arguably better handled in QueryParser itself. This works as a proof of concept for much of the query parser syntax. Examples from the Junit test include: checkMatches("\"j* smyth~\"", "1,2"); //wildcards and fuzzies are OK in phrases checkMatches("\"(jo* -john) smith\"", "2"); // boolean logic works checkMatches("\"jo* smith\"~2", "1,2,3"); // position logic works. checkBadQuery("\"jo* id:1 smith\""); //mixing fields in a phrase is bad checkBadQuery("\"jo* \"smith\" \""); //phrases inside phrases is bad checkBadQuery("\"jo* [sma TO smZ]\" \""); //range queries inside phrases not supported Code plus Junit test to follow...</description>
      <attachments/>
    </issue>
    <issue>
      <id>1518</id>
      <title>Merge Query and Filter classes</title>
      <description>This issue presents a patch, that merges Queries and Filters in a way, that the new Filter class extends Query. This would make it possible, to use every filter as a query. The new abstract filter class would contain all methods of ConstantScoreQuery, deprecate ConstantScoreQuery. If somebody implements the Filter's getDocIdSet()/bits() methods he has nothing more to do, he could just use the filter as a normal query. I do not want to completely convert Filters to ConstantScoreQueries. The idea is to combine Queries and Filters in such a way, that every Filter can automatically be used at all places where a Query can be used (e.g. also alone a search query without any other constraint). For that, the abstract Query methods must be implemented and return a "default" weight for Filters which is the current ConstantScore Logic. If the filter is used as a real filter (where the API wants a Filter), the getDocIdSet part could be directly used, the weight is useless (as it is currently, too). The constant score default implementation is only used when the Filter is used as a Query (e.g. as direct parameter to Searcher.search()). For the special case of BooleanQueries combining Filters and Queries the idea is, to optimize the BooleanQuery logic in such a way, that it detects if a BooleanClause is a Filter (using instanceof) and then directly uses the Filter API and not take the burden of the ConstantScoreQuery (see LUCENE-1345). Here some ideas how to implement Searcher.search() with Query and Filter: User runs Searcher.search() using a Filter as the only parameter. As every Filter is also a ConstantScoreQuery, the query can be executed and returns score 1.0 for all matching documents. User runs Searcher.search() using a Query as the only parameter: No change, all is the same as before User runs Searcher.search() using a BooleanQuery as parameter: If the BooleanQuery does not contain a Query that is subclass of Filter (the new Filter) everything as usual. If the BooleanQuery only contains exactly one Filter and nothing else the Filter is used as a constant score query. If BooleanQuery contains clauses with Queries and Filters the new algorithm could be used: The queries are executed and the results filtered with the filters. For the user this has the main advantage: That he can construct his query using a simplified API without thinking about Filters oder Queries, you can just combine clauses together. The scorer/weight logic then identifies the cases to use the filter or the query weight API. Just like the query optimizer of a RDB.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1536</id>
      <title>if a filter can support random access API, we should use it</title>
      <description>I ran some performance tests, comparing applying a filter via random-access API instead of current trunk's iterator API. This was inspired by LUCENE-1476, where we realized deletions should really be implemented just like a filter, but then in testing found that switching deletions to iterator was a very sizable performance hit. Some notes on the test: Index is first 2M docs of Wikipedia. Test machine is Mac OS X 10.5.6, quad core Intel CPU, 6 GB RAM, java 1.6.0_07-b06-153. I test across multiple queries. 1-X means an OR query, eg 1-4 means 1 OR 2 OR 3 OR 4, whereas +1-4 is an AND query, ie 1 AND 2 AND 3 AND 4. "u s" means "united states" (phrase search). I test with multiple filter densities (0, 1, 2, 5, 10, 25, 75, 90, 95, 98, 99, 99.99999 (filter is non-null but all bits are set), 100 (filter=null, control)). Method high means I use random-access filter API in IndexSearcher's main loop. Method low means I use random-access filter API down in SegmentTermDocs (just like deleted docs today). Baseline (QPS) is current trunk, where filter is applied as iterator up "high" (ie in IndexSearcher's search loop).</description>
      <attachments/>
    </issue>
    <issue>
      <id>1567</id>
      <title>New flexible query parser</title>
      <description>From "New flexible query parser" thread by Micheal Busch in my team at IBM we have used a different query parser than Lucene's in our products for quite a while. Recently we spent a significant amount of time in refactoring the code and designing a very generic architecture, so that this query parser can be easily used for different products with varying query syntaxes. This work was originally driven by Andreas Neumann (who, however, left our team); most of the code was written by Luis Alves, who has been a bit active in Lucene in the past, and Adriano Campos, who joined our team at IBM half a year ago. Adriano is Apache committer and PMC member on the Tuscany project and getting familiar with Lucene now too. We think this code is much more flexible and extensible than the current Lucene query parser, and would therefore like to contribute it to Lucene. I'd like to give a very brief architecture overview here, Adriano and Luis can then answer more detailed questions as they're much more familiar with the code than I am. The goal was it to separate syntax and semantics of a query. E.g. 'a AND b', '+a +b', 'AND(a,b)' could be different syntaxes for the same query. We distinguish the semantics of the different query components, e.g. whether and how to tokenize/lemmatize/normalize the different terms or which Query objects to create for the terms. We wanted to be able to write a parser with a new syntax, while reusing the underlying semantics, as quickly as possible. In fact, Adriano is currently working on a 100% Lucene-syntax compatible implementation to make it easy for people who are using Lucene's query parser to switch. The query parser has three layers and its core is what we call the QueryNodeTree. It is a tree that initially represents the syntax of the original query, e.g. for 'a AND b': AND / \ A B The three layers are: 1. QueryParser 2. QueryNodeProcessor 3. QueryBuilder 1. The upper layer is the parsing layer which simply transforms the query text string into a QueryNodeTree. Currently our implementations of this layer use javacc. 2. The query node processors do most of the work. It is in fact a configurable chain of processors. Each processors can walk the tree and modify nodes or even the tree's structure. That makes it possible to e.g. do query optimization before the query is executed or to tokenize terms. 3. The third layer is also a configurable chain of builders, which transform the QueryNodeTree into Lucene Query objects. Furthermore the query parser uses flexible configuration objects, which are based on AttributeSource/Attribute. It also uses message classes that allow to attach resource bundles. This makes it possible to translate messages, which is an important feature of a query parser. This design allows us to develop different query syntaxes very quickly. Adriano wrote the Lucene-compatible syntax in a matter of hours, and the underlying processors and builders in a few days. We now have a 100% compatible Lucene query parser, which means the syntax is identical and all query parser test cases pass on the new one too using a wrapper. Recent posts show that there is demand for query syntax improvements, e.g improved range query syntax or operator precedence. There are already different QP implementations in Lucene+contrib, however I think we did not keep them all up to date and in sync. This is not too surprising, because usually when fixes and changes are made to the main query parser, people don't make the corresponding changes in the contrib parsers. (I'm guilty here too) With this new architecture it will be much easier to maintain different query syntaxes, as the actual code for the first layer is not very much. All syntaxes would benefit from patches and improvements we make to the underlying layers, which will make supporting different syntaxes much more manageable.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1709</id>
      <title>Parallelize Tests</title>
      <description>The Lucene tests can be parallelized to make for a faster testing system. This task from ANT can be used: http://ant.apache.org/manual/CoreTasks/parallel.html Previous discussion: http://www.gossamer-threads.com/lists/lucene/java-dev/69669 Notes from Mike M.: I'd love to see a clean solution here (the tests are embarrassingly parallelizable, and we all have machines with good concurrency these days)... I have a rather hacked up solution now, that uses "-Dtestpackage=XXX" to split the tests up. Ideally I would be able to say "use N threads" and it'd do the right thing... like the -j flag to make.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1768</id>
      <title>NumericRange support for new query parser</title>
      <description>It would be good to specify some type of "schema" for the query parser in future, to automatically create NumericRangeQuery for different numeric types? It would then be possible to index a numeric value (double,float,long,int) using NumericField and then the query parser knows, which type of field this is and so it correctly creates a NumericRangeQuery for strings like "[1.567..*]" or "(1.787..19.5]". There is currently no way to extract if a field is numeric from the index, so the user will have to configure the FieldConfig objects in the ConfigHandler. But if this is done, it will not be that difficult to implement the rest. The only difference between the current handling of RangeQuery is then the instantiation of the correct Query type and conversion of the entered numeric values (simple Number.valueOf(...) cast of the user entered numbers). Evenerything else is identical, NumericRangeQuery also supports the MTQ rewrite modes (as it is a MTQ). Another thing is a change in Date semantics. There are some strange flags in the current parser that tells it how to handle dates.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1823</id>
      <title>QueryParser with new features for Lucene 3</title>
      <description>I'd like to have a new QueryParser implementation in Lucene 3.1, ideally based on the new QP framework in contrib. It should share as much code as possible with the current StandardQueryParser implementation for easy maintainability. Wish list (feel free to extend): 1. Operator precedence: Support operator precedence for boolean operators 2. Opaque terms: Ability to plugin an external parser for certain syntax extensions, e.g. XML query terms 3. Improved RangeQuery syntax: Use more intuitive &lt;=, =, &gt;= instead of [] and {} 4. Support for trierange queries: See LUCENE-1768 5. Complex phrases: See LUCENE-1486 6. ANY operator: E.g. (a b c d) ANY 3 should match if 3 of the 4 terms occur in the same document 7. New syntax for Span queries: I think the surround parser supports this? 8. Escaped wildcards: See LUCENE-588</description>
      <attachments/>
    </issue>
    <issue>
      <id>2026</id>
      <title>Refactoring of IndexWriter</title>
      <description>I've been thinking for a while about refactoring the IndexWriter into two main components. One could be called a SegmentWriter and as the name says its job would be to write one particular index segment. The default one just as today will provide methods to add documents and flushes when its buffer is full. Other SegmentWriter implementations would do things like e.g. appending or copying external segments [what addIndexes*() currently does]. The second component's job would it be to manage writing the segments file and merging/deleting segments. It would know about DeletionPolicy, MergePolicy and MergeScheduler. Ideally it would provide hooks that allow users to manage external data structures and keep them in sync with Lucene's data during segment merges. API wise there are things we have to figure out, such as where the updateDocument() method would fit in, because its deletion part affects all segments, whereas the new document is only being added to the new segment. Of course these should be lower level APIs for things like parallel indexing and related use cases. That's why we should still provide easy to use APIs like today for people who don't need to care about per-segment ops during indexing. So the current IndexWriter could probably keeps most of its APIs and delegate to the new classes.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2039</id>
      <title>Regex support and beyond in JavaCC QueryParser</title>
      <description>Since the early days the standard query parser was limited to the queries living in core, adding other queries or extending the parser in any way always forced people to change the grammar file and regenerate. Even if you change the grammar you have to be extremely careful how you modify the parser so that other parts of the standard parser are affected by customisation changes. Eventually you had to live with all the limitation the current parser has like tokenizing on whitespaces before a tokenizer / analyzer has the chance to look at the tokens. I was thinking about how to overcome the limitation and add regex support to the query parser without introducing any dependency to core. I added a new special character that basically prevents the parser from interpreting any of the characters enclosed in the new special characters. I choose the forward slash '/' as the delimiter so that everything in between two forward slashes is basically escaped and ignored by the parser. All chars embedded within forward slashes are treated as one token even if it contains other special chars like * []?{} or whitespaces. This token is subsequently passed to a pluggable "parser extension" with builds a query from the embedded string. I do not interpret the embedded string in any way but leave all the subsequent work to the parser extension. Such an extension could be another full featured query parser itself or simply a ctor call for regex query. The interface remains quiet simple but makes the parser extendible in an easy way compared to modifying the javaCC sources. The downsides of this patch is clearly that I introduce a new special char into the syntax but I guess that would not be that much of a deal as it is reflected in the escape method though. It would truly be nice to have more than once extension an have this even more flexible so treat this patch as a kickoff though. Another way of solving the problem with RegexQuery would be to move the JDK version of regex into the core and simply have another method like: protected Query newRegexQuery(Term t) { ... } which I would like better as it would be more consistent with the idea of the query parser to be a very strict and defined parser. I will upload a patch in a second which implements the extension based approach I guess I will add a second patch with regex in core soon too.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2091</id>
      <title>Add BM25 Scoring to Lucene</title>
      <description>http://nlp.uned.es/~jperezi/Lucene-BM25/ describes an implementation of Okapi-BM25 scoring in the Lucene framework, as an alternative to the standard Lucene scoring (which is a version of mixed boolean/TFIDF). I have refactored this a bit, added unit tests and improved the runtime somewhat. I would like to contribute the code to Lucene under contrib.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2215</id>
      <title>paging collector</title>
      <description>http://issues.apache.org/jira/browse/LUCENE-2127?focusedCommentId=12796898&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12796898 Somebody assign this to Aaron McCurry and we'll see if we can get enough votes on this issue to convince him to upload his patch.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2308</id>
      <title>Separately specify a field's type</title>
      <description>This came up from dicussions on IRC. I'm summarizing here... Today when you make a Field to add to a document you can set things index or not, stored or not, analyzed or not, details like omitTfAP, omitNorms, index term vectors (separately controlling offsets/positions), etc. I think we should factor these out into a new class (FieldType?). Then you could re-use this FieldType instance across multiple fields. The Field instance would still hold the actual value. We could then do per-field analyzers by adding a setAnalyzer on the FieldType, instead of the separate PerFieldAnalzyerWrapper (likewise for per-field codecs (with flex), where we now have PerFieldCodecWrapper). This would NOT be a schema! It's just refactoring what we already specify today. EG it's not serialized into the index. This has been discussed before, and I know Michael Busch opened a more ambitious (I think?) issue. I think this is a good first baby step. We could consider a hierarchy of FIeldType (NumericFieldType, etc.) but maybe hold off on that for starters...</description>
      <attachments/>
    </issue>
    <issue>
      <id>2454</id>
      <title>Nested Document query support</title>
      <description>A facility for querying nested documents in a Lucene index as outlined in http://www.slideshare.net/MarkHarwood/proposal-for-nested-document-support-in-lucene</description>
      <attachments/>
    </issue>
    <issue>
      <id>2611</id>
      <title>IntelliJ IDEA and Eclipse setup</title>
      <description>Setting up Lucene/Solr in IntelliJ IDEA or Eclipse can be time-consuming. The attached patches add a new top level directory dev-tools/ with sub-dirs idea/ and eclipse/ containing basic setup files for trunk, as well as top-level ant targets named "idea" and "eclipse" that copy these files into the proper locations. This arrangement avoids the messiness attendant to in-place project configuration files directly checked into source control. The IDEA configuration includes modules for Lucene and Solr, each Lucene and Solr contrib, and each analysis module. A JUnit run configuration per module is included. The Eclipse configuration includes a source entry for each source/test/resource location and classpath setup: a library entry for each jar. For IDEA, once ant idea has been run, the only configuration that must be performed manually is configuring the project-level JDK. For Eclipse, once ant eclipse has been run, the user has to refresh the project (right-click on the project and choose Refresh). If these patches is committed, Subversion svn:ignore properties should be added/modified to ignore the destination IDEA and Eclipse configuration locations. Iam Jambour has written up on the Lucene wiki a detailed set of instructions for applying the 3.X branch patch for IDEA: http://wiki.apache.org/lucene-java/HowtoConfigureIntelliJ</description>
      <attachments/>
    </issue>
    <issue>
      <id>2649</id>
      <title>FieldCache should include a BitSet for matching docs</title>
      <description>The FieldCache returns an array representing the values for each doc. However there is no way to know if the doc actually has a value. This should be changed to return an object representing the values and a BitSet for all valid docs.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2878</id>
      <title>Allow Scorer to expose positions and payloads aka. nuke spans</title>
      <description>Currently we have two somewhat separate types of queries, the one which can make use of positions (mainly spans) and payloads (spans). Yet Span*Query doesn't really do scoring comparable to what other queries do and at the end of the day they are duplicating lot of code all over lucene. Span*Queries are also limited to other Span*Query instances such that you can not use a TermQuery or a BooleanQuery with SpanNear or anthing like that. Beside of the Span*Query limitation other queries lacking a quiet interesting feature since they can not score based on term proximity since scores doesn't expose any positional information. All those problems bugged me for a while now so I stared working on that using the bulkpostings API. I would have done that first cut on trunk but TermScorer is working on BlockReader that do not expose positions while the one in this branch does. I started adding a new Positions class which users can pull from a scorer, to prevent unnecessary positions enums I added ScorerContext#needsPositions and eventually Scorere#needsPayloads to create the corresponding enum on demand. Yet, currently only TermQuery / TermScorer implements this API and other simply return null instead. To show that the API really works and our BulkPostings work fine too with positions I cut over TermSpanQuery to use a TermScorer under the hood and nuked TermSpans entirely. A nice sideeffect of this was that the Position BulkReading implementation got some exercise which now work all with positions while Payloads for bulkreading are kind of experimental in the patch and those only work with Standard codec. So all spans now work on top of TermScorer ( I truly hate spans since today ) including the ones that need Payloads (StandardCodec ONLY)!! I didn't bother to implement the other codecs yet since I want to get feedback on the API and on this first cut before I go one with it. I will upload the corresponding patch in a minute. I also had to cut over SpanQuery.getSpans(IR) to SpanQuery.getSpans(AtomicReaderContext) which I should probably do on trunk first but after that pain today I need a break first . The patch passes all core tests (org.apache.lucene.search.highlight.HighlighterTest still fails but I didn't look into the MemoryIndex BulkPostings API yet)</description>
      <attachments/>
    </issue>
    <issue>
      <id>2899</id>
      <title>Add OpenNLP Analysis capabilities as a module</title>
      <description>Now that OpenNLP is an ASF project and has a nice license, it would be nice to have a submodule (under analysis) that exposed capabilities for it. Drew Farris, Tom Morton and I have code that does: Sentence Detection as a Tokenizer (could also be a TokenFilter, although it would have to change slightly to buffer tokens) NamedEntity recognition as a TokenFilter We are also planning a Tokenizer/TokenFilter that can put parts of speech as either payloads (PartOfSpeechAttribute?) on a token or at the same position. I'd propose it go under: modules/analysis/opennlp</description>
      <attachments/>
    </issue>
    <issue>
      <id>3079</id>
      <title>Faceting module</title>
      <description>Faceting is a hugely important feature, available in Solr today but not [easily] usable by Lucene-only apps. We should fix this, by creating a shared faceting module. Ideally, we factor out Solr's faceting impl, and maybe poach/merge from other impls (eg Bobo browse). Hoss describes some important challenges we'll face in doing this (http://markmail.org/message/5w35c2fr4zkiwsz6), copied here: To look at "faceting" as a concrete example, there are big the reasons faceting works so well in Solr: Solr has total control over the index, knows exactly when the index has changed to rebuild caches, has a strict schema so it can make sense of field types and pick faceting algos accordingly, has multi-phase distributed search approach to get exact counts efficiently across multiple shards, etc... (and there are still a lot of additional enhancements and improvements that can be made to take even more advantage of knowledge solr has because it "owns" the index that we no one has had time to tackle) This is a great list of the things we face in refactoring. It's also important because, if Solr needed to be so deeply intertwined with caching, schema, etc., other apps that want to facet will have the same "needs" and so we really have to address them in creating the shared module. I think we should get a basic faceting module started, but should not cut Solr over at first. We should iterate on the module, fold in improvements, etc., and then, once we can fully verify that cutting over doesn't hurt Solr (ie lose functionality or performance) we can later cutover.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3167</id>
      <title>Make lucene/solr a OSGI bundle through Ant</title>
      <description>We need to make a bundle thriugh Ant, so the binary can be published and no more need the download of the sources. Actually to get a OSGI bundle we need to use maven tools and build the sources. Here the reference for the creation of the OSGI bundle through Maven: https://issues.apache.org/jira/browse/LUCENE-1344 Bndtools could be used inside Ant</description>
      <attachments/>
    </issue>
    <issue>
      <id>3298</id>
      <title>FST has hard limit max size of 2.1 GB</title>
      <description>The FST uses a single contiguous byte[] under the hood, which in java is indexed by int so we cannot grow this over Integer.MAX_VALUE. It also internally encodes references to this array as vInt. We could switch this to a paged byte[] and make the far larger. But I think this is low priority... I'm not going to work on it any time soon.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3312</id>
      <title>Break out StorableField from IndexableField</title>
      <description>In the field type branch we have strongly decoupled Document/Field/FieldType impl from the indexer, by having only a narrow API (IndexableField) passed to IndexWriter. This frees apps up use their own "documents" instead of the "user-space" impls we provide in oal.document. Similarly, with LUCENE-3309, we've done the same thing on the doc/field retrieval side (from IndexReader), with the StoredFieldsVisitor. But, maybe we should break out StorableField from IndexableField, such that when you index a doc you provide two Iterables – one for the IndexableFields and one for the StorableFields. Either can be null. One downside is possible perf hit for fields that are both indexed &amp; stored (ie, we visit them twice, lookup their name in a hash twice, etc.). But the upside is a cleaner separation of concerns in API....</description>
      <attachments/>
    </issue>
    <issue>
      <id>3343</id>
      <title>Comparison operators &gt;,&gt;=,&lt;,&lt;= and = support as RangeQuery syntax in QueryParser</title>
      <description>To offer better interoperability with other search engines and to provide an easier and more straight forward syntax, the operators &gt;, &gt;=, &lt;, &lt;= and = should be available to express an open range query. They should at least work for numeric queries. '=' can be made a synonym for ':'.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3433</id>
      <title>Random access non RAM resident IndexDocValues (CSF)</title>
      <description>There should be a way to get specific IndexDocValues by going through the Directory rather than loading all of the values into memory.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3454</id>
      <title>rename optimize to a less cool-sounding name</title>
      <description>I think users see the name optimize and feel they must do this, because who wants a suboptimal system? but this probably just results in wasted time and resources. maybe rename to collapseSegments or something?</description>
      <attachments/>
    </issue>
    <issue>
      <id>3795</id>
      <title>Replace spatial contrib module with LSP's spatial-lucene module</title>
      <description>I propose that Lucene's spatial contrib module be replaced with the spatial-lucene module within Lucene Spatial Playground (LSP). LSP has been in development for approximately 1 year by David Smiley, Ryan McKinley, and Chris Male and we feel it is ready. LSP is here: http://code.google.com/p/lucene-spatial-playground/ and the spatial-lucene module is intuitively in svn/trunk/spatial-lucene/. I'll add more comments to prevent the issue description from being too long.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4100</id>
      <title>Maxscore - Efficient Scoring</title>
      <description>At Berlin Buzzwords 2012, I will be presenting 'maxscore', an efficient algorithm first published in the IR domain in 1995 by H. Turtle &amp; J. Flood, that I find deserves more attention among Lucene users (and developers). I implemented a proof of concept and did some performance measurements with example queries and lucenebench, the package of Mike McCandless, resulting in very significant speedups. This ticket is to get started the discussion on including the implementation into Lucene's codebase. Because the technique requires awareness about it from the Lucene user/developer, it seems best to become a contrib/module package so that it consciously can be chosen to be used.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4258</id>
      <title>Incremental Field Updates through Stacked Segments</title>
      <description>Shai and I would like to start working on the proposal to Incremental Field Updates outlined here (http://markmail.org/message/zhrdxxpfk6qvdaex).</description>
      <attachments/>
    </issue>
    <issue>
      <id>4345</id>
      <title>Create a Classification module</title>
      <description>Lucene/Solr can host huge sets of documents containing lots of information in fields so that these can be used as training examples (w/ features) in order to very quickly create classifiers algorithms to use on new documents and / or to provide an additional service. So the idea is to create a contrib module (called 'classification') to host a ClassificationComponent that will use already seen data (the indexed documents / fields) to classify new documents / text fragments. The first version will contain a (simplistic) Lucene based Naive Bayes classifier but more implementations should be added in the future.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4956</id>
      <title>the korean analyzer that has a korean morphological analyzer and dictionaries</title>
      <description>Korean language has specific characteristic. When developing search service with lucene &amp; solr in korean, there are some problems in searching and indexing. The korean analyer solved the problems with a korean morphological anlyzer. It consists of a korean morphological analyzer, dictionaries, a korean tokenizer and a korean filter. The korean anlyzer is made for lucene and solr. If you develop a search service with lucene in korean, It is the best idea to choose the korean analyzer.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5052</id>
      <title>bitset codec for off heap filters</title>
      <description>Colleagues, When we filter we don’t care any of scoring factors i.e. norms, positions, tf, but it should be fast. The obvious way to handle this is to decode postings list and cache it in heap (CachingWrappingFilter, Solr’s DocSet). Both of consuming a heap and decoding as well are expensive. Let’s write a posting list as a bitset, if df is greater than segment's maxdocs/8 (what about skiplists? and overall performance?). Beside of the codec implementation, the trickiest part to me is to design API for this. How we can let the app know that a term query don’t need to be cached in heap, but can be held as an mmaped bitset? WDYT?</description>
      <attachments/>
    </issue>
    <issue>
      <id>5189</id>
      <title>Numeric DocValues Updates</title>
      <description>In LUCENE-4258 we started to work on incremental field updates, however the amount of changes are immense and hard to follow/consume. The reason is that we targeted postings, stored fields, DV etc., all from the get go. I'd like to start afresh here, with numeric-dv-field updates only. There are a couple of reasons to that: NumericDV fields should be easier to update, if e.g. we write all the values of all the documents in a segment for the updated field (similar to how livedocs work, and previously norms). It's a fairly contained issue, attempting to handle just one data type to update, yet requires many changes to core code which will also be useful for updating other data types. It has value in and on itself, and we don't need to allow updating all the data types in Lucene at once ... we can do that gradually. I have some working patch already which I'll upload next, explaining the changes.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5205</id>
      <title>SpanQueryParser with recursion, analysis and syntax very similar to classic QueryParser</title>
      <description>This parser extends QueryParserBase and includes functionality from: Classic QueryParser: most of its syntax SurroundQueryParser: recursive parsing for "near" and "not" clauses. ComplexPhraseQueryParser: can handle "near" queries that include multiterms (wildcard, fuzzy, regex, prefix), AnalyzingQueryParser: has an option to analyze multiterms. At a high level, there's a first pass BooleanQuery/field parser and then a span query parser handles all terminal nodes and phrases. Same as classic syntax: term: test fuzzy: roam~0.8, roam~2 wildcard: te?t, test*, t*st regex: /[mb]oat/ phrase: "jakarta apache" phrase with slop: "jakarta apache"~3 default "or" clause: jakarta apache grouping "or" clause: (jakarta apache) boolean and +/-: (lucene OR apache) NOT jakarta; +lucene +apache -jakarta multiple fields: title:lucene author:hatcher Main additions in SpanQueryParser syntax vs. classic syntax: Can require "in order" for phrases with slop with the ~&gt; operator: "jakarta apache"~&gt;3 Can specify "not near": "fever bieber"!~3,10 :: find "fever" but not if "bieber" appears within 3 words before or 10 words after it. Fully recursive phrasal queries with [ and ]; as in: [[jakarta apache]~3 lucene]~&gt;4 :: find "jakarta" within 3 words of "apache", and that hit has to be within four words before "lucene" Can also use [] for single level phrasal queries instead of " as in: [jakarta apache] Can use "or grouping" clauses in phrasal queries: "apache (lucene solr)"~3 :: find "apache" and then either "lucene" or "solr" within three words. Can use multiterms in phrasal queries: "jakarta~1 ap*che"~2 Did I mention full recursion: [[jakarta~1 ap*che]~2 (solr~ /l[ou]+[cs][en]+/)]~10 :: Find something like "jakarta" within two words of "ap*che" and that hit has to be within ten words of something like "solr" or that "lucene" regex. Can require at least x number of hits at boolean level: "apache AND (lucene solr tika)~2 Can use negative only query: -jakarta :: Find all docs that don't contain "jakarta" Can use an edit distance &gt; 2 for fuzzy query via SlowFuzzyQuery (beware of potential performance issues!). Trivial additions: Can specify prefix length in fuzzy queries: jakarta~1,2 (edit distance =1, prefix =2) Can specifiy Optimal String Alignment (OSA) vs Levenshtein for distance &lt;=2: (jakarta~1 (OSA) vs jakarta~&gt;1(Levenshtein) This parser can be very useful for concordance tasks (see also LUCENE-5317 and LUCENE-5318) and for analytical search. Until LUCENE-2878 is closed, this might have a use for fans of SpanQuery. Most of the documentation is in the javadoc for SpanQueryParser. Any and all feedback is welcome. Thank you. Until this is added to the Lucene project, I've added a standalone lucene-addons repo (with jars compiled for the latest stable build of Lucene) on github.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5376</id>
      <title>Add a demo search server</title>
      <description>I think it'd be useful to have a "demo" search server for Lucene. Rather than being fully featured, like Solr, it would be minimal, just wrapping the existing Lucene modules to show how you can make use of these features in a server setting. The purpose is to demonstrate how one can build a minimal search server on top of APIs like SearchManager, SearcherLifetimeManager, etc. This is also useful for finding rough edges / issues in Lucene's APIs that make building a server unnecessarily hard. I don't think it should have back compatibility promises (except Lucene's index back compatibility), so it's free to improve as Lucene's APIs change. As a starting point, I'll post what I built for the "eating your own dog food" search app for Lucene's &amp; Solr's jira issues http://jirasearch.mikemccandless.com (blog: http://blog.mikemccandless.com/2013/05/eating-dog-food-with-lucene.html ). It uses Netty to expose basic indexing &amp; searching APIs via JSON, but it's very rough (lots nocommits).</description>
      <attachments/>
    </issue>
    <issue>
      <id>5527</id>
      <title>Make the Collector API work per-segment</title>
      <description>Spin-off of LUCENE-5299. LUCENE-5229 proposes different changes, some of them being controversial, but there is one of them that I really really like that consists in refactoring the Collector API in order to have a different Collector per segment. The idea is, instead of having a single Collector object that needs to be able to take care of all segments, to have a top-level Collector: public interface Collector { AtomicCollector setNextReader(AtomicReaderContext context) throws IOException; } and a per-AtomicReaderContext collector: public interface AtomicCollector { void setScorer(Scorer scorer) throws IOException; void collect(int doc) throws IOException; boolean acceptsDocsOutOfOrder(); } I think it makes the API clearer since it is now obious setScorer and acceptDocsOutOfOrder need to be called after setNextReader which is otherwise unclear. It also makes things more flexible. For example, a collector could much more easily decide to use different strategies on different segments. In particular, it makes the early-termination collector much cleaner since it can return different atomic collectors implementations depending on whether the current segment is sorted or not. Even if we have lots of collectors all over the place, we could make it easier to migrate by having a Collector that would implement both Collector and AtomicCollector, return this in setNextReader and make current concrete Collector implementations extend this class instead of directly extending Collector.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5569</id>
      <title>Rename AtomicReader to LeafReader</title>
      <description>See LUCENE-5527 for more context: several of us seem to prefer Leaf to Atomic. Talking from my experience, I was a bit confused in the beginning that this thing is named AtomicReader, since Atomic is otherwise used in Java in the context of concurrency. So maybe renaming it to Leaf would help remove this confusion and also carry the information that these readers are used as leaves of top-level readers?</description>
      <attachments/>
    </issue>
    <issue>
      <id>5889</id>
      <title>AnalyzingInfixSuggester should expose commit()</title>
      <description>There is no way short of close() for a user of AnalyzingInfixSuggester to cause it to commit() its underlying index: only refresh() is provided. But callers might want to ensure the index is flushed to disk without closing.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5914</id>
      <title>More options for stored fields compression</title>
      <description>Since we added codec-level compression in Lucene 4.1 I think I got about the same amount of users complaining that compression was too aggressive and that compression was too light. I think it is due to the fact that we have users that are doing very different things with Lucene. For example if you have a small index that fits in the filesystem cache (or is close to), then you might never pay for actual disk seeks and in such a case the fact that the current stored fields format needs to over-decompress data can sensibly slow search down on cheap queries. On the other hand, it is more and more common to use Lucene for things like log analytics, and in that case you have huge amounts of data for which you don't care much about stored fields performance. However it is very frustrating to notice that the data that you store takes several times less space when you gzip it compared to your index although Lucene claims to compress stored fields. For that reason, I think it would be nice to have some kind of options that would allow to trade speed for compression in the default codec.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6198</id>
      <title>two phase intersection</title>
      <description>Currently some scorers have to do a lot of per-document work to determine if a document is a match. The simplest example is a phrase scorer, but there are others (spans, sloppy phrase, geospatial, etc). Imagine a conjunction with two MUST clauses, one that is a term that matches all odd documents, another that is a phrase matching all even documents. Today this conjunction will be very expensive, because the zig-zag intersection is reading a ton of useless positions. The same problem happens with filteredQuery and anything else that acts like a conjunction.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6212</id>
      <title>Remove IndexWriter's per-document analyzer add/updateDocument APIs</title>
      <description>IndexWriter already takes an analyzer up-front (via IndexWriterConfig), but it also allows you to specify a different one for each add/updateDocument. I think this is quite dangerous/trappy since it means you can easily index tokens for that document that don't match at search-time based on the search-time analyzer. I think we should remove this trap in 5.0.</description>
      <attachments/>
    </issue>
  </issues>
</root>
