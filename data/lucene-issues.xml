<?xml version='1.0' encoding='UTF-8'?>
<root>
  <system>lucene</system>
  <issues>
    <issue>
      <id>17</id>
      <title>IndexWriter</title>
      <description>While the input sources are abstracted, the indices are always files in the FileSystem. It would be nice to abstract the IndexWriter to output to other data stores. For example, I would like to trying to use Lucene to index and search a set of short-lived documents while involved in a P2P environment. Ideally, this index (which would be single merge for each peer) can reside in memory rather than in the file system (for reasons of security as much as anything else – I'd prefer not to require permission to write out to the user's filesystem). I think it'd be a nice addition to Lucene. It would make the Lucene engine more easily embedded into other apps.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>31</id>
      <title>The JavaDoc is mixed up!</title>
      <description>static boolean LINUX - True iff running on Windows. Are you sure about this one??? :o) And, yes, 'iff' should be spelled 'if' (check the other descriptions for the same typo). Maybe this set of booleans should instead be a set of integers, because since they are mutually exclusive... and they could be used in the 'switch' block too! Regards.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>36</id>
      <title>Search hits ordering</title>
      <description>Version 1.2 RC5 Hi, Is it possible to change the search hits ordering? I would like to order the hits on a DateField and not on the hit score. Thanks.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>42</id>
      <title>[PATCH] Phonetic Search capability</title>
      <description>I would like Lucene to be able to search based on the way the terms sound, using an algorith like Soundex, Soundex2, or Metaphone.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>43</id>
      <title>Internationalized search</title>
      <description>I would like Lucene to be able to search content in multiple languages, with canadian French and Spanish being the top two that I am concerned with, by just being able to specify the locale at the time of the search.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>44</id>
      <title>Thesauras capability</title>
      <description>I would like Lucene to be able to add terms to a search automatically through a thesauras lookup.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>45</id>
      <title>ACL's on search content</title>
      <description>I would like Lucene to be able to be able to limit its search to specific content by arbitrary information, such as user and company information. We need this kind of capability for our e-commerce website, in that we have company and user rules that would prohibit specific users from being able to see our entire catalog.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>46</id>
      <title>[PATCH] DateField cannot handle dates before January 1970</title>
      <description>Dates before January 1970 cannot be converted to a string representation using the dateToString/timetoString methods of DateField.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>50</id>
      <title>Directory implementation that uses ZIP</title>
      <description>Would is be possible to create an implementation of Directory that uses ZIP to compress the index into a single file, i.e. ZIPDirectory? This would help in distributing indices (with software) to be used on single workstations for read- only purposes.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>60</id>
      <title>One more application is now powered by Lucene</title>
      <description>Hi, we just integrated a Lucene 1.2 based search engine on our company website and are very happy with it We would like to thank you for this useful piece of software and would be pleased if you'd add us to your list of reference applications to encourage others to use it too. We are called Orientation in Objects GmbH and our website can be found under http://www.oio.de BTW we also included a short case study regarding Lucene on our site (currently available in German only ) which might be of interest for your Articles links The article is called Suchengine für ein XML basiertes Content Management System (CMS) which translates to A searchengine for a XML based Content Management System (CMS) and can be found under http://www.oio.de/xmlsuchmaschine-lucene.htm Thanks again best wishes from Mannheim, Germany Kristian Koehler and Steffen Schluff</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>71</id>
      <title>[PATCH] getAllFieldNames diffs for IndexReader</title>
      <description>These diffs were taken from a nightly CVS build. They enable the getAllFieldNames efficient functionality as described on the lucene-dev list on or around 12 November 2002.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>77</id>
      <title>Return multiple fields in Document</title>
      <description>Currently there is no quick way to retrieve multiple fields with the same name, other than retrieving Enumeration of all fields. A useful shortcut would be to provide a method on Document: public Field[] getFields(String name)</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>79</id>
      <title>QueryParser: time not supported in date ranges</title>
      <description>It would be great if time was supported in date ranges. I believe it is a simple change in QueryParser.jj to do this. It should be as simple as changing the following line in QueryParser.jj.getRangeQuery(): DateFormat df = DateFormat.getDateInstance(DateFormat.SHORT); TO DateFormat df = DateFormat.getDateTimeInstance(DateFormat.SHORT, DateFormat.SHORT);</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>84</id>
      <title>Enhanced FSDirectory that allow lock disable via API</title>
      <description>Below is a new version of FSDirectory.java. It is based on 1/30/2003 source. I have added one new version each of getDirectory(File) and getDirectory (String). They take a new third argument 'boolean useLocks'. The previous 'private static final boolean DISABLE_LOCKS' has been changed to 'private static boolean _disableLocks' and is initialized to false. I also added a new method 'private boolean locksDisabledByProp' that checks the 'disableLuceneLocks' system property. Method makeLock now checks the static _disableLocks as the first term in an OR clause, the second of which is a call to locksDisabledByProp. This allows use in an applet that does not have write access to the local filesystem, and when the API is used in that way prevents the query of the system property that is also disallowed in an applet by default (at least in Mozilla/Netscape). From my applet, I can now invoke: Searcher searcher = new IndexSearcher( IndexReader.open(FSDirectory.getDirectory(indexFile, false, false))); and I get an IndexSearcher that will work in the applet with no special permissions other than applet JAR signing. Obviously, email me (jmethot@bea.com) if you have any questions. FSDirectory.java ******************************* package org.apache.lucene.store; /* ==================================================================== The Apache Software License, Version 1.1 * Copyright (c) 2001 The Apache Software Foundation. All rights reserved. * Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * 3. The end-user documentation included with the redistribution, if any, must include the following acknowledgment: "This product includes software developed by the Apache Software Foundation (http://www.apache.org/)." Alternately, this acknowledgment may appear in the software itself, if and wherever such third-party acknowledgments normally appear. * 4. The names "Apache" and "Apache Software Foundation" and "Apache Lucene" must not be used to endorse or promote products derived from this software without prior written permission. For written permission, please contact apache@apache.org. * 5. Products derived from this software may not be called "Apache", "Apache Lucene", nor may "Apache" appear in their name, without prior written permission of the Apache Software Foundation. * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE APACHE SOFTWARE FOUNDATION OR ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ==================================================================== * This software consists of voluntary contributions made by many individuals on behalf of the Apache Software Foundation. For more information on the Apache Software Foundation, please see &lt;http://www.apache.org/&gt;. */ import java.io.IOException; import java.io.File; import java.io.RandomAccessFile; import java.util.Hashtable; import org.apache.lucene.util.Constants; /** Straightforward implementation of {@link Directory} as a directory of files. &lt;p&gt;If the system property 'disableLuceneLocks' has the String value of "true", lock creation will be disabled. * @see Directory @author Doug Cutting */ public final class FSDirectory extends Directory { /** This cache of directories ensures that there is a unique Directory instance per path, so that synchronization on the Directory can be used to synchronize access between readers and writers. * This should be a WeakHashMap, so that entries can be GC'd, but that would require Java 1.2. Instead we use refcounts... */ private static final Hashtable DIRECTORIES = new Hashtable(); private static boolean _disableLocks = false; private boolean locksDisabledByProp() { return Boolean.getBoolean("disableLuceneLocks") || Constants.JAVA_1_1; } /** Returns the directory instance for the named location, with option of disabled locking. * &lt;p&gt;Directories are cached, so that, for a given canonical path, the same FSDirectory instance will always be returned. This permits synchronization on directories. * @param path the path to the directory. @param create if true, create, or erase any existing contents. @param useLocks if false, don't use locks during index reads. Useful in read-only filesystems or from applets. @return the FSDirectory for the named file. */ public static FSDirectory getDirectory(String path, boolean create, boolean useLocks) throws IOException { _disableLocks = !useLocks; return getDirectory(new File(path), create); } /** Returns the directory instance for the named location. * &lt;p&gt;Directories are cached, so that, for a given canonical path, the same FSDirectory instance will always be returned. This permits synchronization on directories. * @param path the path to the directory. @param create if true, create, or erase any existing contents. @return the FSDirectory for the named file. */ public static FSDirectory getDirectory(String path, boolean create) throws IOException { return getDirectory(new File(path), create); } /** Returns the directory instance for the named location, with option of disabled locking. * &lt;p&gt;Directories are cached, so that, for a given canonical path, the same FSDirectory instance will always be returned. This permits synchronization on directories. * @param file the path to the directory. @param create if true, create, or erase any existing contents. @param useLocks if false, don't use locks during index reads. Useful in read-only filesystems or from applets. @return the FSDirectory for the named file. */ public static FSDirectory getDirectory(File file, boolean create, boolean useLocks) throws IOException { _disableLocks = !useLocks; return getDirectory(file, create); } /** Returns the directory instance for the named location. * &lt;p&gt;Directories are cached, so that, for a given canonical path, the same FSDirectory instance will always be returned. This permits synchronization on directories. * @param file the path to the directory. @param create if true, create, or erase any existing contents. @return the FSDirectory for the named file. */ public static FSDirectory getDirectory(File file, boolean create) throws IOException { file = new File(file.getCanonicalPath()); FSDirectory dir; synchronized (DIRECTORIES) Unknown macro: { dir = (FSDirectory)DIRECTORIES.get(file); if (dir == null) { dir = new FSDirectory(file, create); DIRECTORIES.put(file, dir); } else if (create) { dir.create(); } } synchronized (dir) { dir.refCount++; } return dir; } private File directory = null; private int refCount; private FSDirectory(File path, boolean create) throws IOException { directory = path; if (create) create(); if (!directory.isDirectory()) throw new IOException(path + " not a directory"); } private synchronized void create() throws IOException { if (!directory.exists()) if (!directory.mkdir()) throw new IOException("Cannot create directory: " + directory); String[] files = directory.list(); // clear old files for (int i = 0; i &lt; files.length; i++) { File file = new File(directory, files[i]); if (!file.delete()) throw new IOException("couldn't delete " + files[i]); } } /** Returns an array of strings, one for each file in the directory. */ public final String[] list() throws IOException { return directory.list(); } /** Returns true iff a file with the given name exists. */ public final boolean fileExists(String name) throws IOException { File file = new File(directory, name); return file.exists(); } /** Returns the time the named file was last modified. */ public final long fileModified(String name) throws IOException { File file = new File(directory, name); return file.lastModified(); } /** Returns the time the named file was last modified. */ public static final long fileModified(File directory, String name) throws IOException { File file = new File(directory, name); return file.lastModified(); } /** Set the modified time of an existing file to now. */ public void touchFile(String name) throws IOException { File file = new File(directory, name); file.setLastModified(System.currentTimeMillis()); } /** Returns the length in bytes of a file in the directory. */ public final long fileLength(String name) throws IOException { File file = new File(directory, name); return file.length(); } /** Removes an existing file in the directory. */ public final void deleteFile(String name) throws IOException { File file = new File(directory, name); if (!file.delete()) throw new IOException("couldn't delete " + name); } /** Renames an existing file in the directory. */ public final synchronized void renameFile(String from, String to) throws IOException { File old = new File(directory, from); File nu = new File(directory, to); /* This is not atomic. If the program crashes between the call to delete() and the call to renameTo() then we're screwed, but I've been unable to figure out how else to do this... */ if (nu.exists()) if (!nu.delete()) throw new IOException("couldn't delete " + to); if (!old.renameTo(nu)) throw new IOException("couldn't rename " + from + " to " + to); } /** Creates a new, empty file in the directory with the given name. Returns a stream writing this file. */ public final OutputStream createFile(String name) throws IOException { return new FSOutputStream(new File(directory, name)); } /** Returns a stream reading an existing file. */ public final InputStream openFile(String name) throws IOException { return new FSInputStream(new File(directory, name)); } /** Constructs a {@link Lock} with the specified name. Locks are implemented with {@link File#createNewFile() } . * &lt;p&gt;In JDK 1.1 or if system property &lt;I&gt;disableLuceneLocks&lt;/I&gt; is the string "true", locks are disabled. Assigning this property any other string will &lt;B&gt;not&lt;/B&gt; prevent creation of lock files. Locks are also disabled if getDirectory was invoked with the optional third argument useLocks set to false. Disabling locks is useful when using Lucene on read-only medium, such as CD-ROM, or from an applet that does not have filesystem write permission. * @param name the name of the lock file @return an instance of &lt;code&gt;Lock&lt;/code&gt; holding the lock */ public final Lock makeLock(String name) { final File lockFile = new File(directory, name); if (_disableLocks || locksDisabledByProp()) { return new Lock() Unknown macro: { public boolean obtain() throws IOException { return true; } public void release() { return; } public String toString() { return "Lock@locksDisabled"; } } ; } else { return new Lock() Unknown macro: { public boolean obtain() throws IOException { return lockFile.createNewFile(); } public void release() { lockFile.delete(); } public String toString() { return "Lock@" + lockFile; } } ; } } /** Closes the store to future operations. */ public final synchronized void close() throws IOException { if (--refCount &lt;= 0) { synchronized (DIRECTORIES) { DIRECTORIES.remove(directory); } } } /** For debug output. */ public String toString() { return "FSDirectory@" + directory; } } final class FSInputStream extends InputStream { private class Descriptor extends RandomAccessFile { public long position; public Descriptor(File file, String mode) throws IOException { super(file, mode); } } Descriptor file = null; boolean isClone; public FSInputStream(File path) throws IOException { file = new Descriptor(path, "r"); length = file.length(); } /** InputStream methods */ protected final void readInternal(byte[] b, int offset, int len) throws IOException { synchronized (file) { long position = getFilePointer(); if (position != file.position) { file.seek(position); file.position = position; } int total = 0; do { int i = file.read(b, offset+total, len-total); if (i == -1) throw new IOException("read past EOF"); file.position += i; total += i; } while (total &lt; len); } } public final void close() throws IOException { if (!isClone) file.close(); } /** Random-access methods */ protected final void seekInternal(long position) throws IOException { } protected final void finalize() throws IOException { close(); // close the file } public Object clone() { FSInputStream clone = (FSInputStream)super.clone(); clone.isClone = true; return clone; } } final class FSOutputStream extends OutputStream { RandomAccessFile file = null; public FSOutputStream(File path) throws IOException { file = new RandomAccessFile(path, "rw"); } /** output methods: */ public final void flushBuffer(byte[] b, int size) throws IOException { file.write(b, 0, size); } public final void close() throws IOException { super.close(); file.close(); } /** Random-access methods */ public final void seek(long pos) throws IOException { super.seek(pos); file.seek(pos); } public final long length() throws IOException { return file.length(); } protected final void finalize() throws IOException { file.close(); // close the file } }</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>96</id>
      <title>Support for Search Term Highlighting</title>
      <description>Moved from todo.xml: Some of these links broke for me, but I kept them anyway... http://www.geocrawler.org/archives/3/2624/2001/9/50/6553088/ http://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene- dev@jakarta.apache.org&amp;msgId=115271 http://www.iq-computing.de/index.asp?menu=projekte-lucene-highlight http://nagoya.apache.org/eyebrowse/BrowseList?listName=lucene- dev@jakarta.apache.org&amp;by=thread&amp;from=56403 here is a link to search the dev list for discussions about highlighting... http://nagoya.apache.org/eyebrowse/SearchList?listId=&amp;listName=lucene- dev@jakarta.apache.org&amp;searchText=highlight&amp;defaultField=body&amp;Search=Search I believe several changes to help support this were added to 1.3 but the term collection work is still being looked at. I recall two approaches being discussed and maybe merged together. http://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene- dev@jakarta.apache.org&amp;msgId=691046 Perhaps Tatu could fill in a few more details on this bug with pointers to the key messages in the discussion. Were there any patches submitted? Did any of this work end up in the sandbox yet?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>97</id>
      <title>Better support for hits sorted by things other than score.</title>
      <description>An easy, efficient case is to support results sorted by the order documents were added to the index. A little harder and less efficient is support for results sorted by an arbitrary field. http://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene- dev@jakarta.apache.org&amp;msgId=114756 http://www.mail-archive.com/lucene-user@jakarta.apache.org/msg00228.html I don't use the search bean in the contributions, but doesn't it support this?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>98</id>
      <title>[PATCH] Add lastModified() method to Directory</title>
      <description>Moved from todo.xml: Add lastModified() method to Directory, FSDirectory and RamDirectory, so it could be cached in IndexWriter/Searcher manager. I'm not sure what is being asked for here. IndexReader implements: /** Returns the time the index in this directory was last modified. */ public static long lastModified(Directory directory) throws IOException { return directory.fileModified("segments"); } This appears to just be asking to have Directory implement: /** Returns the time this directory was last modified. */ public abstract long lastModified() throws IOException; And FSDirectory something like: /** Returns the time this directory was last modified. */ public long lastModified() throws IOException { return fileModified("segments"); } and something similar in RAMDirectory (although that one would be a little different since its fileModified implementation returns the current time). Is there more to this that I'm missing?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>99</id>
      <title>ability to retrieve the number of occurrences for a phrase</title>
      <description>The ability to retrieve the number of occurrences not only for a term but also for a Phrase. http://www.mail-archive.com/lucene-dev@jakarta.apache.org/msg00101.html</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>100</id>
      <title>[PATCH] Add support for Chinese, Japanese, and Korean to the core build.</title>
      <description>Moved from todo.xml: Che Dong's CJKTokenizer for Chinese, Japanese, and Korean. http://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene- dev@jakarta.apache.org&amp;msgId=330905 and his sigram patch to StandardTokenizer.jj http://nagoya.apache.org/eyebrowse/SearchList?listId=&amp;listName=lucene- dev@jakarta.apache.org&amp;searchText=sigram&amp;defaultField=subject&amp;Search=Search I know there was some discussion about keeping language variant analyzers out of the core a while back, but the sigram change to StandardTokenizer would make the StandardAnalyzer usable for Asian languages. From what I understand about searching in Asian languages the bigram approach used in CJKTokenizer will give better results. I'm not sure of the impact of this change on the QueryParser how/if either of these approaches makes sense along with some of the query syntax. For instance if I had the string of Chinese characters ABCDEFG (notice the lack of spaces between words in this language) and the actual words are AB, CDE and FG how would a Chinese user expect to enter a query that we would do in English as AB +CDE FG? I wish I spoke one of these languages so I would have a better understanding of the search issues. Perhaps Che Dong can give us an idea of if/how a Chinese user would expect to interact with the query syntax. I know I've been struggling with that problem in my own app that needs to support Chinese and Japanese content.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>101</id>
      <title>Selecting a language-specific analyzer according to a locale.</title>
      <description>Moved from todo.xml: Now we rewrite parts of Lucene code in order to use another analyzer. It will be useful to select analyzer without touching code. This was orginally request by Kazuhiro Kazama (kazama@ingrid.org) in http://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene- dev@jakarta.apache.org&amp;msgId=338928 Not sure if this was completed to Kazuhiro Kazama's satisfaction in the current CVS. We can certainly choose which analyzer to use for a given IndexWriter and QueryParser it sounded like he was asking for something like a factory the would create an analyzer based on a locale but unless I don't understand things quite right, searching an index with any analyzer that you didn't create the index with is bound to cause you to have false hits in your results. Perhaps this is fixed or no action should be taken. Can someone with a better understanding of the request comment on this one or close it out?</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>110</id>
      <title>Suggested tag library for searching a lucene index</title>
      <description>The Search custom tag is an iteration tag that loops over the results of criteria passed to the Lucene search engine. This tag supports parameters for passing the search criteria to the search engine and then returns information about the result set to the page programmer through the name used in this tags ID attribute. Any field stored in the search index will be accessible after the start tag has run by calling the result set's someID.getField(String name) method which will locate the value of the field and return it as a string. In the event that no such field exists it will return an empty string. If you need to get a list of the field names ahead of time you can call the tags's getFields() method and it will return you a Set of field names that can be iterated over as you will see in the result.jsp example provided with this documentation. The object passed back through the tag's ID attribute is the current instance of the tag itself, so all of the public information for this instance is available and you will see more reference to this in the tag reference section.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>112</id>
      <title>[PATCH] Add an IndexReader implementation that frees resources when idle and refreshes itself when stale</title>
      <description>Here is a little something I worked on this weekend that I wanted to contribute back as I think others might find it very useful. I extended IndexReader and added support for configuring an idle timeout and refresh interval. It uses a monitoring thread to watch for the reader going idle. When the reader goes idle it is closed. When the index is read again it is re-opened. It uses another thread to periodically check when the reader needs to be refreshed due to a change to index. When the reader is stale, it closes the reader and reopens the index. It is acually delegating all the work to another IndexReader implementation and just handling the threading and synchronization. When it closes a reader, it delegates the close to another thread that waits a bit (configurable how long) before actually closing the reader it was delegating to. This gives any consumers of the original reader a chance to finish up their last action on the reader. This implementation sacrifices a little bit of speed since there is a bit more synchroniztion to deal with and the delegation model puts extra calls on the stack, but it should provide long running applications that have idle periods or frequently changing indices from having to open and close readers all the time or hold open unused resources.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>114</id>
      <title>[PATCH] Add IndexSearcher.close if tag aborts unexpectedly</title>
      <description>The new code submitted yesterday was lacking a fail safe to close any opened index in the event the tag exited abnormally. Also updated the tag descriptor to make the criteria attribute required. I had change the tld during testing and not change it back before I submitted the changes. NOTE: the CollectionTag is not a player in the current taglib but if anyone wants to figure out a way to get the information from that tag into the runtime of the SearchTag either during doInitBody or doAfterBody methods, I am open to suggestion.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>116</id>
      <title>New tags and functionality for lucene-taglib</title>
      <description>HISTORY: 1. Added more robust error handling and the ability to turn it on and off with a throwOnException attribute. (All tags) 2. Added a Column tag for outputting the field names found in a Lucene index. 3. Added a Field tag for retrieving a value for a field in a search result either produced by the Column tag or known in advance. 4. Added new example pages to illustrate how to use the new tags 5. The Collection tag has been deprecated, use the collection attribute of the Search tag instead. 6 Added a lot of new functionality to the search tag, see the index.html file for a full test suite dedicated to the new features.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>122</id>
      <title>[PATCH] Able to pass analyzer for IndexWriter.addDocument() method</title>
      <description>If would be helpfull to be able to pass in an analyzer for IndexWriter.addDocument() method. I am using an analyzer that takes a Locale object so that it can index document with different languages with the correct STOPWORDS, stemmer filter, and lowercase rules. Thus, when I index a list of documents I want to be able to index based on the language of the document, and to do that I need to pass in the languge to the Anaylzer. So, if I could pass in the analyzer during addDocument() that would solve the problem.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>125</id>
      <title>Please increase the default size of HTMLParser summaries or make it ignore graphic's Alt text</title>
      <description>At the top of every page, I have some header graphics w/ Alt text. The problem is that the HTMLParser stores this Alt text in the summary and it shouldn't (all graphics are supposed to have Alt text according to accessibility rules); maybe there should be an option to disable storing Alt text since Lucene has always done this. Even if this is fixed, each of my web pages has a header on the page. Ideally, the summary generator should ignore &lt;Hx&gt; tags (H1, H2, etc.) as well. The header text is the same as the &lt;title&gt; text for the page. This header ends up in the summary as well as the link (the link is the title), so it's wasted space. The end result is that I end up trimming off the first part of the summaries that I get via getParser before storing it in the Lucene index. In the HTMLParser.java file in src\demo\org\apache\lucene\demo\html, the SUMMARY_LENGTH is set to 200, so this effectively is only about 100 for me. Just wanted to give you some feedback instead of just grabbing the source and making my own version of this... This is in 1.3RC3</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>126</id>
      <title>[PATCH] Modifications for retrieval of terms</title>
      <description>Add a getPrefix method to PrefixQuery and to make getEnum(IndexReader reader) in MultiTermQuery a public method. These minor changes allow client code to retrieve matched terms in the index, which consequently makes search term highlighting for prefix and fuzzy queries possible.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>134</id>
      <title>[1.3RC1] QueryParser should handle Query.toString()</title>
      <description>I have a QueryParser that generates a TermQuery with the text between pipes characters ("|"), i.e. the text between the pipes is not analyzed. So, the query : myfield:|3 or 4 words| ...looks for the string "3 or 4 words" in the index. Pretty useful Unfortunately, the string representation (i.e. the result of a call to the Query.toString() method) of this query is : myfield:3 or 4 words ... or in other terms : myfield:3 default_field:or default_field:4 default_field:words ... which, parsed as is, would give me totally different results... and probably none I wonder if it would be possible to have a callback between the Query objects and the QueryParser that have constructed them. This Query object would so delegate its text representation to the QueryParser. This could be done : 1) By creating a makeStringRepresentation method in the QueryParser class : public String makeStringRepresentation(Query query) { String stringRepresentation = null; if (query == null) return null; else if (query instanceOf MySpecialQuery) { ... stringRepresentation = mySpecialRepresentation; } //other custom/non-custom queries else if ... ... //use null to have normal behaviour return stringRepresentation; } 2) By passing a reference to the Query constructor : public Query(QueryParser queryParser) { this.queryParser = queryParser; } 3) By making a test in the toString() method of the Query classes : public String toString(String f) { //callback if the Query object has been constructed by a QueryParser if (this.queryParser != null) { String stringRepresentation; stringRepresentation = queryParser.makeStringRepresentation(this); if (stringRepresentation != null) return stringRepresentation; } else { ... standard Lucene behaviour ... Query objects can live without a QueryParser Does it make sense ?</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>136</id>
      <title>StandardTokenzier with CJK support(sigram)</title>
      <description>diff -ub StandardTokenizer.jj StandardTokenizer.jj.orig — StandardTokenizer.jj Sun Sep 28 01:52:18 2003 +++ StandardTokenizer.jj.orig Sun Sep 28 01:51:57 2003 @@ -54,12 +54,12 @@ options { STATIC = false; - //IGNORE_CASE = true; - //BUILD_PARSER = false; - UNICODE_INPUT = true; +//IGNORE_CASE = true; +//BUILD_PARSER = false; +//UNICODE_INPUT = true; USER_CHAR_STREAM = true; OPTIMIZE_TOKEN_MANAGER = true; - //DEBUG_TOKEN_MANAGER = true; +//DEBUG_TOKEN_MANAGER = true; } PARSER_BEGIN(StandardTokenizer) @@ -89,7 +89,7 @@ TOKEN : { // token patterns // basic word: a sequence of digits &amp; letters -&lt;ALPHANUM: (&lt;LETTER&gt;|&lt;DIGIT&gt;)+ &gt; + &lt;ALPHANUM: (&lt;LETTER&gt;|&lt;DIGIT&gt;)+ &gt; // internal apostrophes: O'Reilly, you're, O'Reilly's // use a post-filter to remove possesives @@ -118,7 +118,6 @@ | &lt;HAS_DIGIT&gt; &lt;P&gt; &lt;ALPHANUM&gt; (&lt;P&gt; &lt;HAS_DIGIT&gt; &lt;P&gt; &lt;ALPHANUM&gt;)+ ) &gt; -| &lt;SIGRAM: (&lt;CJK&gt;) &gt; | &lt;#P: ("_"|"-"|"/"|"."|",") &gt; | &lt;#HAS_DIGIT: // at least one digit (&lt;LETTER&gt;|&lt;DIGIT&gt;)* @@ -127,18 +126,14 @@ &gt; | &lt; #ALPHA: (&lt;LETTER&gt;)+&gt; -| &lt; #LETTER: // alphabets +| &lt; #LETTER: // unicode letters [ "\u0041"-"\u005a", "\u0061"-"\u007a", "\u00c0"-"\u00d6", "\u00d8"-"\u00f6", "\u00f8"-"\u00ff", - "\u0100"-"\u1fff" - ] - &gt; -| &lt; #CJK: // non-alphabets - [ + "\u0100"-"\u1fff", "\u3040"-"\u318f", "\u3300"-"\u337f", "\u3400"-"\u3d2d", @@ -168,7 +163,7 @@ } SKIP : { // skip unrecognized chars -&lt;NOISE: ~[] &gt; + &lt;NOISE: ~[] &gt; } /** Returns the next token in the stream, or null at EOS. @@ -187,7 +182,6 @@ token = &lt;EMAIL&gt; | token = &lt;HOST&gt; | token = &lt;NUM&gt; | token = &lt;SIGRAM&gt; | token = &lt;EOF&gt; ) {</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>139</id>
      <title>[PATCH] Bigram based CJK tokenizer(modified from StopTokenizer)</title>
      <description>/* ==================================================================== The Apache Software License, Version 1.1 * Copyright (c) 2001 The Apache Software Foundation. All rights reserved. * Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * 3. The end-user documentation included with the redistribution, if any, must include the following acknowledgment: "This product includes software developed by the Apache Software Foundation (http://www.apache.org/)." Alternately, this acknowledgment may appear in the software itself, if and wherever such third-party acknowledgments normally appear. * 4. The names "Apache" and "Apache Software Foundation" and "Apache Lucene" must not be used to endorse or promote products derived from this software without prior written permission. For written permission, please contact apache@apache.org. * 5. Products derived from this software may not be called "Apache", "Apache Lucene", nor may "Apache" appear in their name, without prior written permission of the Apache Software Foundation. * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE APACHE SOFTWARE FOUNDATION OR ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ==================================================================== * This software consists of voluntary contributions made by many individuals on behalf of the Apache Software Foundation. For more information on the Apache Software Foundation, please see &lt;http://www.apache.org/&gt;. * $Id: CJKTokenizer.java,v 1.7 2003/05/26 07:14:03 chedong Exp $ */ package org.apache.lucene.analysis.cjk; import org.apache.lucene.analysis.Token; import org.apache.lucene.analysis.Tokenizer; import java.io.Reader; /** &lt;p&gt; CJKTokenizer was modified from StopTokenizer which does a decent job for most European languages. and it perferm other token method for double-byte Characters: the token will return at each two charactors with overlap match.&lt;br&gt; Example: "java C1C2C3C4" will be segment to: "java" "C1C2" "C2C3" "C3C4" it also need filter filter zero length token ""&lt;br&gt; for Digit: digit, '+', '#' will token as letter&lt;br&gt; for more info on Asia language(Chinese Japanese Korean) text segmentation: please search &lt;a href="http://www.google.com/search?q=word+chinese+segment"&gt;google&lt;/a&gt; &lt;/p&gt; * @author Che, Dong */ public final class CJKTokenizer extends Tokenizer { //~ Static fields/initializers --------------------------------------------- /** Max word length */ private static final int MAX_WORD_LEN = 255; /** buffer size: */ private static final int IO_BUFFER_SIZE = 256; //~ Instance fields -------------------------------------------------------- /** word offset, used to imply which character(in ) is parsed */ private int offset = 0; /** the index used only for ioBuffer */ private int bufferIndex = 0; /** data length */ private int dataLen = 0; /** character buffer, store the characters which are used to compose &lt;br&gt; the returned Token */ private final char[] buffer = new char[MAX_WORD_LEN]; /** I/O buffer, used to store the content of the input(one of the &lt;br&gt; members of Tokenizer) */ private final char[] ioBuffer = new char[IO_BUFFER_SIZE]; /** word type: single=&gt;ASCII double=&gt;non-ASCII word=&gt;default */ private String tokenType = "word"; /** tag: previous character is a cached double-byte character "C1C2C3C4" ----(set the C1 isTokened) C1C2 "C2C3C4" ----(set the C2 isTokened) C1C2 C2C3 "C3C4" ----(set the C3 isTokened) "C1C2 C2C3 C3C4" */ private boolean preIsTokened = false; //~ Constructors ----------------------------------------------------------- /** Construct a token stream processing the given input. * @param in I/O reader */ public CJKTokenizer(Reader in) { input = in; } //~ Methods ---------------------------------------------------------------- /** Returns the next token in the stream, or null at EOS. * @return Token * @throws java.io.IOException - throw IOException when read error &lt;br&gt; hanppened in the InputStream * @see "http://java.sun.com/j2se/1.3/docs/api/java/lang/Character.UnicodeBlock.htm l" for detail */ public final Token next() throws java.io.IOException { /** how many character(s) has been stored in buffer */ int length = 0; /** the position used to create Token */ int start = offset; while (true) { /** current charactor */ char c; /** unicode block of current charactor for detail */ Character.UnicodeBlock ub; offset++; if (bufferIndex &gt;= dataLen) { dataLen = input.read(ioBuffer); bufferIndex = 0; } if (dataLen == -1) { if (length &gt; 0) { if (preIsTokened == true) { length = 0; preIsTokened = false; } break; } else { return null; } } else { //get current character c = (char) ioBuffer[bufferIndex++]; //get the UnicodeBlock of the current character ub = Character.UnicodeBlock.of(c); } //if the current character is ASCII or Extend ASCII if ((ub == Character.UnicodeBlock.BASIC_LATIN) || (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS) ) { if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS) { /** convert HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */ int i = (int) c; i = i - 65248; c = (char) i; } // if the current character is a letter or "_" "+" "#" if (Character.isLetterOrDigit(c) || ((c == '_') || (c == '+') || (c == '#')) ) { if (length == 0) { // "javaC1C2C3C4linux" &lt;br&gt; // ^--: the current character begin to token the ASCII // letter start = offset - 1; } else if (tokenType == "double") { // "javaC1C2C3C4linux" &lt;br&gt; // ^--: the previous non-ASCII // : the current character offset--; bufferIndex--; tokenType = "single"; if (preIsTokened == true) { // there is only one non-ASCII has been stored length = 0; preIsTokened = false; break; } else { break; } } // store the LowerCase(c) in the buffer buffer[length++] = Character.toLowerCase(c); tokenType = "single"; // break the procedure if buffer overflowed! if (length == MAX_WORD_LEN) { break; } } else if (length &gt; 0) { if (preIsTokened == true) { length = 0; preIsTokened = false; } else { break; } } } else { // non-ASCII letter, eg."C1C2C3C4" if (Character.isLetter(c)) { if (length == 0) { start = offset - 1; buffer[length++] = c; tokenType = "double"; } else { if (tokenType == "single") { offset--; bufferIndex--; //return the previous ASCII characters break; } else { buffer[length++] = c; tokenType = "double"; if (length == 2) { offset--; bufferIndex--; preIsTokened = true; break; } } } } else if (length &gt; 0) { if (preIsTokened == true) { // empty the buffer length = 0; preIsTokened = false; } else { break; } } } } return new Token(new String(buffer, 0, length), start, start + length, tokenType ); } } /* ==================================================================== The Apache Software License, Version 1.1 * Copyright (c) 2001 The Apache Software Foundation. All rights reserved. * Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * 3. The end-user documentation included with the redistribution, if any, must include the following acknowledgment: "This product includes software developed by the Apache Software Foundation (http://www.apache.org/)." Alternately, this acknowledgment may appear in the software itself, if and wherever such third-party acknowledgments normally appear. * 4. The names "Apache" and "Apache Software Foundation" and "Apache Lucene" must not be used to endorse or promote products derived from this software without prior written permission. For written permission, please contact apache@apache.org. * 5. Products derived from this software may not be called "Apache", "Apache Lucene", nor may "Apache" appear in their name, without prior written permission of the Apache Software Foundation. * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE APACHE SOFTWARE FOUNDATION OR ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ==================================================================== * This software consists of voluntary contributions made by many individuals on behalf of the Apache Software Foundation. For more information on the Apache Software Foundation, please see &lt;http://www.apache.org/&gt;. * $Id: jalopy.xml,v 1.1 2003/04/30 14:36:56 chedong Exp $ */ package org.apache.lucene.analysis.cjk; import org.apache.lucene.analysis.Analyzer; import org.apache.lucene.analysis.StopFilter; import org.apache.lucene.analysis.TokenStream; import java.io.Reader; import java.util.Hashtable; /** Filters CJKTokenizer with StopFilter. * @author Che, Dong */ public class CJKAnalyzer extends Analyzer { //~ Static fields/initializers --------------------------------------------- /** An array containing some common English words that are not usually useful for searching. and some double-byte interpunctions..... */ private static String[] stopWords = { "a", "and", "are", "as", "at", "be", "but", "by", "for", "if", "in", "into", "is", "it", "no", "not", "of", "on", "or", "s", "such", "t", "that", "the", "their", "then", "there", "these", "they", "this", "to", "was", "will", "with", "", "www" } ; //~ Instance fields -------------------------------------------------------- /** stop word list */ private Hashtable stopTable; //~ Constructors ----------------------------------------------------------- /** Builds an analyzer which removes words in STOP_WORDS. */ public CJKAnalyzer() { stopTable = StopFilter.makeStopTable(stopWords); } /** * Builds an analyzer which removes words in the provided array. * * @param stopWords stop word array */ public CJKAnalyzer(String[] stopWords) { stopTable = StopFilter.makeStopTable(stopWords); } //~ Methods ---------------------------------------------------------------- /** get token stream from input * @param fieldName lucene field name @param reader input reader * @return TokenStream */ public final TokenStream tokenStream(String fieldName, Reader reader) { return new StopFilter(new CJKTokenizer(reader), stopTable); } }</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>141</id>
      <title>[PATCH] Provide a java.util.List wrapper around the Hits object</title>
      <description>Saw this on the user list and thought it might be a nice feature to add. I wasn't sure if there would be an objection to modifying Hits to extend AbstractList, so I implemented it both as a separate wrapper class and by making the current Hits object extend AbstractList. There is a separate patch for each. Only one should be applied, I'll leave it to the more experienced folks to vote on which one should be used (if it should be used at all). My preference is for the modification to the Hits object. Eric copied from Tatu's message on the Lucene user list... On Monday 06 October 2003 08:35, Lars Hammer wrote: ... &gt; to iterate the Hits. I thought that Hits was an array of pointers to docs, ^^^ Actually, Hits contains a Vector (could be an array as well), but is not a Collection itself (one can not extend array classes in Java, so no Object besides basic arrays can be arrays or treates as one). Hits be made a Collection, though. In fact, I think it would be a reasonable thing to do, to make Hits be a simple Collection (or perhaps List since it is an ordered collection). You could file an RFE for this, or better yet, implement it. I'd think including such patch for Lucene would make sense as well. &gt; Has anyone any experience in using the &lt;logic:iterate&gt; tag or is it &gt; necessary to write a custom JSP tag which does the iteration?? No, it should be enough to write a simple wrapper that implements Collection, and accesses Hits instance via next() method. + Tatu +</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>142</id>
      <title>[PATCH] DateField support for pre-1970 dates</title>
      <description>I've attached a zip file that contains a set of changes (additions and patches) to enable pre 1970 date support for DateField. (I'm working on an app that needs to index historical dates...) I don't know that this is necessarily the best way to support pre 1970 dates, but its something I've been working with and thought it might work in general... I've added a class (DateFieldDefinition) that discribes how a DateField should be interpreted. (How far plus/minus of 1970 dates should be supported, and what resolution of time differences between dates should be supported.) There are two "well-known" DateFieldDefintions DateFieldDefinition.DEFAULT_DATE_DEF and DateFieldDefinition.HISTORICAL_DATE_DEF. DEFAULT_DATE_DEF is what is currently in place in DateField, i.e. no pre-1970 dates, resolution on date differences of a millisecond. HISTORICAL_DATE_DEF supports dates in the range of 1970 +/- 5,000 years, with a resolution of date differences of a day. I've also added an interface FieldStrategy that describes whether a field is a DateField and which DateFieldDefinition to use for a given field. I modified QueryParser.jj to make use of a FieldStrategy if its available when parsing term queries, range queries and fuzzy queries. QueryParser has been modified to include the following new public methods (existing methods without the FieldStrategy argument remain): static public Query parse(String query, String field, Analyzer analzer, FieldStrategy strategy); public QueryParser(String field, Analyzer analyzer, FieldStrategy strategy); In the absence of a FieldStategy, Queryparser behaves as it does currently with one exception, range queries no longer automatically attempt to treat the field as a date field. (This could be changed, but I'm a fan of parallel semantics between similar functions.) I've added an implementation of FieldStategy, more for illustrative purpose and use in test cases. It assumes that any field that contains '.date' in the name is a DateField and that all dates should be handled with the default definition. Finally, I've modified fuzzy search on date to pick up hits based on how far in time one date is from the query target date rather than on the edit distance of the string representations.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>145</id>
      <title>[PATCH] Token position, phrase matching, stopwords</title>
      <description>The URL I gave is to an archived Lucene-User mailing list post, in which a new user describes surprise at phrase queries succeeding when stopwords appear between phrase tokens in the original text. I think that the default StopFilter.java implementation should implement the position adjusting behavior described in the Lucene API docs: &lt;URL:http://jakarta.apache.org/lucene/docs/api/org/apache/lucene/analysis/Token.html#setPositionIncrement(int)&gt; "Set [the position increment] to values greater than one to inhibit exact phrase matches. If, for example, one does not want phrases to match across removed stop words, then one could build a stop word filter that removes stop words and also sets the increment to the number of stop words removed before each non-stop word. Then exact phrase queries will only match when the terms occur with no intervening stop words."</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>148</id>
      <title>[PATCH] Brasilian Portuguese Analyzer</title>
      <description>Implemented and provided by &lt;atendimento@bizudeanuncio.com&gt;. Suitable for inclusion in the Lucene Sandbox.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>149</id>
      <title>[PATCH] URLDirectory implementation</title>
      <description>August 15th, 2003 contribution from "Lukas Zapletal" &lt;zapletal@inf.upol.cz&gt; Suitable for Lucene Sandbox contribution containing alternate Directory implementations.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>150</id>
      <title>[PATCH] DBDirectory implementation</title>
      <description>Implementation of the Lucene Directory interface which stores data in a JDBC-accessible database. June 2nd, 2003, a contribution from "Anthony Eden" &lt;me@anthonyeden.com&gt;. Original email: Version 1.0 of the DBDirectory library, which implements a Directory which can store indeces in a database is now available for download. There are two versions: Tar GZIP: http://www.anthonyeden.com/download/lucene-dbdirectory-1.0.tar.gz ZIP: http://www.anthonyeden.com/download/lucene-dbdirectory-1.0.zip The source code is included. Please read the README file for instructions on using DBDirectory. I have only tested it with MySQL but would be happy to add other database scripts if anyone would like to submit them. Please post any questions here on the mailing list. Otis, is there anything left to do to get this into the sandbox? Additionally, how will I maintain the code if it is in the sandbox? Will I get write access to the part of the CVS repository which would house DBDirectory? I currently have all of the code in my private CVS. Sincerely, Anthony Eden</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>151</id>
      <title>[PATCH] Clonable RAMDirectory</title>
      <description>A patch for RAMDirectory that makes it clonable. May 22nd, 2003 contribution from "Nick Smith" &lt;nick.smith@techop.ch&gt; Original email: Hi Lucene Developers, Thanks for a great product! I need to be able to 'snapshot' our in-memory indices (RAMDirectory instances). I have been using : RAMDirectory activeDir = new RAMDirectory(); // many inserts, deletes etc RAMDirectory cloneDir = new RAMDirectory(activeDir); but unfortunately this is rather slow for large indices. I have a suggestion - implement java.lang.Cloneable interface in RAMDirectory. I.e to be able to call : RAMDirectory cloneDir = (RAMDirectory)activeDir.clone(); This bypasses the input/output stream handling of the copy constructor by cloneing the underlying buffers that form the directory and is much faster. (Diff attached). Any comments? Regards, Nick</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>152</id>
      <title>[PATCH] KStem for Lucene</title>
      <description>September 10th 2003 contributionn from "Sergio Guzman-Lara" &lt;guzman@cs.umass.edu&gt; Original email: Hi all, I have ported the kstem stemmer to Java and incorporated it to Lucene. You can get the source code (Kstem.jar) from the following website: http://ciir.cs.umass.edu/downloads/ Just click on "KStem Java Implementation" (you will need to register your e-mail, for free of course, with the CIIR --Center for Intelligent Information Retrieval, UMass – and get an access code). Content of Kstem.jar: java/org/apache/lucene/analysis/KStemData1.java java/org/apache/lucene/analysis/KStemData2.java java/org/apache/lucene/analysis/KStemData3.java java/org/apache/lucene/analysis/KStemData4.java java/org/apache/lucene/analysis/KStemData5.java java/org/apache/lucene/analysis/KStemData6.java java/org/apache/lucene/analysis/KStemData7.java java/org/apache/lucene/analysis/KStemData8.java java/org/apache/lucene/analysis/KStemFilter.java java/org/apache/lucene/analysis/KStemmer.java KStemData1.java, ..., KStemData8.java Contain several lists of words used by Kstem KStemmer.java Implements the Kstem algorithm KStemFilter.java Extends TokenFilter applying Kstem To compile unjar the file Kstem.jar to Lucene's "src" directory, and compile it there. What is Kstem? A stemmer designed by Bob Krovetz (for more information see http://ciir.cs.umass.edu/pubfiles/ir-35.pdf). Copyright issues This is open source. The actual license agreement is included at the top of every source file. Any comments/questions/suggestions are welcome, Sergio Guzman-Lara Senior Research Fellow CIIR UMass</description>
      <attachments/>
      <comments>36</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>153</id>
      <title>[PATCH] Arabic Analyzer, Stemmer</title>
      <description>September 28th 2003 contribution from "Pierrick Brihaye" &lt;pierrick.brihaye@wanadoo.fr&gt;. Original email: Hi all, I have written a Lucene Analyzer for arabic. You will find it here : http://perso.wanadoo.fr/pierrick.brihaye/ArabicAnalyzer.jar (provisional adress, anybody interested in hosting it ?) This work is still in beta stage but it gives quite good results In order to make it work, you need : 1) a 1.4+ JVM (because of the native support for regular expressions which are heavily used in the program ; I've been too lazy to use an external package) 2) Apache Jakarta Commons-Collections : http://jakarta.apache.org/commons/collections.html 3) a recent Lucene distribution All this work is based on the amazing Tim Buckwalter's Arabic Morphological Analyzer Version 1.0 (http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2002L49) originaly written in Perl and released under the GPL. The jar contains : a) the compiled classes b) the required data files (dictionaries and compatibility tables) c) 2 command-line test programs d) 3 test documents with different encodings e) the source code f) a README file that will give you a little bit more of information To Lucene developers : I plan to offer this work to Lucene (see the jar hierarchy... and the source file headers . Any objections ? Feedback is very welcome : there are quite a lot of unresolved issues, with the analyzer itselfs as well as with Lucene. mE AlslAmap, cheers, p.b.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>154</id>
      <title>[PATCH] Chinese Tokenizer, Analyzer, Filter</title>
      <description>Date: Thu, 29 Nov 2001 10:18:47 -0800 (PST) From: "Yiyi Sun" &lt;yiyisun@yahoo.com&gt;</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>155</id>
      <title>[PATCH] Better dash handling by StandardTokenizer and QueryParser</title>
      <description>July 11th 2003 contribution from Victor Hadianto &lt;victorh@nuix.com.au&gt; Original email: http://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene-user@jakarta.apache.org&amp;msgNo=4684</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>156</id>
      <title>QueryParser fails when feeding with a stop word with a boosting factor</title>
      <description>so this happens when I use the StandardAnalyzer which contains StopFilter in most of the case the parser autoatically removes the stop words, as one can check at the official demo, found at lucene-1.2/src/demo/org/apache/lucene/demo/SearchFiles this only fails when there is a boosting factor defined for any of the stop word,. like this: the^3</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>160</id>
      <title>contributions XML indexing demo: SAX parsing updates to SAX2</title>
      <description>While using the SAX XML indexing demo in the contributions project, I updated the code for SAX2. Here is the CVS diff for the code update. Erik Hatcher mentioned he could submit this. I hope the attachement is OK if not please contact me. Thanks. diff -u -r1.1 XMLDocumentHandlerSAX.java — XMLDocumentHandlerSAX.java 21 Jun 2002 15:02:51 -0000 1.1 +++ XMLDocumentHandlerSAX.java 3 Nov 2003 22:33:44 -0000 @@ -1,62 +1,96 @@ package org.apache.lucenesandbox.xmlindexingdemo; -import org.xml.sax.*; -import org.xml.sax.helpers.*; -import org.xml.sax.AttributeList; -import javax.xml.parsers.*; - -import org.apache.lucene.document.Document; -import org.apache.lucene.document.Field; - import java.io.File; import java.io.IOException; -public class XMLDocumentHandlerSAX extends HandlerBase -{ /** A buffer for each XML element */ private StringBuffer elementBuffer = new StringBuffer(); - private Document mDocument; - // constructor public XMLDocumentHandlerSAX(File xmlFile) throws ParserConfigurationException, SAXException, IOException { - SAXParserFactory spf = SAXParserFactory.newInstance(); - - SAXParser parser = spf.newSAXParser(); - parser.parse(xmlFile, this); - } - // call at document start public void startDocument() { - mDocument = new Document(); - } - // call at element start public void startElement(String localName, AttributeList atts) throws SAXException { - elementBuffer.setLength(0); - } - // call when cdata found public void characters(char[] text, int start, int length) { - elementBuffer.append(text, start, length); - } - // call at element end public void endElement(String localName) throws SAXException { - mDocument.add(Field.Text(localName, elementBuffer.toString())); - } - public Document getDocument() { - return mDocument; - } +import javax.xml.parsers.ParserConfigurationException; +import javax.xml.parsers.SAXParser; +import javax.xml.parsers.SAXParserFactory; + +import org.apache.lucene.document.Document; +import org.apache.lucene.document.Field; +import org.xml.sax.Attributes; +import org.xml.sax.SAXException; +import org.xml.sax.helpers.DefaultHandler; + +public class XMLDocumentHandlerSAX extends DefaultHandler { + /** A buffer for each XML element */ + private StringBuffer elementBuffer = new StringBuffer(); + + private Document mDocument; + + // constructor + public XMLDocumentHandlerSAX(File xmlFile) + throws ParserConfigurationException, SAXException, IOException { + SAXParserFactory spf = SAXParserFactory.newInstance(); + // use validating parser + spf.setValidating(true); + // make the parser name space aware turn + //spf.setNamespaceAware(true); + + + SAXParser parser = spf.newSAXParser(); + parser.parse(xmlFile, this); + } + + // call at document start + public void startDocument() throws SAXException { + mDocument = new Document(); + } + + // call at element start + public void startElement( + String namespaceURI, + String localName, + String qualifiedName, + Attributes attrs) + throws SAXException { + String eName = localName; + if ("".equals(eName)) { + eName = qualifiedName; // namespaceAware = false + } + // list the attribute(s) + if (attrs != null) { + for (int i = 0; i &lt; attrs.getLength(); i++) Unknown macro: {+ String aName = attrs.getLocalName(i); // Attr name + if ("".equals(aName)) { aName = attrs.getQName(i); }+ // perform application specific action on attribute(s)+ // for now just dump out attribute name and value+ System.out.println("attr " + aName+"="+attrs.getValue(i));+ } + } + elementBuffer.setLength(0); + } + + // call when cdata found + public void characters(char[] text, int start, int length) + throws SAXException { + elementBuffer.append(text, start, length); + } + + // call at element end + public void endElement( + String namespaceURI, + String simpleName, + String qualifiedName) { + + String eName = simpleName; + if ("".equals(eName)) { + eName = qualifiedName; // namespaceAware = false + } + /* + System.out.println( + "endElement eName: " + + eName + + "\teltBuff: " + + elementBuffer.toString()); + */ + mDocument.add(Field.Text(eName, elementBuffer.toString())); + + + } + + public Document getDocument() { + return mDocument; + } }</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>166</id>
      <title>Possibility to update fields of an existing document</title>
      <description>Hi, this is probably gonna be rejected but... We're using lucene to index documents content (word, pdf etc.) with their meta data. The meta data is used to implement access rights to the documents. It is possible (in our system) to change the meta data without the content but currently we have to re-index to entire document (which can take a significant amount of time especially for large PDF's). This results in a performance hit on the server which may not be necessary. Is it possible to add a method to update only some fields of a lucene document and leave the content fields as is? Jeroen</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>171</id>
      <title>[PATCH] French Analyzer</title>
      <description>From: "Patrick TALBOT" &lt;ptalbot@nagora.com&gt; Add to Address Book To: "Lucene Developers List" Subject: RE: French Analyzer Date: Fri, 22 Feb 2002 12:48:20 +0100</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>174</id>
      <title>[PATCH] Indexfiles could cause trouble under Windows</title>
      <description>I tried to index my C:/ drive (using the demo IndexFiles) under Windows 2000 and got several errors: temporary files where I didn't have read access to file.list() returning null which caused nullpointerexception Here is a patch that allowed me to index it without problems. KR, Jean-FranÃ§ois Halleux</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>180</id>
      <title>[PATCH] Language guesser contribution</title>
      <description>Hello, I'd like to contribute this language guesser to Lucene. It contains language guessing interfaces and classes as well as trigram specific classes and some language reference files I generated myself using the trigram file generation utily in there. I included a unit test as well. I didn't do any extensive tests on guessing quality and performance but I would tend to think that they are both OK for a first pass. I thought about writing a custom Analyzer for this but realized that this wouldn't be the way to go and that probably the language decision should be left to the developper, definitely when the Analyzer is used to tokenize a query. Have fun, Jean-FranÃ§ois Halleux</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>182</id>
      <title>[PATCH] SmartDateFormat for QueryParser</title>
      <description>Hi here is the patch to queryparser and additional SmartDateFormat utility that can ease parsing of various date formats, withouth the need to know the server locale. I've implemented as many differen one as I'm aware of. I can write a simple test case if needed.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>185</id>
      <title>[PATCH] Thai Analysis Enhancement</title>
      <description>Unlike other languages, Thai do not have a clear word boundary within a sentence. Words are written consecutively without a delimiter. The Lucene StandardTokenizer currently cannot tokenize a Thai sentence and returns the whole sentence as a token. A special tokenizer to break Thai sentences into words is required.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>187</id>
      <title>[PATCH] minor performance enhancements for DocumentWriter.invertDocument()</title>
      <description>This patch includes two small performance improvements: 1. switch from Hashtable to HashMap and preset the capacity to avoid resizing the HashMap (barely measurable improvement, but easy). 2. add a new Analyzer.tokenStream() method that takes a String instead of a Reader, and call this from within DocumentWriter.invertDocument(). This allows subclasses of Analyzer to provide a more efficient tokenizer for Strings. (The default implementation just uses a StringReader.) I was able to write a variant on LowercaseAnalyzer (not included) that's about 10% faster for my dataset. It works by converting the entire field value with String.toLowerCase() and then using String.substring() to extract the string for each token. This avoids allocating individual char[] arrays inside String for each token, because String.substring() shares its char[] array with the original.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>204</id>
      <title>[PATCH] usage feedback for IndexFiles demo</title>
      <description>Just a small patch that adds "usage" output if the demo is called without a parameter, makes it a little bit friendlier to beginners.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>214</id>
      <title>[PATCH] Field.toString could be more helpful</title>
      <description>org.apache.lucene.document.Field.toString defaults to using Object.toString for some sensible fields. e.g. !isStored &amp;&amp; isIndexed &amp;&amp; !isTokenized fields. This makes debugging slightly more difficult than is really needed. Please find pasted below possible alternative: /** Prints a Field for human consumption. */ public final String toString() { StringBuffer result = new StringBuffer(); if (isStored) { if (isIndexed) { if (isTokenized) { result.append("Text"); } else { result.append("Keyword"); } } else { // XXX warn on tokenized not indexed? result.append("Unindexed"); } } else { if (isIndexed) { if (isTokenized) { result.append("Unstored"); } else { result.append("UnstoredUntokenized"); } } else { result.append("Nonsense_UnstoredUnindexed"); } } result.append('&lt;'); result.append(name); result.append(':'); if (readerValue != null) { result.append(readerValue.toString()); } else { result.append(stringValue); } result.append('&gt;'); return result.toString(); } NB Im working against CVS HEAD</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>217</id>
      <title>[PATCH] new method: Document.remove()</title>
      <description>Here's a patch that adds a remove() method to the Document class (+test case). This is very useful if you have converter classes that return a Lucene Document object but you need to make changes to that object. In my case, I wanted to index PDF files that were saved as BLOBs in a database. The files need to be saved to a temporary file and that file name is given to the PDF converter class. The PDF converter then saves the name of the temporary file name as the file name, which doesn't make sense. So my code needs to remove the 'filename' field and re-add it, this time with the columns primary ID. This is only possible with the attached patch.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>222</id>
      <title>[PATCH] Better "lock obtain timed out" error message</title>
      <description>The attached patch prints the complete path and name of the lock file. This should simplify debugging (it's actually a wish from the Wiki).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>223</id>
      <title>[PATCH] remove unused variables</title>
      <description>Seems I'm the only person who has the "unused variable" warning turned on in Eclipse This patch removes those unused variables and imports (for now only in the "search" package). This doesn't introduce changes in functionality, but it should be reviewed anyway: there might be cases where the variables should be used, but they are not because of a bug.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>229</id>
      <title>[PATCH] Binary stored fields</title>
      <description>Provides a binary Field type that can be used to store byte arrays in the Lucene index. Can be used for a variety of applications from compressed text storage, image storage or as a basis for implementing typed storage (e.g: Integers, Floats, etc.) Based on discussion from lucene-dev list started here: http://marc.theaimsgroup.com/?l=lucene-dev&amp;m=108455161204687&amp;w=2 Directly based on design fleshed out here: http://marc.theaimsgroup.com/?l=lucene-dev&amp;m=108456898230542&amp;w=2 Patch includes updated code and unit tests not included in the patch sent do the lucene-dev list.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>233</id>
      <title>[PATCH] analyzer refactoring based on CVS HEAD from 6/21/2004</title>
      <description>Hello, As mentioned in previous exchanges, notably with Grant Ingersoll, I added some new classes to the "analysis" package to meet the requirements of the feature request in Bugzilla (http://issues.apache.org/bugzilla/show_bug.cgi?id=28182) and did some refactoring while I was under-the-hood. This is an overview of the hierarchies per my changes: -Analyzer --CustomAnalyzer (new abstract class largely based on Grant's BaseAnalyzer) – AbstractAnalyzer (new abstract class) ---RussianAnalyzer ---GermanAnalyzer — etc. -Tokenizer --CloneableTokenizer (new abstract class) ---StandardTokenizer ---CharTokenizer ---CJKTokenizer ---etc. -TokenFilter --CloneableTokenFilter (new abstract class) ---AbstractStemFilter (new abstract class) ----RussianStemFilter ----GermanStemFilter ----etc. -Stemmer (very simple new interface used in AbstractStemFilter) – PorterStemmer --RussianStemmer --etc. In the attached zip file there are 3 diff files (core.analysis, sandbox.analysis, and sandbox.analysis.snowball) and a zip containing the new classes for org.apache.lucene.analysis in the lucene core. I tried to minimize the irrelevant code changes (e.g. style, spaces, etc.) in the diffs while conforming to the code formatting guidelines outlined by Otis. I think there were a number of classes in the "analysis" package that didn't conform so these diffs may have a lot of noise as I reformatted those classe with my IDE, sorry . If the diffs are too painful then let me know and I'll try to prune them. If there is a TODO list specific to Analyzers, are the below items on that list? 1) move German and Russian packages to sandbox (I think this is on the Lucene TODO list) 2) Analyzer class renaming such that dynamic configuration could return classes like Analyzer_ru, Analyzer_de, Analyzer_fr, etc. based on the class naming scheme "Analyzer_ {Locale.toString} " 3) Documentation Question, comments, feedback, criticisms are all welcome...... Regards, RBP PS - Thanks Grant!</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>243</id>
      <title>[PATCH] setIndexInterval() in IndexWriter</title>
      <description>Following a discussion with Doug (see http://article.gmane.org/gmane.comp.jakarta.lucene.devel/5804) here is a patch that add a setIndexInterval() in IndexWriter. This patch adds also a getDirectory method to IndexWriter and modifies SegmentMerger, IndexWriter and TermInfosWriter. This patch passes all tests. Any comments/criticisms welcome. Julien</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>247</id>
      <title>Confusing code line</title>
      <description>Line 81 of TermScorer: if (!(target &gt; docs[pointer])) { Could be replaced with the more readable: if (docs[pointer] &gt;= target) { Sorry for nit picking!</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>248</id>
      <title>[PATCH] Add StopFilter ignoreCase option</title>
      <description>Wanted to have the ability to ignore case in the stop filter. In some cases, I don't want to have to lower case before passing through the stop filter, b/c I may need case preserved for other analysis further down the stream, yet I don't need the stopwords and I don't want to have to apply stopword filters twice.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>254</id>
      <title>[PATCH] pseudo-relevance feedback enhancement</title>
      <description>Hi, A while back I wrote some code to implement automatic query expansion in Lucene. Now I ripped off the application specifics and put it in more or less "test-case style" in the attachment. Apart from the sources and classes, it contains a tiny index consisting of sample data I got from the Lucene FAQ. Almost too small to be meaningful, but with the sample query you'll get at least a different number of hits. I made some arbitrary decisions on where to put the classes. Mainly they ended up in the lucenesandbox-package and should have unique names, hence not overwriting anything. The comments in the source were chiefly written to illustrate the flow to not-so-experienced Lucene users/java programmers. You might find self-evident what they describe. Please refer to the "quick start" section in the readme file to get things going (should be really quick). If you have any questions or comments, I'll appreciate your feedback. Best regards, Rene Hackl</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>265</id>
      <title>[PATCH] to remove synchronized code from TermVectorsReader</title>
      <description>Otis, here the latest and last patch to get rid of all synchronized code from TermVectorsReader. It should include at least 3 files, TermVectorsReader.diff, SegmentReader.diff and the new junit test case TestMultiThreadTermVectors.java. The patch was generated against the current CVS version of TermVectorsReader and SegmentReader. All lucene related junit tests pass fine. best regards Bernhard</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>272</id>
      <title>[PATCH] Remove equals() from internal Comparator of ConjunctionScorer</title>
      <description>As written, the equals() method is not used. The docs of java.util.Comparator have an equals() with a single arg to compare the Comparator itself to another one, which is hardly ever useful. Patch follows</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>273</id>
      <title>Move IndexReader.delete functionality to IndexWriter</title>
      <description>I propose moving the functionality of deleting a document from the index from IndexReader to IndexWriter, not only because because it is misleading that a reader modifies the index, but also because of the following problem: I have a website index, that should be updated but that should also be available for searching while updating. So I open an IndexReader for searching the index for a specific document and for deleting an outdated document from the index and I open an IndexWriter to add updated documents to the index. But there is a conflict between the locks of those two instances. If I want to update a document, I have to close the IndexWriter (because otherwise IndexReader could not obtain the lock for writing to the index) and the IndexReader (because it would complain if the index was modified since opening the reader). Then I have to create a new reader to delete the document from the index, and I have to close this reader as it does not release the write lock after deleting a document, so I would not be able to open an IndexWriter. And now I can open a fresh IndexWriter for adding the new document again and a fresh IndexReader for searching the index for the next document. You see, in the worst case (every document has been modified) the program is only opening and closing readers and writers all the time. If the IndexWriter deleted documents from the index, only this instance would need a write lock and I could only use one IndexReader and one IndexWriter for updating the whole index. If you know a better way to update the index, then let me know. Tilman Giese</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>274</id>
      <title>[PATCH] to store binary fields with compression</title>
      <description>hi all, as promised here is the enhancement for the binary field patch with optional compression. The attachment includes all necessary diffs based on the latest version from CVS. There is also a small junit test case to test the core functionality for binary field compression. The base implementation for binary fields where this patch relies on, can be found in patch #29370. The existing unit tests pass fine. For testing binary fields and compression, I'm creating an index from 2700 plain text files (avg. 6kb per file) and store all file content within that index without using compression. The test was created using the IndexFiles class from the demo distribution. Setting up the index and storing all content without compression took about 60 secs and the final index size was 21 MB. Running the same test, switching compression on, the time to index increase to 75 secs, but the final index size shrinks to 13 MB. This is less than the plain text files them self need in the file system (15 MB) Hopefully this patch helps people dealing with huge index and want to store more than just 300 bytes per document to display a well formed summary. regards Bernhard</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>278</id>
      <title>Fuzziness for date searches</title>
      <description>A "fuzzy date search" could match documents around a given date, giving matches that are near to that date a higher score. See bug #23685.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>279</id>
      <title>[PATCH] Javadoc improvements and minor fixes</title>
      <description>Javadoc improvements for Scorer.java and Weight.java. This also fixes some recent changes introduced minor warnings when building the javadocs and adds a small comment in Similarity.java. The individual patches will be attached.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>285</id>
      <title>David Spencer Spell Checker improved</title>
      <description>hy, i developed a SpellChecker based on the David Spencer code (DSc) but more flexible. the structure of the index is inspired of the DSc (for a 3-4 gram): word: gram3: gram4: 3start: 4start: .. 3end: 4end: .. transposition: This index is a dictonary so there isn't the "freq" field like with DSc version. it's independant of the user index. So we can add words becoming to several fields of several index for example or, why not, to a file with a list of words. The suggestSimilar method return a list of suggests word sorted by the Levenshtein distance and optionaly to the popularity of the word for a specific field in a user index. More of that, this list can be restricted only to words present in a specific field of a user index. See the test case. i hope this code will be put in the lucene sandbox. Nicolas Maisonneuve</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>291</id>
      <title>[PATCH] FSDirectory create() method deletes all files</title>
      <description>hi all, the current implementation of FSDirectory.create(...) silently deletes all files (even empty directories) within the index directory when setting up a new index with create option enabled. Lucene doesn't care when deleting files in the index directory if they belong to lucene or not. I don't think that this is a real bug, but it can be a pain if somebody whants to store some private information in the lucene index directory, e.g some configuration files. Therefore i implemented a FileFilter which knows about the internal lucene file extensions, so that all other files would never get touched when creating a new index. The current patch is an enhancement in FSDirectory only. I don't think that there is a need to make it available in the Directory class and change all it's depending classes. regards Bernhard</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>292</id>
      <title>[PATCH] Comment corrections in MMapDirectory.java</title>
      <description>These comments ended up on the wrong lines after the last changes</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>293</id>
      <title>[PATCH] Scoring terms by density</title>
      <description>These 3 java files provide scoring terms by density. The score is term frequency in the document field divided by the field length. The field length is derived from the default stored norm. It may be necessary to relax some access restrictions in TermQuery.java to make this compile.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>296</id>
      <title>[PATCH] FuzzyTermEnum optimization and refactor</title>
      <description>I took a look at it to see if it could be improved. I saw speed improvements of 20% - 40% by making a couple changes. The patch is here: http://www.hagerfamily.com/patches/FuzzyTermEnumOptimizePatch.txt The Patch is based on the HEAD of the CVS tree as of Oct 22, 2004. What Changed? Since the word was discarded if the edit distance for the word was above a certain threshold, I updated the distance algorithm to abort if at any time during the calculation it is determined that the best possible outcome of the edit distance algorithm is above this threshold. The source code has a great explanation. I also reduced the amount of floating point math, reduced the amount of potential space the array takes in its first dimension, removed the potential divide by 0 error when one term is an empty string, and fixed a bug where an IllegalArgumentException was thrown if the class was somehow initialized wrong, instead of looking at the arguments. The behavior is almost identical. The exception is that similarity is set to 0.0 when it is guaranteed to be below the minimum similarity. Results I saw the biggest improvement from longer words, which makes a sense. My long word was "bridgetown" and I saw a 60% improvement on this. The biggest improvement are for words that are farthest away from the median length of the words in the index. Short words (1-3 characters) saw a 30% improvement. Medium words saw a 10% improvement (5-7 characters). These improvements are with the prefix set to 0.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>300</id>
      <title>[PATCH] Refactoring of SpanScorer</title>
      <description>Refactored some common code in next() and skipTo(). Removed dependency on score value for next() and skipTo(). Passes all current tests at just about the same speed as the current version. Added minimal javadoc. Iirc, there has been some discussion on the dependency of next() and skipTo() on the score value, but I don't remember the conclusion. In case that dependency should stay in, it can be adapted in the refactored code.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>301</id>
      <title>Index Writer constructor flags unclear - and annoying in certain cases</title>
      <description>Wouldn't it make more sense if the constructor for the IndexWriter always created an index if it doesn't exist - and the boolean parameter should be "clear" (instead of "create") So instead of this (from javadoc): IndexWriter public IndexWriter(Directory d, Analyzer a, boolean create) throws IOException Constructs an IndexWriter for the index in d. Text will be analyzed with a. If create is true, then a new, empty index will be created in d, replacing the index already there, if any. Parameters: d - the index directory a - the analyzer to use create - true to create the index or overwrite the existing one; false to append to the existing index Throws: IOException - if the directory cannot be read/written to, or if it does not exist, and create is false We would have this: IndexWriter public IndexWriter(Directory d, Analyzer a, boolean clear) throws IOException Constructs an IndexWriter for the index in d. Text will be analyzed with a. If clear is true, and a index exists at location d, then it will be erased, and a new, empty index will be created in d. Parameters: d - the index directory a - the analyzer to use clear - true to overwrite the existing one; false to append to the existing index Throws: IOException - if the directory cannot be read/written to, or if it does not exist. Its current behavior is kind of annoying, because I have an app that should never clear an existing index, it should always append. So I want create set to false. But when I am starting a brand new index, then I have to change the create flag to keep it from throwing an exception... I guess for now I will have to write code to check if a index actually has content yet, and if it doesn't, change the flag on the fly.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>304</id>
      <title>[PATCH] add boost feature to MultiFieldQueryParser</title>
      <description>add this feature (field1:query)^bfield1 (field2:query)^bfield12 (field3:query)^bfield3 ... (fieldx:query)^bfieldx Google Example: a user search "coco" , my defaults search fields are title and description. But i would like boost title field more than description field to have results more relevant. Change in the MultiFieldQueryParser : add this method public static Query parse(String query, String[] fields, int[] flags, float [] boost, Analyzer analyzer) throws ParseException { { BooleanQuery bQuery = new BooleanQuery(); for (int i = 0; i &lt; fields.length; i++) { Query q = parse(query, fields[i], analyzer); ==&gt; q.setBoost(boost[i]); int flag = flags[i]; switch (flag) { case REQUIRED_FIELD: bQuery.add(q, BooleanClause.Occur.MUST); break; case PROHIBITED_FIELD: bQuery.add(q, BooleanClause.Occur.MUST_NOT); break; default: bQuery.add(q, BooleanClause.Occur.SHOULD); break; } } return bQuery; } }</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>305</id>
      <title>[PATCH] Lock Framework - allows custom lock mechanism</title>
      <description>Proposal: Pluggable Lock Framework for Lucene Date: Nov 2004 Developer: Jeff Patterson (jeffATwebdoyen.com - http://www.webdoyen.com) ------ Abstract: A framework to allow Lucene users to override the default FileSystem locking mechanism with a custom lock mechanism. A Lucene user may develop a new class that extends org.apache.lucene.store.Lock and implement bodies for the following methods: public boolean obtain() - to obtain custom lock public boolean isLocked() - to detect custom lock public void release() - to release custom lock NOTE: When implementing these methods, the developer should make sure to use the this.getLockName() method on the Lock to identify which lock is being manipulated (see Modified Files below for more). After developed, the new class must be added to the classpath (along with any other supporting classes/libraries needed by the new class), and the Lucene framework must be alerted of the new class by way of the "org.apache.lucene.lockClass" -D System property. Example: java -Dorg.apache.lucene.lockClass=foo.MyCustomLocker LuceneTest ------ Modified Files: The following files were modified to support this framework (DIFF files at end): org.apache.lucene.store.Lock The member "lockName" and an accompanying protected getter and setter were added to this class to support naming the lock. This is transparent to the default lock mechanism and is only useful when writing a custom lock. org.apache.lucene.store.FSDirectory Instead of instantiating a default Lock, this class now checks to see if an overridden Lock mechanism is provided, and if so asks the LockFactory (see below) to provide an overridden Lock class. New Files: The following files were added to support this framework: org.apache.lucene.store.LockFactory This class is used to reflect and instantiate by name the custom Lock implementation. Error handing should be modified in this class, but that would have required a more extensive code overhaul. The javadocs for the LockFactory contain a skeleton Java file for a custom lock implementation. ------</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>308</id>
      <title>add "isClosed()" method to IndexSearcher</title>
      <description>I think it would be useful to know if a searcher is closed. It would also be useful to have an isClosed() on the Reader and Writer.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>310</id>
      <title>[PATCH] SegmentReader does unnecessary checks for existence of files</title>
      <description>While investigating some performance issues during queries, I stumbled upon a small issue in SegmentReader in regards to compound files. Specifically, the openNorms() method takes in the directory to use, but then has its own logic as to use that directory or the directory from its base class (IndexReader). When an index has many field infos, we have about 30, this logic about checking for files existing adds a significant overhead. Although this is a small inefficiency in a normal file system, our file system is mounted over nfs, and this check is relatively expensive. The rest of the class doesn't do this sort of check for other files. By changing this code to work like the rest of the methods in the class (i.e. just using the passed in directory), things are a good bit quicker on my end. I don't see any issues with this patch.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>317</id>
      <title>[PATCH] When locks are disabled, IndexWriter.close() throws NullPointerException</title>
      <description>If locks are disabled (via setting the System property 'disableLuceneLocks' to true), IndexWriter throws a NullPointerException on closing. The reason is that the attempt to call writeLock.release() fails because writeLock is null. To correct this, just check for this case before releasing. A (trivial) patch is attached.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>321</id>
      <title>[PATCH] Submissiom of my Tswana Analyzer</title>
      <description>I have developed an Analyzer for one of the South African 11 ofiicial lanuages called Setwsana.The Analyzer works on nourns classifications and nourns in diminutive form plus locatives.It also works on verbs,idebtifies verb roots,extensios of the verbs and suffixes,also recognizes verbs in diminutive form.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>322</id>
      <title>[PATCH] Add IndexSearcher.numDocs() method</title>
      <description>It is convenient method to return number of documents in the index thru available IndexSearcher object. Probably this method should be also introduced in org.apache.lucene.search.Searchable, but this changes more files and at the moment I need it only in IndexSearcher.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>325</id>
      <title>[PATCH] new method expungeDeleted() added to IndexWriter</title>
      <description>We make use the docIDs in lucene. I need a way to compact the docIDs in segments to remove the "holes" created from doing deletes. The only way to do this is by calling IndexWriter.optimize(). This is a very heavy call, for the cases where the index is large but with very small number of deleted docs, calling optimize is not practical. I need a new method: expungeDeleted(), which finds all the segments that have delete documents and merge only those segments. I have implemented this method and have discussed with Otis about submitting a patch. I don't see where I can attached the patch. I will do according to the patch guidleine and email the lucene mailing list. Thanks -John I don't see a place where I can</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>328</id>
      <title>Some utilities for a compact sparse filter</title>
      <description>Two files are attached that might form the basis for an alternative filter implementation that is more memory efficient than one bit per doc when less than about 1/8 of the docs pass through the filter. The document numbers are stored in RAM as VInt's from the Lucene index format. These VInt's encode the difference between two successive document numbers, much like a PositionDelta in the Positions: http://jakarta.apache.org/lucene/docs/fileformats.html The getByteSize() method can be used to verify the compression once a SortedVIntList is constructed. The precise conditions under which this is more memory efficient than one bit per document are not easy to specify in advance.</description>
      <attachments/>
      <comments>32</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>330</id>
      <title>[PATCH] Use filter bits for next() and skipTo() in FilteredQuery</title>
      <description>This improves performance of FilteredQuery by not calling score() on documents that do not pass the filter. This passes the current tests for FilteredQuery, but these tests have not been adapted/extended.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>336</id>
      <title>[PATCH] Add ability to specify the segment name when optimizing an index</title>
      <description>In my application I need to keep the filenames in my index from changing from one index build to the next. I modified the index writer to allow me to supply a segment name when I optimize the index. I thought this might be useful to others, so I'm attaching a patch with my change. This patch is against the CVS HEAD on January 18th, 2005 around 3:30pm Eastern Time</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>339</id>
      <title>date encoding limitation removing</title>
      <description>currently there is some limitation to date encoding in lucene. I think it's because dates should preserve lexicografical ordering, i.e. if one date precedes another date then encoded values should keep same ordering. I know that it can be difficult to integrate it into existing version but there is way to remove this limitation. Date milliseconds can be encoded as unsigned values with prefix that indicates positive or negative value. In more details: I used hex encoding and prefix ‘p’ and ‘n’ for positive and negative values. I got following results: Value -10000 is encoded with nffffffffffffd8f0, -100 - nffffffffffffff9c 0 - p0000000000000000 100 - p0000000000000064 10000 - p0000000000002710 This preserves ordering between values and theirs encoding. Also hex encoding can be replaced with Character.MAX_RADIX encoding. Part of code that do this work: final static char[] digits = { '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' , '8' , '9' , 'a' , 'b' , 'c' , 'd' , 'e' , 'f' , 'g' , 'h' , 'i' , 'j' , 'k' , 'l' , 'm' , 'n' , 'o' , 'p' , 'q' , 'r' , 's' , 't' , 'u' , 'v' , 'w' , 'x' , 'y' , 'z' } ; char prefix; if (time &gt;= 0) { prefix = 'p'; } else { prefix = 'n'; } char[] chars = new char[DATE_LEN + 1]; int index = DATE_LEN; while (time != 0) { int b = (int) (time &amp; 0x0F); chars[index--] = digits[b]; time = time &gt;&gt;&gt; 4; } while (index &gt;= 0) { chars[index--] = '0'; } chars[0] = prefix; return new String(chars);</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>340</id>
      <title>[PATCH] Highlighter: Delegate output escaping to Formatter</title>
      <description>Patch for jakarta-lucene-sandbox/contributions/highlighter CVS version 3rd February 2005 This patch allows the highlighter Formatter to control escaping of the non highlighted text as well as the highlighting of the matching text. The example formatters highlight the matching text using XML/HTML tags. This works fine if the plain text does not contain any characters that need to be escaped for HTML output (i.e. &lt;, &amp;, and "), however this cannot be guaranteed. As the formatter controls the method of highlighting (in the examples this is HTML, but it could be any other form of markup) it should also be responsible for escaping the rest of the output. This patch adds a method, encodeText(String), to the Formatter interface. This is a breaking change. This method is called from the Highlighter with the text that is not passed to the formatter's highlightTerm method. The SimpleHTMLFormatter has a public static method for performing simple HTML escaping called htmlEncode. The SimpleHTMLFormatter, GradientFormatter, and SpanGradientFormatter have been updated to implement the encodeText method and call the htmlEncode method to escape the output. For existing formatter to maintain exactly the same behaviour as before applying this patch they would need to implement the encodeText method to return the argument value without modification, e.g.: public String encodeText(String originalText) { return originalText; }</description>
      <attachments/>
      <comments>12</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>344</id>
      <title>[PATCH] Introduction of QueryFactory interface for Query construction</title>
      <description>To override the default behaviour of the QueryParser you must currently extend it and override the appropriate methods. This seemed to be a little awkward. To enable a (hopefully) more flexible means of creating Query instances it would seem better to introduce a QueryFactory interface and pass an instance of an implementation of this interface to the QueryParser, having the QueryParser callback to it to construct the query as it parses it. With this design you could write something like: QueryParser parser = new QueryParser("defaultField", new StandardAnalyzer(), new QueryFactoryImpl()); Where 'QueryFactoryImpl' is an implementation of the QueryFactory interface. If you wanted to add the ability to lower case all of your queries you could then write: QueryParser parser = new QueryParser("defaultField", new StandardAnalyzer(), new LowerCaseQueryFactory(new QueryFactoryImpl())); Where 'LowerCaseQueryFactory' is a decorator around another QueryFactory instance and it simply lowercases all the terms passed to it before delegating the actual query construction. This is a simple example, but more powerful functionality could be added not by changing the QueryParser, but by creating a new QueryFactory implementation. I have a patch for this which will be forthcoming in a moment.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>357</id>
      <title>Implementation of Directory to check integrity of index</title>
      <description>NullDirectory provides input and output streams that don't themselves do anything useful. That is, there is no real index behind the directory. However one can merge another index with an instance of null index thereby causing a 'read' of the other index. This 'read' will fail if that index is corrupt.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>358</id>
      <title>final classes like Document prevent simple tests and extensions</title>
      <description>Classes like Document or Field are final what make more complicate to write junit tests for custom utility methods or to write custom extensions. Why are these classes final? Wouldn't it be better for instance if Document would be an interface?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>368</id>
      <title>Surround query language</title>
      <description>This is a copy of what I posted about a year ago. The whole thing is hereby licenced under the Apache Licence 2.0, copyright 2005 Apache Software Foundation. For inclusion in Lucene (sandbox perhaps?) it will need at least the following adaptations: renaming of package names (org.surround to somewhere org.apache.lucene ) moves of the source files to corresponding directories Although it uses the identifier sncf in some places I'm not associated with French railroads, but I like the TGV. Regards, Paul Elschot</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>376</id>
      <title>[PATCH] Ant macro for javadocs and javadocs-internal</title>
      <description>This removes the duplication introduced in build.xml when the javadocs-internal build target was added. Regards, Paul Elschot</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>379</id>
      <title>Contribution: Efficient Sorting of DateField/DateTools Encoded Timestamp Long Values</title>
      <description>Hello Tim, As promised, the sort functionality for "long" values is included in the attached files. patchTestSort.txt contains the diff info. for my modifications to the TestSort.java class org.apache.lucene.search.ZIP contains the three new class files for efficient sorting of "long" field values and of encoded timestamp field values as "long" values. Let me know if you have any questions. Regards, Rus</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>382</id>
      <title>[PATCH] Document update contrib (Play with term postings or .. to a easy way to update)</title>
      <description>With this contribution you can add, delete or replace term/document relation A use case is a very fast update of document. Exemple: The update of 1 million of documents containing 2 fields take some seconds (see test case)</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>389</id>
      <title>MatchAllDocsQuery to return all documents</title>
      <description>It would be nice to have a type of query just return all documents from an index.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>390</id>
      <title>Contribution: LuceneIndexAccessor</title>
      <description>As per this post: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200505.mbox/%3c200505182152.07489@danielnaber.de%3e I'm attaching the LuceneIndexAccessor source here. Copyright is now 2005 The Apache Software Foundation. Please note that it won't compile out of the box, but that should be fairly easy to fix using a CVS version of Lucene. Also it makes use of Log4J. I'm fine with moving the classes to any package you like.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>395</id>
      <title>CoordConstrainedBooleanQuery + QueryParser support</title>
      <description>Attached 2 new classes: 1) CoordConstrainedBooleanQuery A boolean query that only matches if a specified number of the contained clauses match. An example use might be a query that returns a list of books where ANY 2 people from a list of people were co-authors, eg: "Lucene In Action" would match ("Erik Hatcher" "Otis Gospodnetić" "Mark Harwood" "Doug Cutting") with a minRequiredOverlap of 2 because Otis and Erik wrote that. The book "Java Development with Ant" would not match because only 1 element in the list (Erik) was selected. 2) CustomQueryParserExample A customised QueryParser that allows definition of CoordConstrainedBooleanQueries. The solution (mis)uses fieldnames to pass parameters to the custom query.</description>
      <attachments/>
      <comments>31</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>399</id>
      <title>WordListLoader.java should be able to read stopwords from a Reader</title>
      <description>WordListLoader should be able to read the stopwords from a Reader. This would (for example) allow stopword lists to be stored as a resource in the jar file of a Lucene application. Diff is attached.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>400</id>
      <title>NGramFilter -- construct n-grams from a TokenStream</title>
      <description>This filter constructs n-grams (token combinations up to a fixed size, sometimes called "shingles") from a token stream. The filter sets start offsets, end offsets and position increments, so highlighting and phrase queries should work. Position increments &gt; 1 in the input stream are replaced by filler tokens (tokens with termText "_" and endOffset - startOffset = 0) in the output n-grams. (Position increments &gt; 1 in the input stream are usually caused by removing some tokens, eg. stopwords, from a stream.) The filter uses CircularFifoBuffer and UnboundedFifoBuffer from Apache Commons-Collections. Filter, test case and an analyzer are attached.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>402</id>
      <title>addition of a previous() method to TermEnum</title>
      <description>the addition of a previous() method to the org.apache.lucene.index.TermEnum will allow efficient backward and forward navigation through the indexed term.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>403</id>
      <title>Alternate Lucene Query Highlighter</title>
      <description>I created a lucene query highlighter (borrowing some code from the one in the sandbox) that my company is using. It better handles phrase queries, doesn't break HTML entities, and has the ability to either highlight terms in an entire document or to highlight fragments from the document. I would like to make it available to anyone who wants it.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>406</id>
      <title>sort missing string fields last</title>
      <description>A SortComparatorSource for string fields that orders documents with the sort field missing after documents with the field. This is the reverse of the default Lucene implementation. The concept and first-pass implementation was done by Chris Hostetter.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>411</id>
      <title>[PATCH] BitSetQuery, FastPrefixQuery, FastWildcardQuery and FastQueryParser</title>
      <description>FastPrefixQuery and FastWildcardQuery rewrites to BitSetQuery instead of OR'ed BooleanQuery's. A BitSetQuery contains a BitSet that desginates which document should be included in the search result. BitSetQuery cannot be used by itself with MultiSearcher as of now.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>421</id>
      <title>Numeric range searching with large value sets</title>
      <description>I have a set of enhancements that build on the numeric sorting cache introduced by Tim Jones and that provide integer and floating point range searches over numeric ranges that are far too large to be implemented via the current term range rewrite mechanism. I'm new to Apache and trying to find out how to attach the source files for the changes for your consideration.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>422</id>
      <title>Performance improvement in search</title>
      <description>When search with a Filter!=null, the scorer gathers all the hits and then throws away the documents not in the filtered set. Seems the work to calculate the score for the documents thrown away is wasted. It would be nice to pass the filter into the scorer, and filter out the documents before scoring is done. Thanks -John</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>423</id>
      <title>thread pool implementation of parallel queries</title>
      <description>This component is a replacement for ParallelMultiQuery that runs a thread pool with queue instead of starting threads for every query execution (so its performance is better).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>426</id>
      <title>Creating a new index vs. opening an existing one</title>
      <description>Currently I have to tell the IndexWriter constructor whether I want a new index created or not. The behaviour I'd like is to open an index if one exists, or create a new one if it doesn't exist. Now I check for a 'segments' file in my code to make the choice: boolean CreateNew=false; // Create a new index ONLY if one doesn't exist already. Even then, we first try to open // an existing index. if (!(new java.io.File(IndexPath+"segments")).exists()) { CreateNew=true; } try { writer = new IndexWriter(IndexPath, new StandardAnalyzer(), false); } catch (IOException e) { if (CreateNew) { writer = new IndexWriter(IndexPath, new StandardAnalyzer(), true); } else { throw e; } } I believe this logic would make more sense inside Lucene.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>430</id>
      <title>Reducing buffer sizes for TermDocs.</title>
      <description>From java-dev: On Friday 09 September 2005 00:34, Doug Cutting wrote: &gt; Paul Elschot wrote: &gt; &gt; I suppose one of these cases are when many terms are used in a query. &gt; &gt; Would it be easily possible to make the buffer size for a term iterator &gt; &gt; depend on the numbers of documents to be iterated? &gt; &gt; Many terms only occur in a few documents, so this could be a &gt; &gt; nice win on total buffer size for the many terms case. &gt; &gt; This would not be too difficult. &gt; &gt; Look in SegmentTermDocs.java. The buffer may be allocated when the &gt; parent's stream is first cloned, but clone() won't allocate a buffer if &gt; the source hasn't had a buffer allocated yet, and nothing should perform &gt; i/o directly on the parent's freqStream, so in practice a buffer should &gt; not be allocated until the first read is performed on the clone. I tried delaying the buffer allocation in BufferedIndexInput by using this clone() method: public Object clone() { BufferedIndexInput clone = (BufferedIndexInput)super.clone(); clone.buffer = null; clone.bufferLength = 0; clone.bufferPosition = 0; clone.bufferStart = getFilePointer(); return clone; } With this all term document iterators seem to be empty, no query in the test cases gives any results, for example TestDemo and TestBoolean2. As far as I can see, this delaying should work, but it doesn't and I have no idea why. End of quote from java-dev. Doug replied that at a glance this clone method looks good. Without this delayed buffer allocation, a reduced buffer size for TermDocs cannot be implemented easily.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>434</id>
      <title>Lucene database bindings</title>
      <description>Code and examples for embedding Lucene in HSQLDB and Derby relational databases.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>435</id>
      <title>[PATCH] BufferedIndexOutput - optimized writeBytes() method</title>
      <description>I have created a patch that optimize writeBytes metod: public void writeBytes(byte[] b, int length) throws IOException { if (bufferPosition &gt; 0) // flush buffer flush(); if (length &lt; BUFFER_SIZE) { flushBuffer(b, length); bufferStart += length; } else { int pos = 0; int size; while (pos &lt; length) { if (length - pos &lt; BUFFER_SIZE) { size = length - pos; } else { size = BUFFER_SIZE; } System.arraycopy(b, pos, buffer, 0, size); pos += size; flushBuffer(buffer, size); bufferStart += size; } } } Its a much more faster now. I know that for indexing this not help much, but for copying files in the IndexStore this is so big improvement. Its about 400% faster that old implementation. The patch was tested with 300MB data, "ant test" sucessfuly finished with no errors.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>436</id>
      <title>[PATCH] TermInfosReader, SegmentTermEnum Out Of Memory Exception</title>
      <description>We've been experiencing terrible memory problems on our production search server, running lucene (1.4.3). Our live app regularly opens new indexes and, in doing so, releases old IndexReaders for garbage collection. But...there appears to be a memory leak in org.apache.lucene.index.TermInfosReader.java. Under certain conditions (possibly related to JVM version, although I've personally observed it under both linux JVM 1.4.2_06, and 1.5.0_03, and SUNOS JVM 1.4.1) the ThreadLocal member variable, "enumerators" doesn't get garbage-collected when the TermInfosReader object is gc-ed. Looking at the code in TermInfosReader.java, there's no reason why it shouldn't be gc-ed, so I can only presume (and I've seen this suggested elsewhere) that there could be a bug in the garbage collector of some JVMs. I've seen this problem briefly discussed; in particular at the following URL: http://java2.5341.com/msg/85821.html The patch that Doug recommended, which is included in lucene-1.4.3 doesn't work in our particular circumstances. Doug's patch only clears the ThreadLocal variable for the thread running the finalizer (my knowledge of java breaks down here - I'm not sure which thread actually runs the finalizer). In our situation, the TermInfosReader is (potentially) used by more than one thread, meaning that Doug's patch doesn't allow the affected JVMs to correctly collect garbage. So...I've devised a simple patch which, from my observations on linux JVMs 1.4.2_06, and 1.5.0_03, fixes this problem. Kieran PS Thanks to daniel naber for pointing me to jira/lucene @@ -19,6 +19,7 @@ import java.io.IOException; import org.apache.lucene.store.Directory; +import java.util.Hashtable; /** This stores a monotonically increasing set of &lt;Term, TermInfo&gt; pairs in a Directory. Pairs are accessed either by Term or by ordinal position the @@ -29,7 +30,7 @@ private String segment; private FieldInfos fieldInfos; private ThreadLocal enumerators = new ThreadLocal(); + private final Hashtable enumeratorsByThread = new Hashtable(); private SegmentTermEnum origEnum; private long size; @@ -60,10 +61,10 @@ } private SegmentTermEnum getEnum() { SegmentTermEnum termEnum = (SegmentTermEnum)enumerators.get(); + SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread()); if (termEnum == null) { termEnum = terms(); - enumerators.set(termEnum); + enumeratorsByThread.put(Thread.currentThread(), termEnum); } return termEnum; } @@ -195,5 +196,15 @@ public SegmentTermEnum terms(Term term) throws IOException { get(term); return (SegmentTermEnum)getEnum().clone(); + } + + /* some jvms might have trouble gc-ing enumeratorsByThread */ + protected void finalize() throws Throwable Unknown macro: {+ try { + // make sure gc can clear up. + enumeratorsByThread.clear(); + } finally { + super.finalize(); + } } } TermInfosReader.java, full source: ====================================== package org.apache.lucene.index; /** Copyright 2004 The Apache Software Foundation * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at * http://www.apache.org/licenses/LICENSE-2.0 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import java.io.IOException; import org.apache.lucene.store.Directory; import java.util.Hashtable; /** This stores a monotonically increasing set of &lt;Term, TermInfo&gt; pairs in a Directory. Pairs are accessed either by Term or by ordinal position the set. */ final class TermInfosReader { private Directory directory; private String segment; private FieldInfos fieldInfos; private final Hashtable enumeratorsByThread = new Hashtable(); private SegmentTermEnum origEnum; private long size; TermInfosReader(Directory dir, String seg, FieldInfos fis) throws IOException { directory = dir; segment = seg; fieldInfos = fis; origEnum = new SegmentTermEnum(directory.openFile(segment + ".tis"), fieldInfos, false); size = origEnum.size; readIndex(); } public int getSkipInterval() { return origEnum.skipInterval; } final void close() throws IOException { if (origEnum != null) origEnum.close(); } /** Returns the number of term/value pairs in the set. */ final long size() { return size; } private SegmentTermEnum getEnum() { SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread()); if (termEnum == null) { termEnum = terms(); enumeratorsByThread.put(Thread.currentThread(), termEnum); } return termEnum; } Term[] indexTerms = null; TermInfo[] indexInfos; long[] indexPointers; private final void readIndex() throws IOException { SegmentTermEnum indexEnum = new SegmentTermEnum(directory.openFile(segment + ".tii"), fieldInfos, true); try { int indexSize = (int)indexEnum.size; indexTerms = new Term[indexSize]; indexInfos = new TermInfo[indexSize]; indexPointers = new long[indexSize]; for (int i = 0; indexEnum.next(); i++) { indexTerms[i] = indexEnum.term(); indexInfos[i] = indexEnum.termInfo(); indexPointers[i] = indexEnum.indexPointer; } } finally { indexEnum.close(); } } /** Returns the offset of the greatest index entry which is less than or equal to term.*/ private final int getIndexOffset(Term term) throws IOException { int lo = 0; // binary search indexTerms[] int hi = indexTerms.length - 1; while (hi &gt;= lo) { int mid = (lo + hi) &gt;&gt; 1; int delta = term.compareTo(indexTerms[mid]); if (delta &lt; 0) hi = mid - 1; else if (delta &gt; 0) lo = mid + 1; else return mid; } return hi; } private final void seekEnum(int indexOffset) throws IOException { getEnum().seek(indexPointers[indexOffset], (indexOffset * getEnum().indexInterval) - 1, indexTerms[indexOffset], indexInfos[indexOffset]); } /** Returns the TermInfo for a Term in the set, or null. */ TermInfo get(Term term) throws IOException { if (size == 0) return null; // optimize sequential access: first try scanning cached enum w/o seeking SegmentTermEnum enumerator = getEnum(); if (enumerator.term() != null // term is at or past current &amp;&amp; ((enumerator.prev != null &amp;&amp; term.compareTo(enumerator.prev) &gt; 0) term.compareTo(enumerator.term()) &gt;= 0)) { int enumOffset = (int)(enumerator.position/enumerator.indexInterval)+1; if (indexTerms.length == enumOffset // but before end of block || term.compareTo(indexTerms[enumOffset]) &lt; 0) return scanEnum(term); // no need to seek } // random-access: must seek seekEnum(getIndexOffset(term)); return scanEnum(term); } /** Scans within block for matching term. */ private final TermInfo scanEnum(Term term) throws IOException { SegmentTermEnum enumerator = getEnum(); while (term.compareTo(enumerator.term()) &gt; 0 &amp;&amp; enumerator.next()) {} if (enumerator.term() != null &amp;&amp; term.compareTo(enumerator.term()) == 0) return enumerator.termInfo(); else return null; } /** Returns the nth term in the set. */ final Term get(int position) throws IOException { if (size == 0) return null; SegmentTermEnum enumerator = getEnum(); if (enumerator != null &amp;&amp; enumerator.term() != null &amp;&amp; position &gt;= enumerator.position &amp;&amp; position &lt; (enumerator.position + enumerator.indexInterval)) return scanEnum(position); // can avoid seek seekEnum(position / enumerator.indexInterval); // must seek return scanEnum(position); } private final Term scanEnum(int position) throws IOException { SegmentTermEnum enumerator = getEnum(); while(enumerator.position &lt; position) if (!enumerator.next()) return null; return enumerator.term(); } /** Returns the position of a Term in the set or -1. */ final long getPosition(Term term) throws IOException { if (size == 0) return -1; int indexOffset = getIndexOffset(term); seekEnum(indexOffset); SegmentTermEnum enumerator = getEnum(); while(term.compareTo(enumerator.term()) &gt; 0 &amp;&amp; enumerator.next()) {} if (term.compareTo(enumerator.term()) == 0) return enumerator.position; else return -1; } /** Returns an enumeration of all the Terms and TermInfos in the set. */ public SegmentTermEnum terms() { return (SegmentTermEnum)origEnum.clone(); } /** Returns an enumeration of terms starting at or after the named term. */ public SegmentTermEnum terms(Term term) throws IOException { get(term); return (SegmentTermEnum)getEnum().clone(); } /* some jvms might have trouble gc-ing enumeratorsByThread */ protected void finalize() throws Throwable { try { // make sure gc can clear up. enumeratorsByThread.clear(); } finally { super.finalize(); } } }</description>
      <attachments/>
      <comments>42</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>438</id>
      <title>add Token.setTermText(), remove final</title>
      <description>The Token class should be more friendly to classes not in it's package: 1) add setTermText() 2) remove final from class and toString() 3) add clone() Support for (1): TokenFilters in the same package as Token are able to do things like "t.termText = t.termText.toLowerCase();" which is more efficient, but more importantly less error prone. Without the ability to change only the term text, a new Token must be created, and one must remember to set all the properties correctly. This exact issue caused this bug: http://issues.apache.org/jira/browse/LUCENE-437 Support for (2): Removing final allows one to subclass Token. I didn't see any performance impact after removing final. I can go into more detail on why I want to subclass Token if anyone is interested. Support for (3): support for a synonym TokenFilter, where one needs to make two tokens from one (same args that support (1), and esp important if instance is a subclass of Token).</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>445</id>
      <title>Contrib.: Thread-safe DelayCloseIndexSearcher.</title>
      <description>Implements search over a single IndexReader, but remains open even if close() is called. This way it can be shared by multiple objects that need to search the index without being aware of the keep-the-index-open-until-it-changes logic. Usage is described in the javadoc.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>446</id>
      <title>search.function - (1) score based on field value, (2) simple score customizability</title>
      <description>FunctionQuery can return a score based on a field's value or on it's ordinal value. FunctionFactory subclasses define the details of the function. There is currently a LinearFloatFunction (a line specified by slope and intercept). Field values are typically obtained from FieldValueSourceFactory. Implementations include FloatFieldSource, IntFieldSource, and OrdFieldSource.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>447</id>
      <title>Make "ant -projecthelp" show the javadocs and docs targets as well</title>
      <description>Added a description to the targets "javadocs" and "docs". This makes ant show them when the executes "ant -projecthelp"</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>448</id>
      <title>optional norms</title>
      <description>For applications with many indexed fields, the norms cause memory problems both during indexing and querying. This patch makes norms optional on a per-field basis, in the same way that term vectors are optional per-field. Overview of changes: Field.omitNorms that defaults to false backward compatible lucene file format change: FieldInfos.FieldBits has a bit for omitNorms IndexReader.hasNorms() method During merging, if any segment includes norms, then norms are included. methods to get norms return the equivalent 1.0f array for backward compatibility The patch was designed for backward compatibility: all current unit tests pass w/o any modifications required compatible with old indexes since the default is omitNorms=false compatible with older/custom subclasses of IndexReader since a default hasNorms() is provided compatible with older/custom users of IndexReader such as Weight/Scorer/explain since a norm array is produced on demand, even if norms were not stored If this patch is accepted (or if the direction is acceptable), performance for scoring could be improved by assuming 1.0f when hasNorms(field)==false.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>449</id>
      <title>NullPointerException when temporary directory not readable</title>
      <description>We have customers reporting errors such as: Caused by: java.lang.NullPointerException at org.apache.lucene.store.FSDirectory.create(FSDirectory.java:200) at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:144) at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:117) at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:205) at com.atlassian.jira.util.LuceneUtils.getIndexWriter(LuceneUtils.java:46) at com.atlassian.jira.issue.index.DefaultIndexManager.getIndexWriter(DefaultIndexManager.java:568) at com.atlassian.jira.issue.index.DefaultIndexManager.indexIssuesAndComments(DefaultIndexManager.java:287) ... 59 more This occurs when the lock directory is unreadable (eg. because Tomcat sets java.io.tmpdir to temp/ and the permissions here are broken). Attached is</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>452</id>
      <title>PrefixQuery is missing the equals() method</title>
      <description>The PrefixQuery is inheriting the java.lang.Object's object default equals method. This makes it hard to have test working of PrefixFilter or any other task requiring equals to work proerply (insertion in Set, etc.). The equal method should be very similar, not to say identical except for class casting, to the equals() of TermQuery.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>454</id>
      <title>lazily create SegmentMergeInfo.docMap</title>
      <description>Since creating the docMap is expensive, and it's only used during segment merging, not searching, defer creation until it is requested. SegmentMergeInfo is also used in MultiTermEnum, the term enumerator for a MultiReader. TermEnum is used by queries such as PrefixQuery, RangeQuery, WildcardQuery, as well as RangeFilter, DateFilter, and sorting the first time (filling the FieldCache). Performance Results: A simple single field index with 555,555 documents, and 1000 random deletions was queried 1000 times with a PrefixQuery matching a single document. Performance Before Patch: indexing time = 121,656 ms querying time = 58,812 ms Performance After Patch: indexing time = 121,000 ms querying time = 598 ms A 100 fold increase in query performance! All lucene unit tests pass.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>457</id>
      <title>FieldCacheImpl take advantage of term info already being sorted</title>
      <description>FieldCacheImpl.getStrings could take advantage of term info already being sorted lexically. Would it be possible to have a "index order" mode which returns an array of ints rather than strings storing a scalar value that increments by one for each new term. Presumably there would be a big memory profile advantage in not holding onto the term value Strings and a lesser one in int comparison being slightly quicker than String.compareTo. Sorry if I have missed something obvious. I don't know the code very well. Regards Sam</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>460</id>
      <title>hashCode improvements</title>
      <description>It would be nice for all Query classes to implement hashCode and equals to enable them to be used as keys when caching.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>464</id>
      <title>Using different analyzer in QueryParser for Keyword Fields and Text Fields</title>
      <description>I have a Text field (call it "text") and a Keyword field (call it "keyword"). I'd like to be able to do the following search: text:"some text" AND keyword:"http://somekeyword" to find documents where the text contains "some" and "text" and the keyword is "http://somekeyword" I want the text part of the query to use the StandardAnalyzer and the keyword portion to use a trivial analyzer that just passes the text string through. In the above case, Lucene uses the QueryParser's analyzer (I used StandardAnalyzer) to parse "http://somekeyword" into "http" and "somekeyword", which is not what I want.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>466</id>
      <title>Need QueryParser support for BooleanQuery.minNrShouldMatch</title>
      <description>Attached 2 new classes: 1) CoordConstrainedBooleanQuery A boolean query that only matches if a specified number of the contained clauses match. An example use might be a query that returns a list of books where ANY 2 people from a list of people were co-authors, eg: "Lucene In Action" would match ("Erik Hatcher" "Otis Gospodnetić" "Mark Harwood" "Doug Cutting") with a minRequiredOverlap of 2 because Otis and Erik wrote that. The book "Java Development with Ant" would not match because only 1 element in the list (Erik) was selected. 2) CustomQueryParserExample A customised QueryParser that allows definition of CoordConstrainedBooleanQueries. The solution (mis)uses fieldnames to pass parameters to the custom query.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>467</id>
      <title>Use Float.floatToRawIntBits over Float.floatToIntBits</title>
      <description>Copied From my Email: Float.floatToRawIntBits (in Java1.4) gives the raw float bits without normalization (like (int)&amp;floatvar would in C). Since it doesn't do normalization of NaN values, it's faster (and hopefully optimized to a simple inline machine instruction by the JVM). On my Pentium4, using floatToRawIntBits is over 5 times as fast as floatToIntBits. That can really add up in something like Similarity.floatToByte() for encoding norms, especially if used as a way to compress an array of float during query time as suggested by Doug.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>468</id>
      <title>Searchable.java: The info in the @deprecated tags do not refer to the search(Weight, etc...) versions...</title>
      <description>E.g. The javadoc for void search(Query query, Filter filter, HitCollector results) states: Deprecated. use search(Query, Filter, HitCollector) instead. instead of: Deprecated. use search(Weight, Filter, HitCollector) instead.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>474</id>
      <title>High Frequency Terms/Phrases at the Index level</title>
      <description>We should be able to find the all the high frequency terms/phrases ( where frequency is the search criteria / benchmark)</description>
      <attachments/>
      <comments>13</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>475</id>
      <title>RAMDirectory(Directory dir, boolean closeDir) constructor uses memory inefficiently.</title>
      <description>recently I found that RAMDirectory(Directory dir, boolean closeDir) constructor uses memory inefficiently. files from source index are read entirely intro memory as single byte array which is after all is thrown away. And if I want to load my 200M optimized, compound format index to memory for faster search I should give JVM at least 400Mb memory limit. For larger indexes this can be an issue. I've attached patch how to solve this problem.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>476</id>
      <title>BooleanQuery add public method that returns number of clauses this query</title>
      <description>BooleanQuery add public method getClausesCount() that returns number of clauses this query. current ways of getting clauses count are: 1). int clausesCount = booleanQuery.getClauses().length; or</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>482</id>
      <title>JE Directory Implementation</title>
      <description>I've created a port of DbDirectory to JE</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>484</id>
      <title>Java Content Repository Directory Implementation</title>
      <description>This is version 1.0 of my Java Content Repository Implementation (http://jcp.org/aboutJava/communityprocess/final/jsr170/index.html). This was first discussed in the following thread: http://www.mail-archive.com/java-dev%40lucene.apache.org/msg02926.html The implementation is now using fixed-size buffers. I've not joined an Ant build file because my TestCase is using Apache Jackrabbbit (JSR-170) and they don't currently don't offer binaries (the situation will probably change soon). The actual classes only needs the "jcr-1.0.jar" which can be found in the specification download http://jcp.org/aboutJava/communityprocess/final/jsr170/index.html. Comments are welcomed. Regards, Nicolas Bélisle Laval University Library</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>485</id>
      <title>IndexWriter.mergeSegments should not hold the commit lock while cleaning up.</title>
      <description>Same happens in IndexWriter.addIndexes(IndexReader[] readers). The commit lock should be obtained whenever the Index structure/version is read or written. It should be kept for as short a period as possible. The write lock is needed to make sure only one IndexWriter or IndexReader instance can update the index (multiple IndexReaders can of course use the index for searching). The list of files that can be deleted is stored in the file "deletable". It is only read or written by the IndexWriter instance that holds the write lock, so there's no need to have the commit lock to to update it. On my production system deleting the obsolete segment files after a mergeSegments() happens can occasionally take several seconds and the commit lock blocks the searcher machines from updating their IndexReader instance. Even on a standalone machine, the time to update the segments file is about 3ms, the time to delete the obsolete segments about 30ms.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>487</id>
      <title>Database as a lucene index target</title>
      <description>I've written an extension for the Directory object called DBDirectory, that allows you to read and write a Lucene index to a database instead of a file system. This is done using blobs. Each blob represents a "file". Also, each blob has a name which is equivalent to the filename and a prefix, which is equivalent to a directory on a file system. This allows you to create multiple Lucene indexes in a single database schema. The solution uses two tables: LUCENE_INDEX - which holds the index files as blobs LUCENE_LOCK - holds the different locks Attached is my proposed solution. This solution is still very basic, but it does the job. The solution supports Oracle and mysql To use this solution: 1. Place the files: DBDirectory in src/java/org/apache/lucene/store TestDBIndex in src/test/org/apache/lucene/index objects-mysql.sql in src/db objects-oracle.sql in src/db 2. Edit the parameters for the database connection in TestDBIndex 3. Create the database tables using the objects-mysql.sql script (assuming you're using mysql) 4. Build Lucene 5. Run TestDBIndex with the database driver in the classpath I've tested the solution on mysql, but it should work on Oracle, I will test that in a few days. Amir</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>494</id>
      <title>Analyzer for preventing overload of search service by queries with common terms in large indexes</title>
      <description>An analyzer used primarily at query time to wrap another analyzer and provide a layer of protection which prevents very common words from being passed into queries. For very large indexes the cost of reading TermDocs for a very common word can be high. This analyzer was created after experience with a 38 million doc index which had a term in around 50% of docs and was causing TermQueries for this term to take 2 seconds. Use the various "addStopWords" methods in this class to automate the identification and addition of stop words found in an already existing index.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>495</id>
      <title>Suggested Patches to MultiPhraseQuery and QueryTermExtractor (for use with HighLighter)</title>
      <description>I encountered a problem with the Highlighter, where it was not recognizing MultiPhraseQuery. To fix this, I developed the following two patches: ===================================================== 1. Addition to org.apache.lucene.search.MultiPhraseQuery: Add the following method: /** Returns the set of terms in this phrase. */ public Term[] getTerms() { ArrayList allTerms = new ArrayList(); Iterator iterator = termArrays.iterator(); while (iterator.hasNext()) { Term[] terms = (Term[])iterator.next(); for (int i = 0, n = terms.length; i &lt; n; ++i) { allTerms.add(terms[i]); } } return (Term[])allTerms.toArray(new Term[0]); } ===================================================== 2. Patch to org.apache.lucene.search.highlight.QueryTermExtractor: a) Add the following import: import org.apache.lucene.search.MultiPhraseQuery; b) Add the following code to the end of the getTerms(...) method: else if(query instanceof MultiPhraseQuery) getTermsFromMultiPhraseQuery((MultiPhraseQuery) query, terms, fieldName); } c) Add the following method: private static final void getTermsFromMultiPhraseQuery(MultiPhraseQuery query, HashSet terms, String fieldName) { Term[] queryTerms = query.getTerms(); int i; for (i = 0; i &lt; queryTerms.length; i++) { if((fieldName==null)||(queryTerms[i].field()==fieldName)) { terms.add(new WeightedTerm(query.getBoost(),queryTerms[i].text())); } } } ===================================================== Can the team update the repository? Thanks Michael Harhen</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>496</id>
      <title>New tool for reseting the (length)norm of fields after changing Similarity</title>
      <description>I've written a little tool that seems like it can/will be very handy as I tweak my custom similarity. I think it would make a good addition to contrib/miscellaneous. Class and Tests to be attached shortly...</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>497</id>
      <title>update copyright (and licence) prior to release of 1.9</title>
      <description>As discussed in email earlier today, it wouldn't hurt to update the Copyright on all of the source files before release 1.9. Rather then try to submit a path with all the changes, here's a oneliner that should work on any unix box to update in mass. If it sees a Copyright string it recognizes, it preserves the start year and adds/replaces the end year... find name *.java | xargs perl -pi -e 's/Copyright ((c) )?(200[0-5])(\d+)? (The )?Apache Software Foundation/Copyright $ {2} -2006 The Apache Software Foundation/;' ...it would make sense for someone with commit permissions to run that themselves. It also cleans up a few that have a " (c) " in them that doesn't seem standard across the rest of the files, and makes sure that the ASF is refered to as "The" ASF. Even after all that, there are a few that may need cleaned up by hand... ./src/test/org/apache/lucene/store/TestLock.java: * Copyright (c) 2001,2004 The Apache Software Foundation. All rights ./src/test-deprecated/org/apache/lucene/index/DocHelper.java: * Copyright 2004. Center For Natural Language Processing ./contrib/analyzers/src/java/org/apache/lucene/analysis/cn/ChineseAnalyzer.java: * Copyright: Copyright (c) 2001 ./contrib/analyzers/src/java/org/apache/lucene/analysis/cn/ChineseFilter.java: * Copyright: Copyright (c) 2001 ./contrib/analyzers/src/java/org/apache/lucene/analysis/cn/ChineseTokenizer.java: * Copyright: Copyright (c) 2001 ...the first is just an anoying format, the rest either have non ASF copyrights, or dual copyrights (!?) It also may be a good time to take a look at all the (non-JavaCC generated) java files that don't mention the Apache License, Version 2.0 ... @asimov:~/svn/lucene/java$ find src -name *.java | xargs grep -L "Generated By:JavaCC" | xargs grep -L LICENSE-2.0 src/java/org/apache/lucene/search/SortComparatorSource.java src/java/org/apache/lucene/search/SortComparator.java src/test/org/apache/lucene/index/TestTermVectorsReader.java src/test/org/apache/lucene/index/TestSegmentTermEnum.java src/test/org/apache/lucene/index/TestFieldInfos.java src/test/org/apache/lucene/index/TestIndexWriter.java src/test/org/apache/lucene/store/TestLock.java src/test/org/apache/lucene/store/_TestHelper.java src/test/org/apache/lucene/search/TestRangeQuery.java src/test/org/apache/lucene/TestHitIterator.java src/test/org/apache/lucene/document/TestBigBinary.java src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java src/test-deprecated/org/apache/lucene/index/TestTermVectorsReader.java src/test-deprecated/org/apache/lucene/index/store/FSDirectoryTestCase.java src/test-deprecated/org/apache/lucene/index/TestSegmentTermEnum.java src/test-deprecated/org/apache/lucene/index/DocHelper.java src/test-deprecated/org/apache/lucene/index/TestIndexWriter.java src/test-deprecated/org/apache/lucene/search/TestRangeQuery.java</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>499</id>
      <title>Documentation improvements for 1.9 release</title>
      <description>I've poked arround the 1.9-rc1 builds and noticed a few simple documentation things that could be cleaned up, a patch will follow that... 1) Adds some additional info to the README.txt 2) Updates the version info in queryparsersyntax.xml and fileformats.html, and advises people with older versions how to find the correct documentation for their version 3) Builds javadocs for all of the contrib modules (the list was incomplete)</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>502</id>
      <title>TermScorer caches values unnecessarily</title>
      <description>TermScorer aggressively caches the doc and freq of 32 documents at a time for each term scored. When querying for a lot of terms, this causes a lot of garbage to be created that's unnecessary. The SegmentTermDocs from which it retrieves its information doesn't have any optimizations for bulk loading, and it's unnecessary. In addition, it has a SCORE_CACHE, that's of limited benefit. It's caching the result of a sqrt that should be placed in DefaultSimilarity, and if you're only scoring a few documents that contain those terms, there's no need to precalculate the SQRT, especially on modern VMs. Enclosed is a patch that replaces TermScorer with a version that does not cache the docs or feqs. In the case of a lot of queries, that saves 196 bytes/term, the unnecessary disk IO, and extra SQRTs which adds up.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>503</id>
      <title>Contrib: ThaiAnalyzer to enable Thai full-text search in Lucene</title>
      <description>Thai text don't have space between words. Usually, a dictionary-based algorithm is used to break string into words. For Lucene to be usable for Thai, an Analyzer that know how to break Thai words is needed. I've implemented such Analyzer, ThaiAnalyzer, using ICU4j DictionaryBasedBreakIterator for word breaking. I'll upload the code later. I'm normally a C++ programmer and very new to Java. Please review the code for any problem. One possible problem is that it requires ICU4j. I don't know whether this is OK.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>505</id>
      <title>MultiReader.norm() takes up too much memory: norms byte[] should be made into an Object</title>
      <description>MultiReader.norms() is very inefficient: it has to construct a byte array that's as long as all the documents in every segment. This doubles the memory requirement for scoring MultiReaders vs. Segment Readers. Although this is cached, it's still a baseline of memory that is unnecessary. The problem is that the Normalization Factors are passed around as a byte[]. If it were instead replaced with an Object, you could perform a whole host of optimizations a. When reading, you wouldn't have to construct a "fakeNorms" array of all 1.0fs. You could instead return a singleton object that would just return 1.0f. b. MultiReader could use an object that could delegate to NormFactors of the subreaders c. You could write an implementation that could use mmap to access the norm factors. Or if the index isn't long lived, you could use an implementation that reads directly from the disk. The patch provided here replaces the use of byte[] with a new abstract class called NormFactors. NormFactors has two methods on it public abstract byte getByte(int doc) throws IOException; // Returns the byte[doc] public float getFactor(int doc) throws IOException; // Calls Similarity.decodeNorm(getByte(doc)) There are four implementations of this abstract class 1. NormFactors.EmptyNormFactors - This replaces the fakeNorms with a singleton that only returns 1.0 2. NormFactors.ByteNormFactors - Converts a byte[] to a NormFactors for backwards compatibility in constructors. 3. MultiNormFactors - Multiplexes the NormFactors in MultiReader to prevent the need to construct the gigantic norms array. 4. SegmentReader.Norm - Same class, but now extends NormFactors to provide the same access. In addition, Many of the Query and Scorer classes were changes to pass around NormFactors instead of byte[], and to call getFactor() instead of using the byte[]. I have kept around IndexReader.norms(String) for backwards compatibiltiy, but marked it as deprecated. I believe that the use of ByteNormFactors in IndexReader.getNormFactors() will keep backward compatibility with other IndexReader implementations, but I don't know how to test that.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>506</id>
      <title>Optimize Memory Use for Short-Lived Indexes (Do not load TermInfoIndex if you know the queries ahead of time)</title>
      <description>Summary: Provide a way to avoid loading the TermInfoIndex into memory if you know all the terms you are ever going to query. In our search environment, we have a large number of indexes (many thousands), any of which may be queried by any number of hosts. These indexes may be very large (~1M document), but since we have a low term/doc ratio, we have 7-11M terms. With an index interval of 128, that means ~70-90K terms. On loading the index, it instantiates a Term, a TermInfo, a String, and a char[]. When the document is long lived, this makes some sense because you can quickly search the list of terms using binary search. However, since we throw away the Indexes very often, a lot of garbage is created per query Here's an example where we load a large index 10 times. This corresponds to 7MB of garbage per query. percent live alloc'ed stack class rank self accum bytes objs bytes objs trace name 1 4.48% 4.48% 4678736 128946 23393680 644730 387749 char[] 3 3.95% 12.61% 4126272 128946 20631360 644730 387751 org.apache.lucene.index.TermInfo 6 2.96% 22.71% 3094704 128946 15473520 644730 387748 java.lang.String 8 1.98% 26.97% 2063136 128946 10315680 644730 387750 org.apache.lucene.index.Term This adds up after a while. Since we know exactly which Terms we're going to search for before even opening the index, there's no need to allocate this much memory. Upon opening the index, we can go through the TII in sequential order and retrieve the entries into the main term dictionary and reduce the storage requirements dramatically. This reduces the amount of garbage generated by querying by about 60% if you only make 1 query/index with a 77% increase in throughput. This is accomplished by factoring out the "index loading" aspects of TermInfosReader into a new file, SegmentTermInfosReader. TermInfosReader becomes a base class to allow access to terms. A new class, PrefetchedTermInfosReader will, upon startup, sort the passed in terms and retrieve the IndexEntries for those terms. IndexReader and SegmentReader are modified to take new constructor methods that take a Collection of Terms that correspond to the total set of terms that will ever be searched in the life of the index. In order to support the "skipping" behavior, some changes need to be made to SegmentTermEnum: specifically, we need to be able to go back an entry in order to retrieve the previous TermInfo and IndexPointer. This is because, unlike the normal case, with the index we want to return the value right before the intended field (so that we can be behind the desired termin the main dictionary). For example, if we're looking for "apple" in the index, and the two adjacent values are "abba" and "argon", we want to return "abba" instead of "argon". That way we won't miss any terms in the real index. This code is confusing; it should probably be moved to an subclass of TermBuffer, but that required more code. Not wanting to modify TermBuffer to keep it small, also lead to the odd NPE catch in SegmentTermEnum.java. Stickler for contracts may want to rename SegmentTermEnum.skipTo() to a different name because it implements a different contract: but it would be useful for anyone trying to skip around in the TII, so I figured it was the right thing to do.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>507</id>
      <title>CLONE -[PATCH] remove unused variables</title>
      <description>Seems I'm the only person who has the "unused variable" warning turned on in Eclipse This patch removes those unused variables and imports (for now only in the "search" package). This doesn't introduce changes in functionality, but it should be reviewed anyway: there might be cases where the variables should be used, but they are not because of a bug.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>509</id>
      <title>Performance optimization when retrieving a single field from a document</title>
      <description>If you just want to retrieve a single field from a Document, the only way to do it is to retrieve all the fields from the Document and then search it. This patch is an optimization that allows you retrieve a specific field from a document without instantiating a lot of field and string objects. This reduces our memory consumption on a per query basis by around around 20% when a lot of documents are returned. I've added a lot of comments saying you should only call it if you only ever need one field. There's also a unit test.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>510</id>
      <title>IndexOutput.writeString() should write length in bytes</title>
      <description>We should change the format of strings written to indexes so that the length of the string is in bytes, not Java characters. This issue has been discussed at: http://www.mail-archive.com/java-dev@lucene.apache.org/msg01970.html We must increment the file format number to indicate this change. At least the format number in the segments file should change. I'm targetting this for 2.1, i.e., we shouldn't commit it to trunk until after 2.0 is released, to minimize incompatible changes between 1.9 and 2.0 (other than removal of deprecated features).</description>
      <attachments/>
      <comments>33</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>513</id>
      <title>Remove superfluous comment in MMapDirectory.java</title>
      <description>See title, and I prefer my name to be removed from the source code.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>519</id>
      <title>NIO FS implementation to avoid synchronization</title>
      <description>Alternative implementation of FSDirectory that uses NIO to avoid sychronization on the file pointer. To use this, invoke Java with the System property org.apache.lucene.FSDirectory.class set to org.apache.lucene.store.NIOFSDirectory.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>520</id>
      <title>Ability to abort hit collection</title>
      <description>If the HitCollector.collect method returned a boolean value rather than void, this value could be used to determine whether any further hits should be reported. This would speed up things a bit when all you need is a confirmation that a query produces some hits (e.g. for generating suggestions).</description>
      <attachments/>
      <comments>9</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>522</id>
      <title>SpanFuzzyQuery</title>
      <description>This is my SpanFuzzyQuery. It is released under the Apache licensence. Just paste it in. package se.snigel.lucene; import org.apache.lucene.index.IndexReader; import org.apache.lucene.index.Term; import org.apache.lucene.search.*; import org.apache.lucene.search.spans.SpanOrQuery; import org.apache.lucene.search.spans.SpanQuery; import org.apache.lucene.search.spans.SpanTermQuery; import org.apache.lucene.search.spans.Spans; import java.io.IOException; import java.util.Collection; import java.util.LinkedList; /** @author Karl Wettin &lt;kalle@snigel.net&gt; */ public class SpanFuzzyQuery extends SpanQuery { public final static float defaultMinSimilarity = 0.7f; public final static int defaultPrefixLength = 0; private final Term term; private final float minimumSimilarity; private final int prefixLength; private BooleanQuery rewrittenFuzzyQuery; public SpanFuzzyQuery(Term term) { this(term, defaultMinSimilarity, defaultPrefixLength); } public SpanFuzzyQuery(Term term, float minimumSimilarity, int prefixLength) { this.term = term; this.minimumSimilarity = minimumSimilarity; this.prefixLength = prefixLength; if (minimumSimilarity &gt;= 1.0f) { throw new IllegalArgumentException("minimumSimilarity &gt;= 1"); } else if (minimumSimilarity &lt; 0.0f) { throw new IllegalArgumentException("minimumSimilarity &lt; 0"); } if (prefixLength &lt; 0) { throw new IllegalArgumentException("prefixLength &lt; 0"); } } public Query rewrite(IndexReader reader) throws IOException { FuzzyQuery fuzzyQuery = new FuzzyQuery(term, minimumSimilarity, prefixLength); rewrittenFuzzyQuery = (BooleanQuery) fuzzyQuery.rewrite(reader); BooleanClause[] clauses = rewrittenFuzzyQuery.getClauses(); SpanQuery[] spanQueries = new SpanQuery[clauses.length]; for (int i = 0; i &lt; clauses.length; i++) { BooleanClause clause = clauses[i]; TermQuery termQuery = (TermQuery) clause.getQuery(); spanQueries[i] = new SpanTermQuery(termQuery.getTerm()); spanQueries[i].setBoost(termQuery.getBoost()); } SpanOrQuery query = new SpanOrQuery(spanQueries); query.setBoost(fuzzyQuery.getBoost()); return query; } /** Expert: Returns the matches for this query in an index. Used internally to search for spans. */ public Spans getSpans(IndexReader reader) throws IOException { throw new UnsupportedOperationException("Query should have been rewritten"); } /** Returns the name of the field matched by this query.*/ public String getField() { return term.field(); } /** Returns a collection of all terms matched by this query.*/ public Collection getTerms() { if (rewrittenFuzzyQuery == null) { throw new RuntimeException("Query must be rewritten prior to calling getTerms()!"); } else { LinkedList&lt;Term&gt; terms = new LinkedList&lt;Term&gt;(); BooleanClause[] clauses = rewrittenFuzzyQuery.getClauses(); for (int i = 0; i &lt; clauses.length; i++) { BooleanClause clause = clauses[i]; TermQuery termQuery = (TermQuery) clause.getQuery(); terms.add(termQuery.getTerm()); } return terms; } } /** Prints a query to a string, with &lt;code&gt;field&lt;/code&gt; as the default field for terms. &lt;p&gt;The representation used is one that is supposed to be readable by {@link org.apache.lucene.queryParser.QueryParser QueryParser} . However, there are the following limitations: &lt;ul&gt; &lt;li&gt;If the query was created by the parser, the printed representation may not be exactly what was parsed. For example, characters that need to be escaped will be represented without the required backslash.&lt;/li&gt; &lt;li&gt;Some of the more complicated queries (e.g. span queries) don't have a representation that can be parsed by QueryParser.&lt;/li&gt; &lt;/ul&gt; */ public String toString(String field) { return "spans(" + rewrittenFuzzyQuery.toString() + ")"; } }</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>524</id>
      <title>Current implementation of fuzzy and wildcard queries inappropriately implemented as Boolean query rewrites</title>
      <description>The implementation of MultiTermQuery in terms of BooleanQuery introduces several problems: 1) Collisions with maximum clause limit on boolean queries which throws an exception. This is most problematic because it is difficult to ascertain in advance how many terms a fuzzy query or wildcard query might involve. 2) The boolean disjunctive scoring is not appropriate for either fuzzy or wildcard queries. In effect the score is divided by the number of terms in the query which has nothing to do with the relevancy of the results. 3) Performance of disjunctive boolean queries for large term sets is quite sub-optimal</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>525</id>
      <title>A standard Lucene install that works for simple web sites</title>
      <description>I'm new to Lucene. I would like to be able to download a blob, install it, set a few settings, preferably in a GUI, and be on the air with search enabled on my static web site. What I find on the Examples page is nothing like this. It is a collection of stuff that leads me to believe that I'll have to become expert in all sorts of Lucene arcana before I can get to my goal.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>528</id>
      <title>Optimization for IndexWriter.addIndexes()</title>
      <description>One big performance problem with IndexWriter.addIndexes() is that it has to optimize the index both before and after adding the segments. When you have a very large index, to which you are adding batches of small updates, these calls to optimize make using addIndexes() impossible. It makes parallel updates very frustrating. Here is an optimized function that helps out by calling mergeSegments only on the newly added documents. It will try to avoid calling mergeSegments until the end, unless you're adding a lot of documents at once. I also have an extensive unit test that verifies that this function works correctly if people are interested. I gave it a different name because it has very different performance characteristics which can make querying take longer.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>532</id>
      <title>[PATCH] Indexing on Hadoop distributed file system</title>
      <description>In my current project we needed a way to create very large Lucene indexes on Hadoop distributed file system. When we tried to do it directly on DFS using Nutch FsDirectory class - we immediately found that indexing fails because DfsIndexOutput.seek() method throws UnsupportedOperationException. The reason for this behavior is clear - DFS does not support random updates and so seek() method can't be supported (at least not easily). Well, if we can't support random updates - the question is: do we really need them? Search in the Lucene code revealed 2 places which call IndexOutput.seek() method: one is in TermInfosWriter and another one in CompoundFileWriter. As we weren't planning to use CompoundFileWriter - the only place that concerned us was in TermInfosWriter. TermInfosWriter uses IndexOutput.seek() in its close() method to write total number of terms in the file back into the beginning of the file. It was very simple to change file format a little bit and write number of terms into last 8 bytes of the file instead of writing them into beginning of file. The only other place that should be fixed in order for this to work is in SegmentTermEnum constructor - to read this piece of information at position = file length - 8. With this format hack - we were able to use FsDirectory to write index directly to DFS without any problems. Well - we still don't index directly to DFS for performance reasons, but at least we can build small local indexes and merge them into the main index on DFS without copying big main index back and forth.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>12</commenters>
    </issue>
    <issue>
      <id>534</id>
      <title>CRUD facade on top of Lucene</title>
      <description>29 mar 2006 kl. 15.43 Erik Hatcher wrote: Lazy loaded fields will be a nice addition to Lucene. I'm curious why the flag is set at indexing time rather than it being something that is controlled during retrieval somehow. I'm not sure what that API would look like, but it seems its a decision to be addressed during searching and reading of an index rather than during indexing itself. I reply: Lazy is nice. Thumbs up! But. I believe the field text persistency feature foolishly lead people to choose Lucene for persistency. I would prefer if we had identity safe Document and a CRUD pattern that bound them with an object instance. A standard implementation could act just as the current fields do. Perhaps this has already been discussed in the past. Perhaps I should settle with having a facade around Lucene rather than inside to save a few clock ticks. Perhaps everybody is not an OO-fundamentalist. Lucene to me is a set of tokens I can search to find my object instances already residing in memory. I use Prevayler for object persistency. As things I wrote required more indices I became sick keeping track of writers, searcher, readers, documents, et.c.. So I wrote a facade on top of Lucene that takes care of all the for me. It all comes down to four classes: final IndexHandler - one instance per Directory. Builds the index. CRUD&lt;E extends Tokenized &gt; - Create, Read, Update, Delete and searches. Tokenized&lt;E id class&gt; - Any class that shoud be accessible from a Lucene search. Hit&lt;E extends Tokenized&gt; In the end, I do something like this: LinkedList&lt;Hit&lt;MyClass&gt;&gt; hits = getCRUD().search(new TermQuery(new Term("foo", "bar")), getIndexHandler().getSearcher()); I have put it on the Jira in case anyone is interested. Made a small example. With some refactoring the pattern would support other implementations than Lucene.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>537</id>
      <title>Refactor of spell check</title>
      <description>I use the same ngram index for multiple categories, but only want to spell check per category. The old implementation did not support this as it used docFreq as controller source. The spell check returns suggestions with score and not just the suggested word. TokenFrequencyVector replace the IndexReader used for docFreq. LuceneTokenFrequencyVector wraps an IndexReader and works just as the old implementation. LuceneQueryDictionary creates an ngram dictionary based on a query and not the whole index. MultiTokenFrequencyVector treats a number of TokenFrequencyVector:s as one. I.e. for use when spell checking in multiple contexts. TokenFrequencyVectorMap is a HashMap facade. Comes with static factory to create the vector based on the the tokens in a specific field from a search. I use the TokenFrequencyVectorMap to build one vector per category and instanciate a MultiTokenFrequencyVector for each user query. Could probably save a couple of clock ticks by buffering MultiVectors rather than creating new once all the time. Also it seems as the ngram-code might not be thread safe. This also include the source in CVS. Have not succeded to prove it when when testing, only in the live environment. Each instance of Spellchecker only suggest once. And it takes quite some resources to create new instances of the spellchecker as it is designed today. Might get back on that subject.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>539</id>
      <title>Fix for deprecations in contrib/surround</title>
      <description>Fix for deprecations in contrib/surround.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>543</id>
      <title>Wildcard query with no wildcard characters in the term throws StringIndexOutOfBounds exception</title>
      <description>Query q1 = new WildcardQuery(new Term("Text", "a")); Hits hits = searcher.search(q1); Caught Exception java.lang.StringIndexOutOfBoundsException : String index out of range: -1 at java.lang.String.substring(Unknown Source) at org.apache.lucene.search.WildcardTermEnum.&lt;init&gt;(WildcardTermEnum.java:65) at org.apache.lucene.search.WildcardQuery.getEnum (WildcardQuery.java:38) at org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:54) at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:137) at org.apache.lucene.search.Query.weight (Query.java:92) at org.apache.lucene.search.Hits.&lt;init&gt;(Hits.java:41) at org.apache.lucene.search.Searcher.search(Searcher.java:44) at org.apache.lucene.search.Searcher.search(Searcher.java:36) at QuickTest.main(QuickTest.java:45) From Erik Hatcher Feel free to log this as a bug report in our JIRA issue tracker. It seems like a reasonable change to make, such that a WildcardQuery without a wildcard character would behave like TermQuery.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>544</id>
      <title>MultiFieldQueryParser field boost multiplier</title>
      <description>Allows specific boosting per field, e.g. +(name:foo^1 description:foo^0.1). Went from String[] field to MultiFieldQueryParser.FieldSetting[] field in constructor.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>545</id>
      <title>Field Selection and Lazy Field Loading</title>
      <description>The patch to come shortly implements a Field Selection and Lazy Loading mechanism for Document loading on the IndexReader. It introduces a FieldSelector interface that defines the accept method: FieldSelectorResult accept(String fieldName); (Perhaps we want to expand this to take in other parameters such as the field metadata (term vector, etc.)) Anyone can implement a FieldSelector to define how they want to load fields for a Document. The FieldSelectorResult can be one of four values: LOAD, LAZY_LOAD, NO_LOAD, LOAD_AND_BREAK. The FieldsReader, as it is looping over the FieldsInfo, applies the FieldSelector to determine what should be done with the current field. I modeled this after the java.io.FileFilter mechanism. There are two implementations to date: SetBasedFieldSelector and LoadFirstFieldSelector. The former takes in two sets of field names, one to load immed. and one to load lazily. The latter returns LOAD_AND_BREAK on the first field encountered. See TestFieldsReader for examples. It should support UTF-8 (I borrowed code from Issue 509, thanks!). See TestFieldsReader for examples I added an expert method on IndexInput named skipChars that takes in the number of characters to skip. This is a compromise on changing the file format of the fields to better support seeking. It does some of the work of readChars, but not all of it. It doesn't require buffer storage and it doesn't do the bitwise operations. It just reads in the appropriate number of bytes and promptly ignores them. This is useful for skipping non-binary, non-compressed stored fields. The biggest change is by far the introduction of the Fieldable interface (as per Doug's suggestion from a mailing list email on Lazy Field loading from a while ago). Field is now a Fieldable. All uses of Field have been changed to use Fieldable. FieldsReader.LazyField also implements Fieldable. Lazy Field loading is now implemented. It has a major caveat (that is Documented) in that it clones the underlying IndexInput upon lazy access to the Field value. IT IS UNDEFINED whether a Lazy Field can be loaded after the IndexInput parent has been closed (although, from what I saw, it does work). I thought about adding a reattach method, but it seems just as easy to reload the document. See the TestFieldsReader and DocHelper for examples. I updated a couple of other tests to reflect the new fields that are on the DocHelper document. All tests pass.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>547</id>
      <title>Directory implementation for Applets</title>
      <description>This directory implementation can be used inside of applets, where the index files are located on the server. Also teh applet is not required to be signed, as no calls to the System.getProperty are made.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>550</id>
      <title>InstantiatedIndex - faster but memory consuming index</title>
      <description>Represented as a coupled graph of class instances, this all-in-memory index store implementation delivers search results up to a 100 times faster than the file-centric RAMDirectory at the cost of greater RAM consumption. Performance seems to be a little bit better than log2n (binary search). No real data on that, just my eyes. Populated with a single document InstantiatedIndex is almost, but not quite, as fast as MemoryIndex. At 20,000 document 10-50 characters long InstantiatedIndex outperforms RAMDirectory some 30x, 15x at 100 documents of 2000 charachters length, and is linear to RAMDirectory at 10,000 documents of 2000 characters length. Mileage may vary depending on term saturation.</description>
      <attachments/>
      <comments>100</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>553</id>
      <title>MultiFieldDisjunctionMaxQueryParser</title>
      <description>Following on from Lucene-323, here is a replacement for the old DistributingMultiFieldQueryParser that works with the new QueryParser. This is essentially a cut-and-paste of MultiFieldQueryParser, using DisjunctionMaxQuery instead of BooleanQuery as the combing operator for the expansion of query terms across the multiple fields being searched. This is an important adjunct to DisjunctionMaxQuery as it provides a way to generate this query from input query strings. It would be possible to generalize MultiFieldQueryParser to parameterize the combining operator, but this would require modifying it rather than simply adding a new class.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>558</id>
      <title>Selective field loading</title>
      <description>Provides a new api, IndexReader.document(int doc, String[] fields). A document containing only the specified fields is created. The other fields of the document are not loaded, although unfortunately uncompressed strings still have to be scanned because the length information in the index is for UTF-8 encoded chars and not bytes. This is useful for applications that need quick access to a small subset of the fields. It can be used in conjunction with or for some uses instead of ParallelReader. This is a much smaller change for a simpler use case than Lucene-545. No existing API's are affected. All the tests pass and new tests are added to verify the feature.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>559</id>
      <title>Turkish Analyzer for Lucene</title>
      <description>I have developed an Analyzer for Turkish, thanks to German Language Analyzer and Brazillian Language Analyzers. This Turkish Analyzer supports iso-8859-9 character set(Turkish) and have a nice stop words set. I hope it can help to Turkish developers who use lucene(i searched many hours for a turkish analyzer for lucene but couldnt find, so i coded and sending it here.)</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>563</id>
      <title>IndexReader currently has javadoc errors</title>
      <description>Current trunk has some javadoc errors in IndexReader and some more in contrib.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>566</id>
      <title>Esperanto Analyzer</title>
      <description>Esperanto stemmer and analyzer from Brion Vibber: http://cvs.sourceforge.net/viewcvs.py/wikipedia/lucene-search/org/wikimedia/lsearch/ I asked Brion to follow up to this issue and attach his code with ASL 2.0.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>568</id>
      <title>[PATCH]Multiple threads performance enhancement when querying.</title>
      <description>This improvement will reduce the wait when TermInfosReader calls ensureIndexIsRead(). The small trick like below: .... private void ensureIndexIsRead() throws IOException { if (indexTerms != null) // index already read return; // do nothing synchronized(this){ System.out.println("Read index@--@"); if(indexTerms != null) { System.out.println ("Someone read it.return-_-"); return ; } readIndex (); } } private synchronized void readIndex() throws IOException{ Term[] m_indexTerms = null; try { int indexSize = (int)indexEnum.size; // otherwise read index m_indexTerms = new Term[indexSize]; indexInfos = new TermInfo[indexSize]; indexPointers = new long[indexSize]; for (int i = 0; indexEnum.next(); i++) { m_indexTerms[i] = indexEnum.term(); indexInfos[i] = indexEnum.termInfo(); indexPointers[i] = indexEnum.indexPointer; } } finally { indexEnum.close(); indexEnum = null; indexTerms = m_indexTerms; } }</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>570</id>
      <title>Expose directory on IndexReader</title>
      <description>It would be really useful to expose the index directory on the IndexReader class.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>574</id>
      <title>support for vjc java compiler, also known as J#</title>
      <description>The issue is to provide build/makefile support for a 3rd Java compiler. To add a new directory named "vjc" under the "src" directory, following the pattern of the "gcj" directory which provides build/makefile support for the GNU Java compiler. The "vjc" folder would include a readme.txt file describing particular compiler and platform issues and specific build notes. It would also contain the "solution" and "project" files needed to build from the standard Java Lucene sources. Any platform-specific source code would also be contained in this folder. A platform-specific patch file is also contained here, which ideally would be automatically applied by the build process. Work along these lines so far is available at http://alum.mit.edu/www/gjc/lucene-java-vjc.html</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>577</id>
      <title>SweetSpotSimiliarity</title>
      <description>This is a new Similarity implimention for the contrib/miscellaneous/ package, it provides a Similiarty designed for people who know the "sweetspot" of their data. three major pieces of functionality are included: 1) a lengthNorm which creates a "platuea" of values. 2) a baseline tf that provides a fixed value for tf's up to a minimum, at which point it becomes a sqrt curve (this is used by the tf(int) function. 3) a hyperbolic tf function which is best explained by graphing the equation. this isn't used by default, but is available for subclasses to call from their own tf functions. All constants used in all functions are configurable. In the case of lengthNorm, the constants are configurable per field, as well as allowing for defaults for unspecified fields.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>578</id>
      <title>Summer of Code GDATA Server --Project structure and simple version to start with--</title>
      <description>This is the initial issue for the GDATA - Server project (Google Summer of Code). The purpose of the issue is to create the project structure in the svn repository to kick off the project. The source e.g. the project will be located at URL: http://svn.apache.org/repos/asf/lucene/java/trunk/contrib The attachment includes the diff text file and the jar files included in the lib directory as a tar.gz file. To get some information about the project see http://wiki.apache.org/general/SimonWillnauer/SummerOfCode2006</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>580</id>
      <title>Pre-analyzed fields</title>
      <description>Adds the possibility to set a TokenStream at Field constrution time, available as tokenStreamValue in addition to stringValue, readerValue and binaryValue. There might be some problems with mixing stored fields with the same name as a field with tokenStreamValue.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>581</id>
      <title>Index, a new generalization super root</title>
      <description>This update adds a new super class Index, extended by Directory. It allows for transparent communication between your application and the persistency mechanism. It takes issue 550 one step closer to backwards compability. Term and Document are no longer final classes. InterfaceIndexWriter I also added these two in the Index. abstract index.openIndexWriter() abstract index.openIndexReader(); There is some bonus material that use this: Decorators for Index InterfaceIndexWriter IndexReader IndexSearcher NotifiableIndex, adds notification of changes to any Index. CreateListener DeleteListener OptimizationListener (not implemented) ContentUpdateListener, reacts to any change of the index. Optimized for some implementations. AutofreshedSearcher, contains a searcher that is always up to date with the index and have a buffer with old searchers that will be closed when nobody is using them anymore (hopefully).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>582</id>
      <title>Don't throw TooManyClauses exception</title>
      <description>I wonder if it would make sense to fall back to a ConstantScoreQuery instead of throwing a TooManyClauses exception?</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>584</id>
      <title>Decouple Filter from BitSet</title>
      <description>package org.apache.lucene.search; public abstract class Filter implements java.io.Serializable { public abstract AbstractBitSet bits(IndexReader reader) throws IOException; } public interface AbstractBitSet { public boolean get(int index); } It would be useful if the method =Filter.bits()= returned an abstract interface, instead of =java.util.BitSet=. Use case: there is a very large index, and, depending on the user's privileges, only a small portion of the index is actually visible. Sparsely populated =java.util.BitSet=s are not efficient and waste lots of memory. It would be desirable to have an alternative BitSet implementation with smaller memory footprint. Though it is possibly to derive classes from =java.util.BitSet=, it was obviously not designed for that purpose. That's why I propose to use an interface instead. The default implementation could still delegate to =java.util.BitSet=.</description>
      <attachments/>
      <comments>142</comments>
      <commenters>14</commenters>
    </issue>
    <issue>
      <id>586</id>
      <title>Very inefficient implementation of MultiTermDocs.skipTo</title>
      <description>In our application anytime the index was unoptimized/contained more than one segment there was a sharp drop in performance, which amounted to over 50ms per search on average. We would consistently see this drop anytime an index went from an optimized state to an unoptimized state. I tracked down the issue to the implementation of MultiTermDocs.skipTo function (found in MultiReader.java). Optimized indexes do not use this class during search but unoptimized indexes do. The comment on this function even explicitly states 'As yet unoptimized implementation.' It was implemented just by calling 'next' over and over so even if it knew it could skip ahead hundreds of thousands of hits it would not. So I re-implemented the function very similar to how the MultiTermDocs.next function was implemented and tested it out on or application for correctness and performance and it passed all our tests and the performance penalty of having multiple segments vanished. We have already put the new jar onto our production machines. Here is my implementation of skipTo, which closely mirrors the accepted implementation of 'next', please feel free to test it and commit it. /** Much more optimized implementation. Could be optimized fairly easily to skip entire segments */ public boolean skipTo(int target) throws IOException Unknown macro: { if (current != null &amp;&amp; current.skipTo(target-base)) { return true; } else if (pointer &lt; readers.length) { base = starts[pointer]; current = termDocs(pointer++); return skipTo(target); } else return false; }</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>589</id>
      <title>Demo HTML parser doesn't work for international documents</title>
      <description>Javacc assumes ASCII so it won't work with, say, japanese documents. Ideally it would read the charset from the HTML markup, but that can by tricky. For now assuming unicode would do the trick: Add the following line marked with a + to HTMLParser.jj: options { STATIC = false; OPTIMIZE_TOKEN_MANAGER = true; //DEBUG_LOOKAHEAD = true; //DEBUG_TOKEN_MANAGER = true; + UNICODE_INPUT = true; }</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>591</id>
      <title>Add meta keywords to HTMLParser</title>
      <description>It would be good if the HTMLParser could give us the keywords specified in the meta tags, so that we can index them. In HTMLParser.jj: void addMetaTag() { metaTags.setProperty(currentMetaTag, currentMetaContent); currentMetaTag = null; currentMetaContent = null; return; } One way to do it: void addMetaTag() throws IOException { metaTags.setProperty(currentMetaTag, currentMetaContent); if (currentMetaTag.equalsIgnoreCase("keywords")) { pipeOut.write(currentMetaContent); } currentMetaTag = null; currentMetaContent = null; return; }</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>592</id>
      <title>Create compound file after addIndexes but before rewrite of segments</title>
      <description>When compound file format is used new 'segments' file is written before cfs is created. If there is an exception (disk full, etc.) or it is opened before cfs exists, segments points to non-existing file. This is a small change in index/IndexWriter.java, just a swap a block of code beginning with if(useCompoundFile) ...</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>594</id>
      <title>Javadoc - Field constructor with Reader needs comment about retained reference</title>
      <description>If you don't dig into the Lucene internals, it isn't obvious the Field constructor http://lucene.apache.org/java/docs/api/org/apache/lucene/document/Field.html#Field%28java.lang.String,%20java.io.Reader%29 retains a reference to the reader for use later on. It would be useful to have a comment added to the Javadoc saying something like: Note: A reference to java.io.Reader is retained by the field. Reader is read from when the Document which this field is added to is itself added to the index. Without this, the caller is liable to do silly things like closing the stream after constructing the org.apache.lucene.document.Field.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>598</id>
      <title>GData Server MileStone 1 Revision</title>
      <description>Some Improvements to the GData Server. CRUD actions for Entries implemented / tested StorageComponent storing entries / feeds / users Dynamic Feed elements like links added. Decoupled all server components (storage / ReqeustHandler etc) using lookup service Added some JavaDoc</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>600</id>
      <title>ParallelWriter companion to ParallelReader</title>
      <description>A new class ParallelWriter is provided that serves as a companion to ParallelReader. ParallelWriter meets all of the doc-id synchronization requirements of ParallelReader, subject to: 1. ParallelWriter.addDocument() is synchronized, which might have an adverse effect on performance. The writes to the sub-indexes are, however, done in parallel. 2. The application must ensure that the ParallelReader is never reopened inside ParallelWriter.addDocument(), else it might find the sub-indexes out of sync. 3. The application must deal with recovery from ParallelWriter.addDocument() exceptions. Recovery must restore the synchronization of doc-ids, e.g. by deleting any trailing document(s) in one sub-index that were not successfully added to all sub-indexes, and then optimizing all sub-indexes. A new interface, Writable, is provided to abstract IndexWriter and ParallelWriter. This is in the same spirit as the existing Searchable and Fieldable classes. This implementation uses java 1.5. The patch applies against today's svn head. All tests pass, including the new TestParallelWriter.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>601</id>
      <title>RAMDirectory implements Serializable</title>
      <description>RAMDirectory is for some reason not serializable.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>602</id>
      <title>[PATCH] Filtering tokens for position and term vector storage</title>
      <description>This patch provides a new TokenSelector mechanism to select tokens of interest and creates two new IndexWriter configuration parameters: termVectorTokenSelector and positionsTokenSelector. termVectorTokenSelector, if non-null, selects which index tokens will be stored in term vectors. If positionsTokenSelector is non-null, then any tokens it rejects will have only their first position stored in each document (it is necessary to store one position to keep the doc freq properly to avoid the token being garbage collected in merges). This mechanism provides a simple solution to the problem of minimzing index size overhead cause by storing extra tokens that facilitate queries, in those cases where the mere existence of the extra tokens is sufficient. For example, in my test data using reverse tokens to speed prefix wildcard matching, I obtained the following index overheads: 1. With no TokenSelectors: 60% larger with reverse tokens than without 2. With termVectorTokenSelector rejecting reverse tokens: 36% larger 3. With both positionsTokenSelector and termVectorTokenSelector rejecting reverse tokens: 25% larger It is possible to obtain the same effect by using a separate field that has one occurrence of each reverse token and no term vectors, but this can be hard or impossible to do and a performance problem as it requires either rereading the content or storing all the tokens for subsequent processing. The solution with TokenSelectors is very easy to use and fast. Otis, thanks for leaving a comment in QueryParser.jj with the correct production to enable prefix wildcards! With this, it is a straightforward matter to override the wildcard query factory method and use reverse tokens effectively.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>605</id>
      <title>Make Explanation include information about match/non-match</title>
      <description>As discussed, I'm looking into the possibility of improving the Explanation class to include some basic info about the "match" status of the Explanation – independent of the value... http://www.nabble.com/BooleanWeight.normalize%28float%29-doesn%27t-normalize-prohibited-clauses--t1596471.html#a4347644 This is neccesary to deal with things like LUCENE-451</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>606</id>
      <title>Change behavior of ParallelReader.document(int)</title>
      <description>Currently, the returned documents contain, for each field, the stored data from all enclosed IndexReaders which contain the corresponding field. That is, a call to ParallelReader.document(doc).getFields(fieldName) returns an array of possibly several Field objects. Since null entries are disallowed, there is no way to determine to which IndexReader the field data exactly belongs. On the other side, a search for a term on that field only yields results if that term was contained in the first matching IndexReader which contained the field. Thus, when merging the ParallelReader contents to another IndexWriter, the indexed data does not correspond to the stored information. I am not sure whether this can be considered a bug (in some cases, this may exactly be required). However I would like to see an option to change this behaviour. I suggest a parameter for ParallelReader which specifies whether stored data from all IndexReaders or only from the one which is repsonsible for the field's indexed data will be returned by ParallelReader.document(int). Please find my proposed implementation attached, as well as a JUnit testcase.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>608</id>
      <title>deprecate Document.fields(), add getFields()</title>
      <description>A simple API improvement that I'm going to commit if nobody objects.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>615</id>
      <title>[patch] javadoc and comment updates for BooleanClause.</title>
      <description>Javadoc and comment updates for BooleanClause, one minor code simplification.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>623</id>
      <title>RAMDirectory.close() should have a comment about not releasing any resources</title>
      <description>I wrongly assumed that calling RAMDirectory.close() would free up the memory occupied by the RAMDirectory. It might be helpful to add a javadoc comment that warns users that RAMDirectory.close() is a no-op, since it might be a common assumption that close() would release resources.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>624</id>
      <title>Segment size limit for compound files</title>
      <description>Hello everyone, I implemented an improvement targeting compound file usage. Compound files are used to decrease the number of index files, because operating systems can't handle too many open file descriptors. On the other hand, a disadvantage of compound file format is the worse performance compared to multi-file indexes: http://www.gossamer-threads.com/lists/lucene/java-user/8950 In the book "Lucene in Action" it's said that compound file format is about 5-10% slower than multi-file format. The patch I'm proposing here adds the ability to the IndexWriter to use compound format only for segments, that do not contain more documents than a specific limit "CompoundFileSegmentSizeLimit", which the user can set. Due to the exponential merges, a lucene index usually contains only a few very big segments, but much more small segments. The best performance is actually just needed for the big segments, whereas a slighly worse performance for small segments shouldn't play a big role in the overall search performance. Consider the following example: Index Size: 1,500,000 Merge factor: 10 Max buffered docs: 100 Number of indexed fields: 10 Max. OS file descriptors: 1024 in the worst case a not-optimized index could contain the following amount of segments: 1 x 1,000,000 9 x 100,000 9 x 10,000 9 x 1,000 9 x 100 That's 37 segments. A multi-file format index would have: 37 segments * (7 files per segment + 10 files for indexed fields) = 629 files ==&gt; only about 2 open indexes per machine could be handled by the operating system A compound-file format index would have: 37 segments * 1 cfs file = 37 files ==&gt; about 27 open indexes could be handled by the operating system, but performance would be 5-10% worse. A compound-file format index with CompoundFileSegmentSizeLimit = 1,000,000 would have: 36 segments * 1 cfs file + 1 segment * (7 + 10 files) = 53 ==&gt; about 20 open indexes could be handled by the OS The OS can handle now 20 instead of just 2 open indexes, while maintaining the multi-file format performance. I'm going to create diffs on the current HEAD and will attach the patch files soon. Please let me know what you think about this improvement.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>625</id>
      <title>Query auto completer</title>
      <description>A trie that helps users to type in their query. Made for AJAX, works great with ruby on rails common scripts &lt;http://script.aculo.us/&gt;. Similar to the Google labs suggester. Trained by user queries. Optimizable. Uses an in memory corpus. Serializable.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>626</id>
      <title>Extended spell checker with phrase support and adaptive user session analysis.</title>
      <description>Extensive javadocs available in patch, but I also try to keep it compiled here: http://ginandtonique.org/~kalle/javadocs/didyoumean/org/apache/lucene/search/didyoumean/package-summary.html#package_description A semi-retarded reinforcement learning thingy backed by algorithmic second level suggestion schemes that learns from and adapts to user behavior as queries change, suggestions are accepted or declined, etc. Except for detecting spelling errors it considers context, composition/decomposition and a few other things. heroes of light and magik -&gt; heroes of might and magic vinci da code -&gt; da vinci code java docs -&gt; javadocs blacksabbath -&gt; black sabbath Depends on LUCENE-550</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>629</id>
      <title>Performance improvement for merging stored, compressed fields</title>
      <description>Hello everyone, currently the merging of stored, compressed fields is not optimal for the following reason: every time a stored, compressed field is being merged, the FieldsReader uncompresses the data, hence the FieldsWriter has to compress it again when it writes the merged fields data (.fdt) file. The uncompress/compress step is unneccessary and slows down the merge performance significantly. This patch improves the merge performance by avoiding the uncompress/compress step. In the following I give an overview of the changes I made: Added a new FieldSelectorResult constant named "LOAD_FOR_MERGE" to org.apache.lucene.document.FieldSelectorResult SegmentMerger now uses an FieldSelector to get stored fields from the FieldsReader. This FieldSelector's accept() method returns the FieldSelectorResult "LOAD_FOR_MERGE" for every field. Added a new inner class to FieldsReader named "FieldForMerge", which extends org.apache.lucene.document.AbstractField. This class holds the field properties and its data. If a field has the FieldSelectorResult "LOAD_FOR_MERGE", then the FieldsReader creates an instance of "FieldForMerge" and does not uncompress the field's data. FieldsWriter checks if the field it is about to write is an instanceof FieldsReader.FieldForMerge. If true, then it does not compress the field data. To test the performance I index about 350,000 text files and store the raw text in a stored, compressed field in the lucene index. I use a merge factor of 10. The final index has a size of 366MB. After building the index, I optimize it to measure the pure merge performance. Here are the performance results: old version: Time for Indexing: 36.7 minutes Time for Optimizing: 4.6 minutes patched version: Time for Indexing: 20.8 minutes Time for Optimizing: 0.5 minutes The results show that the index build time improved by about 43%, and the optimizing step is more than 8x faster. A diff of the final indexes (old and patched version) shows, that they are identical. Furthermore, all junit testcases succeeded with the patched version. Regards, Michael Busch</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>631</id>
      <title>GData Server - Milestone 3 Patch, Bugfixes, Documentation</title>
      <description>For Milestone 3 added Features: Update Delete Concurrency Version control Second storage impl. based on Db4o. (Distributed Storage) moved all configuration in one single config file. removed dependencies in testcases. added schema validation for and all xml files in the project (Configuration etc.) added JavaDoc much better Performance after reusing some resources added recovering component to lucene based storage to recover entries after a server crash or OOM Error (very simple) solved test case fail on hyperthread / multi core machines (@ hossman: give it a go) @Yonik &amp;&amp; Doug could you get that stuff in the svn please regards simon</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>635</id>
      <title>[PATCH] Decouple locking implementation from Directory implementation</title>
      <description>This is a spinoff of http://issues.apache.org/jira/browse/LUCENE-305. I've opened this new issue to capture that it's wider scope than LUCENE-305. This is a patch originally created by Jeff Patterson (see above link) and then modified as described here: http://issues.apache.org/jira/browse/LUCENE-305#action_12418493 with some small additional changes: For each FSDirectory.getDirectory(), I made a corresponding version that also accepts a LockFactory instance. So, you can construct an FSDirectory with your own LockFactory. Cascaded defaulting for FSDirectory's LockFactory implementation: if you pass in a LockFactory instance, it's used; else if setDisableLocks was called, we use NoLockFactory; else, if the system property "org.apache.lucene.store.FSDirectoryLockFactoryClass" is defined, we use that; finally, we'll use the original locking implementation (SimpleFSLockFactory). The gist is that all locking code has been moved out of *Directory and into subclasses of a new abstract LockFactory class. You can now set the LockFactory of a Directory to change how it does locking. For example, you can create an FSDirectory but set its locking to SingleInstanceLockFactory (if you know all writing/reading will take place a single JVM). The changes pass all unit tests (on Ubuntu Linux Sun Java 1.5 and Windows XP Sun Java 1.4), and I added another TestCase to test the LockFactory code. Note that LockFactory defaults are not changed: FSDirectory defaults to SimpleFSLockFactory and RAMDirectory defaults to SingleInstanceLockFactory. Next step (separate issue) is to create a LockFactory that uses the OS native locks (through java.nio).</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>636</id>
      <title>[PATCH] Differently configured Lucene 'instances' in same JVM</title>
      <description>Currently Lucene can be configured using system properties. When running multiple 'instances' of Lucene for different purposes in the same JVM, it is not possible to use different settings for each 'instance'. I made changes to some Lucene classes so you can pass a configuration to that class. The Lucene 'instance' will use the settings from that configuration. The changes do not effect the API and/or the current behavior so are backwards compatible. In addition to the changes above I also made the SegmentReader and SegmentTermDocs extensible outside of their package. I would appreciate the inclusion of these changes but don't mind creating a separate issue for them.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>639</id>
      <title>[PATCH] Slight performance improvement for readVInt() of IndexInput</title>
      <description>By unrolling the loop in readVInt() I was able to get a slight, about 1.8 %, performance improvement for this method. The test program invoked the method over 17 million times on each run. I ran the performance tests on: Windows XP Pro SP2 Sun JDK 1.5.0_07 YourKit 5.5.4 Lucene trunk</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>640</id>
      <title>[PATCH] Performance improvement for doNext() of ConjunctionScorer</title>
      <description>I was able to improve the performance of 'doNext() of 'ConjunctionScorer' by over 20% by storing the result of some calls that were performed multiple times in local variables. The test program invoked the method over 400,000 times on each run. I ran the performance tests on: Windows XP Pro SP2 Sun JDK 1.5.0_07 YourKit 5.5.4 Lucene trunk</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>642</id>
      <title>GData Server IndexComponent</title>
      <description>New Feature added: -&gt; Indexcomponent. -&gt; Content extraction from entries. -&gt; Custom content ext. strategies added. -&gt; user defined index schema. -&gt; extended gdata-config.xml schema (xsd) -&gt; Indexcomponent UnitTests -&gt; Spellchecking on some JavaDoc. ############## New jars included: nekoHTML.jar xercesImpl.jar @yonik: don't miss the '+' button to add directories</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>644</id>
      <title>Contrib: another highlighter approach</title>
      <description>Mark Harwoods highlighter package is a great contribution to Lucene, I've used it a lot! However, when you have large documents (fields), highlighting can be quite time consuming if you increase the number of bytes to analyze with setMaxDocBytesToAnalyze(int). The default value of 50k is often too low for indexed PDFs etcetera, which results in empty highlight strings. This is an alternative approach using term position vectors only to build fragment info objects. Then a StringReader can read the relevant fragments and skip() between them. This is a lot faster. Also, this method uses the entire field for finding the best fragments so you're always guaranteed to get a highlight snippet. Because this method only works with fields which have term positions stored one can check if this method works for a particular field using following code (taken from TokenSources.java): TermFreqVector tfv = (TermFreqVector) reader.getTermFreqVector(docId, field); if (tfv != null &amp;&amp; tfv instanceof TermPositionVector) { // use FulltextHighlighter } else { // use standard Highlighter } Someone else might find this useful so I'm posting the code here.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>646</id>
      <title>[PATCH] fix various small issues with the "getting started" demo pages</title>
      <description>This patch contains numerous small fixes for the "getting started" pages on the Lucene Java web site. Here are the rough fixes: To results.jsp: changed StopAnalyzer -&gt; StandardAnalyzer changed references of "url" to "path" (field "url" is never set and was therefore always null) remove prefix of "../webapps" from path so clicking through works Fixed typos, grammar and other cosmetic things. Modernized some things that have changed with time (names of JAR files, which languages have analyzers, etc.) Added outbound links to Javadocs, Wiki, Lucene static web site, external sites, when appropriate. Removed exact version of Tomcat for the demo web app (I think all recent versions of Tomcat will work as described) Other small changes... Net/net I think this is an improved version of what's available on the site today.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>648</id>
      <title>Allow changing of ZIP compression level for compressed fields</title>
      <description>In response to this thread: http://www.gossamer-threads.com/lists/lucene/java-user/38810 I think we should allow changing the compression level used in the call to java.util.zip.Deflator in FieldsWriter.java. Right now it's hardwired to "best": compressor.setLevel(Deflater.BEST_COMPRESSION); Unfortunately, this can apparently cause the zip library to take a very long time (10 minutes for 4.5 MB in the above thread) and so people may want to change this setting. One approach would be to read the default from a Java system property, but, it seems recently (pre 2.0 I think) there was an effort to not rely on Java System properties (many were removed). A second approach would be to add static methods (and static class attr) to globally set the compression level? A third method would be in document.Field class, eg a setCompressLevel/getCompressLevel? But then every time a document is created with this field you'd have to call setCompressLevel since Lucene doesn't have a global Field schema (like Solr). Any other ideas / prefererences for either of these methods?</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>649</id>
      <title>Fixed Spelling mailinglist.xml</title>
      <description>Just fixed some spelling in the mailinglist.xml in /java/trunk/xdocs</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>651</id>
      <title>Poor performance race condition in FieldCacheImpl</title>
      <description>A race condition exists in FieldCacheImpl that causes a significant performance degradation if multiple threads concurrently request a value that is not yet cached. The degradation is particularly noticable in large indexes and when there a many concurent requests for the cached value. For the full discussion see the mailing list thread 'Poor performance "race condition" in FieldSortedHitQueue' (http://www.gossamer-threads.com/lists/lucene/java-user/38717).</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>652</id>
      <title>Compressed fields should be "externalized" (from Fields into Document)</title>
      <description>Right now, as of 2.0 release, Lucene supports compressed stored fields. However, after discussion on java-dev, the suggestion arose, from Robert Engels, that it would be better if this logic were moved into the Document level. This way the indexing level just stores opaque binary fields, and then Document handles compress/uncompressing as needed. This approach would have prevented issues like LUCENE-629 because merging of segments would never need to decompress. See this thread for the recent discussion: http://www.gossamer-threads.com/lists/lucene/java-dev/38836 When we do this we should also work on related issue LUCENE-648.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>653</id>
      <title>GData-server storage fix activation depth</title>
      <description>Fixed nullpointer exception while rendering feeds with big amount of extensions. DB4O context.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>656</id>
      <title>FieldsInfo uses deprecated API</title>
      <description>The class FieldsInfo.java uses deprecated API in method "public void add(Document doc)" I rused the replacement and created the patch -&gt; see attachment</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>657</id>
      <title>FuzzyQuery should not be final</title>
      <description>I am trying to extend the FuzzyQuery to further filter the TermEnum. (I am indexing stem forms and original forms, but I only want to match original forms with a fuzzy term, otherwise I get to much noise). However, FuzzyQuery is a final class and I cannot extend it. As discussed in the mailing list (http://www.gossamer-threads.com/lists/lucene/java-dev/38756), we want to make the private variables and inner classes protected. I am attaching a patch for FuzzyQuery.java that implements this. I ran all unit tests and they passed without errors. Andreas.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>660</id>
      <title>GData html render preview</title>
      <description>GData output is usually ATOM / RSS e.g plain xml. This feature enables users or admins to preview the server output as html transformed by user defined xsl stylesheet. Stylesheet is configurable per service. That's just a cool feature for developing and for users wanna use the server for simple blog or feed server. regards simon</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>662</id>
      <title>Extendable writer and reader of field data</title>
      <description>As discussed on the dev mailing list, I have modified Lucene to allow to define how the data of a field is writen and read in the index. Basically, I have introduced the notion of IndexFormat. It is in fact a factory of FieldsWriter and FieldsReader. So the IndexReader, the indexWriter and the SegmentMerger are using this factory and not doing a "new FieldsReader/Writer()". I have also introduced the notion of FieldData. It handles every data of a field, and also the writing and the reading in a stream. I have done this way because in the current design of Lucene, Fiedable is an interface, so methods with a protected or package visibility cannot be defined. A FieldsWriter just writes data into a stream via the FieldData of the field. A FieldsReader instanciates a FieldData depending on the field name. Then it use the field data to read the stream. And finnaly it instanciates a Field with the field data. About compatibility, I think it is kept, as I have writen a DefaultIndexFormat that provides some DefaultFieldsWriter and DefaultFieldsReader. These implementations do the exact job that is done today. To acheive this modification, some classes and methods had to be moved from private and/or final to public or protected. About the lazy fields, I have implemented them in a more general way in the implementation of the abstract class FieldData, so it will be totally transparent for the Lucene user that will extends FieldData. The stream is kept in the fieldData and used as soon as the stringValue (or something else) is called. Implementing this way allowed me to handle the recently introduced LOAD_FOR_MERGE; it is just a lazy field data, and when read() is called on this lazy field data, the saved input stream is directly copied in the output stream. I have a last issue with this patch. The current design allow to read an index in an old format, and just do a writer.addIndexes() into a new format. With the new design, you cannot, because the writer will use the FieldData.write provided by the reader. enjoy !</description>
      <attachments/>
      <comments>17</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>663</id>
      <title>New feature rich higlighter for Lucene.</title>
      <description>Well, I refactored (took) some code from two previous highlighters. This highlighter: + use TermPositionVector where available + use Analyzer if no TermPositionVector found or is forced to use it. + support for all lucene queries (Term, Phrase with slops, Prefix, Wildcard, Range) except Fuzzy Query (can be implemented easly) has no support for scoring (yet) use same prefix,postfix for accepted terms (yet) ? It's written in Java5 In next release I'd like to add support for Fuzzy, "coloring" f.e. diffrent color for terms btw. phrase terms (slops), scoring of fragments It's apache licensed - I hope so I put licene statement in every file</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>664</id>
      <title>[PATCH] small fixes to the new scoring.html doc</title>
      <description>This is an awesome initiative. We need more docs that cleanly explain the inner workings of Lucene in general... thanks Grant &amp; Steve &amp; others! I have a few small initial proposed fixes, largely just adding some more description around the components of the formula. But also a couple typos, another link out to Wikipedia, a missing closing ), etc. I've only made it through the "Understanding the Scoring Formula" section so far.</description>
      <attachments/>
      <comments>27</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>668</id>
      <title>Incremental Search</title>
      <description>Add full support for long searches executed in steps. QueryFilters only support narrowing down a set of results, they don't allow extending it with OR clauses. They also don't allow to add new clauses to SpanQueries. This facility is needed because repeatedly creating new queries and executing a full, more complicated query at each step is not efficient for search sessions where you may have chains of hundreds of such steps.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>671</id>
      <title>Hashtable based Document</title>
      <description>I've attached a Document based on a hashtable and a performance test case. It performs better in most cases (all but enumeration by my measurement), but likely uses a larger memory footprint. The Document testcase will fail since it accesses the "fields" variable directly and gets confused when it's not the list it expected it to be. If nothing else we would be interested in at least being able to extend Document, which is currently declared final. (Anyone know the performance gains on declaring a class final?) Currently we have to maintain a copy of lucene which has methods and classes definalized and overriden. There are other classes as well that could be declared non-final (Fieldable comes to mind) since it's possible to make changes for project specific situations in those aswell but that's off-topic.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>672</id>
      <title>new merge policy</title>
      <description>New merge policy developed in the course of http://issues.apache.org/jira/browse/LUCENE-565 http://issues.apache.org/jira/secure/attachment/12340475/newMergePolicy.Sept08.patch</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>675</id>
      <title>Lucene benchmark: objective performance test for Lucene</title>
      <description>We need an objective way to measure the performance of Lucene, both indexing and querying, on a known corpus. This issue is intended to collect comments and patches implementing a suite of such benchmarking tests. Regarding the corpus: one of the widely used and freely available corpora is the original Reuters collection, available from http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz or http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz. I propose to use this corpus as a base for benchmarks. The benchmarking suite could automatically retrieve it from known locations, and cache it locally.</description>
      <attachments/>
      <comments>44</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>676</id>
      <title>Promote solr's PrefixFilter into Java Lucene's core</title>
      <description>Solr's PrefixFilter class is not specific to Solr and seems to be of interest to core lucene users (PyLucene in this case). Promoting it into the Lucene core would be helpful.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>677</id>
      <title>Italian Analyzer</title>
      <description>Hello everybody, I have written an Italian analyzer based on the Porter's stemming algorithm as found at http://www.snowball.tartarus.org. I would like to contribute it to the Lucene sandbox. The classes come with thorough unit tests. Regards, Federico Grilli</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>678</id>
      <title>[PATCH] LockFactory implementation based on OS native locks (java.nio.*)</title>
      <description>The current default locking for FSDirectory is SimpleFSLockFactory. It uses java.io.File.createNewFile for its locking, which has this spooky warning in Sun's javadocs: Note: this method should not be used for file-locking, as the resulting protocol cannot be made to work reliably. The FileLock facility should be used instead. So, this patch provides a LockFactory implementation based on FileLock (using java.nio.*). All unit tests pass with this patch, on OS X (10.4.8), Linux (Ubuntu 6.06), and Windows XP SP2. Another benefit of native locks is the OS automatically frees them if the JVM exits before Lucene can free its locks. Many people seem to hit this (old lock files still on disk) now. I've created this new class: org.apache.lucene.store.NativeFSLockFactory and added a couple test cases to the existing TestLockFactory. I've left SimpleFSLockFactory as the default locking for FSDirectory for now. I think we should get some usage / experience with NativeFSLockFactory and then later on make it the default locking implementation? I also tested changing FSDirectory's default locking to NativeFSLockFactory and all unit tests still pass (on the above platforms). One important note about locking over NFS: some NFS servers and/or clients do not support it, or, it's a configuration option or mode that must be explicitly enabled. When it's misconfigured it's able to take a long time (35 seconds in my case) before throwing an exception. To handle this, I acquire &amp; release a random test lock on creating the NativeFSLockFactory to verify locking is configured properly. A few other small changes in the patch: Added a "failure reason" to Lock.java so that in obtain(lockWaitTimeout), if there is a persistent IOException in trying to obtain the lock, this can be messaged &amp; included in the "Lock obtain timed out" that's raised. Corrected javadoc in SimpleFSLockFactory: it previously said the wrong system property for overriding lock class via system properties Fixed unhandled IOException when opening an IndexWriter for create, if the locks dir does not exist (just added lockDir.exists() check in clearAllLocks method of SimpleFSLockFactory &amp; NativeFSLockFactory. Fixed a few small unrelated issues with TestLockFactory, and also fixed tests to accept NativeFSLockFactory as the default locking implementation for FSDirectory. Fixed a typo in javadoc in FieldsReader.java Added some more javadoc for the LockFactory.setLockPrefix</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>682</id>
      <title>QueryParser with Locale Based Operators (French included)</title>
      <description>Here is a version of the QueryParser that can "understand" the AND, OR and NOT keyword in other languages. If activated, "a ET b" should return the same query as "a AND b", namely: "+a +b" "a OU b" should return the same query as "a OR b", namely: "a b" "a SAUF b" should return the same query as "a NOT b", namely: "a -b" Here are its main points : 1) Patched from revision 454774 of lucene 2.1dev (trunk) (probably could be used with other versions) 2) The "ant test" target is still successful when the modified QueryParser is used 3) It doesn't break actual code 4) The default behavior is the same as before 5) It has to be deliberately activated 6) It use ResourceBundle to find the keywords translation 7) Comes with FRENCH translation 8) Comes with JUnit testCases 9) Adds 1 public method to QueryParser 10) Expands the TOKEN &lt;TERM&gt; 11) Use TOKEN_MGR_DECLS to set some field for the TokenManager</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>685</id>
      <title>Extract interface from IndexWriter</title>
      <description>org.apache.lucene.index.IndexWriter should probably implement an interface to allow us to more easily write unit tests that use it. As it stands, it's a complex class that's hard to stub/mock. For example, an interface which had methods such as addDocument(), close() and optimize().</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>687</id>
      <title>Performance improvement: Lazy skipping on proximity file</title>
      <description>Hello, I'm proposing a patch here that changes org.apache.lucene.index.SegmentTermPositions to avoid unnecessary skips and reads on the proximity stream. Currently a call of next() or seek(), which causes a movement to a document in the freq file also moves the prox pointer to the posting list of that document. But this is only necessary if actual positions have to be retrieved for that particular document. Consider for example a phrase query with two terms: the freq pointer for term 1 has to move to document x to answer the question if the term occurs in that document. But only if term 2 also matches document x, the positions have to be read to figure out if term 1 and term 2 appear next to each other in document x and thus satisfy the query. A move to the posting list of a document can be quite expensive. It has to be skipped to the last skip point before that document and then the documents between the skip point and the desired document have to be scanned, which means that the VInts of all positions of those documents have to be read and decoded. An improvement is to move the prox pointer lazily to a document only if nextPosition() is called. This will become even more important in the future when the size of the proximity file increases (e. g. by adding payloads to the posting lists). My patch implements this lazy skipping. All unit tests pass. I also attach a new unit test that works as follows: Using a RamDirectory an index is created and test docs are added. Then the index is optimized to make sure it only has a single segment. This is important, because IndexReader.open() returns an instance of SegmentReader if there is only one segment in the index. The proxStream instance of SegmentReader is package protected, so it is possible to set proxStream to a different object. I am using a class called SeeksCountingStream that extends IndexInput in a way that it is able to count the number of invocations of seek(). Then the testcase searches the index using a PhraseQuery "term1 term2". It is known how many documents match that query and the testcase can verify that seek() on the proxStream is not called more often than number of search hits. Example: Number of docs in the index: 500 Number of docs that match the query "term1 term2": 5 Invocations of seek on prox stream (old code): 29 Invocations of seek on prox stream (patched version): 5 Michael</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>688</id>
      <title>Add a FAQ entry explaning release naming.</title>
      <description>People get confused when they see a -dev release when they build from source. Explain why.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>691</id>
      <title>Bob Carpenter's FuzzyTermEnum refactoring</title>
      <description>I'll just paste Bob's complete email here. I refactored the org.apache.lucene.search.FuzzyTermEnum edit distance implementation. It now only uses a single pair of arrays, and those never get resized. That required turning the order of text/target around in the loops. You'll see that with the pair of arrays method, they get re-used hand-over-hand, and are assigned to local variables in the tight loops. I removed the calculation of min distance and replaced it with a boolean – the min wasn't needed, only the test vs. the max. I also flipped some variables around so there's one less addition in the very inner loop and the arrays are now looping in the ordinary way (starting at 0 with a &lt; comparison). I also commented out the redundant definition of the public close() [which just called super.close() and had none of its own doc.] I also just compute the max distance each time rather than fiddling with an array – it's just a little arithmetic done once per term, but that could be put back. I also rewrote min(int,int,int) to get rid of intermediate assignments. Is there a lib for this kind of thing? An intermediate refactoring that does the hand-over-hand with the existing array and resizing strategy is in FuzzyTermEnum.intermed.java. I ran the unit tests as follows on both versions (my hat's off to the build.xml author(s) – this all just worked out of the box and was really easy to follow the first through): C:\java\lucene-2.0.0&gt;ant -Djunit.includes="" -Dtestcase=TestFuzzyQuery test Buildfile: build.xml javacc-uptodate-check: javacc-notice: init: common.compile-core: [javac] Compiling 1 source file to C:\java\lucene-2.0.0\build\classes\java compile-core: compile-demo: common.compile-test: compile-test: test: [junit] Testsuite: org.apache.lucene.search.TestFuzzyQuery [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.453 sec BUILD SUCCESSFUL Total time: 2 seconds Does anyone have regression/performance test harnesses? The unit tests were pretty minimal (which is a good thing!). It'd be nice to test the min impl (ternary vs. if/then) and the assumption about not allocating an array of max distances. All told, the refactored version should be a modest speed improvement, mainly from unfolding the arrays so they're local one-dimensional refs. I don't know what the protocol is for one-off contributions. I'm happy with the Apache license, so that shouldn't be a problem. I also don't know whether you use tabs or spaces – I untabified the final version and used your two-space format in emacs. Bob Carpenter package org.apache.lucene.search; /** Copyright 2004 The Apache Software Foundation * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at * http://www.apache.org/licenses/LICENSE-2.0 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import org.apache.lucene.index.IndexReader; import org.apache.lucene.index.Term; import java.io.IOException; /** Subclass of FilteredTermEnum for enumerating all terms that are similiar to the specified filter term. * &lt;p&gt;Term enumerations are always ordered by Term.compareTo(). Each term in the enumeration is greater than all that precede it. */ public final class FuzzyTermEnum extends FilteredTermEnum { /* This should be somewhere around the average long word. If it is longer, we waste time and space. If it is shorter, we waste a little bit of time growing the array as we encounter longer words. */ private static final int TYPICAL_LONGEST_WORD_IN_INDEX = 19; /* Allows us save time required to create a new array everytime similarity is called. These are slices that will be reused during dynamic programming hand-over-hand style. */ private final int[] d0; private final int[] d1; private float similarity; private boolean endEnum = false; private Term searchTerm = null; private final String field; private final String text; private final String prefix; private final float minimumSimilarity; private final float scale_factor; /** Creates a FuzzyTermEnum with an empty prefix and a minSimilarity of 0.5f. &lt;p&gt; After calling the constructor the enumeration is already pointing to the first valid term if such a term exists. * @param reader @param term @throws IOException @see #FuzzyTermEnum(IndexReader, Term, float, int) */ public FuzzyTermEnum(IndexReader reader, Term term) throws IOException { this(reader, term, FuzzyQuery.defaultMinSimilarity, FuzzyQuery.defaultPrefixLength); } /** * Creates a FuzzyTermEnum with an empty prefix. * &lt;p&gt; * After calling the constructor the enumeration is already pointing to the first * valid term if such a term exists. * * @param reader * @param term * @param minSimilarity * @throws IOException * @see #FuzzyTermEnum(IndexReader, Term, float, int) */ public FuzzyTermEnum(IndexReader reader, Term term, float minSimilarity) throws IOException { this(reader, term, minSimilarity, FuzzyQuery.defaultPrefixLength); } /** * Constructor for enumeration of all terms from specified &lt;code&gt;reader&lt;/code&gt; which share a prefix of * length &lt;code&gt;prefixLength&lt;/code&gt; with &lt;code&gt;term&lt;/code&gt; and which have a fuzzy similarity &gt; * &lt;code&gt;minSimilarity&lt;/code&gt;. * &lt;p&gt; * After calling the constructor the enumeration is already pointing to the first * valid term if such a term exists. * * @param reader Delivers terms. * @param term Pattern term. * @param minSimilarity Minimum required similarity for terms from the reader. Default value is 0.5f. * @param prefixLength Length of required common prefix. Default value is 0. * @throws IOException */ public FuzzyTermEnum(IndexReader reader, Term term, final float minSimilarity, final int prefixLength) throws IOException { super(); if (minSimilarity &gt;= 1.0f) throw new IllegalArgumentException("minimumSimilarity cannot be greater than or equal to 1"); else if (minSimilarity &lt; 0.0f) throw new IllegalArgumentException("minimumSimilarity cannot be less than 0"); if(prefixLength &lt; 0) throw new IllegalArgumentException("prefixLength cannot be less than 0"); this.minimumSimilarity = minSimilarity; this.scale_factor = 1.0f / (1.0f - minimumSimilarity); this.searchTerm = term; this.field = searchTerm.field(); //The prefix could be longer than the word. //It's kind of silly though. It means we must match the entire word. final int fullSearchTermLength = searchTerm.text().length(); final int realPrefixLength = prefixLength &gt; fullSearchTermLength ? fullSearchTermLength : prefixLength; this.text = searchTerm.text().substring(realPrefixLength); this.prefix = searchTerm.text().substring(0, realPrefixLength); this.d0 = new int[this.text.length()+1]; this.d1 = new int[this.d0.length]; setEnum(reader.terms(new Term(searchTerm.field(), prefix))); } /** * The termCompare method in FuzzyTermEnum uses Levenshtein distance to * calculate the distance between the given term and the comparing term. */ protected final boolean termCompare(Term term) { if (field == term.field() &amp;&amp; term.text().startsWith(prefix)) { final String target = term.text().substring(prefix.length()); this.similarity = similarity(target); return (similarity &gt; minimumSimilarity); } endEnum = true; return false; } public final float difference() { return (float)((similarity - minimumSimilarity) * scale_factor); } public final boolean endEnum() { return endEnum; } /****************************** * Compute Levenshtein distance ******************************/ /** * Finds and returns the smallest of three integers */ private static final int min(int a, int b, int c) { // removed assignments to use double ternary return (a &lt; b) ? ((a &lt; c) ? a : c) : ((b &lt; c) ? b: c); // alt form is: // if (a &lt; b) { if (a &lt; c) return a; else return c; } // if (b &lt; c) return b; else return c; } /** * &lt;p&gt;Similarity returns a number that is 1.0f or less (including negative numbers) * based on how similar the Term is compared to a target term. It returns * exactly 0.0f when * &lt;pre&gt; * editDistance &lt; maximumEditDistance&lt;/pre&gt; * Otherwise it returns: * &lt;pre&gt; * 1 - (editDistance / length)&lt;/pre&gt; * where length is the length of the shortest term (text or target) including a * prefix that are identical and editDistance is the Levenshtein distance for * the two words.&lt;/p&gt; * * &lt;p&gt;Embedded within this algorithm is a fail-fast Levenshtein distance * algorithm. The fail-fast algorithm differs from the standard Levenshtein * distance algorithm in that it is aborted if it is discovered that the * mimimum distance between the words is greater than some threshold. * * &lt;p&gt;To calculate the maximum distance threshold we use the following formula: * &lt;pre&gt; * (1 - minimumSimilarity) * length&lt;/pre&gt; * where length is the shortest term including any prefix that is not part of the * similarity comparision. This formula was derived by solving for what maximum value * of distance returns false for the following statements: * &lt;pre&gt; * similarity = 1 - ((float)distance / (float) (prefixLength + Math.min(textlen, targetlen))); * return (similarity &gt; minimumSimilarity);&lt;/pre&gt; * where distance is the Levenshtein distance for the two words. * &lt;/p&gt; * &lt;p&gt;Levenshtein distance (also known as edit distance) is a measure of similiarity * between two strings where the distance is measured as the number of character * deletions, insertions or substitutions required to transform one string to * the other string. * @param target the target word or phrase * @return the similarity, 0.0 or less indicates that it matches less than the required * threshold and 1.0 indicates that the text and target are identical */ private synchronized final float similarity(final String target) { final int m = target.length(); final int n = text.length(); if (n == 0) { //we don't have anything to compare. That means if we just add //the letters for m we get the new word return prefix.length() == 0 ? 0.0f : 1.0f - ((float) m / prefix.length()); } if (m == 0) { return prefix.length() == 0 ? 0.0f : 1.0f - ((float) n / prefix.length()); } final int maxDistance = calculateMaxDistance(m); if (maxDistance &lt; Math.abs(m-n)) { //just adding the characters of m to n or vice-versa results in //too many edits //for example "pre" length is 3 and "prefixes" length is 8. We can see that //given this optimal circumstance, the edit distance cannot be less than 5. //which is 8-3 or more precisesly Math.abs(3-8). //if our maximum edit distance is 4, then we can discard this word //without looking at it. return 0.0f; } int[] dLast = d0; // set locals for efficiency int[] dCurrent = d1; for (int j = 0; j &lt;= n; j++) dCurrent[j] = j; for (int i = 0; i &lt; m; ) { final char s_i = target.charAt; int[] dTemp = dLast; dLast = dCurrent; // previously: d[i-i] dCurrent = dTemp; // previously: d[i] boolean prune = (dCurrent[0] = ++i) &gt; maxDistance; // true if d[i][0] is too large for (int j = 0; j &lt; n; j++) { dCurrent[j+1] = (s_i == text.charAt(j)) ? min(dLast[j+1]+1, dCurrent[j]+1, dLast[j]) : min(dLast[j+1], dCurrent[j], dLast[j])+1; if (prune &amp;&amp; dCurrent[j+1] &lt;= maxDistance) prune = false; } // (prune==false) iff (dCurrent[j] &lt; maxDistance) for some j if (prune) { return 0.0f; } } // this will return less than 0.0 when the edit distance is // greater than the number of characters in the shorter word. // but this was the formula that was previously used in FuzzyTermEnum, // so it has not been changed (even though minimumSimilarity must be // greater than 0.0) return 1.0F - dCurrent[n]/(float)(prefix.length() + Math.min(n,m)); } private int calculateMaxDistance(int m) { return (int) ((1-minimumSimilarity) * (Math.min(text.length(), m) + prefix.length())); } /* This is redundant public void close() throws IOException { super.close(); //call super.close() and let the garbage collector do its work. } */ } package org.apache.lucene.search; /** * Copyright 2004 The Apache Software Foundation * * Licensed under the Apache License, Version 2.0 (the "License"); * you may not use this file except in compliance with the License. * You may obtain a copy of the License at * * http://www.apache.org/licenses/LICENSE-2.0 * * Unless required by applicable law or agreed to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language governing permissions and * limitations under the License. */ import org.apache.lucene.index.IndexReader; import org.apache.lucene.index.Term; import java.io.IOException; /** Subclass of FilteredTermEnum for enumerating all terms that are similiar * to the specified filter term. * * &lt;p&gt;Term enumerations are always ordered by Term.compareTo(). Each term in * the enumeration is greater than all that precede it. */ public final class FuzzyTermEnum extends FilteredTermEnum { /* This should be somewhere around the average long word. * If it is longer, we waste time and space. If it is shorter, we waste a * little bit of time growing the array as we encounter longer words. */ private static final int TYPICAL_LONGEST_WORD_IN_INDEX = 19; /* Allows us save time required to create a new array * everytime similarity is called. These are slices that * will be reused during dynamic programming hand-over-hand * style. They get resized, if necessary, by growDistanceArrays(int). */ private int[] d0; private int[] d1; private float similarity; private boolean endEnum = false; private Term searchTerm = null; private final String field; private final String text; private final String prefix; private final float minimumSimilarity; private final float scale_factor; /** * Creates a FuzzyTermEnum with an empty prefix and a minSimilarity of 0.5f. * &lt;p&gt; * After calling the constructor the enumeration is already pointing to the first * valid term if such a term exists. * * @param reader * @param term * @throws IOException * @see #FuzzyTermEnum(IndexReader, Term, float, int) */ public FuzzyTermEnum(IndexReader reader, Term term) throws IOException { this(reader, term, FuzzyQuery.defaultMinSimilarity, FuzzyQuery.defaultPrefixLength); } /** Creates a FuzzyTermEnum with an empty prefix. &lt;p&gt; After calling the constructor the enumeration is already pointing to the first valid term if such a term exists. * @param reader @param term @param minSimilarity @throws IOException @see #FuzzyTermEnum(IndexReader, Term, float, int) */ public FuzzyTermEnum(IndexReader reader, Term term, float minSimilarity) throws IOException { this(reader, term, minSimilarity, FuzzyQuery.defaultPrefixLength); } /** Constructor for enumeration of all terms from specified &lt;code&gt;reader&lt;/code&gt; which share a prefix of length &lt;code&gt;prefixLength&lt;/code&gt; with &lt;code&gt;term&lt;/code&gt; and which have a fuzzy similarity &gt; &lt;code&gt;minSimilarity&lt;/code&gt;. &lt;p&gt; After calling the constructor the enumeration is already pointing to the first valid term if such a term exists. * @param reader Delivers terms. @param term Pattern term. @param minSimilarity Minimum required similarity for terms from the reader. Default value is 0.5f. @param prefixLength Length of required common prefix. Default value is 0. @throws IOException */ public FuzzyTermEnum(IndexReader reader, Term term, final float minSimilarity, final int prefixLength) throws IOException { super(); if (minSimilarity &gt;= 1.0f) throw new IllegalArgumentException("minimumSimilarity cannot be greater than or equal to 1"); else if (minSimilarity &lt; 0.0f) throw new IllegalArgumentException("minimumSimilarity cannot be less than 0"); if(prefixLength &lt; 0) throw new IllegalArgumentException("prefixLength cannot be less than 0"); this.minimumSimilarity = minSimilarity; this.scale_factor = 1.0f / (1.0f - minimumSimilarity); this.searchTerm = term; this.field = searchTerm.field(); //The prefix could be longer than the word. //It's kind of silly though. It means we must match the entire word. final int fullSearchTermLength = searchTerm.text().length(); final int realPrefixLength = prefixLength &gt; fullSearchTermLength ? fullSearchTermLength : prefixLength; this.text = searchTerm.text().substring(realPrefixLength); this.prefix = searchTerm.text().substring(0, realPrefixLength); growDistanceArrays(TYPICAL_LONGEST_WORD_IN_INDEX); setEnum(reader.terms(new Term(searchTerm.field(), prefix))); } /** The termCompare method in FuzzyTermEnum uses Levenshtein distance to calculate the distance between the given term and the comparing term. */ protected final boolean termCompare(Term term) Unknown macro: { if (field == term.field() &amp;&amp; term.text().startsWith(prefix)) { final String target = term.text().substring(prefix.length()); this.similarity = similarity(target); return (similarity &gt; minimumSimilarity); } endEnum = true; return false; } public final float difference() { return (float)((similarity - minimumSimilarity) * scale_factor); } public final boolean endEnum() { return endEnum; } /****************************** Compute Levenshtein distance ******************************/ /** Finds and returns the smallest of three integers */ private static final int min(int a, int b, int c) { // removed assignments to use double ternary return (a &lt; b) ? ((a &lt; c) ? a : c) : ((b &lt; c) ? b: c); // alt form is: // if (a &lt; b) { if (a &lt; c) return a; else return c; } // if (b &lt; c) return b; else return c; } /** &lt;p&gt;Similarity returns a number that is 1.0f or less (including negative numbers) based on how similar the Term is compared to a target term. It returns exactly 0.0f when &lt;pre&gt; editDistance &lt; maximumEditDistance&lt;/pre&gt; Otherwise it returns: &lt;pre&gt; 1 - (editDistance / length)&lt;/pre&gt; where length is the length of the shortest term (text or target) including a prefix that are identical and editDistance is the Levenshtein distance for the two words.&lt;/p&gt; * &lt;p&gt;Embedded within this algorithm is a fail-fast Levenshtein distance algorithm. The fail-fast algorithm differs from the standard Levenshtein distance algorithm in that it is aborted if it is discovered that the mimimum distance between the words is greater than some threshold. * &lt;p&gt;To calculate the maximum distance threshold we use the following formula: &lt;pre&gt; (1 - minimumSimilarity) * length&lt;/pre&gt; where length is the shortest term including any prefix that is not part of the similarity comparision. This formula was derived by solving for what maximum value of distance returns false for the following statements: &lt;pre&gt; similarity = 1 - ((float)distance / (float) (prefixLength + Math.min(textlen, targetlen))); return (similarity &gt; minimumSimilarity);&lt;/pre&gt; where distance is the Levenshtein distance for the two words. &lt;/p&gt; &lt;p&gt;Levenshtein distance (also known as edit distance) is a measure of similiarity between two strings where the distance is measured as the number of character deletions, insertions or substitutions required to transform one string to the other string. @param target the target word or phrase @return the similarity, 0.0 or less indicates that it matches less than the required threshold and 1.0 indicates that the text and target are identical */ private synchronized final float similarity(final String target) { final int m = target.length(); final int n = text.length(); if (n == 0) { //we don't have anything to compare. That means if we just add //the letters for m we get the new word return prefix.length() == 0 ? 0.0f : 1.0f - ((float) m / prefix.length()); } if (m == 0) { return prefix.length() == 0 ? 0.0f : 1.0f - ((float) n / prefix.length()); } final int maxDistance = calculateMaxDistance(m); if (maxDistance &lt; Math.abs(m-n)) { //just adding the characters of m to n or vice-versa results in //too many edits //for example "pre" length is 3 and "prefixes" length is 8. We can see that //given this optimal circumstance, the edit distance cannot be less than 5. //which is 8-3 or more precisesly Math.abs(3-8). //if our maximum edit distance is 4, then we can discard this word //without looking at it. return 0.0f; } //let's make sure we have enough room in our array to do the distance calculations. if (d0.length &lt;= m) { growDistanceArrays(m); } int[] dLast = d0; // set local vars for efficiency ~ the old d[i-1] int[] dCurrent = d1; // ~ the old d[i] for (int j = 0; j &lt;= m; j++) dCurrent[j] = j; for (int i = 0; i &lt; n; ) { final char s_i = text.charAt; int[] dTemp = dLast; dLast = dCurrent; // previously: d[i-i] dCurrent = dTemp; // previously: d[i] boolean prune = (dCurrent[0] = ++i) &gt; maxDistance; // true if d[i][0] is too large for (int j = 0; j &lt; m; j++) { dCurrent[j+1] = (s_i == target.charAt(j)) ? min(dLast[j+1]+1, dCurrent[j]+1, dLast[j]) : min(dLast[j+1], dCurrent[j], dLast[j])+1; if (prune &amp;&amp; dCurrent[j+1] &lt;= maxDistance) prune = false; } // (prune==false) iff (dCurrent[j] &lt; maxDistance) for some j if (prune) { return 0.0f; } } // this will return less than 0.0 when the edit distance is // greater than the number of characters in the shorter word. // but this was the formula that was previously used in FuzzyTermEnum, // so it has not been changed (even though minimumSimilarity must be // greater than 0.0) return 1.0F - dCurrent[m]/(float)(prefix.length() + Math.min(n,m)); } /** Grow the second dimension of the array slices, so that we can calculate the Levenshtein difference. */ private void growDistanceArrays(int m) { d0 = new int[m+1]; d1 = new int[m+1]; } private int calculateMaxDistance(int m) { return (int) ((1-minimumSimilarity) * (Math.min(text.length(), m) + prefix.length())); } /* This is redundant public void close() throws IOException { super.close(); //call super.close() and let the garbage collector do its work. } */ }</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>692</id>
      <title>Hangul Jamo (Korean) support in StandardTokenizer.jj</title>
      <description>One of our users reported their inability to search some Korean strings. This is because the Hangul Jamo Unicode block is not included in the StandardTokenizer.jj file. I'm attaching a patch which fixes this, from Young-Ho Cha.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>695</id>
      <title>Improve BufferedIndexInput.readBytes() performance</title>
      <description>During a profiling session, I discovered that BufferedIndexInput.readBytes(), the function which reads a bunch of bytes from an index, is very inefficient in many cases. It is efficient for one or two bytes, and also efficient for a very large number of bytes (e.g., when the norms are read all at once); But for anything in between (e.g., 100 bytes), it is a performance disaster. It can easily be improved, though, and below I include a patch to do that. The basic problem in the existing code was that if you ask it to read 100 bytes, readBytes() simply calls readByte() 100 times in a loop, which means we check byte after byte if the buffer has another character, instead of just checking once how many bytes we have left, and copy them all at once. My version, attached below, copies these 100 bytes if they are available at bulk (using System.arraycopy), and if less than 100 are available, whatever is available gets copied, and then the rest. (as before, when a very large number of bytes is requested, it is read directly into the final buffer). In my profiling, this fix caused amazing performance improvement: previously, BufferedIndexInput.readBytes() took as much as 25% of the run time, and after the fix, this was down to 1% of the run time! However, my scenario is not the typical Lucene code, but rather a version of Lucene with added payloads, and these payloads average at 100 bytes, where the original readBytes() did worst. I expect that my fix will have less of an impact on "vanilla" Lucene, but it still can have an impact because it is used for things like reading fields. (I am not aware of a standard Lucene benchmark, so I can't provide benchmarks on a more typical case). In addition to the change to readBytes(), my attached patch also adds a new unit test to BufferedIndexInput (which previously did not have a unit test). This test simulates a "file" which contains a predictable series of bytes, and then tries to read from it with readByte() and readButes() with various sizes (many thousands of combinations are tried) and see that exactly the expected bytes are read. This test is independent of my new readBytes() inplementation, and can be used to check the old implementation as well. By the way, it's interesting that BufferedIndexOutput.writeBytes was already efficient, and wasn't simply a loop of writeByte(). Only the reading code was inefficient. I wonder why this happened.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>701</id>
      <title>Lock-less commits</title>
      <description>This is a patch based on discussion a while back on lucene-dev: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3c44E5B16D.4010805@mikemccandless.com%3e The approach is a small modification over the original discussion (see Retry Logic below). It works correctly in all my cross-machine test case, but I want to open it up for feedback, testing by users/developers in more diverse environments, etc. This is a small change to how lucene stores its index that enables elimination of the commit lock entirely. The write lock still remains. Of the two, the commit lock has been more troublesome for users since it typically serves an active role in production. Whereas the write lock is usually more of a design check to make sure you only have one writer against the index at a time. The basic idea is that filenames are never reused ("write once"), meaning, a writer never writes to a file that a reader may be reading (there is one exception: the segments.gen file; see "RETRY LOGIC" below). Instead it writes to generational files, ie, segments_1, then segments_2, etc. Besides the segments file, the .del files and norm files (.sX suffix) are also now generational. A generation is stored as an "_N" suffix before the file extension (eg, _p_4.s0 is the separate norms file for segment "p", generation 4). One important benefit of this is it avoids files contents caching entirely (the likely cause of errors when readers open an index mounted on NFS) since the file is always a new file. With this patch I can reliably instantiate readers over NFS when a writer is writing to the index. However, with NFS, you are still forced to refresh your reader once a writer has committed because "point in time" searching doesn't work over NFS (see LUCENE-673 ). The changes are fully backwards compatible: you can open an old index for searching, or to add/delete docs, etc. I've added a new unit test to test these cases. All units test pass, and I've added a number of additional unit tests, some of which fail on WIN32 in the current lucene but pass with this patch. The "fileformats.xml" has been updated to describe the changes to the files (but XXX references need to be fixed before committing). There are some other important benefits: Readers are now entirely read-only. Readers no longer block one another (false contention) on initialization. On hitting contention, we immediately retry instead of a fixed (default 1.0 second now) pause. No file renaming is ever done. File renaming has caused sneaky access denied errors on WIN32 (see LUCENE-665 ). (Yonik, I used your approach here to not rename the segments_N file(try segments_(N-1) on hitting IOException on segments_N): the separate ".done" file did not work reliably under very high stress testing when a directory listing was not "point in time"). On WIN32, you can now call IndexReader.setNorm() even if other readers have the index open (fixes a pre-existing minor bug in Lucene). On WIN32, You can now create an IndexWriter with create=true even if readers have the index open (eg see www.gossamer-threads.com/lists/lucene/java-user/39265) . Here's an overview of the changes: Every commit writes to the next segments_(N+1). Loading the segments_N file (&amp; opening the segments) now requires retry logic. I've captured this logic into a new static class: SegmentInfos.FindSegmentsFile. All places that need to do something on the current segments file now use this class. No more deletable file. Instead, the writer computes what's deletable on instantiation and updates this in memory whenever files can be deleted (ie, when it commits). Created a common class index.IndexFileDeleter shared by reader &amp; writer, to manage deletes. Storing more information into segments info file: whether it has separate deletes (and which generation), whether it has separate norms, per field (and which generation), whether it's compound or not. This is instead of relying on IO operations (file exists calls). Note that this fixes the current misleading FileNotFoundException users now see when an _X.cfs file is missing (eg http://www.nabble.com/FileNotFound-Exception-t6987.html). Fixed some small things about RAMDirectory that were not filesystem-like (eg opening a non-existent IndexInput failed to raise IOException; renames were not atomic). I added a stress test against a RAMDirectory (1 writer thread &amp; 2 reader threads) that uncovered these. Added option to not remove old files when create=true on creating FSDirectory; this is so the writer can do its own [more sophisticated because it retries on errors] removal. Removed all references to commit lock, COMMIT_LOCK_TIMEOUT, etc. (This is an API change). Extended index/IndexFileNames.java and index/IndexFileNameFilter.java with logic for computing generational file names. Changed index/IndexFileNameFilter.java to use a HashSet to check file extentsions for better performance. Fixed the test case TestIndexReader.testLastModified: it was incorrectly (I think?) comparing lastModified to version, of the index. I fixed that and then added a new test case for version. Retry Logic (in index/SegmentInfos.java) If a reader tries to load the segments just as a writer is committing, it may hit an IOException. This is just normal contention. In current Lucene contention causes a [default] 1.0 second pause then retry. With lock-less the contention causes no added delay beyond the time to retry. When this happens, we first try segments_(N-1) if present, because it could be segments_N is still being written. If that fails, we re-check to see if there is now a newer segments_M where M &gt; N and advance if so. Else we retry segments_N once more (since it could be it was in process previously but must now be complete since segments_(N-1) did not load). In order to find the current segments_N file, I list the directory and take the biggest segments_N that exists. However, under extreme stress testing (5 threads just opening &amp; closing readers over and over), on one platform (OS X) I found that the directory listing can be incorrect (stale) by up to 1.0 seconds. This means the listing will show a segments_N file but that file does not exist (fileExists() returns false). In order to handle this (and other such platforms), I switched to a hybrid approach (originally proposed by Doron Cohen in the original thread): on committing, the writer writes to a file "segments.gen" the generation it just committed. It writes 2 identical longs into this file. The retry logic, on detecting that the directory listing is stale falls back to the contents of this file. If that file is consistent (the two longs are identical), and, the generation is indeed newer than the dir listing, it will use that. Finally, if this approach is also stale, we fallback to stepping through sequential generations (up to a maximum # tries). If all 3 methods fail, we throw the original exception we hit. I added a static method SegmentInfos.setInfoStream() which will print details of retry attempts. In the patch it's set to System.out right now (we should turn off before a real commit) so if there are problems we can see what retry logic had done.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>703</id>
      <title>Change QueryParser to use ConstantScoreRangeQuery in preference to RangeQuery by default</title>
      <description>Change to QueryParser to default to using new ConstantScoreRangeQuery in preference to RangeQuery for range queries. This implementation is generally preferable because it a) Runs faster b) Does not have the scarcity of range terms unduly influence score c) avoids any "TooManyBooleanClauses" exception. However, if applications really need to use the old-fashioned RangeQuery and the above points are not required then the "useOldRangeQuery" property can be used to revert to old behaviour. The patch includes extra Junit tests for this flag and all other Junit tests pass</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>704</id>
      <title>Lucene should have a "write once" mode</title>
      <description>This is a spinoff of LUCENE-701 If your directory is residing on a "write once" filesystem (eg Hadoop), we need for Lucene to have a mode where it doesn't write to the same file more than once, nor (I think?) do things like rewind a file to overwrite parts of it. Lockless commits (LUCENE-701 ) gets us closer to this goal because it always commits to a new segments_N+1 file (and new files for deletes/separate norms), but, it still re-writes to a "segments.gen" file. This file is often "optional" (it's only necessary if directory listing can be stale on the platform/filesystem). The only other place I know of is in CompoundFileWriter.close(). That method writes 0's into the header and then rewinds and rewrites those 0s with the actual offsets into the compound file. I think (on quick inspection) that pre-computing the offsets and writing everything in one pass should be simple. Does anyone know of other places that re-use filenames or rewind/seek and rewrite bytes? We should create a "setWriteOnceMode()" or something like that.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>705</id>
      <title>CompoundFileWriter should pre-set its file length</title>
      <description>I've read that if you are writing a large file, it's best to pre-set the size of the file in advance before you write all of its contents. This in general minimizes fragmentation and improves IO performance against the file in the future. I think this makes sense (intuitively) but I haven't done any real performance testing to verify. Java has the java.io.File.setLength() method (since 1.2) for this. We can easily fix CompoundFileWriter to call setLength() on the file it's writing (and add setLength() method to IndexOutput). The CompoundFileWriter knows exactly how large its file will be. Another good thing is: if you are going run out of disk space, then, the setLength call should fail up front instead of failing when the compound file is actually written. This has two benefits: first, you find out sooner that you will run out of disk space, and, second, you don't fill up the disk down to 0 bytes left (always a frustrating experience!). Instead you leave what space was available and throw an IOException. My one hesitation here is: what if out there there exists a filesystem that can't handle this call, and it throws an IOException on that platform? But this is balanced against possible easy-win improvement in performance. Does anyone have any feedback / thoughts / experience relevant to this?</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>706</id>
      <title>Index File Format - Example for frequency file .frq is wrong</title>
      <description>Reported by Johan Stuyts - http://www.nabble.com/Possible-documentation-error--p7012445.html - Frequency file example says: For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 15, 22, 3 It should be: For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 15, 8, 3</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>707</id>
      <title>Lucene Java Site docs</title>
      <description>It would be really nice if the Java site docs where consistent with the rest of the Lucene family (namely, with navigation tabs, etc.) so that one can easily go between Nutch, Hadoop, etc.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>708</id>
      <title>Setup nightly build website links and docs</title>
      <description>Per discussion on mailing list, we are going to setup a Nightly Build link on the website linking to the docs (and javadocs) generated by the nightly build process. The build process may need to be modified to complete this task. Going forward, the main website will, for the most part, only be updated per releases (I imagine exceptions will be made for News items and per committer's discretion). The Javadocs linked to from the main website will always be for the latest release.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>709</id>
      <title>[PATCH] Enable application-level management of IndexWriter.ramDirectory size</title>
      <description>IndexWriter currently only supports bounding of in the in-memory index cache using maxBufferedDocs, which limits it to a fixed number of documents. When document sizes vary substantially, especially when documents cannot be truncated, this leads either to inefficiencies from a too-small value or OutOfMemoryErrors from a too large value. This simple patch exposes IndexWriter.flushRamSegments(), and provides access to size information about IndexWriter.ramDirectory so that an application can manage this based on total number of bytes consumed by the in-memory cache, thereby allow a larger number of smaller documents or a smaller number of larger documents. This can lead to much better performance while elimianting the possibility of OutOfMemoryErrors. The actual job of managing to a size constraint, or any other constraint, is left up the applicatation. The addition of synchronized to flushRamSegments() is only for safety of an external call. It has no significant effect on internal calls since they all come from a sychronized caller.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>710</id>
      <title>Implement "point in time" searching without relying on filesystem semantics</title>
      <description>This was touched on in recent discussion on dev list: http://www.gossamer-threads.com/lists/lucene/java-dev/41700#41700 and then more recently on the user list: http://www.gossamer-threads.com/lists/lucene/java-user/42088 Lucene's "point in time" searching currently relies on how the underlying storage handles deletion files that are held open for reading. This is highly variable across filesystems. For example, UNIX-like filesystems usually do "close on last delete", and Windows filesystem typically refuses to delete a file open for reading (so Lucene retries later). But NFS just removes the file out from under the reader, and for that reason "point in time" searching doesn't work on NFS (see LUCENE-673 ). With the lockless commits changes (LUCENE-701 ), it's quite simple to re-implement "point in time searching" so as to not rely on filesystem semantics: we can just keep more than the last segments_N file (as well as all files they reference). This is also in keeping with the design goal of "rely on as little as possible from the filesystem". EG with lockless we no longer re-use filenames (don't rely on filesystem cache being coherent) and we no longer use file renaming (because on Windows it can fails). This would be another step of not relying on semantics of "deleting open files". The less we require from filesystem the more portable Lucene will be! Where it gets interesting is what "policy" we would then use for removing segments_N files. The policy now is "remove all but the last one". I think we would keep this policy as the default. Then you could imagine other policies: Keep past N day's worth Keep the last N Keep only those in active use by a reader somewhere (note: tricky how to reliably figure this out when readers have crashed, etc.) Keep those "marked" as rollback points by some transaction, or marked explicitly as a "snaphshot". Or, roll your own: the "policy" would be an interface or abstract class and you could make your own implementation. I think for this issue we could just create the framework (interface/abstract class for "policy" and invoke it from IndexFileDeleter) and then implement the current policy (delete all but most recent segments_N) as the default policy. In separate issue(s) we could then create the above more interesting policies. I think there are some important advantages to doing this: "Point in time" searching would work on NFS (it doesn't now because NFS doesn't do "delete on last close"; see LUCENE-673 ) and any other Directory implementations that don't work currently. Transactional semantics become a possibility: you can set a snapshot, do a bunch of stuff to your index, and then rollback to the snapshot at a later time. If a reader crashes or machine gets rebooted, etc, it could choose to re-open the snapshot it had previously been using, whereas now the reader must always switch to the last commit point. Searchers could search the same snapshot for follow-on actions. Meaning, user does search, then next page, drill down (Solr), drill up, etc. These are each separate trips to the server and if searcher has been re-opened, user can get inconsistent results (= lost trust). But with, one series of search interactions could explicitly stay on the snapshot it had started with.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>711</id>
      <title>BooleanWeight should size the weights Vector correctly</title>
      <description>The weights field on BooleanWeight uses a Vector that will always be sized exactly the same as the outer class' clauses Vector, therefore can be sized correctly in the constructor. This is a trivial memory saving enhancement.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>714</id>
      <title>Use a System.arraycopy more than a for</title>
      <description>In org.apache.lucene.index.DocumentWriter. The patch will explain by itself. I didn't make any performance test, but I think it is obvious that it will be faster. All tests passed.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>716</id>
      <title>Support unicode escapes in QueryParser</title>
      <description>As suggested by Yonik in http://issues.apache.org/jira/browse/LUCENE-573 the QueryParser should be able to handle unicode escapes, i. e. \uXXXX. I have already working and tested code. It is based on the patch i submitted for LUCENE-573, so once this is (hopefully ) committed, I will submit another patch here.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>718</id>
      <title>possible build.xml addition to ensure 1.4 class compatibility</title>
      <description>As encountered recently, setting the "source" and "target" values for the java compiler don't acctually test that the classes/methods are 1.4 compatible – just that the language syntax/features are... http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6333296 ...i've come up with one possible solution, that's really feels like a hack, but i wanted to throw it out here for comment, in a nutshell: 1) we support a new optional javac.bootclasspath property indicating with path the compiler should use. 2) people compiling with 1.4 can ignore that property 3) anyone who has a 1.5 compiler by default, can set this proprety to point at a 1.4 copy of the rt.jar – which is not inlcuded (users would need to install it themselves) 4) as part of the "init" target the build file will attempt to compile a java class that is syntactically correct in java 1.4, but utilizes a method only available in 1.5 ... if this class compiles cleanly, the task will fail. 5) java 1.5 users that aren't concerned about submitting compatible patches back to the comunity and don't want to hassle with a 1.4 version of rt.jar, can set a"javac.trustbootclasspath" and go about their merry way. The main idea here being that if someone has both JVMs installed and accidently uses the wrong one to test something before submitting a patch or committing, their build will either fail with a helpful message, or compile against the correct set of core classes anyway if they've done a small amount of setup. Caveats to commiting this: a) it's a hack, so i don't wnat to commit unless multiple people like it b) at the moment, all "successful" ant executions print a confusing compiler error as right off the bat, it would be better if we could supress that somehow. c) the BUILD.txt should be updated accordingly.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>719</id>
      <title>IndexModifier does not support all APIs from IndexWriter/IndexReader</title>
      <description>IndexModifer should probably provide all of the methods defined in both IndexReader and IndexWriter. Currently it does not (e.g., IndexReader.maxDoc() is not available). One way to do this might be to have IndexReader and IndexWriter interfaces. For example: package org.apache.lucene.index.interface; public interface IndexReader { ... int maxDoc(); ... } Then the IndexReader and IndexWriter classes could implement those interfaces: package org.apache.lucene.index; public abstract class IndexReader implements org.apache.lucene.index.interface IndexReader { ... public int maxDoc() { ... } ... } The IndexModifier could then implement both: public class IndexModifier implements org.apache.lucene.index.interface IndexReader, org.apache.lucene.index.interface IndexWriter { ... } Anywhere an IndexWriter or IndexReader was needed, one would require an object which implemented the appropriate interface: package org.apache.lucene.index; public class MultiReader extends IndexReader { ... MultiReader(org.apache.lucene.index.interface.IndexReader[] subReaders) { ... } ... } Just a thought....</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>721</id>
      <title>Code coverage reports</title>
      <description>Hi all, We should be able to measure the code coverage of our unit testcases. I believe it would be very helpful for the committers, if they could verify before committing a patch if it does not reduce the coverage. Furthermore people could take a look in the code coverage reports to figure out where work needs to be done, i. e. where additional testcases are neccessary. It would be nice if we could add a page to the Lucene website showing the report, generated by the nightly build. Maybe you could add that to your preview page (LUCENE-707), Grant? I attach a patch here that uses the tool EMMA to generate the code coverage reports. EMMA is a very nice open-source tool released under the CPL (same license as junit). The patch adds three targets to common-build.xml: emma-check: verifys if both emma.jar and emma_ant.jar are in the ant classpath emma-instrument: instruments the compiled code generate-emma-report: generates an html code coverage report The following steps are neccessary in order to generate a code coverage report: add emma.jar and emma_ant.jar to your ant classpath (download emma from http://emma.sourceforge.net/) execute ant target 'emma-instrument' (depends on compile-test, so it will compile all core and test classes) execute ant target 'test' to run the unit tests execute ant target 'generate-emma-report' To view the emma report open build/test/emma/index.html</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>723</id>
      <title>QueryParser support for MatchAllDocs</title>
      <description>It seems like there really should be QueryParser support for MatchAllDocsQuery. I propose : (brings back memories of DOS</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>724</id>
      <title>Oracle JVM implementation for Lucene DataStore also a preliminary implementation for an Oracle Domain index using Lucene</title>
      <description>Here a preliminary implementation of the Oracle JVM Directory data store which replace a file system by BLOB data storage. The reason to do this is: Using traditional File System for storing the inverted index is not a good option for some users. Using BLOB for storing the inverted index running Lucene outside the Oracle database has a bad performance because there are a lot of network round trips and data marshalling. Indexing relational data stores such as tables with VARCHAR2, CLOB or XMLType with Lucene running outside the database has the same problem as the previous point. The JVM included inside the Oracle database can scale up to 10.000+ concurrent threads without memory leaks or deadlock and all the operation on tables are in the same memory space!! With these points in mind, I uploaded the complete Lucene framework inside the Oracle JVM and I runned the complete JUnit test case successful, except for some test such as the RMI test which requires special grants to open ports inside the database. The Lucene's test cases run faster inside the Oracle database (11g) than the Sun JDK 1.5, because the classes are automatically JITed after some executions. I had implemented and OJVMDirectory Lucene Store which replaces the file system storage with a BLOB based storage, compared with a RAMDirectory implementation is a bit slower but we gets all the benefits of the BLOB storage (backup, concurrence control, and so on). The OJVMDirectory is cloned from the source at http://issues.apache.org/jira/browse/LUCENE-150 (DBDirectory) but with some changes to run faster inside the Oracle JVM. At this moment, I am working in a full integration with the SQL Engine using the Data Cartridge API, it means using Lucene as a new Oracle Domain Index. With this extension we can create a Lucene Inverted index in a table using: create index it1 on t1(f2) indextype is LuceneIndex parameters('test'); assuming that the table t1 has a column f2 of type VARCHAR2, CLOB or XMLType, after this, the query against the Lucene inverted index can be made using a new Oracle operator: select * from t1 where contains(f2, 'Marcelo') = 1; the important point here is that this query is integrated with the execution plan of the Oracle database, so in this simple example the Oracle optimizer see that the column "f2" is indexed with the Lucene Domain index, then using the Data Cartridge API a Java code running inside the Oracle JVM is executed to open the search, a fetch all the ROWID that match with "Marcelo" and get the rows using the pointer, here the output: SELECT STATEMENT ALL_ROWS 3 1 115 TABLE ACCESS(BY INDEX ROWID) LUCENE.T1 3 1 115 DOMAIN INDEX LUCENE.IT1 Another benefits of using the Data Cartridge API is that if the table T1 has insert, update or delete rows operations a corresponding Java method will be called to automatically update the Lucene Index. There is a simple HTML file with some explanation of the code. The install.sql script is not fully tested and must be lunched into the Oracle database, not remotely. Best regards, Marcelo. For Oracle users the big question is, Why do I use Lucene instead of Oracle Text which is implemented in C? I think that the answer is too simple, Lucene is open source and anybody can extend it and add the functionality needed For Lucene users which try to use Lucene as enterprise search engine, the Oracle JVM provides an highly scalable container which can scale up to 10.000+ concurrent session and with the facility of querying table in the same memory space.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>725</id>
      <title>NovelAnalyzer - wraps your choice of Lucene Analyzer and filters out all "boilerplate" text</title>
      <description>This is a class I have found to be useful for analyzing small (in the hundreds) collections of documents and removing any duplicate content such as standard disclaimers or repeated text in an exchange of emails. This has applications in sampling query results to identify key phrases, improving speed-reading of results with similar content (eg email threads/forum messages) or just removing duplicated noise from a search index. To be more generally useful it needs to scale to millions of documents - in which case an alternative implementation is required. See the notes in the Javadocs for this class for more discussion on this</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>726</id>
      <title>Remove usage of deprecated method Document.fields()</title>
      <description>The classes DocumentWriter, FieldsWriter, and ParallelReader use the deprecated method Document.fields(). This simple patch changes these three classes to use Document.getFields() instead. All unit tests pass.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>729</id>
      <title>non-recursive MultiTermDocs</title>
      <description>A non-recursive implementation of MultiTermDocs.next() and skipTo() would be nice as it's currently possible to get a stack overflow in very rare situations.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>730</id>
      <title>Restore top level disjunction performance</title>
      <description>This patch restores the performance of top level disjunctions. The introduction of BooleanScorer2 had impacted this as reported on java-user on 21 Nov 2006 by Stanislav Jordanov.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>732</id>
      <title>Support DateTools in QueryParser</title>
      <description>The QueryParser currently uses the deprecated class DateField to create RangeQueries with date values. However, most users probably use DateTools to store date values in their indexes, because this is the recommended way since DateField has been deprecated. In that case RangeQueries with date values produced by the QueryParser won't work with those indexes. This patch replaces the use of DateField in QueryParser by DateTools. Because DateTools can produce date values with different resolutions, this patch adds the following methods to QueryParser: /** Sets the default date resolution used by RangeQueries for fields for which no specific date resolutions has been set. Field specific resolutions can be set with {@link #setDateResolution(String, DateTools.Resolution)} . @param dateResolution the default date resolution to set */ public void setDateResolution(DateTools.Resolution dateResolution); /** Sets the date resolution used by RangeQueries for a specific field. @param field field for which the date resolution is to be set @param dateResolution date resolution to set */ public void setDateResolution(String fieldName, DateTools.Resolution dateResolution); (I also added the corresponding getter methods). Now the user can set a default date resolution used for all fields or, with the second method, field specific date resolutions. The initial default resolution, which is used if the user does not set a different resolution, is DateTools.Resolution.DAY. Please let me know if you think we should use a different resolution as default. I extended TestQueryParser to test this new feature. All unit tests pass.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>735</id>
      <title>Simple tool to back-convert from lockless to pre-lockless file format</title>
      <description>Simple tool to back-convert from lockless to pre-lockless file format The changes for lockless commits (LUCENE-701 ) are fairly minor and so creating a tool to convert a lockless format index back to a pre-lockless format index is 1) fairly simple, and 2) useful at least for brave souls who want to try lockless but have the freedom to roll back to Lucene 2.0, using the same index, if anything goes wrong. I will attach an initial patch. This has not yet received extensive testing so please be extremely careful if you use this in production! I've only done minimal testing so far: using IndexFiles to produce an index under lockless, converting it to pre-lockless, and then doing searches against that index with 2.0. More testing is clearly needed to ensure separate deletes, separate norms, etc, are working correctly. The tool prints details of what it did, eg: &gt;&gt; java org.apache.lucene.index.ConvertPreLockless index 3 segments in index segment 0: not compound file format has deletions rename _a_2.del to _a.del no separate norms segment 1: not compound file format has deletions rename _b_1.del to _b.del no separate norms segment 2: not compound file format has deletions rename _c_1.del to _c.del no separate norms wrote "segments" file rename segments_8 to segments_8.old Caveats: Tread very carefully! Test first in a sandox, etc. Make sure you only run this tool on an index that is not in use by any reader/writers, else you could have problems: the tool currently does not acquire the write lock even though it's modifying the index. On Windows only: if your index has any un-referenced files (ie, files that should have been deleted but were in use at the time) at the time you run this tool, then they will never be deleted (ie, pre-lockless Lucene won't know to delete them).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>737</id>
      <title>Provision of encryption/decryption services API to support Field.Store.Encrypted</title>
      <description>Attached are proposed modifications to Lucene 2.0 to support Field.Store.Encrypted. The rational behind this proposal is simple. Since Lucene can store data in the index, it effectively makes the data portable. It is conceivable that some of the data may be sensitive in nature, hence the option to encrypt it. Both the data and its index are encrypted in this implementation. This is only an initial implementation. It has the following several restrictions, all of which can be resolved if required, albeit with some effort and more changes to Lucene: 1) binary and compressed fields cannot be encrypted as well (a plaintext once encrypted becomes binary). 2) Field.Store.Encrypted implies Field.Store.Yes This makes sense but it forces one to store the data in the same index where the tokens are stored. It may be preferable at times to have two indeces, one for tokens, the other for the data. 3) As implemented, it uses RC4 encryption from BouncyCastle. This is an open source package, very simple to use which has the advantage of guaranteeing that the length of the encrypted field is the same as the original plaintext. As of Java 1.5 (5.0) Sun provides an RC4 equivalent in its Java Cryptography Extension, but unfortunately not in Java 1.4. The BouncyCastle RC4 is not the only algorythm available, others not depending on third party code can be used, but it was just the simplest to implement for this first attempt. 4) The attachements are modifications in diff form based on an early (I think August or September '06) repository snapshot of Lucene 2.0 subsequently updated from the Lucene repository on 29/11/06. They may need some additional work to merge with the latest version in the Lucene repository. They also include a couple of JUnit test programs which explain, as well as test, the usage. You will need the BouncyCastle .jar (bcprov-jdk14-134.jar) to run them. I did not attach it to minimize the size of the attachements, but it can be downloaded free from: http://www.bouncycastle.org/latest_releases.html 5) Searching an encrypted field is restricted to single terms, no phrase or boolean searches allowed yet, and the term has to be encrypted by the application before searching it. (ref. attached JUnit test programs) To the extent that I have tested it, the code works as intended and does not appear to introduce any regression problems, but more testing by others would be desirable. I don't propose at this stage to do any further work with this API extensions unless there is some expression of interest and direction from the Lucene Developers team. I have an application ready to roll which uses the proposed Lucene encryption API additions (please see http://www.kbforge.com/index.html). The application is not yet available for downloading simply because I am not sure if the Lucene licence allows me to do so. I would appreciate your advice in this regard. My application is free but its source code is not available (yet). I should add that encryption does not have to be an integral part of Lucene, it can be just part of the end application, but somehow it seems to me that Field.Store.Encrypted belongs in the same category as compression and binary values. I would be happy to receive your feedback. victor negrin</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>738</id>
      <title>read/write .del as d-gaps when the deleted bit vector is sufficiently sparse</title>
      <description>.del file of a segment maintains info on deleted documents in that segment. The file exists only for segments having deleted docs, so it does not exists for newly created segments (e.g. resulted from merge). Each time closing an index reader that deleted any document, the .del file is rewritten. In fact, since the lock-less commits change a new (generation of) .del file is created in each such occasion. For small indexes there is no real problem with current situation. But for very large indexes, each time such an index reader is closed, creating such new bit-vector seems like unnecessary overhead in cases that the bit vector is sparse (just a few docs were deleted). For instance, for an index with a segment of 1M docs, the sequence: {open reader; delete 1 doc from that segment; close reader;} would write a file of ~128KB. Repeat this sequence 8 times: 8 new files of total size of 1MB are written to disk. Whether this is a bottleneck or not depends on the application deletes pattern, but for the case that deleted docs are sparse, writing just the d-gaps would save space and time. I have this (simple) change to BitVector running and currently trying some performance tests to, yet, convince myself on the worthiness of this.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>739</id>
      <title>Performance improvement for SegmentMerger.mergeNorms()</title>
      <description>This patch makes two improvements to SegmentMerger.mergeNorms(): 1) When the SegmentMerger merges the norm values it allocates a new byte array to buffer the values for every field of every segment. The size of such an array equals the size of the corresponding segment, so if large segments are being merged, those arrays become very large, too. We can easily reduce the number of array allocations by reusing a byte array to buffer the norm values that only grows, if a segment is larger than the previous one. 2) Before a norm value is written it is checked if the corresponding document is deleted. If not, the norm is written using IndexOutput.writeByte(byte[]). This patch introduces an optimized case for segments that do not have any deleted docs. In this case the frequent call of IndexReader.isDeleted(int) can be avoided and the more efficient method IndexOutput.writeBytes(byte[], int) can be used. This patch only changes the method SegmentMerger.mergeNorms(). All unit tests pass.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>741</id>
      <title>Field norm modifier (CLI tool)</title>
      <description>I took Chris' LengthNormModifier (contrib/misc) and modified it slightly, to allow us to set fake norms on an existing fields, effectively making it equivalent to Field.Index.NO_NORMS. This is related to LUCENE-448 (NO_NORMS patch) and LUCENE-496 (LengthNormModifier contrib from Chris).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>742</id>
      <title>SpanOrQuery.java: simplification and test</title>
      <description>The current SpanOrQuery.java has some unnessary attributes. After removing these, I found that there was no existing test for it, so I added some tests to TestSpans.java.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>743</id>
      <title>IndexReader.reopen()</title>
      <description>This is Robert Engels' implementation of IndexReader.reopen() functionality, as a set of 3 new classes (this was easier for him to implement, but should probably be folded into the core, if this looks good).</description>
      <attachments/>
      <comments>95</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>745</id>
      <title>Make inspection of BooleanQuery more efficient</title>
      <description>Just attempting to inspect a BooleanQuery allocates two new arrays. This could be cheaper.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>746</id>
      <title>Incorrect error message in AnalyzingQueryParser.getPrefixQuery</title>
      <description>The error message of getPrefixQuery is incorrect when tokens were added, for example by a stemmer. The message is "token was consumed" even if tokens were added. Attached is a patch, which when applied gives a better description of what actually happened.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>750</id>
      <title>don't use finalizers for FSIndexInput clones</title>
      <description>finalizers are expensive, and we should avoid using them where possible. It looks like this helped to tickle some kind of bug (looks like a JVM bug?) http://www.nabble.com/15-minute-hang-in-IndexInput.clone%28%29-involving-finalizers-tf2826906.html#a7891015</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>753</id>
      <title>Use NIO positional read to avoid synchronization in FSIndexInput</title>
      <description>As suggested by Doug, we could use NIO pread to avoid synchronization on the underlying file. This could mitigate any MT performance drop caused by reducing the number of files in the index format.</description>
      <attachments/>
      <comments>82</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>755</id>
      <title>Payloads</title>
      <description>This patch adds the possibility to store arbitrary metadata (payloads) together with each position of a term in its posting lists. A while ago this was discussed on the dev mailing list, where I proposed an initial design. This patch has a much improved design with modifications, that make this new feature easier to use and more efficient. A payload is an array of bytes that can be stored inline in the ProxFile (.prx). Therefore this patch provides low-level APIs to simply store and retrieve byte arrays in the posting lists in an efficient way. API and Usage ------------------------------ The new class index.Payload is basically just a wrapper around a byte[] array together with int variables for offset and length. So a user does not have to create a byte array for every payload, but can rather allocate one array for all payloads of a document and provide offset and length information. This reduces object allocations on the application side. In order to store payloads in the posting lists one has to provide a TokenStream or TokenFilter that produces Tokens with payloads. I added the following two methods to the Token class: /** Sets this Token's payload. */ public void setPayload(Payload payload); /** Returns this Token's payload. */ public Payload getPayload(); In order to retrieve the data from the index the interface TermPositions now offers two new methods: /** Returns the payload length of the current term position. This is invalid until {@link #nextPosition()} is called for * the first time. * * @return length of the current payload in number of bytes */ int getPayloadLength(); /** Returns the payload data of the current term position. * This is invalid until {@link #nextPosition()} is called for the first time. This method must not be called more than once after each call of {@link #nextPosition()} . However, payloads are loaded lazily, so if the payload data for the current position is not needed, this method may not be called at all for performance reasons. @param data the array into which the data of this payload is to be stored, if it is big enough; otherwise, a new byte[] array is allocated for this purpose. @param offset the offset in the array into which the data of this payload is to be stored. @return a byte[] array containing the data of this payload @throws IOException */ byte[] getPayload(byte[] data, int offset) throws IOException; Furthermore, this patch indroduces the new method IndexOutput.writeBytes(byte[] b, int offset, int length). So far there was only a writeBytes()-method without an offset argument. Implementation details ------------------------------ One field bit in FieldInfos is used to indicate if payloads are enabled for a field. The user does not have to enable payloads for a field, this is done automatically: The DocumentWriter enables payloads for a field, if one ore more Tokens carry payloads. The SegmentMerger enables payloads for a field during a merge, if payloads are enabled for that field in one or more segments. Backwards compatible: If payloads are not used, then the formats of the ProxFile and FreqFile don't change Payloads are stored inline in the posting list of a term in the ProxFile. A payload of a term occurrence is stored right after its PositionDelta. Same-length compression: If payloads are enabled for a field, then the PositionDelta is shifted one bit. The lowest bit is used to indicate whether the length of the following payload is stored explicitly. If not, i. e. the bit is false, then the payload has the same length as the payload of the previous term occurrence. In order to support skipping on the ProxFile the length of the payload at every skip point has to be known. Therefore the payload length is also stored in the skip list located in the FreqFile. Here the same-length compression is also used: The lowest bit of DocSkip is used to indicate if the payload length is stored for a SkipDatum or if the length is the same as in the last SkipDatum. Payloads are loaded lazily. When a user calls TermPositions.nextPosition() then only the position and the payload length is loaded from the ProxFile. If the user calls getPayload() then the payload is actually loaded. If getPayload() is not called before nextPosition() is called again, then the payload data is just skipped. Changes of file formats ------------------------------ FieldInfos (.fnm) The format of the .fnm file does not change. The only change is the use of the sixth lowest-order bit (0x20) of the FieldBits. If this bit is set, then payloads are enabled for the corresponding field. ProxFile (.prx) ProxFile (.prx) --&gt; &lt;TermPositions&gt;^TermCount TermPositions --&gt; &lt;Positions&gt;^DocFreq Positions --&gt; &lt;PositionDelta, Payload?&gt;^Freq Payload --&gt; &lt;PayloadLength?, PayloadData&gt; PositionDelta --&gt; VInt PayloadLength --&gt; VInt PayloadData --&gt; byte^PayloadLength For payloads disabled (unchanged): PositionDelta is the difference between the position of the current occurrence in the document and the previous occurrence (or zero, if this is the first occurrence in this document). For Payloads enabled: PositionDelta/2 is the difference between the position of the current occurrence in the document and the previous occurrence. If PositionDelta is odd, then PayloadLength is stored. If PositionDelta is even, then the length of the current payload equals the length of the previous payload and thus PayloadLength is omitted. FreqFile (.frq) SkipDatum --&gt; DocSkip, PayloadLength?, FreqSkip, ProxSkip PayloadLength --&gt; VInt For payloads disabled (unchanged): DocSkip records the document number before every SkipInterval th document in TermFreqs. Document numbers are represented as differences from the previous value in the sequence. For payloads enabled: DocSkip/2 records the document number before every SkipInterval th document in TermFreqs. If DocSkip is odd, then PayloadLength follows. If DocSkip is even, then the length of the payload at the current skip point equals the length of the payload at the last skip point and thus PayloadLength is omitted. This encoding is space efficient for different use cases: If only some fields of an index have payloads, then there's no space overhead for the fields with payloads disabled. If the payloads of consecutive term positions have the same length, then the length only has to be stored once for every term. This should be a common case, because users probably use the same format for all payloads. If only a few terms of a field have payloads, then we don't waste much space because we benefit again from the same-length-compression since we only have to store the length zero for the empty payloads once per term. All unit tests pass.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>756</id>
      <title>Maintain norms in a single file .nrm</title>
      <description>Non-compound indexes are ~10% faster at indexing, and perform 50% IO activity comparing to compound indexes. But their file descriptors foot print is much higher. By maintaining all field norms in a single .nrm file, we can bound the number of files used by non compound indexes, and possibly allow more applications to use this format. More details on the motivation for this in: http://www.nabble.com/potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-tf2826909.html (in particular http://www.nabble.com/Re%3A-potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-p7910403.html).</description>
      <attachments/>
      <comments>26</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>759</id>
      <title>Add n-gram tokenizers to contrib/analyzers</title>
      <description>It would be nice to have some n-gram-capable tokenizers in contrib/analyzers. Patch coming shortly.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>761</id>
      <title>Clone proxStream lazily in SegmentTermPositions</title>
      <description>In SegmentTermPositions the proxStream should be cloned lazily, i. e. at the first time nextPosition() is called. Then the initialization costs of TermPositions are not higher anymore compared to TermDocs and thus there is no reason anymore for Scorers to use TermDocs instead of TermPositions. In fact, all Scorers should use TermPositions, because custom subclasses of existing scorers might want to access payloads, which is only possible via TermPositions. We could further merge SegmentTermDocs and SegmentTermPositions into one class and deprecate the interface TermDocs. I'm going to attach a patch once the payloads feature (LUCENE-755) is committed.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>762</id>
      <title>[PATCH] Efficiently retrieve sizes of field values</title>
      <description>Sometimes an application would like to know how large a document is before retrieving it. This can be important for memory management or choosing between algorithms, especially in cases where documents might be very large. This patch extends the existing FieldSelector mechanism with two new FieldSelectorResults: SIZE and SIZE_AND_BREAK. SIZE creates fields on the retrieved document that store field sizes instead of actual values. SIZE_AND_BREAK is especially efficient if one field comprises the bulk of the document size (e.g., the body field) and can thus be used as a reasonable size approximation.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>764</id>
      <title>Document the temporary free space requirements of IndexWriter methods</title>
      <description>Just opening an issue to track fixes to javadocs around Directory space usage of optimize(), addIndexes, addDocument. This came out of a recent thread on the users list around unexpectedly high temporary disk usage during optimize(): http://www.gossamer-threads.com/lists/lucene/java-user/43475</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>767</id>
      <title>maxDoc should be explicitly stored in the index, not derived from file length</title>
      <description>This is a spinoff of LUCENE-140 In general we should rely on "as little as possible" from the file system. Right now, maxDoc is derived by checking the file length of the FieldsReader index file (.fdx) which makes me nervous. I think we should explicitly store it instead. Note that there are no known cases where this is actually causing a problem. There was some speculation in the discussion of LUCENE-140 that it could be one of the possible, but in digging / discussion there were no specifically relevant JVM bugs found (yet!). So this would be a defensive fix at this point.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>769</id>
      <title>[PATCH] Performance improvement for some cases of sorted search</title>
      <description>It's a small addition to Lucene that significantly lowers memory consumption and improves performance for sorted searches with frequent index updates and relatively big indexes (&gt;1mln docs) scenario. This solution supports only single-field sorting currently (which seem to be quite popular use case). Multiple fields support can be added without much trouble. The solution is this: documents from the sorting set (instead of given field's values from the whole index - current FieldCache approach) are cached in a WeakHashMap so the cached items are candidates for GC. Their fields values are then fetched from the cache and compared while sorting.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>770</id>
      <title>CfsExtractor tool</title>
      <description>A tool for extracting the content of a CFS file, in order to go from a compound index to a multi-file index. This may be handy for people who want to go back to multi-file index format now that field norms are in a single file - LUCENE-756. Most of this code already existed and was hiding in IndexReader.main. I'll commit tomorrow, unless I hear otherwise. I think I should also remove IndexReader.main then. Ja?</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>771</id>
      <title>Change default write lock file location to index directory (not java.io.tmpdir)</title>
      <description>Now that readers are read-only, we no longer need to store lock files in a different global lock directory than the index directory. This has been a source of confusion and caused problems to users in the past. Furthermore, once the write lock is stored in the index directory, it no longer needs the big digest prefix that was previously required to make sure lock files in the global lock directory, from different indexes, did not conflict. This way, all files related to an index will appear in a single directory. And you can easily list that directory to see if a "write.lock" is present to check whether a writer is open on the index. Note that this change just affects how FSDirectory creates its default lockFactory if no lockFactory was specified. It is still possible (just no longer the default) to pick a different directory to store your lock files by pre-instantiating your own LockFactory. As part of this I would like to remove LOCK_DIR and the no-argument constructor, in SimpleFSLockFactory and NativeFSLockFactory. I don't think we should have the notion of a global default lock directory anymore. This is actually an API change. However, neither SimpleFSLockFactory nor NativeFSLockFactory haver been released yet, so I think this API removal is allowed? Finally I want to deprecate (but not yet remove, because this has been in the API for many releases) the static LOCK_DIR that's in FSDirectory. But it's now entirely unused. See here for discussion leading to this: http://www.gossamer-threads.com/lists/lucene/java-dev/43940</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>773</id>
      <title>Deprecate "create" method in FSDirectory.getDirectory in favor of IndexWriter's "create"</title>
      <description>It's confusing that there is a create=true|false at the FSDirectory level and then also another create=true|false at the IndexWriter level. Which one should you use when creating an index? Our users have been confused by this in the past: http://www.gossamer-threads.com/lists/lucene/java-user/4792 I think in general we should try to have one obvious way to achieve something (like Python: http://en.wikipedia.org/wiki/Python_philosophy). And the fact that there are now two code paths that are supposed to do the same (similar?) thing, can more easily lead to sneaky bugs. One case of LUCENE-140 (already fixed in trunk but not past releases), which inspired this issue, can happen if you send create=false to the FSDirectory and create=true to the IndexWriter. Finally, as of lockless commits, it is now possible to open an existing index for "create" while readers are still using the old "point in time" index, on Windows. (At least one user had tried this previously and failed). To do this, we use the IndexFileDeleter class (which retries on failure) and we also look at the segments file to determine the next segments_N file to write to. With future issues like LUCENE-710 even more "smarts" may be required to know what it takes to "create" a new index into an existing directory. Given that we have have quite a few Directory implemenations, I think these "smarts" logically should live in IndexWriter (not replicated in each Directory implementation), and we should leave the Directory as an interface that knows how to make changes to some backing store but does not itself try to make any changes.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>775</id>
      <title>Searcher code creating Hits is somewhat messy</title>
      <description>This patch makes sure all Hits-resulting queries sent to Searcher pass though the same methods, rather than an ad hoc Hits call per method call. Did it so it would be easier for me to implement this decorated searcher cache of mine. I could not find any implementations overriding the methods I set final, so I think it is allright.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>776</id>
      <title>Use WeakHashMap instead of Hashtable in FSDirectory</title>
      <description>I was just reading the FSDirectory java code, then I found this : /** This cache of directories ensures that there is a unique Directory instance per path, so that synchronization on the Directory can be used to synchronize access between readers and writers. * This should be a WeakHashMap, so that entries can be GC'd, but that would require Java 1.2. Instead we use refcounts... */ private static final Hashtable DIRECTORIES = new Hashtable(); Since Lucene is now requiring at least 1.2 (for ThreadLocal for instance, which is using BTW some WeakHashMap), maybe it is time to change ?</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>777</id>
      <title>SpanWithinQuery - A SpanNotQuery that allows a specified number of intersections</title>
      <description>A SpanNotQuery that allows a specified number of intersections.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>778</id>
      <title>Allow overriding a Document</title>
      <description>In our application, we have some kind of generic API that is handling how we are using Lucene. The different other applications are using this API with different semantics, and are using the Lucene fields quite differently. We wrote some usefull functions to do this mapping. Today, as the Document class cannot be overriden, we are obliged to make a document wrapper by application, ie some MyAppDocument and MyOtherAppDocument which have a property holding a real Lucene Document. Then, when MyApp or MyOtherApp want to use our generic lucene API, we have to "get out" the Lucene document, ie do some genericLuceneAPI.writeDoc(myAppDoc.getLuceneDocument()). This work fine, but it becomes quite tricky to use the other function of our generic API which is genericLuceneAPI.writeDocs(Collection&lt;Document&gt; docs). I don't know the rational behind making final Document, but removing it will allow more object-oriented code.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>779</id>
      <title>behaviour of Lucene in tokenizing - causes query problems</title>
      <description>I recently submitted a problem with queries to Atlassian Support team and they have pointed out that it is an issue with Lucene. I'm copying the contents from my support request to Atlassian. Please help. Question to Atlassian: When I use "text search" to query an item from summary it gives me zero match. But when I use query through custom field, I get results displayed properly. In the attachment, I did a query for no: 10012363 using text search and I know this entry is part of the summary for several issues. But the result says "no matching issues found". But when I selected, Vendor name and number, I get proper results. I tried re-indexing, still no help Response from Atlassian: The reason why you couldn't receive any results from the query is due to the behaviour of Lucene (an Apache-developed search and index library being used in JIRA) in tokenizing. For example, Invoice Number-123456-API ASSOCIATES-10012363 is tokenized to: Invoice Number-123456-API ASSOCIATES-10012363 In this case, Number-123456-API and ASSOCIATES-10012363 are recognized as NUM (e.g. floating point, serial, model numbers, ip addresses, etc.). (Hope this doesn't sound too technical to you.) You can somehow call this a limitation of the third party library. If you are keen to have it improved, feel free to raise a support request at our issue tracker or perhaps try asking it at Lucene's issue tracker.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>780</id>
      <title>Generalize directory copy operation</title>
      <description>The copy operation in RAMDirectory(Directory) constructor can be used more generally to copy one directory to another. Why bound it only to RAMDirectory?. For example, I build index in RAMDirectory but I need it to persist in FSDirectory. I created a patch to solve it.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>782</id>
      <title>Add a jruby interface and a luke-like utiltiy app</title>
      <description>Still pretty early, but available for anyone with sufficient interest and bravado.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>783</id>
      <title>Store all metadata in human-readable segments file</title>
      <description>Various index-reading components in Lucene need metadata in addition to data. This metadata is presently stored in arbitrary binary headers and spread out over several files. We should move to concentrate it in a single file, and this file should be encoded using a human-readable, extensible, standardized data serialization language – either XML or YAML. Making metadata human-readable makes debugging easier. Centralizing it makes debugging easier still. Developers benefit from being able to scan and locate relevant information quickly and with less debug printing. Users get a new window through which to peer into the index structure. Since metadata is written to a separate file, there would no longer be a need to seek back to the beginning of any data file to finish a header, solving issue LUCENE-532. Special-case parsing code needed for extracting metadata supplied by different index formats can be pared down. If a value is no longer necessary, it can just be ignored/discarded. Removing headers from the data files simplifies them and makes the file format easier to implement. With headers removed, all or nearly all data structures can take the form of records stacked end to end, so that once a decoder has been selected, an iterator can read the file from top to tail. To an extent, this allows us to separate our data-processing algorithms from our serialization algorithms, decoupling Lucene's code base from its file format. For instance, instead of further subclassing TermDocs to deal with "flexible indexing" formats, we might replace it with a PostingList which returns a subclass of Posting. The deserialization code would be wholly contained within the Posting subclass rather than spread out over several subclasses of TermDocs. YAML and XML are equally well suited for the task of storing metadata, but in either case a complete parser would not be needed – a small subset of the language will do. KinoSearch 0.20's custom-coded YAML parser occupies about 600 lines of C – not too bad, considering how miserable C's string handling capabilities are.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>786</id>
      <title>Extended javadocs in spellchecker</title>
      <description>Added some javadocs that explains why the spellchecker does not work as one might expect it to. http://www.nabble.com/SpellChecker%3A%3AsuggestSimilar%28%29-Question-tf3118660.html#a8640395 &gt; Without having looked at the code for a long time, I think the problem is what the &gt; lucene scoring consider to be best. First the grams are searched, resulting in a number &gt; of hits. Then the edit-distance is calculated on each hit. "Genetics" is appearently the &gt; third most similar hit according to Lucene, but the best according to Levenshtein. &gt; &gt; I.e. Lucene does not use edit-distance as similarity. You need to get a bunch of best hits &gt; in order to find the one with the smallest edit-distance. I took a look at the code, and my assessment seems to be right.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>790</id>
      <title>contrib/benchmark - few improvements and a bug fix</title>
      <description>Benchmark byTask was slightly improved: 1. fixed a bug in the "child-should-not-report" mechanism. If a task sequence contained only simple tasks it worked as expected (i.e. child tasks did not report times/memory) but if a child was a task sequence, then its children would report - they should not - this was fixed, so this property is now "penetrating/inherited" all the way down. 2. doc size control now possible also for the Reuters doc maker. (allowing to index N docs of size C characters each.) 3. TrecDocMaker was added - it reads as input the .gz files used in Trec - e.g. .gov data - this can be handy to benchmark Lucene on these large collections. Similar to the Reuters collection, the doc-maker scans the input directory for all the files and extracts documents from the files. Here there are multiple documents in each input file. Unlike the Reuters collection, we cannot provide a 'loader' for these collections - they are available from http://trec.nist.gov - for research purposes. 4. a new BasicDocMaker abstract class handles most of doc-maker tasks, including creating docs with specific size, so adding new doc-makers for other data is now much simpler.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>791</id>
      <title>Update the Wiki</title>
      <description>The wiki needs updating. For starters, the URL is still Jakarta. I think infrastructure needs to be contacted to do this move. If someone is so inclined, it might be useful to go through and cleanup/organize what is there.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>794</id>
      <title>Extend contrib Highlighter to properly support PhraseQuery, SpanQuery, ConstantScoreRangeQuery</title>
      <description>This patch adds a new Scorer class (SpanQueryScorer) to the Highlighter package that scores just like QueryScorer, but scores a 0 for Terms that did not cause the Query hit. This gives 'actual' hit highlighting for the range of SpanQuerys, PhraseQuery, and ConstantScoreRangeQuery. New Query types are easy to add. There is also a new Fragmenter that attempts to fragment without breaking up Spans. See http://issues.apache.org/jira/browse/LUCENE-403 for some background. There is a dependency on MemoryIndex.</description>
      <attachments/>
      <comments>97</comments>
      <commenters>18</commenters>
    </issue>
    <issue>
      <id>796</id>
      <title>Change Visibility of fields[] in MultiFieldQueryParser</title>
      <description>In MultiFieldQueryParser the two methods protected Query getFuzzyQuery(String field, String termStr, float minSimilarity) throws ParseException protected Query getWildcardQuery(String field, String termStr) throws ParseException are intended to be overwritten if one would like to avoid fuzzy and wildcard queries. However, the String[] fields attribute of this class is private and hence it is not accessible in subclasses of MFQParser. If you just change it to protected this issue should be solved.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>798</id>
      <title>Factory for RangeFilters that caches sections of ranges to reduce disk reads</title>
      <description>RangeFilters can be cached using CachingWrapperFilter but are only re-used if a user happens to use exactly the same upper/lower bounds. This class demonstrates a caching approach where sections of ranges are cached as bitsets and these are re-used/combined to construct large range filters if they fall within the required range. This can improve the "cache hit" ratio and avoid going to disk to read large lists of Doc ids from TermDocs. This class needs some more work to add thread safety but I'm making it available to gather feedback on the design at this early stage before making robust.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>801</id>
      <title>build.xml in cnotrib/benchmark should auto build core java and demo if required</title>
      <description>Currently one needs to build core jar and demo jar before building/running benchmark. This is not very convenient. Change it to use core classes and demo classes (instead of jars). build core and demo by dependency if required.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>802</id>
      <title>lucene jars should include LiCENSE and NOTICE</title>
      <description>The Lucene jars created by the build should include the LICENSE and NOTICE files in META-INF.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>803</id>
      <title>add svn ignores for eclipse artifacts</title>
      <description>Be nice to ignore the files eclipse puts into the project root as we do the .idea file for intellij. The two files are .project .classpath I'm gonna lie and say there's a patch available for this because an svn diff patch with propery changes can't be applied with patch anyway.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>805</id>
      <title>New Lucene Demo</title>
      <description>The much maligned demo, while useful, could use a breath of fresh air. This issue is to start collecting requirements about what people would like to see in a demo and what they don't like in the current one. Ideas (not necessarily in order of importance): 1. More in-depth tutorial explaining indexing/searching 2. Multilingual support/demonstration 3. Better demonstration of querying capabilities: Spans, Phrases, Wildcards, Filters, sorting, etc. 4. Dealing with different content types and pointers to resources 5. Wiki use cases links – I think it would be cool to solicit people to contribute use cases to the docs. 6. Demonstration of contrib packages, esp. Highlighter 7. Performance issues/factors/tradeoffs. Lucene lessons learned and best practices Advanced tutorials: 1. Hadoop + Lucene 2. Writing custom analyzers/filters/tokenizers 3. Changing Scoring 4. Payloads (when they are committed) Please contribute what else you would like to see. I may be able to address some of these issues for my ApacheCon talk, but not all of them.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>806</id>
      <title>Synchronization bottleneck in FieldSortedHitQueue with many concurrent readers</title>
      <description>The below is from a post by (my colleague) Paul Smith to the java-users list: — Hi ho peoples. We have an application that is internationalized, and stores data from many languages (each project has it's own index, mostly aligned with a single language, maybe 2). Anyway, I've noticed during some thread dumps diagnosing some performance issues, that there appears to be a potential synchronization bottleneck using Locale-based sorting of Strings. I don't think this problem is the root cause of our performance problem, but I thought I'd mention it here. Here's the stack dump of a thread waiting: "http-1001-Processor245" daemon prio=1 tid=0x31434da0 nid=0x3744 waiting for monitor entry [0x2cd44000..0x2cd45f30] at java.text.RuleBasedCollator.compare(RuleBasedCollator.java) waiting to lock &lt;0x6b1e8c68&gt; (a java.text.RuleBasedCollator) at org.apache.lucene.search.FieldSortedHitQueue$4.compare(FieldSortedHitQueue.java:320) at org.apache.lucene.search.FieldSortedHitQueue.lessThan(FieldSortedHitQueue.java:114) at org.apache.lucene.util.PriorityQueue.upHeap(PriorityQueue.java:120) at org.apache.lucene.util.PriorityQueue.put(PriorityQueue.java:47) at org.apache.lucene.util.PriorityQueue.insert(PriorityQueue.java:58) at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:90) at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:97) at org.apache.lucene.search.TopFieldDocCollector.collect(TopFieldDocCollector.java:47) at org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:291) at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:132) at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:110) at com.aconex.index.search.FastLocaleSortIndexSearcher.search(FastLocaleSortIndexSearcher.java:90) ..... In our case we had 12 threads waiting like this, while one thread had the lock on the RuleBasedCollator. Turns out RuleBasedCollator's.compare(...) method is synchronized. I wonder if a ThreadLocal based collator would be better here... ? There doesn't appear to be a reason for other threads searching the same index to wait on this sort. Be just as easy to use their own. (Is RuleBasedCollator a "heavy" object memory wise? Wouldn't have thought so, per thread) Thoughts? — I've investigated this somewhat, and agree that this is a potential problem with a series of possible workarounds. Further discussion (including proof-of-concept patch) to follow.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>807</id>
      <title>Minor improvement to JavaDoc for ScoreDocComparator</title>
      <description>About to attach a very small patch for ScoreDocComparator which broadens the contract of compare(ScoreDoc, ScoreDoc) to follow the same semantics as java.util.Comparator.compare() – allow any integer to be returned, rather than specifically -1/0/-1. Note that this behaviour must already be acceptable; the anonymous ScoreDocComparators returned by FieldSortedHitQueue.comparatorStringLocale() already return the result of Collator.compare(), which is not tied to this -1/0/1 restriction.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>810</id>
      <title>provide a isClosed() method in Searcher class</title>
      <description>In our project we need to build a searcher pooling system, so a isClosed() method is required for testing if searcher is closed then try to re-open that. we dont want to close IndexReader since it is fixed for a searcher.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>816</id>
      <title>Manage dependencies in the build with ivy</title>
      <description>There were issues about making the 2.1 release : http://www.nabble.com/-VOTE--release-Lucene-2.1-tf3228536.html#a8994721 Then the discussion started to talk about maven, and also about ivy. I propose here a draft, a proof of concept of an ant + ivy build. I made this build parallel to the actual one, so people can evaluate it. Note that I have only ivy-ified the core, the demo and the contrib/benchmark. The other contrib projects can be ivy-ified quite easily. The build system is in the common-build directory. In this directory we have : common-build.xml : the main common build which handle dependencies with ivy common-build-project.xml : build a java project, core, demo, or a contrib one common-build-webapp.xml : extend common-build-project and have some tasks about building a war common-build-modules.xml : allow to build sevral projects, just using some subant task common-build-gcj.xml : build with gcj. It work once, need to be fixed ivyconf.xml, ivyconf.properties : ivy configuration build.xml : a little task to generate the ivyconf.xml to use with the eclipse ivy plugin eclipse directory : contains some XSL/XML to generate .classpath and .project To test it and see how ivy is cool : cd contrib/benchmark ant -f build-ivy.xml buildeep and look at the new local-libs directory at the root of the lucene directory !</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>817</id>
      <title>Manage dependencies in the build with ivy</title>
      <description>There were issues about making the 2.1 release : http://www.nabble.com/-VOTE--release-Lucene-2.1-tf3228536.html#a8994721 Then the discussion started to talk about maven, and also about ivy. I propose here a draft, a proof of concept of an ant + ivy build. I made this build parallel to the actual one, so people can evaluate it. Note that I have only ivy-ified the core, the demo and the contrib/benchmark. The other contrib projects can be ivy-ified quite easily. The build system is in the common-build directory. In this directory we have : common-build.xml : the main common build which handle dependencies with ivy common-build-project.xml : build a java project, core, demo, or a contrib one common-build-webapp.xml : extend common-build-project and have some tasks about building a war common-build-modules.xml : allow to build sevral projects, just using some subant task common-build-gcj.xml : build with gcj. It work once, need to be fixed ivyconf.xml, ivyconf.properties : ivy configuration build.xml : a little task to generate the ivyconf.xml to use with the eclipse ivy plugin eclipse directory : contains some XSL/XML to generate .classpath and .project To test it and see how ivy is cool : cd contrib/benchmark ant -f build-ivy.xml buildeep and look at the new local-libs directory at the root of the lucene directory !</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>822</id>
      <title>Make FieldSelector usable from Searchable</title>
      <description>Seems reasonable that you would want to be able to specify a FieldSelector from Searchable because many systems do not use IndexSearcher (where you can get a Reader), but instead use Searchable or Searcher so that Searchers and MultiSearchers can be used in a polymorphic manner.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>824</id>
      <title>IndexWriter#addIndexesNoOptimize has redundent try/catch</title>
      <description>With the new transaction code, the try/catch clause at the beginning of IndexWriter#addIndexesNoOptimize is redundant.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>826</id>
      <title>Language detector</title>
      <description>A formula 1A token/ngram-based language detector. Requires a paragraph of text to avoid false positive classifications. Depends on contrib/analyzers/ngrams for tokenization, Weka for classification (logistic support vector models) feature selection and normalization of token freuencies. Optionally Wikipedia and NekoHTML for training data harvesting. Initialized like this: LanguageRoot root = new LanguageRoot(new File("documentClassifier/language root")); root.addBranch("uralic"); root.addBranch("fino-ugric", "uralic"); root.addBranch("ugric", "uralic"); root.addLanguage("fino-ugric", "fin", "finnish", "fi", "Suomi"); root.addBranch("proto-indo european"); root.addBranch("germanic", "proto-indo european"); root.addBranch("northern germanic", "germanic"); root.addLanguage("northern germanic", "dan", "danish", "da", "Danmark"); root.addLanguage("northern germanic", "nor", "norwegian", "no", "Norge"); root.addLanguage("northern germanic", "swe", "swedish", "sv", "Sverige"); root.addBranch("west germanic", "germanic"); root.addLanguage("west germanic", "eng", "english", "en", "UK"); root.mkdirs(); LanguageClassifier classifier = new LanguageClassifier(root); if (!new File(root.getDataPath(), "trainingData.arff").exists()) { classifier.compileTrainingData(); // from wikipedia } classifier.buildClassifier(); Training set build from Wikipedia is the pages describing the home country of each registred language in the language to train. Above example pass this test: (testEquals is the same as assertEquals, just not required. Only one of them fail, see comment.) assertEquals("swe", classifier.classify(sweden_in_swedish).getISO()); testEquals("swe", classifier.classify(norway_in_swedish).getISO()); testEquals("swe", classifier.classify(denmark_in_swedish).getISO()); testEquals("swe", classifier.classify(finland_in_swedish).getISO()); testEquals("swe", classifier.classify(uk_in_swedish).getISO()); testEquals("nor", classifier.classify(sweden_in_norwegian).getISO()); assertEquals("nor", classifier.classify(norway_in_norwegian).getISO()); testEquals("nor", classifier.classify(denmark_in_norwegian).getISO()); testEquals("nor", classifier.classify(finland_in_norwegian).getISO()); testEquals("nor", classifier.classify(uk_in_norwegian).getISO()); testEquals("fin", classifier.classify(sweden_in_finnish).getISO()); testEquals("fin", classifier.classify(norway_in_finnish).getISO()); testEquals("fin", classifier.classify(denmark_in_finnish).getISO()); assertEquals("fin", classifier.classify(finland_in_finnish).getISO()); testEquals("fin", classifier.classify(uk_in_finnish).getISO()); testEquals("dan", classifier.classify(sweden_in_danish).getISO()); // it is ok that this fails. dan and nor are very similar, and the document about norway in danish is very small. testEquals("dan", classifier.classify(norway_in_danish).getISO()); assertEquals("dan", classifier.classify(denmark_in_danish).getISO()); testEquals("dan", classifier.classify(finland_in_danish).getISO()); testEquals("dan", classifier.classify(uk_in_danish).getISO()); testEquals("eng", classifier.classify(sweden_in_english).getISO()); testEquals("eng", classifier.classify(norway_in_english).getISO()); testEquals("eng", classifier.classify(denmark_in_english).getISO()); testEquals("eng", classifier.classify(finland_in_english).getISO()); assertEquals("eng", classifier.classify(uk_in_english).getISO()); I don't know how well it works on lots of lanugages, but this fits my needs for now. I'll try do more work on considering the language trees when classifying. It takes a bit of time and RAM to build the training data, so the patch contains a pre-compiled arff-file.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>831</id>
      <title>Complete overhaul of FieldCache API/Implementation</title>
      <description>Motivation: 1) Complete overhaul the API/implementation of "FieldCache" type things... a) eliminate global static map keyed on IndexReader (thus eliminating synch block between completley independent IndexReaders) b) allow more customization of cache management (ie: use expiration/replacement strategies, disk backed caches, etc) c) allow people to define custom cache data logic (ie: custom parsers, complex datatypes, etc... anything tied to a reader) d) allow people to inspect what's in a cache (list of CacheKeys) for an IndexReader so a new IndexReader can be likewise warmed. e) Lend support for smarter cache management if/when IndexReader.reopen is added (merging of cached data from subReaders). 2) Provide backwards compatibility to support existing FieldCache API with the new implementation, so there is no redundent caching as client code migrades to new API.</description>
      <attachments/>
      <comments>162</comments>
      <commenters>16</commenters>
    </issue>
    <issue>
      <id>833</id>
      <title>Indexing of Subversion Repositories.</title>
      <description>It would be a big help if Lucene had the ability to index Subversion (or CVS, or whatever) repositories, including revision history. Searches (beyond basic text of the source code) might include: path:/branches/mybranch Foo history:Foo</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>834</id>
      <title>Payload Queries</title>
      <description>Now that payloads have been implemented, it will be good to make them searchable via one or more Query mechanisms. See http://wiki.apache.org/lucene-java/Payload_Planning for some background information and https://issues.apache.org/jira/browse/LUCENE-755 for the issue that started it all.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>835</id>
      <title>An IndexReader with run-time support for synonyms</title>
      <description>These classes provide support for enabling the use of synonyms for terms in an existing index. While Analyzers can be used at Query-parse time or Index-time to inject synonyms these are not always satisfactory means of providing support for synonyms: Index-time injection of synonyms is less flexible because changing the lists of synonyms requires an index rebuild. Query-parse-time injection is awkward because special support is required in the parser/query logic to recognise and cater for the tokens that appear in the same position. Additionally, any statistical analysis of the index content via TermEnum/TermDocs etc does not consider the synonyms unless specific code is added. What is perhaps more useful is a transparent wrapper for the IndexReader that provides a synonym-ized view of the index without requiring specialised support in the calling code. All of the TermEnum/TermDocs interfaces remain the same but behind the scenes synonyms are being considered/applied silently. The classes supplied here provide this "virtual" view of the index and all queries or other code that examines this index using the special reader benefit from this view without requiring specialized code. A Junit test illustrates this code in action.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>836</id>
      <title>Benchmarks Enhancements (precision/recall, TREC, Wikipedia)</title>
      <description>Would be great if the benchmark contrib had a way of providing precision/recall benchmark information ala TREC. I don't know what the copyright issues are for the TREC queries/data (I think the queries are available, but not sure about the data), so not sure if the is even feasible, but I could imagine we could at least incorporate support for it for those who have access to the data. It has been a long time since I have participated in TREC, so perhaps someone more familiar w/ the latest can fill in the blanks here. Another option is to ask for volunteers to create queries and make judgments for the Reuters data, but that is a bit more complex and probably not necessary. Even so, an Apache licensed set of benchmarks may be useful for the community as a whole. Hmmm.... Wikipedia might be another option instead of Reuters to setup as a download for benchmarking, as it is quite large and I believe the licensing terms are quite amenable. Having a larger collection would be good for stressing Lucene more and would give many users a demonstration of how Lucene handles large collections. At any rate, this kind of information could be useful for people looking at different indexing schemes, formats, payloads and different query strategies.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>841</id>
      <title>Replace UTF8 characters in stemmer code with integer values.</title>
      <description>BrazillianStemmer, GermanStemmer, FrenchStemmer and DutchStemmer all contains UTF characters in the java code. All environments does not handle that. It really ought to be integer values instead. I'll come up with a patch sooner or later.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>843</id>
      <title>improve how IndexWriter uses RAM to buffer added documents</title>
      <description>I'm working on a new class (MultiDocumentWriter) that writes more than one document directly into a single Lucene segment, more efficiently than the current approach. This only affects the creation of an initial segment from added documents. I haven't changed anything after that, eg how segments are merged. The basic ideas are: Write stored fields and term vectors directly to disk (don't use up RAM for these). Gather posting lists &amp; term infos in RAM, but periodically do in-RAM merges. Once RAM is full, flush buffers to disk (and merge them later when it's time to make a real segment). Recycle objects/buffers to reduce time/stress in GC. Other various optimizations. Some of these changes are similar to how KinoSearch builds a segment. But, I haven't made any changes to Lucene's file format nor added requirements for a global fields schema. So far the only externally visible change is a new method "setRAMBufferSize" in IndexWriter (and setMaxBufferedDocs is deprecated) so that it flushes according to RAM usage and not a fixed number documents added.</description>
      <attachments/>
      <comments>43</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>847</id>
      <title>Factor merge policy out of IndexWriter</title>
      <description>If we factor the merge policy out of IndexWriter, we can make it pluggable, making it possible for apps to choose a custom merge policy and for easier experimenting with merge policy variants.</description>
      <attachments/>
      <comments>62</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>848</id>
      <title>Add supported for Wikipedia English as a corpus in the benchmarker stuff</title>
      <description>Add support for using Wikipedia for benchmarking.</description>
      <attachments/>
      <comments>57</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>849</id>
      <title>contrib/benchmark: configurable HTML Parser, external classes to path, exhaustive doc maker</title>
      <description>"doc making" enhancements: 1. Allow configurable html parser, with a new html.parser property. Currently TrecDocMaker is using the Demo html parser. With this new property this can be overridden. 2. allow to add external class path, so the benchmark can be used with modified makers/parsers without having to add code to Lucene. Run benchmark with e.g. "ant run-task -Dbenchmark.ext.classpath=/myproj/myclasses" 3. allow to crawl a doc maker until exhausting all its files/docs once, without having to know in advance how many docs it can make. This can be useful for instance if the input data is in zip files.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>850</id>
      <title>Easily create queries that transform subquery scores arbitrarily</title>
      <description>Refactor DisMaxQuery into SubQuery(Query|Scorer) that admits easy subclassing. An example is given for multiplicatively combining scores. Note: patch is not clean; for demonstration purposes only.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>851</id>
      <title>Pruning</title>
      <description>Greets, A thread on java-dev a couple of months ago drew my attention to a technique used by Nutch for cutting down the number of hits that have to be processed: if you have an algorithm for ordering documents by importance, and you sort them so that the lowest document numbers have the highest rank, then most of your high-scoring hits are going to occur early on in the hit-collection process. Say you're looking for the top 100 matches – the odds are pretty good that after you've found 1000 hits, you've gotten most of the good stuff. It may not be necessary to score the other e.g. 5,000,000 hits. To pull this off in Nutch, they run the index through a post process whereby documents are re-ordered by page score using the IndexSorter class. Unfortunately, post-processing does not live happily with incremental indexing. However, if we ensure that document numbers are ordered according to our criteria within each segment, that's almost as good. Say we're looking for 100 hits, as before; what we do is collect a maximum of 1000 hits per segment. If we are dealing with an index made up of 25 segments, that's 25,000 hits max we'll have to process fully – the rest we can skip over. That's not as quick as only processing 1000 hits then stopping in a fully optimized index, but it's a lot better than churning through all 5,000,000 hits. A lot of those hits from the smallest segments will be garbage; we'll get most of our good hits from a few large segments most of the time. But that's fine – the cost to process any one segment is small. Writing a low-level scoring loop which implements pruning per segment is straightforward. KinoSearch's version (in C) is below. To control the amount of pruning, we need a high-level Searcher.setPruneFactor API, which sets a multiplier; the number of hits-per-segment which must be processed is determined by multiplying the number of hits you need by pruneFactor. Here's code from KS for deriving hits-per-seg: process prune_factor if supplied my $seg_starts; my $hits_per_seg = 2**31 - 1; if ( defined $self-&gt; {prune_factor} and defined $args{num_wanted} ) { my $prune_count = $self-&gt;{prune_factor} * $args {num_wanted} ; if ( $prune_count &lt; $hits_per_seg ) { # don't exceed I32_MAX $hits_per_seg = $prune_count; $seg_starts = $reader-&gt;get_seg_starts; } } What I have not yet written is the index-time mechanism for sorting documents. In Nutch, they use the norms from a known indexed, non-tokenized field ("site"). However, in Lucene and KS, we can't count on any existing fields. Document boost isn't stored directly, either. The obvious answer is to start storing it, which would suffice for Nutch-like uses. However, it may make sense to to avoid coupling document ordering to boost in order to influence pruning without affecting scores. The sort ordering information needs a permanent home in the index, since it will be needed whenever segment merging occurs. The fixed-width per-document storage in Lucene's .fdx file seems like a good place. If we use one float per document, we can simply put it before or after the 64-bit file pointer and seek into the file after multiplying the doc num by 12 rather than 8. During indexing, we'd keep the ordering info in an array; after all documents for a segment have been added, we create an array of sorted document numbers. When flushing the postings, their document numbers get remapped using the sorted array. Then we rewrite the .fdx file (and also the .tvx file), moving the file pointers (and ordering info) to remapped locations. The fact that the .fdt file is now "out of order" is isn't a problem – optimizing sequential access to that file isn't important. This issue is closely tied to LUCENE-843, "Improve how IndexWriter uses RAM to buffer added documents", and LUCENE-847, "Factor merge policy out of IndexWriter". Michael McCandless, Steven Parks, Ning Li, anybody else... comments? Suggestions? Marvin Humphrey Rectangular Research http://www.rectangular.com/ -------------------------------------------------------------------- void Scorer_collect(Scorer *self, HitCollector *hc, u32_t start, u32_t end, u32_t hits_per_seg, VArray *seg_starts) { u32_t seg_num = 0; u32_t doc_num_thresh = 0; u32_t hits_this_seg = 0; u32_t hits_thresh = hits_per_seg; /* get to first doc */ if ( !Scorer_Skip_To(self, start) ) return; /* execute scoring loop */ do { u32_t doc_num = Scorer_Doc(self); Tally *tally; if (hits_this_seg &gt;= hits_thresh || doc_num &gt;= doc_num_thresh) { if (doc_num &gt;= end) { /* bail out of loop if we've hit the user-spec'd end */ return; } else if (seg_starts == NULL || seg_starts-&gt;size == 0) { /* must supply seg_starts to enable pruning */ hits_thresh = U32_MAX; doc_num_thresh = end; } else if (seg_num == seg_starts-&gt;size) { /* we've finished the last segment */ return; } else { /* get start of upcoming segment */ Int this_start = (Int)VA_Fetch(seg_starts, seg_num); Int next_start = (Int)VA_Fetch(seg_starts, seg_num + 1); u32_t this_seg_start = this_start-&gt;value; seg_num++; /* skip docs as appropriate if we're pruning */ if (doc_num &lt; this_seg_start) { if ( Scorer_Skip_To(self, this_seg_start) ) doc_num = Scorer_Doc(self); else return; } /* set the last doc_num we'll score this upcoming segment */ doc_num_thresh = next_start == NULL ? end /* last segment */ : next_start-&gt;value; } /* start over counting hits for the new segment */ hits_this_seg = 0; } /* this doc is in range, so collect it */ tally = Scorer_Tally(self); hc-&gt;collect(hc, doc_num, tally-&gt;score); hits_this_seg++; } while (Scorer_Next(self)); }</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>852</id>
      <title>spellchecker: make hard-coded values configurable</title>
      <description>the class org.apache.lucene.search.spell.SpellChecker uses the following hard-coded values in its method indexDictionary: writer.setMergeFactor(300); writer.setMaxBufferedDocs(150); this poses problems when the spellcheck index is created on systems with certain limits, i.e. in unix environments where the ulimit settings are restricted for the user (http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428). there are several ways to circumvent this: 1. add another indexDictionary method with additional parameters: public void indexDictionary (Dictionary dict, int mergeFactor, int maxBufferedDocs) throws IOException 2. add setter methods for mergeFactor and maxBufferedDocs (see code in http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428 ) 3. Make SpellChecker subclassing easier as suggested by Chris Hostetter (see reply http://www.gossamer-threads.com/lists/lucene/java-dev/47463#47463) thanx, karin</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>853</id>
      <title>Caching does not work when using RMI</title>
      <description>Filters and caching uses transient maps so that caching does not work if you are using RMI and a remote searcher I want to add a new RemoteCachededFilter that will make sure that the caching is done on the remote searcher side</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>854</id>
      <title>Create merge policy that doesn't periodically inadvertently optimize</title>
      <description>The current merge policy, at every maxBufferedDocs * power-of-mergeFactor docs added, will do a fully cascaded merge, which is the same as an optimize. I think this is not good because at that "optimization poin", the particular addDocument call is [surprisingly] very expensive. While, amortized over all addDocument calls, the cost is low, the cost is paid "up front" and in a very "bunched up" manner. I think of this as "pay it forward": you are paying the full cost of an optimize right now on the expectation / hope that you will be adding a great many more docs. But, if you don't add that many more docs, then, the amortized cost for your index is in fact far higher than it should have been. Better to "pay as you go" instead. So we could make a small change to the policy by only merging the first mergeFactor segments once we hit 2X the merge factor. With mergeFactor=10, when we have created the 20th level 0 (just flushed) segment, we merge the first 10 into a level 1 segment. Then on creating another 10 level 0 segments, we merge the second set of 10 level 0 segments into a level 1 segment, etc. With this new merge policy, an index that's a bit bigger than a current "optimization point" would then have a lower amortized cost per document. Plus the merge cost is less "bunched up" and less "pay it forward": instead you pay for what you are actually using. We can start by creating this merge policy (probably, combined with with the "by size not by doc count" segment level computation from LUCENE-845) and then later decide whether we should make it the default merge policy.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>855</id>
      <title>MemoryCachedRangeFilter to boost performance of Range queries</title>
      <description>Currently RangeFilter uses TermEnum and TermDocs to find documents that fall within the specified range. This requires iterating through every single term in the index and can get rather slow for large document sets. MemoryCachedRangeFilter reads all &lt;docId, value&gt; pairs of a given field, sorts by value, and stores in a SortedFieldCache. During bits(), binary searches are used to find the start and end indices of the lower and upper bound values. The BitSet is populated by all the docId values that fall in between the start and end indices. TestMemoryCachedRangeFilterPerformance creates a 100K RAMDirectory-backed index with random date values within a 5 year range. Executing bits() 1000 times on standard RangeQuery using random date intervals took 63904ms. Using MemoryCachedRangeFilter, it took 876ms. Performance increase is less dramatic when you have less unique terms in a field or using less number of documents. Currently MemoryCachedRangeFilter only works with numeric values (values are stored in a long[] array) but it can be easily changed to support Strings. A side "benefit" of storing the values are stored as longs, is that there's no longer the need to make the values lexographically comparable, i.e. padding numeric values with zeros. The downside of using MemoryCachedRangeFilter is there's a fairly significant memory requirement. So it's designed to be used in situations where range filter performance is critical and memory consumption is not an issue. The memory requirements are: (sizeof(int) + sizeof(long)) * numDocs. MemoryCachedRangeFilter also requires a warmup step which can take a while to run in large datasets (it took 40s to run on a 3M document corpus). Warmup can be called explicitly or is automatically called the first time MemoryCachedRangeFilter is applied using a given field. So in summery, MemoryCachedRangeFilter can be useful when: Performance is critical Memory is not an issue Field contains many unique numeric values Index contains large amount of documents</description>
      <attachments/>
      <comments>40</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>856</id>
      <title>Optimize segment merging</title>
      <description>With LUCENE-843, the time spent indexing documents has been substantially reduced and now the time spent merging is a sizable portion of indexing time. I ran a test using the patch for LUCENE-843, building an index of 10 million docs, each with ~5,500 byte plain text, with term vectors (positions + offsets) on and with 2 small stored fields per document. RAM buffer size was 32 MB. I didn't optimize the index in the end, though optimize speed would also improve if we optimize segment merging. Index size is 86 GB. Total time to build the index was 8 hrs 38 minutes, 5 hrs 40 minutes of which was spent merging. That's 65.6% of the time! Most of this time is presumably IO which probably can't be reduced much unless we improve overall merge policy and experiment with values for mergeFactor / buffer size. These tests were run on a Mac Pro with 2 dual-core Intel CPUs. The IO system is RAID 0 of 4 drives, so, these times are probably better than the more common case of a single hard drive which would likely be slower IO. I think there are some simple things we could do to speed up merging: Experiment with buffer sizes – maybe larger buffers for the IndexInputs used during merging could help? Because at a default mergeFactor of 10, the disk heads must do alot of seeking back and forth between these 10 files (and then to the 11th file where we are writing). Use byte copying when possible, eg if there are no deletions on a segment we can almost (I think?) just copy things like prox postings, stored fields, term vectors, instead of full parsing to Jave objects and then re-serializing them. Experiment with mergeFactor / different merge policies. For example I think LUCENE-854 would reduce time spend merging for a given index size. This is currently just a place to list ideas for optimizing segment merges. I don't plan on working on this until after LUCENE-843. Note that for "autoCommit=false", this optimization is somewhat less important, depending on how often you actually close/open a new IndexWriter. In the extreme case, if you open a writer, add 100 MM docs, close the writer, then no segment merges happen at all.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>857</id>
      <title>Remove BitSet caching from QueryFilter</title>
      <description>Since caching is built into the public BitSet bits(IndexReader reader) method, I don't see a way to deprecate that, which means I'll just cut it out and document it in CHANGES.txt. Anyone who wants QueryFilter caching will be able to get the caching back by wrapping the QueryFilter in the CachingWrapperFilter.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>858</id>
      <title>link from Lucene web page to API docs</title>
      <description>There should be a way to link from e.g. http://lucene.apache.org/java/docs/gettingstarted.html to the API docs, but not just to the start page with the frame set but to a specific page, e.g. this: http://lucene.zones.apache.org:8080/hudson/job/Lucene-Nightly/javadoc/overview-summary.html#overview_description To make this work a way to set a relative link is needed.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>859</id>
      <title>Expose the number of deleted docs in index/segment</title>
      <description>Use case: We've got a lot of large, mostly search-only indices. These indices are not re-optimized once "deployed". Docs in them do not get updated, but they do get deleted. After a while, the number of deleted docs grows, but it's hard to tell how many documents have been deleted. Exposing the number of deleted docs via *Reader.deletedDocs() method let's you get to this number. I'm attaching patch that touches the following: M src/test/org/apache/lucene/index/TestSegmentReader.java M src/java/org/apache/lucene/index/MultiReader.java M src/java/org/apache/lucene/index/IndexReader.java M src/java/org/apache/lucene/index/FilterIndexReader.java M src/java/org/apache/lucene/index/ParallelReader.java M src/java/org/apache/lucene/index/SegmentReader.java SegmentReader also got a public static main(String[]) that takes 1 command-line parameter, a path to the index to check, and prints out the number of deleted docs.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>860</id>
      <title>site should call project "Lucene Java", not just "Lucene"</title>
      <description>To avoid confusion with the top-level Lucene project, the Lucene Java website should refer to itself as Lucene Java.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>864</id>
      <title>contrib/benchmark files need eol-style set</title>
      <description>The following files in contrib/benchmark don't have eol-style set to native, so when they are checked out, they don't get converted. ./build.xml: ./CHANGES.txt: ./conf/sample.alg: ./conf/standard.alg: ./conf/sloppy-phrase.alg: ./conf/deletes.alg: ./conf/micro-standard.alg: ./conf/compound-penalty.alg:</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>866</id>
      <title>Multi-level skipping on posting lists</title>
      <description>To accelerate posting list skips (TermDocs.skipTo(int)) Lucene uses skip lists. The default skip interval is set to 16. If we want to skip e. g. 100 documents, then it is not necessary to read 100 entries from the posting list, but only 100/16 = 6 skip list entries plus 100%16 = 4 entries from the posting list. This speeds up conjunction (AND) and phrase queries significantly. However, the skip interval is always a compromise. If you have a very big index with huge posting lists and you want to skip over lets say 100k documents, then it is still necessary to read 100k/16 = 6250 entries from the skip list. For big indexes the skip interval could be set to a higher value, but then after a big skip a long scan to the target doc might be necessary. A solution for this compromise is to have multi-level skip lists that guarantee a logarithmic amount of skips to any target in the posting list. This patch implements such an approach in the following way: Example for skipInterval = 3: c (skip level 2) c c c (skip level 1) x x x x x x x x x x (skip level 0) d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d (posting list) 3 6 9 12 15 18 21 24 27 30 (df) d - document x - skip data c - skip data with child pointer Skip level i contains every skipInterval-th entry from skip level i-1. Therefore the number of entries on level i is: floor(df / ((skipInterval ^ (i + 1))). Each skip entry on a level i&gt;0 contains a pointer to the corresponding skip entry in list i-1. This guarantees a logarithmic amount of skips to find the target document. Implementations details: I factored the skipping code out of SegmentMerger and SegmentTermDocs to simplify those classes. The two new classes AbstractSkipListReader and AbstractSkipListWriter implement the skipping functionality. While AbstractSkipListReader and Writer take care of writing and reading the multiple skip levels, they do not implement an actual skip data format. The two new subclasses DefaultSkipListReader and Writer implement the skip data format that is currently used in Lucene (with two file pointers for the freq and prox file and with payload length information). I added this extra layer to be prepared for flexible indexing and different posting list formats. File format changes: I added the new parameter 'maxSkipLevels' to the term dictionary and increased the version of this file. If maxSkipLevels is set to one, then the format of the freq file does not change at all, because we only have one skip level as before. For backwards compatibility maxSkipLevels is set to one automatically if an index without the new parameter is read. In case maxSkipLevels &gt; 1, then the frq file changes as follows: FreqFile (.frq) --&gt; &lt;TermFreqs, SkipData&gt;^TermCount SkipData --&gt; &lt;&lt;SkipLevelLength, SkipLevel&gt;^(Min(maxSkipLevels, floor(log(DocFreq/log(skipInterval))) - 1)&gt;, SkipLevel&gt; SkipLevel --&gt; &lt;SkipDatum&gt;DocFreq/(SkipInterval(Level + 1)) Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not needed, and 2) the format of this file does not change for maxSkipLevels=1 then. All unit tests pass with this patch.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>868</id>
      <title>Making Term Vectors more accessible</title>
      <description>One of the big issues with term vector usage is that the information is loaded into parallel arrays as it is loaded, which are then often times manipulated again to use in the application (for instance, they are sorted by frequency). Adding a callback mechanism that allows the vector loading to be handled by the application would make this a lot more efficient. I propose to add to IndexReader: abstract public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException; and a similar one for the all fields version Where TermVectorMapper is an interface with a single method: void map(String term, int frequency, int offset, int position); The TermVectorReader will be modified to just call the TermVectorMapper. The existing getTermFreqVectors will be reimplemented to use an implementation of TermVectorMapper that creates the parallel arrays. Additionally, some simple implementations that automatically sort vectors will also be created. This is my first draft of this API and is subject to change. I hope to have a patch soon. See http://www.gossamer-threads.com/lists/lucene/java-user/48003?search_string=get%20the%20total%20term%20frequency;#48003 for related information.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>869</id>
      <title>Make FSIndexInput and FSIndexOutput inner classes of FSDirectory</title>
      <description>I would like make FSIndexInput and FSIndexOutput protected, static, inner classes of FSDirectory. Currently these classes are located in the same source file as FSDirectory, which means that classes outside the store package can not extend them. I don't see any performance impacts or other side effects of this trivial patch. All unit tests pass.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>870</id>
      <title>add concurrent merge policy</title>
      <description>Provide the ability to handle merges in one or more concurrent threads, i.e., concurrent with other IndexWriter operations. I'm factoring the code from LUCENE-847 for this.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>872</id>
      <title>have Hits implement Iterable</title>
      <description>for compatibilty with the enhanced for loop it is required that the Hits class implements the interface Iterable. no further code changes required as the method iterator required from the interface already is present.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>874</id>
      <title>Automatic reopen of IndexSearcher/IndexReader</title>
      <description>To improve performance, a single instance of IndexSearcher should be used. However, if the index is updated, it's hard to close/reopen it, because multiple threads may be accessing it at the same time. Lucene should include an out-of-the-box solution to this problem. Either a new class should be implemented to manage this behaviour (singleton IndexSearcher, plus detection of a modified index, plus safely closing and reopening the IndexSearcher) or this could be behind the scenes by the IndexSearcher class.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>876</id>
      <title>contrib/gdata has many javadoc warnings</title>
      <description>All other javadoc warnings/errors were resolved in LUCENE-875. Resolving the warnings also in gdata would allow to enable the 'fail-on-javadoc-warning' logic in 'ant javadocs' target.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>878</id>
      <title>Concept Search</title>
      <description>I have looked around on your web site as well as some documentation but have not found anything to do with Concept Search. My definition of Concept Search is as follows: 1. I would have a file (list) of various phrases / N-grams which I would like to Lucene to use as a search basis without having to type in all these phrases manually, and have Lucene return the results as it would normally if a single search query was entered. 2. An example would be - find Unable to render embedded object: File (Wild_Animals) not found. - where the "!" would indicate that this is a search that would use a file (ie Wild_Animals.txt) and read in the various phrases within this file and perform the search in the corpus for these phrases. 3. The contents of Wild_Animals.txt could look like this: BUFFALO BEAR MOOSE COYOTE WOLF MOUNTAIN GOAT MOUNTAIN SHEEP DALL SHEEP DEER KODIAK BEAR BROWN BEAR BLACK BEAR etc etc etc 4. Is my idea of a Concept Search feasible / doable??? If so, can you point me to any documentation that exists whereby this could be done within Lucene Please send any info you have on this to me - Charles_S_Patridge@prodigy.net Thank you in advance for your time and efforts.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>879</id>
      <title>Document number integrity merge policy</title>
      <description>This patch allows for document numbers stays the same even after merge of segments with deletions. Consumer needs to do this: indexWriter.setSkipMergingDeletedDocuments(false); The effect will be that deleted documents are replaced by a new Document() in the merged segment, but not marked as deleted. This should probably be some policy thingy that allows for different solutions such as keeping the old document, et c. Also see http://www.nabble.com/optimization-behaviour-tf3723327.html#a10418880</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>882</id>
      <title>Spellchecker doesn't need to store ngrams</title>
      <description>The spellchecker in contrib stores the ngrams although this doesn't seem to be necessary. This patch changes that, I will commit it unless someone objects. This improves indexing speed and index size. Some numbers on a small test I did: Input of the original index: 2200 text files, index size 5.3 MB, indexing took 17 seconds Spell index before patch: about 60.000 documents, index size 13 MB, indexing took 62 seconds Spell index after patch: about 60.000 documents, index size 6.3 MB, indexing took 52 seconds BTW, the test case fails even before this patch. I'll probaby submit another issue about how to fix that.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>884</id>
      <title>Query Syntax page does not make it clear that wildcard searches are not allowed in Phrase Queries</title>
      <description>The queryparsersyntax page which is where I expect most novices (such as myself) start with lucene seems to indicate that wildcards can be used in phrase terms Quoting: 'Terms: A query is broken up into terms and operators. There are two types of terms: Single Terms and Phrases. A Single Term is a single word such as "test" or "hello". A Phrase is a group of words surrounded by double quotes such as "hello dolly". .... Wildcard Searches Lucene supports single and multiple character wildcard searches. To perform a multiple character wildcard search use the "*" symbol. Multiple character wildcard searches looks for 0 or more characters. For example, to search for test, tests or tester, you can use the search: test* You can also use the wildcard searches in the middle of a term. ' there is nothing to indicate in the section on Wildcard Searches that it can be performed only on Single word terms not Phrase terms. Chris argues 'that there is nothing in the description of a Phrase to indicate that it can be anything other then what it says "a group of words surrounded by double quotes" .. at no point does it suggest that other types of queries or syntax can be used inside the quotes. likewise the discussion of Wildcards makes no mention of phrases to suggest that wildcard characters can be used in a phrase.' but I don't accept this because there is nothing in the description of a Single Term either to indicate it can use wildcards either. Wildcards are only mentioned in the Wildcard section and there it says thay can be used in a term, it does not restrict the type of term I Propose a simple solution modify: Lucene supports single and multiple character wildcard searches. to Lucene supports single and multiple character wildcard searches within single terms. (Chris asked for a patch, but Im not sure how to do this, but the change is simple enough)</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>885</id>
      <title>clean up build files so contrib tests are run more easily</title>
      <description>Per mailing list discussion... http://www.nabble.com/Tests%2C-Contribs%2C-and-Releases-tf3768924.html#a10655448 Tests for contribs should be run when "ant test" is used, existing "test" target renamed to "test-core"</description>
      <attachments/>
      <comments>19</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>886</id>
      <title>spellchecker cleanup</title>
      <description>Some cleanup, attached here so it can be tracked if necessary: javadoc improvements; don't print exceptions to stderr but re-throw them; new constructor for a new test case. I will commit this soon.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>887</id>
      <title>Interruptible segment merges</title>
      <description>Adds the ability to IndexWriter to interrupt an ongoing merge. This might be necessary when Lucene is e. g. running as a service and has to stop indexing within a certain period of time due to a shutdown request. A solution would be to add a new method shutdown() to IndexWriter which satisfies the following two requirements: if a merge is happening, abort it flush the buffered docs but do not trigger a merge See also discussions about this feature on java-dev: http://www.gossamer-threads.com/lists/lucene/java-dev/49008</description>
      <attachments/>
      <comments>17</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>888</id>
      <title>Improve indexing performance by increasing internal buffer sizes</title>
      <description>In working on LUCENE-843, I noticed that two buffer sizes have a substantial impact on overall indexing performance. First is BufferedIndexOutput.BUFFER_SIZE (also used by BufferedIndexInput). Second is CompoundFileWriter's buffer used to actually build the compound file. Both are now 1 KB (1024 bytes). I ran the same indexing test I'm using for LUCENE-843. I'm indexing ~5,500 byte plain text docs derived from the Europarl corpus (English). I index 200,000 docs with compound file enabled and term vector positions &amp; offsets stored plus stored fields. I flush documents at 16 MB RAM usage, and I set maxBufferedDocs carefully to not hit LUCENE-845. The resulting index is 1.7 GB. The index is not optimized in the end and I left mergeFactor @ 10. I ran the tests on a quad-core OS X 10 machine with 4-drive RAID 0 IO system. At 1 KB (current Lucene trunk) it takes 622 sec to build the index; if I increase both buffers to 8 KB it takes 554 sec to build the index, which is an 11% overall gain! I will run more tests to see if there is a natural knee in the curve (buffer size above which we don't really gain much more performance). I'm guessing we should leave BufferedIndexInput's default BUFFER_SIZE at 1024, at least for now. During searching there can be quite a few of this class instantiated, and likely a larger buffer size for the freq/prox streams could actually hurt search performance for those searches that use skipping. The CompoundFileWriter buffer is created only briefly, so I think we can use a fairly large (32 KB?) buffer there. And there should not be too many BufferedIndexOutputs alive at once so I think a large-ish buffer (16 KB?) should be OK.</description>
      <attachments/>
      <comments>31</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>889</id>
      <title>Standard tokenizer with punctuation output</title>
      <description>This patch adds punctuation (comma, period, question mark and exclamation point) tokens as output from the StandardTokenizer, and filters them out in the StandardFilter. (I needed them for text classification reasons.)</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>892</id>
      <title>CompoundFileReader's openInput produces streams that may do an extra buffer copy</title>
      <description>Spinoff of LUCENE-888. The class for reading from a compound file (CompoundFileReader) has a primary stream which is a BufferedIndexInput when that stream is from an FSDirectory (which is the norm). That is one layer of buffering. Then, when its openInput is called, a CSIndexInput is created which also subclasses from BufferedIndexInput. That's a second layer of buffering. When a consumer actually uses that CSIndexInput to read, and a call to readByte or readBytes runs out of what's in the first buffer, it will go to refill its buffer. But that refill calls the first BufferedIndexInput which in turn may refill its buffer (a double copy) by reading the underlying stream. Not sure how to fix it yet but we should change things to not do the extra buffer copy.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>893</id>
      <title>Increase buffer sizes used during searching</title>
      <description>Spinoff of LUCENE-888. In LUCENE-888 we increased buffer sizes that impact indexing and found substantial (10-18%) overall performance gains. It's very likely that we can also gain some performance for searching by increasing the read buffers in BufferedIndexInput used by searching. We need to test performance impact to verify and then pick a good overall default buffer size, also being careful not to add too much overall HEAP RAM usage because a potentially very large number of BufferedIndexInput instances are created during searching (# segments X # index files per segment).</description>
      <attachments/>
      <comments>12</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>896</id>
      <title>Let users set Similarity for MoreLikeThis</title>
      <description>Let users set Similarity used for MoreLikeThis For discussion, see: http://www.nabble.com/MoreLikeThis-API-changes--tf3838535.html</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>897</id>
      <title>Change how Core and Contrib javadocs are hosted</title>
      <description>Change the site javadocs to: 1. separate contrib javadocs from core javadocs 2. Optionally, include a unified view as well.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>904</id>
      <title>Calculate MD5 checksums in target &lt;dist-all&gt;</title>
      <description>Trivial patch that extends the ant target &lt;dist-all&gt; to calculate the MD5 checksums for the dist files.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>906</id>
      <title>Elision filter for simple french analyzing</title>
      <description>If you don't wont to use stemming, StandardAnalyzer miss some french strangeness like elision. "l'avion" wich means "the plane" must be tokenized as "avion" (plane). This filter could be used with other latin language if elision exists.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>928</id>
      <title>field level search</title>
      <description>right now there is field level search avilable. But if we dont want to search on field but on the whole document then to the query parse the whole contet is to be passed. If we append the titles (captions) to say index content(which is having all the captions) for every caption indexcontent is created. to avoid the index content so for a query without the "field:quey" how is it possible and what to pass to the query analyser as a input so it searches on teh whole document.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>935</id>
      <title>Improve maven artifacts</title>
      <description>There are a couple of things we can improve for the next release: "*pom.xml" files should be renamed to "*pom.xml.template" artifacts "lucene-parent" should extend "apache-parent" add source jars as artifacts update &lt;generate-maven-artifacts&gt; task to work with latest version of maven-ant-tasks.jar metadata filenames should not contain "local"</description>
      <attachments/>
      <comments>23</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>937</id>
      <title>Make CachingTokenFilter faster</title>
      <description>The LinkedList used by CachingTokenFilter is accessed using the get() method. Direct access on a LinkedList is slow and an Iterator should be used instead. For more than a handful of tokens, the difference in speed grows exponentially.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>939</id>
      <title>Check for boundary conditions in FieldInfos</title>
      <description>In FieldInfos there are three methods in which we don't check for boundary conditions but catch e. g. an IndexOutOfBoundsException or a NPE. I think this isn't good code style and is probably not even faster than checking explicitly. "Exceptions should not be used to alter the flow of a program as part of normal execution." Also this can be irritating when you're trying to debug an IndexOutOfBoundsException that is thrown somewhere else in your program and you place a breakpoint on that exception. The three methods are: public int fieldNumber(String fieldName) { try { FieldInfo fi = fieldInfo(fieldName); if (fi != null) return fi.number; } catch (IndexOutOfBoundsException ioobe) { return -1; } return -1; } public String fieldName(int fieldNumber) { try { return fieldInfo(fieldNumber).name; } catch (NullPointerException npe) { return ""; } } public FieldInfo fieldInfo(int fieldNumber) { try { return (FieldInfo) byNumber.get(fieldNumber); } catch (IndexOutOfBoundsException ioobe) { return null; } }</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>943</id>
      <title>ComparatorKey in Locale based sorting</title>
      <description>This is a reply/follow-up on Chris Hostetter's message on Lucene developers list (aug 2006): http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3cPine.LNX.4.58.0608211050330.5081@hal.rescomp.berkeley.edu%3e &gt; perhaps it would be worthwhile for comparatorStringLocale to convert the String[] it gets back from FieldCache.DEFAULT.getStrings to a new CollationKey[]? or maybe even for FieldCache.DEFAULT.getStrings to be deprecated, and replaced with a FieldCache.DEFAULT.getCollationKeys(reader,field,Collator)? I think the best is to keep the default behavior as it is today. There is a cost of building caches for sort fields which I think not everyone wants. However for some international production environments there are indeed possible performance gains in comparing precalculated keys instead of comparing strings with rulebased collators. Since Lucene's Sort architecture is pluggable it is easy to create a custom locale-based comparator, which utilizes the built-in caching/warming mechanism of FieldCache, and may be used in SortField constructor. I'm not sure whether there should be classes for this in Lucene core or not, but it could be nice to have the option of performance vs. memory consumption in localized sorting without having to use additional jars.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>944</id>
      <title>Remove deprecated methods in BooleanQuery</title>
      <description>Remove deprecated methods setUseScorer14 and getUseScorer14 in BooleanQuery, and adapt javadocs.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>947</id>
      <title>Some improvements to contrib/benchmark</title>
      <description>I've made some small improvements to the contrib/benchmark, mostly merging in the ad-hoc benchmarking code I've been using in LUCENE-843: Fixed thread safety of DirDocMaker's usage of SimpleDateFormat Print the props in sorted order Added new config "autocommit=true|false" to CreateIndexTask Added new config "ram.flush.mb=int" to AddDocTask Added new configs "doc.term.vector.positions=true|false" and "doc.term.vector.offsets=true|false" to BasicDocMaker Added WriteLineDocTask.java, so you can make an alg that uses this to build up a single file containing one document per line in a single file. EG this alg converts the reuters-out tree into a single file that has ~1000 bytes per body field, saved to work/reuters.1000.txt: docs.dir=reuters-out doc.maker=org.apache.lucene.benchmark.byTask.feeds.DirDocMaker line.file.out=work/reuters.1000.txt doc.maker.forever=false {WriteLineDoc(1000)} : * Each line has tab-separted TITLE, DATE, BODY fields. Created feeds/LineDocMaker.java that creates documents read from the file created by WriteLineDocTask.java. EG this alg indexes all documents created above: analyzer=org.apache.lucene.analysis.SimpleAnalyzer directory=FSDirectory doc.add.log.step=500 docs.file=work/reuters.1000.txt doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker doc.tokenized=true doc.maker.forever=false ResetSystemErase CreateIndex {AddDoc} : * CloseIndex RepSumByPref AddDoc I'll attach initial patch shortly.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>953</id>
      <title>Snowball has new Stemmers available</title>
      <description>I noticed Porter's Snowball stemmers have some new versions available, among them Turkish, Hungarian, etc. It also looks like the packaging has changed a little bit. It would be useful to build a new version and update Lucene's contrib to include these new versions. Would also be helpful if we could submit a patch back to the Snowball project to put an abstract declaration of the stem() method into the SnowballProgram. http://snowball.tartarus.org/</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>954</id>
      <title>Toggle score normalization in Hits</title>
      <description>The current implementation of the "Hits" class sometimes performs score normalization. In particular, whenever the top-ranked score is bigger than 1.0, it is normalized to a maximum of 1.0. In this case, Hits may return different score results than TopDocs-based methods. In my scenario (a federated search system), Hits delievered just plain wrong results. I was merging results from several sources, all having homogeneous statistics (similar to MultiSearcher, but over the Internet using HTTP/XML-based protocols). Sometimes, some of the sources had a top-score greater than 1, so I ended up with garbled results. I suggest to add a switch to enable/disable this score-normalization at runtime. My patch (attached) has an additional peformance benefit, since score normalization now occurs only when Hits#score() is called, not when creating the Hits result list. Whenever scores are not required, you save one multiplication per retrieved hit (i.e., at least 100 multiplications with the current implementation of Hits).</description>
      <attachments/>
      <comments>19</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>956</id>
      <title>phonem conversion from aspell dictionnary</title>
      <description>First step to improve Spellchecker's suggestions : phonem conversion for differents languages. The conversion code is build from aspell file description. The patch contains class for managing english, french, wallon and swedish. If it's work well, other available dictionnary from aspell project can be built.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>959</id>
      <title>Document Vector-&gt;ArrayList</title>
      <description>Document Vector should be changed to ArrayList. Document is not advertised to be thread safe, and it's doubtful that anyone modifies a Document from multiple threads.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>960</id>
      <title>SpanQueryFilter addition</title>
      <description>Similar to the QueryFilter (or whatever it is called now) the SpanQueryFilter is a regular Lucene Filter, but it also can return Spans-like information. This is useful if you not only want to filter based on a Query, but you then want to be able to compare how a given match from a new query compared to the positions of the filtered SpanQuery. Patch to come shortly also contains a caching mechanism for the SpanQueryFilter</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>964</id>
      <title>Remove DocumentWriter</title>
      <description>DocumentWriter has been replaced by DocumentsWriter (from LUCENE-843) so we need to remove it &amp; fix the unit tests that directly use it...</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>965</id>
      <title>Implement a state-of-the-art retrieval function in Lucene</title>
      <description>We implemented the axiomatic retrieval function, which is a state-of-the-art retrieval function, to replace the default similarity function in Lucene. We compared the performance of these two functions and reported the results at http://sifaka.cs.uiuc.edu/hfang/lucene/Lucene_exp.pdf. The report shows that the performance of the axiomatic retrieval function is much better than the default function. The axiomatic retrieval function is able to find more relevant documents and users can see more relevant documents in the top-ranked documents. Incorporating such a state-of-the-art retrieval function could improve the search performance of all the applications which were built upon Lucene. Most changes related to the implementation are made in AXSimilarity, TermScorer and TermQuery.java. However, many test cases are hand coded to test whether the implementation of the default function is correct. Thus, I also made the modification to many test files to make the new retrieval function pass those cases. In fact, we found that some old test cases are not reasonable. For example, in the testQueries02 of TestBoolean2.java, the query is "+w3 xx", and we have two documents "w1 xx w2 yy w3" and "w1 w3 xx w2 yy w3". The second document should be more relevant than the first one, because it has more occurrences of the query term "w3". But the original test case would require us to rank the first document higher than the second one, which is not reasonable.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>966</id>
      <title>A faster JFlex-based replacement for StandardAnalyzer</title>
      <description>JFlex (http://www.jflex.de/) can be used to generate a faster (up to several times) replacement for StandardAnalyzer. Will add a patch and a simple benchmark code in a while.</description>
      <attachments/>
      <comments>41</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>967</id>
      <title>Add "tokenize documents only" task to contrib/benchmark</title>
      <description>I've been looking at performance improvements to tokenization by re-using Tokens, and to help benchmark my changes I've added a new task called ReadTokens that just steps through all fields in a document, gets a TokenStream, and reads all the tokens out of it. EG this alg just reads all Tokens for all docs in Reuters collection: doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker doc.maker.forever=false {ReadTokens &gt; : *</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>969</id>
      <title>Optimize the core tokenizers/analyzers &amp; deprecate Token.termText</title>
      <description>There is some "low hanging fruit" for optimizing the core tokenizers and analyzers: Re-use a single Token instance during indexing instead of creating a new one for every term. To do this, I added a new method "Token next(Token result)" (Doron's suggestion) which means TokenStream may use the "Token result" as the returned Token, but is not required to (ie, can still return an entirely different Token if that is more convenient). I added default implementations for both next() methods in TokenStream.java so that a TokenStream can choose to implement only one of the next() methods. Use "char[] termBuffer" in Token instead of the "String termText". Token now maintains a char[] termBuffer for holding the term's text. Tokenizers &amp; filters should retrieve this buffer and directly alter it to put the term text in or change the term text. I only deprecated the termText() method. I still allow the ctors that pass in String termText, as well as setTermText(String), but added a NOTE about performance cost of using these methods. I think it's OK to keep these as convenience methods? After the next release, when we can remove the deprecated API, we should clean up Token.java to no longer maintain "either String or char[]" (and the initTermBuffer() private method) and always use the char[] termBuffer instead. Re-use TokenStream instances across Fields &amp; Documents instead of creating a new one for each doc. To do this I added an optional "reusableTokenStream(...)" to Analyzer which just defaults to calling tokenStream(...), and then I implemented this for the core analyzers. I'm using the patch from LUCENE-967 for benchmarking just tokenization. The changes above give 21% speedup (742 seconds -&gt; 585 seconds) for LowerCaseTokenizer -&gt; StopFilter -&gt; PorterStemFilter chain, tokenizing all of Wikipedia, on JDK 1.6 -server -Xmx1024M, Debian Linux, RAID 5 IO system (best of 2 runs). If I pre-break Wikipedia docs into 100 token docs then it's 37% faster (1236 sec -&gt; 774 sec), I think because of re-using TokenStreams across docs. I'm just running with this alg and recording the elapsed time: analyzer=org.apache.lucene.analysis.LowercaseStopPorterAnalyzer doc.tokenize.log.step=50000 docs.file=/lucene/wikifull.txt doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker doc.tokenized=true doc.maker.forever=false {ReadTokens &gt; : * See this thread for discussion leading up to this: http://www.gossamer-threads.com/lists/lucene/java-dev/51283 I also fixed Token.toString() to work correctly when termBuffer is used (and added unit test).</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>971</id>
      <title>Create enwiki indexable data as line-per-article rather than file-per-article</title>
      <description>Create a line per article rather than a file. Consume with indexLineFile task.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>975</id>
      <title>Position based TermVectorMapper</title>
      <description>As part of the new TermVectorMapper approach to TermVectors, the ensuing patch loads term vectors and stores the term info by position. This should let people directly index into a term vector given a position. Actually, it does it through Maps, b/c the array based bookkeeping is a pain given the way positions are stored. The map looks like: Map&lt;String, Map&lt;Integer, TVPositionInfo&gt;&gt; where the String is the field name, the integer is the position, and TVPositionInfo is a storage mechanism for the terms and offsets that occur at a position. It should handle multiple terms per position (which is always my downfall! ) I have not tested performance of this approach.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>977</id>
      <title>internal hashing improvements</title>
      <description>Internal power-of-two closed hashtable traversal in DocumentsWriter and CharArraySet could be better. Here is the current method of resolving collisions: if (text2 != null &amp;&amp; !equals(text, len, text2)) { final int inc = code*1347|1; do { code += inc; pos = code &amp; mask; text2 = entries[pos]; } while (text2 != null &amp;&amp; !equals(text, len, text2)); The problem is that two different hashCodes with the same lower bits will keep picking the same slots (the upper bits will be ignored). This is because multiplication (*1347) only really shifts bits to the left... so given that the two codes already matched on the right, they will both pick the same increment, and this will keep them on the same path through the table (even though it's being added to numbers that differ on the left). To resolve this, some bits need to be moved to the right when calculating the increment.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>979</id>
      <title>Remove Deprecated Benchmarking Utilities from contrib/benchmark</title>
      <description>The old Benchmark utilities in contrib/benchmark have been deprecated and should be removed in 2.9 of Lucene.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>981</id>
      <title>NewAnalyzerTask</title>
      <description>NewAnalyzerTask (patch to follow) allows a contrib/benchmark algorithm to change Analyzers during a run. This is useful when comparing Analyzers {"NewAnalyzer" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) &gt; is a sample declaration in an algorithm file.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>982</id>
      <title>Create new method optimize(int maxNumSegments) in IndexWriter</title>
      <description>Spinning this out from the discussion in LUCENE-847. I think having a way to "slightly optimize" your index would be useful for many applications. The current optimize() call is very expensive for large indices because it always optimizes fully down to 1 segment. If we add a new method which instead is allowed to stop optimizing once it has &lt;= maxNumSegments segments in the index, this would allow applications to eg optimize down to say &lt;= 10 segments after doing a bunch of updates. This should be a nice compromise of gaining good speedups of searching while not spending the full (and typically very high) cost of optimizing down to a single segment. Since LUCENE-847 is now formalizing an API for decoupling merge policy from IndexWriter, if we want to add this new optimize method we need to take it into account in LUCENE-847.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>983</id>
      <title>Enable IndexReader to merge tail segments on demand, in RAM, when opening</title>
      <description>Spinoff from LUCENE-845. In LUCENE-845, the IndexWriter must pay a high cost (O(N^2) merge cost) for keeping the number of segments "always small" in the case where flushes of very small segments (1 doc as worst case) happen frequently. This happens in "low latency" applications. This is because IndexWriter must be ready "at every moment" for an IndexReader to open the index. But, if we allow IndexReader to use some RAM (give it a RAM buffer) to load the long tail of small segments into a RAMDirectory, and then merge them (in RAM), this allows IndexReader to still have good performance on the index without IndexWriter paying this high merge cost. This effectively allows us to optimize the tail segments "on demand" when a reader needs to use them. When we combine this with LUCENE-743 (efficient "re-open" of a reader) then we should be able to efficiently handle low latency applications.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>986</id>
      <title>Refactor segmentInfos from IndexReader into its subclasses</title>
      <description>References to segmentInfos in IndexReader cause different kinds of problems for subclasses of IndexReader, like e. g. MultiReader. Only subclasses of IndexReader that own the index directory, namely SegmentReader and MultiSegmentReader, should have a SegmentInfos object and be able to access it. Further information: http://www.gossamer-threads.com/lists/lucene/java-dev/51808 http://www.gossamer-threads.com/lists/lucene/java-user/52460 A part of the refactoring work was already done in LUCENE-781</description>
      <attachments/>
      <comments>11</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>988</id>
      <title>Benchmarker tasks for the TPB data collection</title>
      <description>Very simple DocMaker and QueryMaker for the TPB data collection (~150,000 content items, ~500,000 comments to the contents and ~3,700,000 user queries). URL to dataset: http://thepiratebay.org/tor/3783572/db_dump_and_query_log_from_piratebay.org__summer_of_2006</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>989</id>
      <title>Statistics from ValueSourceQuery</title>
      <description>Patch forthcoming that adds a DocValuesStats object that is optionally computer for a ValueSourceQuery. This ~replaces the getMin/Max/Avg from the DocValues which were previously unaccessible without reasonably heavy subclassing. in addition it add a few more stats and provides a single object to encapsulate all statistics going forward. The stats object is tied to the ValueSourceQuery so that the values can be cached without having to maintain the full set of DocValues. Test and javadocs included. will</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>990</id>
      <title>ParallelMultiSearcher.search with a custom HitCollector should run parallel</title>
      <description>The ParallelMultiSearcher.search(Weight weight, Filter filter, final HitCollector results) should search over its underlying Searchers in parallel, like the TopDocs versions of the search() method. There's a @todo for this in the method's Javadoc comment.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>994</id>
      <title>Change defaults in IndexWriter to maximize "out of the box" performance</title>
      <description>This is follow-through from LUCENE-845, LUCENE-847 and LUCENE-870; I'll commit this once those three are committed. Out of the box performance of IndexWriter is maximized when flushing by RAM instead of a fixed document count (the default today) because documents can vary greatly in size. Likewise, merging performance should be faster when merging by net segment size since, to minimize the net IO cost of merging segments over time, you want to merge segments of equal byte size. Finally, ConcurrentMergeScheduler improves indexing speed substantially (25% in a simple initial test in LUCENE-870) because it runs the merges in the backround and doesn't block add/update/deleteDocument calls. Most machines have concurrency between CPU and IO and so it makes sense to default to this MergeScheduler. Note that these changes will break users of ParallelReader because the parallel indices will no longer have matching docIDs. Such users need to switch IndexWriter back to flushing by doc count, and switch the MergePolicy back to LogDocMergePolicy. It's likely also necessary to switch the MergeScheduler back to SerialMergeScheduler to ensure deterministic docID assignment. I think the combination of these three default changes, plus other performance improvements for indexing (LUCENE-966, LUCENE-843, LUCENE-963, LUCENE-969, LUCENE-871, etc.) should make for some sizable performance gains Lucene 2.3!</description>
      <attachments/>
      <comments>30</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>995</id>
      <title>Add open ended range query syntax to QueryParser</title>
      <description>The QueryParser fails to generate open ended range queries. Parsing e.g. "date:[1990 TO *]" gives zero results, but ConstantRangeQuery("date","1990",null,true,true) does produce the expected results. "date:[* TO 1990]" gives the same results as ConstantRangeQuery("date",null,"1990",true,true).</description>
      <attachments/>
      <comments>21</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>996</id>
      <title>Parsing mixed inclusive/exclusive range queries</title>
      <description>The current query parser doesn't handle parsing a range query (i.e. ConstantScoreRangeQuery) with mixed inclusive/exclusive bounds.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>997</id>
      <title>Add search timeout support to Lucene</title>
      <description>This patch is based on Nutch-308. This patch adds support for a maximum search time limit. After this time is exceeded, the search thread is stopped, partial results (if any) are returned and the total number of results is estimated. This patch tries to minimize the overhead related to time-keeping by using a version of safe unsynchronized timer. This was also discussed in an e-mail thread. http://www.nabble.com/search-timeout-tf3410206.html#a9501029</description>
      <attachments/>
      <comments>54</comments>
      <commenters>12</commenters>
    </issue>
    <issue>
      <id>999</id>
      <title>Searcher class should have an abstract declaration of doc(int n, FieldSelector)</title>
      <description>Add: abstract public Document doc(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException; to Searcher, since it is defined by Searchable anyway, which Searcher implements. This would allows people using Searcher to have access to the FieldSelector mechanism. If there are no objections, I will commit this change by Monday, as I don't think it will break anything, because all derived classes already have to implement it anyway.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1000</id>
      <title>queryparsersyntax.html escaping section needs beefed up</title>
      <description>the query syntax documentation is currently lacking several key pieces of info: 1) that unicode style escapes are valid 2) that any character can be escaped with a backslash, not just special chars. ..we should probably beef up the "Escaping Special Characters" section</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1001</id>
      <title>Add Payload retrieval to Spans</title>
      <description>It will be nice to have access to payloads when doing SpanQuerys. See http://www.gossamer-threads.com/lists/lucene/java-dev/52270 and http://www.gossamer-threads.com/lists/lucene/java-dev/51134 Current API, added to Spans.java is below. I will try to post a patch as soon as I can figure out how to make it work for unordered spans (I believe I have all the other cases working). /** * Returns the payload data for the current span. * This is invalid until {@link #next()} is called for * the first time. * This method must not be called more than once after each call * of {@link #next()}. However, payloads are loaded lazily, * so if the payload data for the current position is not needed, * this method may not be called at all for performance reasons.&lt;br&gt; * &lt;br&gt; * &lt;p&gt;&lt;font color="#FF0000"&gt; * WARNING: The status of the &lt;b&gt;Payloads&lt;/b&gt; feature is experimental. * The APIs introduced here might change in the future and will not be * supported anymore in such a case.&lt;/font&gt; * * @return a List of byte arrays containing the data of this payload * @throws IOException */ // TODO: Remove warning after API has been finalized List/*&lt;byte[]&gt;*/ getPayload() throws IOException; /** * Checks if a payload can be loaded at this position. * &lt;p/&gt; * Payloads can only be loaded once per call to * {@link #next()}. * &lt;p/&gt; * &lt;p&gt;&lt;font color="#FF0000"&gt; * WARNING: The status of the &lt;b&gt;Payloads&lt;/b&gt; feature is experimental. * The APIs introduced here might change in the future and will not be * supported anymore in such a case.&lt;/font&gt; * * @return true if there is a payload available at this position that can be loaded */ // TODO: Remove warning after API has been finalized public boolean isPayloadAvailable();</description>
      <attachments/>
      <comments>36</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1007</id>
      <title>Flexibility to turn on/off any flush triggers</title>
      <description>See discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/53186 Provide the flexibility to turn on/off any flush triggers - ramBufferSize, maxBufferedDocs and maxBufferedDeleteTerms. One of ramBufferSize and maxBufferedDocs must be enabled.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1015</id>
      <title>FieldCache should support longs and doubles</title>
      <description>Would be nice if FieldCache supported longs and doubles</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1016</id>
      <title>TermVectorAccessor, transparent vector space access</title>
      <description>This class visits TermVectorMapper and populates it with information transparent by either passing it down to the default terms cache (documents indexed with Field.TermVector) or by resolving the inverted index.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1017</id>
      <title>BoostingTermQuery performance</title>
      <description>I have been experimenting with payloads and BoostingTermQuery, which I think are excellent additions to Lucene core. Currently, BoostingTermQuery extends SpanQuery. I would suggest changing this class to extend TermQuery and refactor the current version to something like 'BoostingSpanQuery'. The reason is rooted in performance. In my testing, I compared query throughput using TermQuery against 2 versions of BoostingTermQuery - the current one that extends SpanQuery and one that extends TermQuery (which I've included, below). Here are the results (qps = queries per second): TermQuery: 200 qps BoostingTermQuery (extends SpanQuery): 97 qps BoostingTermQuery (extends TermQuery): 130 qps Here is a version of BoostingTermQuery that extends TermQuery. I had to modify TermQuery and TermScorer to make them public. A code review would be in order, and I would appreciate your comments on this suggestion. Peter</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1019</id>
      <title>CustomScoreQuery should support multiple ValueSourceQueries</title>
      <description>CustomScoreQuery's constructor currently accepts a subQuery, and a ValueSourceQuery. I would like it to accept multiple ValueSourceQueries. The workaround of nested CustomScoreQueries works for simple cases, but it quickly becomes either cumbersome to manage, or impossible to implement the desired function. This patch implements CustomMultiScoreQuery with my desired functionality, and refactors CustomScoreQuery to implement the special case of a CustomMultiScoreQuery with 0 or 1 ValueSourceQueries. This keeps the CustomScoreQuery API intact. This patch includes basic tests, more or less taken from the original implementation, and customized a bit to cover the new cases.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1020</id>
      <title>Basic tool for checking &amp; repairing an index</title>
      <description>This has been requested a number of times on the mailing lists. Most recently here: http://www.gossamer-threads.com/lists/lucene/java-user/53474 I think we should provide a basic tool out of the box.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1024</id>
      <title>MarkovChainTokenFilter</title>
      <description>Merges chains of tokens to a single token, ngrams on a phrase word level rathar than on a word character level. Quite useful for text mining purposes of the vector space.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1025</id>
      <title>Document clusterer</title>
      <description>A two-dimensional desicion tree in conjunction with cosine coefficient similarity is the base of this document clusterer. It uses Lucene for tokenization and length normalization. Example output of 3500 clustered news articles dated the thee first days of January 2004 from a number of sources can be found here: &lt; http://ginandtonique.org/~kalle/LUCENE-1025/out_4.0.txt &gt;. One thing missing is automatic calculation of cluster boundaries. Not impossible to implement, nor is it really needed. 4.5 in the URL above is that distance. The example was calculated limited to the top 1000 terms from instance, divided with siblings and re-pruned all the way to the root. On my dual core it took about 100ms to insert a new document in the tree, no matter if it contained 100 or 10,000 instances. 1GB RAM held about 10,000 news articles. Next steps for this code is persistency of the tree using BDB or a even perhaps something similar to the Lucene segmented solution. Perhaps even using Lucene Directory. The plan is to keep this clusterer synchronized with the index, allowing really speedy "more like this" features. Later on I'll introduce map/reduce for better training speed. This code is far from perfect, nor is the results as good as many other products. Knowing I didn't put in more than a few handful of hours, this works quite well. By displaying neighboring clusters (as in the example) one will definetly get more related documents at a fairly low false-positive cost. Perhaps it would be interesting to analyse user behavior to find out if any of them could be merged. Perhaps some reinforcement learning? There are no ROC-curves, precision/recall-values nor tp/fp-rates as I have no manually clustered corpus for me to compare with. I've been looking for an archive of the Lucene-users forum for demonstrational use, but could not find it. Any ideas on where I can find that? It could for instance be neat to tweak this code to identify frequently asked questions and match it with an answer in the Wiki, but perhaps an SVM, NB or something-implementation would be better suited for that. Don't hesitate to comment on this if you have an idea, request or question.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1026</id>
      <title>Provide a simple way to concurrently access a Lucene index from multiple threads</title>
      <description>For building interactive indexes accessed through a network/internet (multiple threads). This builds upon the LuceneIndexAccessor patch. That patch was not very newbie friendly and did not properly handle MultiSearchers (or at the least made it easy to get into trouble). This patch simplifies things and provides out of the box support for sharing the IndexAccessors across threads. There is also a simple test class and example SearchServer to get you started. Future revisions will be zipped. Works pretty solid as is, but could use the ability to warm new Searchers.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1030</id>
      <title>"Read-only" IndexReaders</title>
      <description>IndexReaders are able to perform certain write operations on an index (setNorm(), deleteDocument()). This makes issues like LUCENE-743 more complicated and also duplicate code for acquiring locks and write transactions is necessary. Therefore we should make IndexReaders "read-only" in the future. However, we have to find a good solution to support setting norms and deleting documents by docid with the IndexWriter. See related discussions in LUCENE-743 and http://www.gossamer-threads.com/lists/lucene/java-dev/52017.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1031</id>
      <title>Fixes a handful of misspellings/mistakes in changes.txt</title>
      <description>There are a handful of misspellings/mistakes in changes.txt. This patch fixes them. Avoided the one or two British to English conversions &lt;g&gt;</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1032</id>
      <title>CJKAnalyzer should convert half width katakana to full width katakana</title>
      <description>Some of our Japanese customers are reporting errors when performing searches using half width characters. The desired behavior is that a document containing half width characters should be returned when performing a search using full width equivalents or when searching by the half width character itself. Currently, a search will not return any matches for half width characters. Here is a test case outlining desired behavior (this may require a new Analyzer). public class TestJapaneseEncodings extends TestCase { byte[] fullWidthKa = new byte[]{(byte) 0xE3, (byte) 0x82, (byte) 0xAB}; byte[] halfWidthKa = new byte[]{(byte) 0xEF, (byte) 0xBD, (byte) 0xB6}; public void testAnalyzerWithHalfWidth() throws IOException { Reader r1 = new StringReader(makeHalfWidthKa()); TokenStream stream = new CJKAnalyzer().tokenStream("foo", r1); assertNotNull(stream); Token token = stream.next(); assertNotNull(token); assertEquals(makeFullWidthKa(), token.termText()); } public void testAnalyzerWithFullWidth() throws IOException { Reader r1 = new StringReader(makeFullWidthKa()); TokenStream stream = new CJKAnalyzer().tokenStream("foo", r1); assertEquals(makeFullWidthKa(), stream.next().termText()); } private String makeFullWidthKa() throws UnsupportedEncodingException { return new String(fullWidthKa, "UTF-8"); } private String makeHalfWidthKa() throws UnsupportedEncodingException { return new String(halfWidthKa, "UTF-8"); } }</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1034</id>
      <title>Add new API method to retrieve document field data in a batch</title>
      <description>I've read in many forums about people who need to retrieve document data for a large number of search results. In our case, we need to retrieve up to 10,000 results (sometimes more) from an index of over 100 million documents (our index is about 65 GB). This can sometimes take a couple minutes. In one of my attempts to improve performance, I modified the IndexReader interface to provide a method which looks like: public Document[] documents(int[] n, FieldSelector fieldSelector); Instead of retrieving document data one at a time, I would request data for many document numbers in one shot. The idea was to optimize the seeks on disk so that in the FieldsReader, the seeks for the indexStream would be done first, then all the seeks in the fieldStream would be completed. For a large number of documents, this yielded a 20% speed improvement. The improvement was not as much as I was looking for, but I felt that the improvement was significant enough that I would request changes to the IndexReader interface. I'm providing patches for the files that I needed to change for our application. These patches are against the 2.2 release.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1035</id>
      <title>Optional Buffer Pool to Improve Search Performance</title>
      <description>Index in RAMDirectory provides better performance over that in FSDirectory. But many indexes cannot fit in memory or applications cannot afford to spend that much memory on index. On the other hand, because of locality, a reasonably sized buffer pool may provide good improvement over FSDirectory. This issue aims at providing such an optional buffer pool layer. In cases where it fits, i.e. a reasonable hit ratio can be achieved, it should provide a good improvement over FSDirectory.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1038</id>
      <title>TermVectorMapper.setDocumentNumber()</title>
      <description>Passes down the index of the document whose term vector is currently beeing mapped, once for each top level call to a term vector reader. See http://www.nabble.com/Allowing-IOExceptions-in-TermVectorMapper--tf4687704.html#a13397341</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1039</id>
      <title>Bayesian classifiers using Lucene as data store</title>
      <description>Bayesian classifiers using Lucene as data store. Based on the Naive Bayes and Fisher method algorithms as described by Toby Segaran in "Programming Collective Intelligence", ISBN 978-0-596-52932-1. Have fun. Poor java docs, but the TestCase shows how to use it: public class TestClassifier extends TestCase { public void test() throws Exception { InstanceFactory instanceFactory = new InstanceFactory() { public Document factory(String text, String _class) { Document doc = new Document(); doc.add(new Field("class", _class, Field.Store.YES, Field.Index.NO_NORMS)); doc.add(new Field("text", text, Field.Store.YES, Field.Index.NO, Field.TermVector.NO)); doc.add(new Field("text/ngrams/start", text, Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.YES)); doc.add(new Field("text/ngrams/inner", text, Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.YES)); doc.add(new Field("text/ngrams/end", text, Field.Store.NO, Field.Index.TOKENIZED, Field.TermVector.YES)); return doc; } Analyzer analyzer = new Analyzer() { private int minGram = 2; private int maxGram = 3; public TokenStream tokenStream(String fieldName, Reader reader) { TokenStream ts = new StandardTokenizer(reader); ts = new LowerCaseFilter(ts); if (fieldName.endsWith("/ngrams/start")) { ts = new EdgeNGramTokenFilter(ts, EdgeNGramTokenFilter.Side.FRONT, minGram, maxGram); } else if (fieldName.endsWith("/ngrams/inner")) { ts = new NGramTokenFilter(ts, minGram, maxGram); } else if (fieldName.endsWith("/ngrams/end")) { ts = new EdgeNGramTokenFilter(ts, EdgeNGramTokenFilter.Side.BACK, minGram, maxGram); } return ts; } }; public Analyzer getAnalyzer() { return analyzer; } }; Directory dir = new RAMDirectory(); new IndexWriter(dir, null, true).close(); Instances instances = new Instances(dir, instanceFactory, "class"); instances.addInstance("hello world", "en"); instances.addInstance("hallå världen", "sv"); instances.addInstance("this is london calling", "en"); instances.addInstance("detta är london som ringer", "sv"); instances.addInstance("john has a long mustache", "en"); instances.addInstance("john har en lång mustache", "sv"); instances.addInstance("all work and no play makes jack a dull boy", "en"); instances.addInstance("att bara arbeta och aldrig leka gör jack en trist gosse", "sv"); instances.addInstance("shrimp sandwich", "en"); instances.addInstance("räksmörgås", "sv"); instances.addInstance("it's now or never", "en"); instances.addInstance("det är nu eller aldrig", "sv"); instances.addInstance("to tie up at a landing-stage", "en"); instances.addInstance("att angöra en brygga", "sv"); instances.addInstance("it's now time for the children's television shows", "en"); instances.addInstance("nu är det dags för barnprogram", "sv"); instances.flush(); testClassifier(instances, new NaiveBayesClassifier()); testClassifier(instances, new FishersMethodClassifier()); instances.close(); } private void testClassifier(Instances instances, BayesianClassifier classifier) throws IOException { assertEquals("sv", classifier.classify(instances, "detta blir ett test")[0].getClassification()); assertEquals("en", classifier.classify(instances, "this will be a test")[0].getClassification()); // test training data instances. all ought to match! for (int documentNumber = 0; documentNumber &lt; instances.getIndexReader().maxDoc(); documentNumber++) { if (!instances.getIndexReader().isDeleted(documentNumber)) { Map&lt;Term, Double&gt; features = instances.extractFeatures(instances.getIndexReader(), documentNumber, classifier.isNormalized()); Document document = instances.getIndexReader().document(documentNumber); assertEquals(document.get("class"), classifier.classify(instances, features)[0].getClassification()); } } }</description>
      <attachments/>
      <comments>13</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1043</id>
      <title>Speedup merging of stored fields when field mapping "matches"</title>
      <description>Robert Engels suggested the following idea, here: http://www.gossamer-threads.com/lists/lucene/java-dev/54217 When merging in the stored fields from a segment, if the field name -&gt; number mapping is identical then we can simply bulk copy the entire entry for the document rather than re-interpreting and then re-writing the actual stored fields. I've pulled the code from the above thread and got it working on the current trunk.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1047</id>
      <title>Change MergePolicy &amp; MergeScheduler to be abstract base classes instead of an interfaces</title>
      <description>This gives us freedom to add methods with default base implementation over time w/o breaking backwards compatibility. Thanks to Hoss for raising this!</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1049</id>
      <title>Simple toString() for BooleanFilter</title>
      <description>While working with BooleanFilter I wanted a basic toString() for debugging. This is what I came up. It works ok for me.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1051</id>
      <title>Separate javadocs for core and contribs</title>
      <description>A while ago we had a discussion on java-dev about separating the javadocs for the contrib modules instead of having only one big javadoc containing the core and contrib classes. This patch: Adds new targets to build.xml: "javadocs-all" Generates Javadocs for the core, demo, and contrib classes "javadocs-core" Generates Javadocs for the core classes "javadocs-demo" Generates Javadocs for the demo classes "javadocs-contrib" Using contrib-crawl it generates the Javadocs for all contrib modules, except "similarity" (currently empty) and gdata. Adds submenues to the Javadocs link on the Lucene site with links to the different javadocs Includes the javadocs in the maven artifacts Remarks: I removed the ant target "javadocs-internal", because I didn't want to add corresponding targets for all new javadocs target. Instead I defined a new property "javadoc.access", so now "ant -Djavadoc.access=package" can be used in combination with any of the javadocs targets. Is this ok? I didn't include gdata (yet) because it uses build files that don't extend Lucenes standard build files. Here's a preview: http://people.apache.org/~buschmi/site-preview/index.html Please let me know what you think about these changes!</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1052</id>
      <title>Add an "termInfosIndexDivisor" to IndexReader</title>
      <description>The termIndexInterval, set during indexing time, let's you tradeoff how much RAM is used by a reader to load the indexed terms vs cost of seeking to the specific term you want to load. But the downside is you must set it at indexing time. This issue adds an indexDivisor to TermInfosReader so that on opening a reader you could further sub-sample the the termIndexInterval to use less RAM. EG a setting of 2 means every 2 * termIndexInterval is loaded into RAM. This is particularly useful if your index has a great many terms (eg you accidentally indexed binary terms). Spinoff from this thread: http://www.gossamer-threads.com/lists/lucene/java-dev/54371</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1056</id>
      <title>IndexReader.flush()</title>
      <description>With the new IndexReader.reopen() feature (LUCENE-743) it makes sense to also add a flush() method to IndexReader. Yonik pointed out this usecase: reader.deleteDocument() reader.flush() writer = new IndexWriter() writer.addDocument() writer.close() reader.reopen() reader.deleteDocument()</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1058</id>
      <title>New Analyzer for buffering tokens</title>
      <description>In some cases, it would be handy to have Analyzer/Tokenizer/TokenFilters that could siphon off certain tokens and store them in a buffer to be used later in the processing pipeline. For example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field. Patch to follow, but I am still not sure about a couple of things, mostly how it plays with the new reuse API. See http://www.gossamer-threads.com/lists/lucene/java-dev/54397?search_string=BufferingAnalyzer;#54397</description>
      <attachments/>
      <comments>37</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1059</id>
      <title>bad java practices which affect performance (result of code inspection)</title>
      <description>IntelliJ IDEA found the following issues in the Lucense source code and tests: 1) explicit for loops where calls to System.arraycopy() should have been 2) calls to Boolean constructor (in stead of the appropriate static method/field) 3) instantiation of unnecessary Integer instances for toString, instead of calling the static one 4) String concatenation using + inside a call to StringBuffer.append(), in stead of chaining the append calls all minor issues. patch is forthcoming.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1060</id>
      <title>Correct 2 minor javadoc mistakes in core, javadoc.access=private</title>
      <description>Patches Token.java and TermVectorsReader.java</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1061</id>
      <title>Adding a factory to QueryParser to instantiate query instances</title>
      <description>With the new efforts with Payload and scoring functions, it would be nice to plugin custom query implementations while using the same QueryParser. Included is a patch with some refactoring the QueryParser to take a factory that produces query instances.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1064</id>
      <title>Make TopDocs constructor public</title>
      <description>TopDocs constructor is package visible. This prevents instantiating it from outside this package. For example, I wrote a HitColletor that couldn't extend directly from TopDocCollector. I need to create a new TopDocs instance, however since the c'tor is package visible, I can't do that. For now, I completely duplicated the code, but I hope you'll fix it soon.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1066</id>
      <title>better explain output</title>
      <description>Very simple patch that slightly improves output of idf: show both docFreq and numDocs.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1073</id>
      <title>Add unit test showing how to do a "live backup" of an index</title>
      <description>The question of how to backup an index comes up every so often on the lists. Backing up and index is also clearly an important fundamental admin task that many applications need to do for fault tolerance. In the past you were forced to stop &amp; block all changes to your index, perform the backup, and then resume changes. But many applications cannot afford a potentially long pause in their indexing. With the addition of DeletionPolicy (LUCENE-710), it's now possible to do a "live backup", which means backup your index in the background without pausing ongoing changes to the index. This SnapshotDeletionPolicy just has to mark the chosen commit point as not deletable, until the backup finishes.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1074</id>
      <title>Workaround in Searcher.java for gcj bug#15411 no longer needed</title>
      <description>This gcj bug has meanwhile been fixed, see: http://gcc.gnu.org/bugzilla/show_bug.cgi?id=15411</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1076</id>
      <title>Allow MergePolicy to select non-contiguous merges</title>
      <description>I started work on this but with LUCENE-1044 I won't make much progress on it for a while, so I want to checkpoint my current state/patch. For backwards compatibility we must leave the default MergePolicy as selecting contiguous merges. This is necessary because some applications rely on "temporal monotonicity" of doc IDs, which means even though merges can re-number documents, the renumbering will always reflect the order in which the documents were added to the index. Still, for those apps that do not rely on this, we should offer a MergePolicy that is free to select the best merges regardless of whether they are continuguous. This requires fixing IndexWriter to accept such a merge, and, fixing LogMergePolicy to optionally allow it the freedom to do so.</description>
      <attachments/>
      <comments>40</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1077</id>
      <title>New Analysis Contributions</title>
      <description>With the advent of the new TeeTokenFilter and SinkTokenizer, there now exists some interesting new things that can be done in the analysis phase of indexing. See LUCENE-1058. This patch provides some new implementations of SinkTokenizer that may be useful.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1078</id>
      <title>Cleanup some unused and unnecessary code</title>
      <description>Several classes in trunk have some unused and unnecessary code. This includes unused fields, unused automatic variables, unused imports and unnecessary assignments. Attached it a patch to clean these up.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1079</id>
      <title>DocValues cleanup: constructor &amp; getInnerArray()</title>
      <description>DocValues constructor taking a numDocs parameter is not very clean. Get rid of this. Also, it's optional getInnerArray() method is not very clean. This is necessary for better testing, but currently tests will fail if it is not implemented. Modify it to throw UnSupportedOp exception (rather than returning an empty array). Modify tests to not fail but just warn if the tested iml does not override it. These changes should make it easier to implement DocValues for other ValueSource's, e.g. above payloads, with or without caching.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1080</id>
      <title>Make Token.DEFAULT_TYPE public</title>
      <description>Make Token.DEFAULT_TYPE public so that TokenFilters using the reusable Token model have a way of setting the type back to the default. No patch necessary. I will commit soon.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1083</id>
      <title>JDiff report of changes between different versions of Lucene</title>
      <description>I think that a helpful addition to the release process for Lucene would be JDiff reports of the API changes between different versions. I am attaching reports of the differences between 1.9.1 and 2.2.0 and also between 2.1.0 and 2.2.0. The reports could be changed to only show the public methods. The start page is changes.html. This is the Ant target I added to the top-level build.xml file in the JDiff directory to produce a report: &lt;target name="lucene" depends="dist"&gt; &lt;taskdef name="jdiff" classname="jdiff.JDiffAntTask" classpath="${dist.dir}/antjdiff.jar" /&gt; &lt;jdiff destdir="${reports.dir}/lucene" verbose="on" stats="on" docchanges="on"&gt; &lt;old name="1.9.1"&gt; &lt;dirset dir="${examples.dir}/lucene-1.9.1/src/java" includes="org/**" /&gt; &lt;/old&gt; &lt;new name="2.2.0"&gt; &lt;dirset dir="${examples.dir}/lucene-2.2.0/src/java" includes="org/**" /&gt; &lt;/new&gt; &lt;/jdiff&gt; &lt;/target&gt; Disclaimer: I'm the author of JDiff</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1084</id>
      <title>increase default maxFieldLength?</title>
      <description>To my understanding, Lucene 2.3 will easily index large documents. So shouldn't we get rid of the 10,000 default limit for the field length? 10,000 isn't that much and as Lucene doesn't have any error logging by default, this is a common problem for users that is difficult to debug if you don't know where to look. A better new default might be Integer.MAX_VALUE.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1085</id>
      <title>search.function should support all capabilities of Solr's search.function</title>
      <description>Lucene search.function does not allow Solr to move to use it, and so Solr currently maintains its own version of this package. Enhance Lucene's search.function so that Solr can move to use it, and avoid this redundancy.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1088</id>
      <title>PriorityQueue 'wouldBeInserted' method</title>
      <description>This is a request for a new method in PriorityQueue public boolean wouldBeInserted(Object element) // returns true if doc would be inserted, without inserting This would allow an application to prevent duplicate entries from being added to the queue. Here is a reference to the discussion behind this request: http://www.nabble.com/FieldSortedHitQueue-enhancement-to9733550.html#a9733550</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1089</id>
      <title>Add insertWithOverflow to PriorityQueue</title>
      <description>This feature proposes to add an insertWithOverflow to PriorityQueue so that callers can reuse the objects that are being dropped off the queue. Also, it changes heap to protected for easier extensibility of PQ</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1090</id>
      <title>remove relative paths assumptions from benchmark code</title>
      <description>Also see Eric comments in: http://www.nabble.com/forum/ViewPost.jtp?post=14347924&amp;framed=y Benchmark's config.xml relies on relative paths, more or less like this; base-dir conf-dir work-dir docs-dir indexes-dir These assumptions are also in the Java code, and so it is inconvenient for using absolute paths, e.g. for specifying a docs dir that is not under work-dir. Relax this by modifying in build.xml to replace "value" and "line" props by "location" and "file" and by requiring absolute paths in the Java code.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1093</id>
      <title>SpanFirstQuery modification to aid term boosting based on position.</title>
      <description>This is a request for a modification to SpanFirstQuery that would allow term boosting based on a term's distance from the beginning of the document. This modification to SpanFirstQuery would be that the Spans returned by SpanFirstQuery.getSpans() must always return 0 from its start() method. Then the slop passed to sloppyFreq(slop) would be the distance from the beginning of the indexed field to the end of the Spans of the SpanQuery passed to SpanFirstQuery. Here is the discussion behind this issue: http://www.nabble.com/Can-I-do-boosting-based-on-term-postions--to11939423.html#a11939423</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1095</id>
      <title>StopFilter should have option to incr positionIncrement after stop word</title>
      <description>I've seen this come up on the mailing list a few times in the last month, so i'm filing a known bug/improvement arround it... StopFilter should have an option that if set, records how many stop words are "skipped" in a row, and then sets that value as the positionIncrement on the "next" token that StopFilter does return.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1098</id>
      <title>Small performance enhancement for StandardAnalyzer</title>
      <description>The class StandardAnalyzer has an inner class, SavedStreams, which is used internally for maintaining some state. This class doesn't use the implicit reference to the enclosing class, so it can be made static and reduce some memory requirements. A patch will be attached shortly.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1099</id>
      <title>Making Tokenizer.reset(Reader) public</title>
      <description>In order to implement reusableTokenStream and be able to reset a Tokenizer, Tokenizer defines a reset(Reader) method. The problem is that this method is protected. I need to call this reset(Reader) method without having to know in advance what will be the type of the Tokenizer (I plan to have several). I noticed that almost all Tokenizer extensions define this method as public, and I wonder if this can be changed for Tokenizer also (I can't simply create my general Tokenizer extension and inherit from it because I want to use StandardTokenizer as well).</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1102</id>
      <title>EnwikiDocMaker id field</title>
      <description>The EnwikiDocMaker is fairly usable outside of the benchmarking class, but it would benefit from indexing the ID field of the docs. Patch to follow that adds an ID field.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1103</id>
      <title>WikipediaTokenizer</title>
      <description>I have extended StandardTokenizer to recognize Wikipedia syntax and mark tokens with certain attributes. It isn't necessarily complete, but it does a good enough job for it to be consumed and improved by others. It sets the Token.type() value depending on the Wikipedia syntax (links, internal links, bold, italics, etc.) based on my pass at http://en.wikipedia.org/wiki/Wikipedia:Tutorial I have only tested it with the benchmarking EnwikiDocMaker wikipedia stuff and it seems to do a decent job. Caveats: I am not sure how to best handle testing, since the content is licensed under GNU Free Doc License, I believe I can't copy and paste a whole document into the unit test. I have hand coded one doc and have another one that just generally runs over the benchmark Wikipedia download. One more question is where to put it. It could go in analysis, but the tests at least will have a dependency on Benchmark. I am thinking of adding a new contrib/wikipedia where this could grow to have other wikipedia things (perhaps we would move EnwikiDocMaker there????) and reverse the dependency on Benchmark. I will post a patch over the next few days.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1116</id>
      <title>contrib.benchmark.quality package improvements</title>
      <description>Few fixes and improvements for the search quality benchmark package: flush report and logger at the end (otherwise long submission reports might miss last lines). add run-tag-name to submission report (API change). add control over max-#queries to run (useful at debugging a quality evaluation setup). move control over max-docs-to-retrieve from benchmark constructor to a setter method (API change). add computation of Mean Reciprocal Rank (MRR) in QualityStats. QualityStats fixed to not fail if there are no results to average. Add a TREC queries reader adequate for the 1MQ track (track started 2007). All tests pass, will commit this in 1-2 days if there is no objection.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1118</id>
      <title>core analyzers should not produce tokens &gt; N (100?) characters in length</title>
      <description>Discussion that led to this: http://www.gossamer-threads.com/lists/lucene/java-dev/56103 I believe nearly any time a token &gt; 100 characters in length is produced, it's a bug in the analysis that the user is not aware of. These long tokens cause all sorts of problems, downstream, so it's best to catch them early at the source. We can accomplish this by tacking on a LengthFilter onto the chains for StandardAnalyzer, SimpleAnalyzer, WhitespaceAnalyzer, etc. Should we do this in 2.3? I realize this is technically a break in backwards compatibility, however, I think it must be incredibly rare that this break would in fact break something real in the application?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1119</id>
      <title>Optimize TermInfosWriter.add</title>
      <description>I found one more optimization, in how terms are written in TermInfosWriter. Previously, each term required a new Term() and a new String(). Looking at the cpu time (using YourKit), I could see this was adding a non-trivial cost to flush() when indexing Wikipedia. I changed TermInfosWriter.add to accept char[] directly, instead. I ran a quick test building first 200K docs of Wikipedia. With this fix it took 231.31 sec (best of 3) and without the fix it took 236.05 sec (best of 3) = ~2% speedup.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1120</id>
      <title>Use bulk-byte-copy when merging term vectors</title>
      <description>Indexing all of Wikipedia, with term vectors on, under the YourKit profiler, shows that 26% of the time (!!) was spent merging the vectors. This was without offsets &amp; positions, which would make matters even worse. Depressingly, merging, even with ConcurrentMergeScheduler, cannot in fact keep up with the flushing of new segments in this test, and this is on a strong IO system (Mac Pro with 4 drive RAID 0 array, 4 CPU cores). So, just like Robert's idea to merge stored fields with bulk copying whenever the field name-&gt;number mapping is "congruent" (LUCENE-1043), we can do the same with term vectors. It's a little trickier because the term vectors format doesn't quite make it easy to bulk-copy because it doesn't directly encode the offset into the tvf file. I worked out a patch that changes the tvx format slightly, by storing the absolute position in the tvf file for the start of each document into the tvx file, just like it does for tvd now. This adds an extra 8 bytes (long) in the tvx file, per document. Then, I removed a vLong (the first "position" stored inside the tvd file), which makes tvd contents fully position independent (so you can just copy the bytes). This adds up to 7 bytes per document (less for larger indices) that have term vectors enabled, but I think this small increase in index size is acceptable for the gains in indexing performance? With this change, the time spent merging term vectors dropped from 26% to 3%. Of course, this only applies if your documents are "regular". I think in the future we could have Lucene try hard to assign the same field number for a given field name, if it had been seen before in the index... Merging terms now dominates the merge cost (~20% over overall time building the Wikipedia index). I also beefed up TestBackwardsCompatibility unit test: test a non-CFS and a CFS of versions 1.9, 2.0, 2.1, 2.2 index formats, and added some term vector fields to these indices.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1121</id>
      <title>Use nio.transferTo when copying large blocks of bytes</title>
      <description>When building a CFS file, and also when merging stored fields (and term vectors, with LUCENE-1120), we copy large blocks of bytes at once. We currently do this with an intermediate buffer. But, nio.transferTo should be somewhat faster on OS's that offer low level IO APIs for moving blocks of bytes between files.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1122</id>
      <title>queryparser whitespace escaping and documentation?</title>
      <description>as noted in this solr thread... http://www.nabble.com/PhraseQuery-and-WildcardQuery-to14503609.html#a14503609 ...it's possible to escape a whitespace character in the value of a term or prefix query by using a backslash so that the QueryParser will not treat it as "special" (ie: won't split on it when dividing the input into chunks for analysis). at a minimum, this should be noted here... http://lucene.apache.org/java/docs/queryparsersyntax.html#Escaping%20Special%20Characters ...but it got me wondering... is this a side effect of something else, or will QueryParser really respect this everywhere? even in field names? (i haven't tested) ... i think this is a result of QueryParser allowing you to escape any character even if it isn't "special" to the syntax shouldn't a space be considered "special" since it does trigger certain behavior? ... so shouldn't QueryParser.escape(String) escape spaces as well as the other special characters?</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1124</id>
      <title>short circuit FuzzyQuery.rewrite when input token length is small compared to minSimilarity</title>
      <description>I found this (unreplied to) email floating around in my Lucene folder from during the holidays... From: Timo Nentwig To: java-dev Subject: Fuzzy makes no sense for short tokens Date: Mon, 31 Dec 2007 16:01:11 +0100 Message-Id: &lt;200712311601.12255.lucene@nitwit.de&gt; Hi! it generally makes no sense to search fuzzy for short tokens because changing even only a single character of course already results in a high edit distance. So it actually only makes sense in this case: if( token.length() &gt; 1f / (1f - minSimilarity) ) E.g. changing one character in a 3-letter token (foo) results in an edit distance of 0.6. And if minSimilarity (which is by default: 0.5 :-) is higher we can save all the expensive rewrite() logic. I don't know much about FuzzyQueries, but this reasoning seems sound ... FuzzyQuery.rewrite should be able to completely skip all TermEnumeration in the event that the input token is shorter then some simple math on the minSimilarity. (i'm not smart enough to be certain that the math above is right however ... it's been a while since i looked at Levenstein distances ... tests needed)</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1126</id>
      <title>Simplify StandardTokenizer JFlex grammar</title>
      <description>Summary of thread entitled "Fullwidth alphanumeric characters, plus a question on Korean ranges" begun by Daniel Noll on java-user, and carried over to java-dev: On 01/07/2008 at 5:06 PM, Daniel Noll wrote: &gt; I wish the tokeniser could just use Character.isLetter and &gt; Character.isDigit instead of having to know all the ranges itself, since &gt; the JRE already has all this information. Character.isLetter does &gt; return true for CJK characters though, so the ranges would still come in &gt; handy for determining what kind of letter they are. I don't support &gt; JFlex has a way to do this... The DIGIT macro could be replaced by JFlex's predefined character class [:digit:], which has the same semantics as java.lang.Character.isDigit(). Although JFlex's predefined character class [:letter:] (same semantics as java.lang.Character.isLetter()) includes CJK characters, there is a way to handle this using JFlex's regex negation syntax !. From the JFlex documentation: [T]he expression that matches everything of a not matched by b is !(!a|b) So to exclude CJ characters from the LETTER macro: LETTER = ! ( ! [:letter:] | {CJ} ) Since [:letter:] includes all of the Korean ranges, there's no reason (AFAICT) to treat them separately; unlike Chinese and Japanese characters, which are individually tokenized, the Korean characters should participate in the same token boundary rules as all of the other letters. I looked at some of the differences between Unicode 3.0.0, which Java 1.4.2 supports, and Unicode 5.0, the latest version, and there are lots of new and modified letter and digit ranges. This stuff gets tweaked all the time, and I don't think Lucene should be in the business of trying to track it, or take a position on which Unicode version users' data should conform to. Switching to using JFlex's [:letter:] and [:digit:] predefined character classes ties (most of) these decisions to the user's choice of JVM version, and this seems much more reasonable to me than the current status quo. I will attach a patch shortly.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1127</id>
      <title>TokenSources.getTokenStream(Document...)</title>
      <description>Sometimes, one already has the Document, and just needs to generate a TokenStream from it, so I am going to add a convenience method to TokenSources. Sometimes, you also already have just the string, so I will add a convenience method for that.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1128</id>
      <title>Add Highlighting benchmark support to contrib/benchmark</title>
      <description>I would like to be able to test the performance (speed, initially) of the Highlighter in a standard way. Patch to follow that adds the Highlighter as a dependency benchmark and adds in tasks extending the ReadTask to perform highlighting on retrieved documents.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1131</id>
      <title>Add numDeletedDocs to IndexReader</title>
      <description>Add numDeletedDocs to IndexReader. Basically, the implementation is as simple as doing: public int numDeletedDocs() { return deletedDocs == null ? 0 : deletedDocs.count(); } in SegmentReader. Patch to follow to include in all IndexReader extensions.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1133</id>
      <title>WikipediaTokenizer needs a way of not tokenizing certain parts of the text</title>
      <description>It would be nice if the WikipediaTokenizer had a way of, via a flag, leaving categories, links, etc. as single tokens (or at least some parts of them) Thus, if we came across [[Category:My Big Idea]] there would be a way of outputting, as a single token "My Big Idea". Optionally, it would be good to output both "My Big Idea" and the individual tokens as well. I am not sure of how to do this in JFlex, so any insight would be appreciated.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1136</id>
      <title>add ability to not count sub-task doLogic increment to contri/benchmark</title>
      <description>Sometimes, you want to run a sub-task like CloseIndex, and include the time it takes to run, but not include the count that it returns when reporting rec/s. We could adopt this approach: if a task is preceded by a "-" character, then, do not count the value returned by doLogic. See discussion leading to this here: http://www.gossamer-threads.com/lists/lucene/java-dev/57081</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1137</id>
      <title>Token type as BitSet: typeBits()</title>
      <description>It is sometimes useful to have a more compact, easy to parse, type representation for Token than the current type() String. This patch adds a BitSet onto Token, defaulting to null, with accessors for setting bit flags on a Token. This is useful for communicating information about a token to TokenFilters further down the chain. For example, in the WikipediaTokenizer, the possibility exists that a token could be both a category and bold (or many other variations), yet it is difficult to communicate this without adding in a lot of different Strings for type. Unlike using the payload information (which could serve this purpose), the BitSet does not get added to the index (although one could easily convert it to a payload.)</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1139</id>
      <title>Various small improvements to contrib/benchmark</title>
      <description>I've worked out a few small improvements to contrib/benchmark: Refactored the common code in Open/CreateIndexTask that sets the configuration for the IndexWriter. This also fixes a bug in OpenIndexTasks that prevented you from disabling flushing by RAM. Added a new config property for LineDocMaker: doc.reuse.fields=true|false which turns on/off reusing of Field/Document by LineDocMaker. This lets us measure performance impact of sharing Field/Document vs not, and also turn it off when necessary (eg if you have your own consumer that uses private threads). Added merge.scheduler &amp; merge.policy config options. Added param for OptimizeTask, which expects an int and calls optimize(maxNumSegments) with that param. Added param for CloseIndex(true|false) – if you pass false that means close the index, aborting any running merges</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1142</id>
      <title>Updated Snowball package</title>
      <description>Updated Snowball contrib package New org.tartarus.snowball java package with patched SnowballProgram to be abstract to avoid using reflection. Introducing Hungarian, Turkish and Romanian stemmers Introducing constructor SnowballFilter(SnowballProgram) It is possible there have been some changes made to the some of there stemmer algorithms between this patch and the current SVN trunk of Lucene, an index might thus not be compatible with new stemmers! The API is backwards compatibile and the test pass.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1145</id>
      <title>DisjunctionSumScorer small tweak</title>
      <description>Move ScorerDocQueue initialization from next() and skipTo() methods to the Constructor. Makes DisjunctionSumScorer a bit faster (less than 1% on my tests). Downside (if this is one, I cannot judge) would be throwing IOException from DisjunctionSumScorer constructors as we touch HardDisk there. I see no problem as this IOException does not propagate too far (the only modification I made is in BooleanScorer2) if (scorerDocQueue == null) { initScorerDocQueue(); } Attached test is just quick &amp; dirty rip of TestScorerPerf from standard Lucene test package. Not included as patch as I do not like it. All test pass, patch made on trunk revision 613923</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1146</id>
      <title>ConjunctionScorer small (ca. 3.5%) optimization</title>
      <description>ConjunctionScorer initialization is done lazy in next() and skipTo() methods, using one if(firstTime) check, this patch moves this initialization to the Constructor. Constructor already throws an IOException. speed-up on jdk 5 &amp; 6 is in the 3.5% - 4% range. Speed-up was measured with standard TestScorerPerf test in Lucene test package (very dense bit sets) . Similar issue is with: https://issues.apache.org/jira/browse/LUCENE-1145?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel patch made on trunk revision: 614219</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1147</id>
      <title>add option to CheckIndex to only check certain segments</title>
      <description>Simple patch to add -segment option to CheckIndex tool, to have it only check the particular segment, instead of all segments, from your index.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1148</id>
      <title>Create a new sub-class of SpanQuery to enable use of a RangeQuery within a SpanQuery</title>
      <description>Our users express queries using a syntax which enables them to embed various query types within SpanQuery instances. One feature they've been asking for is the ability to embed a numeric range query so they could, for example, find documents matching "[2.0 2.75]MHz". The attached patch adds the capability and I hope others will find it useful.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1149</id>
      <title>add XA transaction support</title>
      <description>Need to add XA transaction support to Lucene. Without XA support, it is difficult to keep disparate resources (e.g. database) in sync with the Lucene index. A review of the XA support added to Hibernate might be a good start (although Hibernate almost always uses a XA capable backing store database). It would be ideal to have a combined IndexReaderWriter instance, then create a XAIndexReaderWriter which wraps it. The implementation might be as simple as a XA log file which lists the XA transaction id, and the segments XXX number(s), since Lucene already allows you to rollback to a previous version (??? for sure, or does it only allow you to abort the current commit). If operating under a XA transaction, the no explicit commits or rollbacks should be allowed on the instance. The index would be committed during XA prepare(), and then if needed rolledback when requested. The XA commit() would be a no-op. There is a lot more to this but this should get the ball rolling.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1151</id>
      <title>Fix StandardAnalyzer to not mis-identify HOST as ACRONYM by default</title>
      <description>Coming out of the discussion around back compatibility, it seems best to default StandardAnalyzer to properly fix LUCENE-1068, while preserving the ability to get the back-compatible behavior in the rare event that it's desired. This just means changing the replaceInvalidAcronym = false with = true, and, adding a clear entry to CHANGES.txt that this very slight non back compatible change took place. Spinoff from here: http://www.gossamer-threads.com/lists/lucene/java-dev/57517#57517 I'll commit that change in a day or two.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1155</id>
      <title>BoostingTermQuery#defaultTermBoost</title>
      <description>This patch allows a null payload to mean something different than 1f. (I have this use case where 99% of my tokens share the same rather large token position payload boost.) Index: src/java/org/apache/lucene/search/payloads/BoostingTermQuery.java =================================================================== --- src/java/org/apache/lucene/search/payloads/BoostingTermQuery.java (revision 615215) +++ src/java/org/apache/lucene/search/payloads/BoostingTermQuery.java (working copy) @@ -41,11 +41,16 @@ */ public class BoostingTermQuery extends SpanTermQuery{ + private Float defaultTermBoost = null; public BoostingTermQuery(Term term) { super(term); } + public BoostingTermQuery(Term term, Float defaultTermBoost) { + super(term); + this.defaultTermBoost = defaultTermBoost; + } protected Weight createWeight(Searcher searcher) throws IOException { return new BoostingTermWeight(this, searcher); @@ -107,7 +112,9 @@ payload = positions.getPayload(payload, 0); payloadScore += similarity.scorePayload(term.field(), payload, 0, positions.getPayloadLength()); payloadsSeen++; - + } else if (defaultTermBoost != null) { + payloadScore += defaultTermBoost; + payloadsSeen++; } else { //zero out the payload? } @@ -146,7 +153,14 @@ } + public Float getDefaultTermBoost() { + return defaultTermBoost; + } + public void setDefaultTermBoost(Float defaultTermBoost) { + this.defaultTermBoost = defaultTermBoost; + } + public boolean equals(Object o) { if (!(o instanceof BoostingTermQuery)) return false;</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1157</id>
      <title>Formatable changes log (CHANGES.txt is easy to edit but not so friendly to read by Lucene users)</title>
      <description>Background in http://www.nabble.com/formatable-changes-log-tt15078749.html</description>
      <attachments/>
      <comments>49</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>1161</id>
      <title>Punctuation handling in StandardTokenizer (and WikipediaTokenizer)</title>
      <description>It would be useful, in the StandardTokenizer, to be able to have more control over in-word punctuation is handled. For instance, it is not always desirable to split on dashes or other punctuation. In other cases, one may want to output the split tokens plus a collapsed version of the token that removes the punctuation. For example, Solr's WordDelimiterFilter provides some nice capabilities here, but it can't do it's job when using the StandardTokenizer because the StandardTokenizer already makes the decision on how to handle it without giving the user any choice. I think, in JFlex, we can have a back-compatible way of letting users make decisions about punctuation that occurs inside of a token. Such as e-bay or i-pod, thus allowing for matches on iPod and eBay.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1162</id>
      <title>Improve architecture of FieldSortedHitQueue</title>
      <description>Per the discussion (quite some time ago) on issue LUCENE-806, I'd like to propose an architecture change to the way FieldSortedHitQueue works, and in particular the way it creates SortComparatorSources. I think (I hope) that anyone who looks at the FSHQ code will agree that the class does a lot and much of it's repetitive stuff that really has no business being in that class. I am about to attach a patch which, in and of itself, doesn't really achieve much that's concrete but does tidy things up a great deal and makes it easier to plug in different behaviours. I then have a subsequent patch which provides a fairly simple and flexible example of how you might replace an implementation, in this case the field-local-String-comparator version from LUCENE-806. The downside to this patch is that it involved changing the signature of SortComparatorSource.newComparator to take a Locale. There would be ways around this (letting FieldSortedHitQueue take in either a SortComparatorSource or some new, improved interface which takes a Locale (and possibly extends SortComparatorSource). I'm open to this but personally I think that the Locale version makes sense and would suggest that the code would be nicer by breaking the API (and hence targeting this to, presumably, 3.0 at a minimum). This code does not include specific tests (I will add these, if people like the general idea I'm proposing here) but all current tests pass with this change. Patch to follow.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1165</id>
      <title>Reduce exposure of nightly build documentation</title>
      <description>From LUCENE-1157 - ..the nightly build documentation is too prominent. A search for "indexwriter api" on Google or Yahoo! returns nightly documentation before released documentation. (https://issues.apache.org/jira/browse/LUCENE-1157?focusedCommentId=12565820#action_12565820)</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1166</id>
      <title>A tokenfilter to decompose compound words</title>
      <description>A tokenfilter to decompose compound words you find in many germanic languages (like German, Swedish, ...) into single tokens. An example: Donaudampfschiff would be decomposed to Donau, dampf, schiff so that you can find the word even when you only enter "Schiff". I use the hyphenation code from the Apache XML project FOP (http://xmlgraphics.apache.org/fop/) to do the first step of decomposition. Currently I use the FOP jars directly. I only use a handful of classes from the FOP project. My question now: Would it be OK to copy this classes over to the Lucene project (renaming the packages of course) or should I stick with the dependency to the FOP jars? The FOP code uses the ASF V2 license as well. What do you think?</description>
      <attachments/>
      <comments>33</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1172</id>
      <title>Small speedups to DocumentsWriter</title>
      <description>Some small fixes that I found while profiling indexing Wikipedia, mainly using our own quickSort instead of Arrays.sort. Testing first 200K docs of Wikipedia shows a speedup from 274.6 seconds to 270.2 seconds. I'll commit in a day or two.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1181</id>
      <title>Token reuse is not ideal for avoiding array copies</title>
      <description>The way the Token API is currently written results in two unnecessary array copies which could be avoided by changing the way it works. 1. setTermBuffer(char[],int,int) calls resizeTermBuffer(int) which copies the original term text even though it's about to be overwritten. #1 should be trivially fixable by introducing a private resizeTermBuffer(int,boolean) where the new boolean parameter specifies whether the existing term data gets copied over or not. 2. setTermBuffer(char[],int,int) copies what you pass in, instead of actually setting the term buffer. Setting aside the fact that the setTermBuffer method is misleadingly named, consider a token filter which performs Unicode normalisation on each token. How it has to be implemented at present: once: create a reusable char[] for storing the normalisation result every token: use getTermBuffer() and getTermLength() to get the buffer and relevant length normalise the original string into our temporary buffer (if it isn't big enough, grow the temp buffer size.) setTermBuffer(byte[],int,int) - this does an extra copy. The following sequence would be much better: once: create a reusable char[] for storing the normalisation result every token: use getTermBuffer() and getTermLength() to get the buffer and relevant length normalise the original string into our temporary buffer (if it isn't big enough, grow the temp buffer size.) setTermBuffer(byte[],int,int) sets in our buffer by reference set the term buffer which used to be in the Token such that it becomes our new temp buffer. The latter sequence results in no copying with the exception of the normalisation itself, which is unavoidable.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1183</id>
      <title>TRStringDistance uses way too much memory (with patch)</title>
      <description>The implementation of TRStringDistance is based on version 2.1 of org.apache.commons.lang.StringUtils#getLevenshteinDistance(String, String), which uses an un-optimized implementation of the Levenshtein Distance algorithm (it uses way too much memory). Please see Bug 38911 (http://issues.apache.org/bugzilla/show_bug.cgi?id=38911) for more information. The commons-lang implementation has been heavily optimized as of version 2.2 (3x speed-up). I have reported the new implementation to TRStringDistance.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1184</id>
      <title>Allow SnapshotDeletionPolicy to be reused across writer close/open</title>
      <description>If you re-use the same instance of SnapshotDeletionPolicy across a close/open of your writer, and you had a snapshot open, it can still be removed when the 2nd writer is opened. This is because SDP is comparing IndexCommitPoint instances. The fix is to instead compare segments file names. I've also changed the inner class IndexFileDeleter.CommitPoint to be static so an instance of SnapshotDeletionPolicy does not hold references to IndexFileDeleter, DocumentsWriter, etc. Spinoff from here: http://markmail.org/message/bojgqfgyxkkv4fyb</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1185</id>
      <title>[PATCH] Avoid checking for TermBuffer in SegmentTermEnum#scanTo</title>
      <description>It seems that SegmentTermEnum#scanTo is a critical method which is called very often, especially whenever we iterate over a sequence of terms in an index. When that method is called, the first thing happens is that it checks whether a temporary TermBuffer "scratch" has already been initialized. In fact, this is not necessary. We can simply declare and initialize the "scratch"-Buffer at the class-level (right now, the initial value is null). Java's lazy-loading capabilities allow this without adding any memory footprint for cases where we do not need that buffer. The attached patch takes care of this. We now save one comparison per term. In addition to that, the patch renames "scratch" to "scanBuffer", which aligns with the naming of the other two buffers that are declared in the class.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1186</id>
      <title>[PATCH] Clear ThreadLocal instances in close()</title>
      <description>As already found out in LUCENE-436, there seems to be a garbage collection problem with ThreadLocals at certain constellations, resulting in an OutOfMemoryError. The resolution there was to remove the reference to the ThreadLocal value when calling the close() method of the affected classes (see FieldsReader and TermInfosReader). For Java &lt; 5.0, this can effectively be done by calling threadLocal.set(null); for Java &gt;= 5.0, we would call threadLocal.remove() Analogously, this should be done in any class which creates ThreadLocal values Right now, two classes of the core API make use of ThreadLocals, but do not properly remove their references to the ThreadLocal value 1. org.apache.lucene.index.SegmentReader 2. org.apache.lucene.analysis.Analyzer For SegmentReader, I have attached a simple patch. For Analyzer, there currently is no patch because Analyzer does not provide a close() method (future to-do?)</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1187</id>
      <title>Things to be done now that Filter is independent from BitSet</title>
      <description>(Aside: where is the documentation on how to mark up text in jira comments?) The following things are left over after LUCENE-584 : For Lucene 3.0 Filter.bits() will have to be removed. There is a CHECKME in IndexSearcher about using ConjunctionScorer to have the boolean behaviour of a Filter. I have not looked into Filter caching yet, but I suppose there will be some room for improvement there. Iirc the current core has moved to use OpenBitSetFilter and that is probably what is being cached. In some cases it might be better to cache a SortedVIntList instead. Boolean logic on DocIdSetIterator is already available for Scorers (that inherit from DocIdSetIterator) in the search package. This is currently implemented by ConjunctionScorer, DisjunctionSumScorer, ReqOptSumScorer and ReqExclScorer. Boolean logic on BitSets is available in contrib/misc and contrib/queries DisjunctionSumScorer calls score() on its subscorers before the score value actually needed. This could be a reason to introduce a DisjunctionDocIdSetIterator, perhaps as a superclass of DisjunctionSumScorer. To fully implement non scoring queries a TermDocIdSetIterator will be needed, perhaps as a superclass of TermScorer. The javadocs in org.apache.lucene.search using matching vs non-zero score: I'll investigate this soon, and provide a patch when necessary. An early version of the patches of LUCENE-584 contained a class Matcher, that differs from the current DocIdSet in that Matcher has an explain() method. It remains to be seen whether such a Matcher could be useful between DocIdSet and Scorer. The semantics of scorer.skipTo(scorer.doc()) was discussed briefly. This was also discussed at another issue recently, so perhaps it is wortwhile to open a separate issue for this. Skipping on a SortedVIntList is done using linear search, this could be improved by adding multilevel skiplist info much like in the Lucene index for documents containing a term. One comment by me of 3 Dec 2008: A few complete (test) classes are deprecated, it might be good to add the target release for removal there.</description>
      <attachments/>
      <comments>40</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1188</id>
      <title>equals and hashCode implementation in org.apache.lucene.search.* package</title>
      <description>I would like to talk about the implementation of equals and hashCode method in org.apache.lucene.search.* package. Example One: org.apache.lucene.search.spans.SpanTermQuery (Super Class) &lt;- org.apache.lucene.search.payloads.BoostingTermQuery (Sub Class) Observation: BoostingTermQuery defines equals but inherits hashCode from SpanTermQuery. Definition of equals is a code clone of SpanTermQuery with a change in class name. Intention: I believe the intention of equals redefinition in BoostingTermQuery is not to make the objects of SpanTermQuery and BoostingTermQuery comparable. ie. spanTermQuery.equals(boostingTermQuery) == false &amp;&amp; boostingTermQuery.equals(spanTermQuery) == false. Problem: With current implementation, the intention might not be respected as a result of symmetric property violation of equals contract i.e. spanTermQuery.equals(boostingTermQuery) == true (can be) &amp;&amp; boostingTermQuery.equals(spanTermQuery) == false. (always) (Note: Provided their state variables are equal) Solution: Change implementation of equals in SpanTermQuery from: SpanTermQuery.java public boolean equals(Object o) { if (!(o instanceof SpanTermQuery)) return false; SpanTermQuery other = (SpanTermQuery)o; return (this.getBoost() == other.getBoost()) &amp;&amp; this.term.equals(other.term); } To: SpanTermQuery.java public boolean equals(Object o) { if(o == this) return true; if(o == null || o.getClass() != this.getClass()) return false; // if (!(o instanceof SpanTermQuery)) // return false; SpanTermQuery other = (SpanTermQuery)o; return (this.getBoost() == other.getBoost()) &amp;&amp; this.term.equals(other.term); } Advantage: BoostingTermQuery.equals and BoostingTermQuery.hashCode is not needed while still preserving the same intention as before. Any further subclassing that does not add new state variables in the extended classes of SpanTermQuery, does not have to redefine equals and hashCode. Even if a new state variable is added in a subclass, the symmetric property of equals contract will still be respected irrespective of implementation (i.e. instanceof / getClass) of equals and hashCode in the subclasses. Example Two: org.apache.lucene.search.CachingWrapperFilter (Super Class) &lt;- org.apache.lucene.search.CachingWrapperFilterHelper (Sub Class) Observation: Same as Example One. Problem: Same as Example one. Solution: Change equals in CachingWrapperFilter from: CachingWrapperFilter.java public boolean equals(Object o) { if (!(o instanceof CachingWrapperFilter)) return false; return this.filter.equals(((CachingWrapperFilter)o).filter); } To: CachingWrapperFilter.java public boolean equals(Object o) { // if (!(o instanceof CachingWrapperFilter)) return false; if(o == this) return true; if(o == null || o.getClass() != this.getClass()) return false; return this.filter.equals(((CachingWrapperFilter)o).filter); } Advantage: Same as Example One. Here, CachingWrapperFilterHelper.equals and CachingWrapperFilterHelper.hashCode is not needed. Example Three: org.apache.lucene.search.MultiTermQuery (Abstract Parent) &lt;- org.apache.lucene.search.FuzzyQuery (Concrete Sub) &lt;- org.apache.lucene.search.WildcardQuery (Concrete Sub) Observation (Not a problem): WildcardQuery defines equals but inherits hashCode from MultiTermQuery. Definition of equals contains just super.equals invocation. FuzzyQuery has few state variables added that are referenced in its equals and hashCode. Intention: I believe the intention here is not to make objects of FuzzyQuery and WildcardQuery comparable. ie. fuzzyQuery.equals(wildCardQuery) == false &amp;&amp; wildCardQuery.equals(fuzzyQuery) == false. Proposed Implementation: How about changing the implementation of equals in MultiTermQuery from: MultiTermQuery.java public boolean equals(Object o) { if (this == o) return true; if (!(o instanceof MultiTermQuery)) return false; final MultiTermQuery multiTermQuery = (MultiTermQuery) o; if (!term.equals(multiTermQuery.term)) return false; return getBoost() == multiTermQuery.getBoost(); } To: MultiTermQuery.java public boolean equals(Object o) { if (this == o) return true; // if (!(o instanceof MultiTermQuery)) return false; if(o == null || o.getClass() != this.getClass()) return false; final MultiTermQuery multiTermQuery = (MultiTermQuery) o; if (!term.equals(multiTermQuery.term)) return false; return getBoost() == multiTermQuery.getBoost(); } Advantage: Same as above. Here, WildcardQuery.equals is not needed as it does not define any new state. (FuzzyQuery.equals is still needed because FuzzyQuery defines new state.)</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1190</id>
      <title>a lexicon object for merging spellchecker and synonyms from stemming</title>
      <description>Some Lucene features need a list of referring word. Spellchecking is the basic example, but synonyms is an other use. Other tools can be used smoothlier with a list of words, without disturbing the main index : stemming and other simplification of word (anagram, phonetic ...). For that, I suggest a Lexicon object, wich contains words (Term + frequency), wich can be built from Lucene Directory, or plain text files. Classical TokenFilter can be used with Lexicon (LowerCaseFilter and ISOLatin1AccentFilter should be the most useful). Lexicon uses a Lucene Directory, each Word is a Document, each meta is a Field (word, ngram, phonetic, fields, anagram, size ...). Above a minimum size, number of differents words used in an index can be considered as stable. So, a standard Lexicon (built from wikipedia by example) can be used. A similarTokenFilter is provided. A spellchecker will come soon. A fuzzySearch implementation, a neutral synonym TokenFilter can be done. Unused words can be remove on demand (lazy delete?) Any criticism or suggestions?</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1191</id>
      <title>If IndexWriter hits OutOfMemoryError it should not commit</title>
      <description>While progress has been made making IndexWriter robust to OOME, I think there is still a real risk that an OOME at a bad time could put IndexWriter into a bad state such that if close() is called and somehow it succeeds without hitting another OOME, it risks introducing messing up the index. I'd like to detect if OOME has been hit in any of the methods that alter IW's state, and if so, do not commit changes to the index. If close is called after hitting OOME, I think writer should instead abort. Attached patch just adds try/catch clauses to catch OOME, note that it was hit, and re-throw it. Then, sync() refuses to commit a new segments_N if OOME was hit, and close instead calls abort when OOME was hit. All tests pass. I plan to commit in a day or two.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1194</id>
      <title>Add deleteByQuery to IndexWriter</title>
      <description>This has been discussed several times recently: http://markmail.org/message/awlt4lmk3533epbe http://www.gossamer-threads.com/lists/lucene/java-user/57384#57384 If we add deleteByQuery to IndexWriter then this is a big step towards allowing IndexReader to be readonly. I took the approach suggested in that first thread: I buffer delete queries just like we now buffer delete terms, holding the max docID that the delete should apply to. Then, I also decoupled flushing deletes (mapping term or query --&gt; actual docIDs that need deleting) from flushing added documents, and now I flush deletes only when a merge is started, or on commit() or close(). SegmentMerger now exports the docID map it used when merging, and I use that to renumber the max docIDs of all pending deletes. Finally, I turned off tracking of memory usage of pending deletes since they now live beyond each flush. Deletes are now only explicitly flushed if you set maxBufferedDeleteTerms to something other than DISABLE_AUTO_FLUSH. Otherwise they are flushed at the start of every merge.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1195</id>
      <title>Performance improvement for TermInfosReader</title>
      <description>Currently we have a bottleneck for multi-term queries: the dictionary lookup is being done twice for each term. The first time in Similarity.idf(), where searcher.docFreq() is called. The second time when the posting list is opened (TermDocs or TermPositions). The dictionary lookup is not cheap, that's why a significant performance improvement is possible here if we avoid the second lookup. An easy way to do this is to add a small LRU cache to TermInfosReader. I ran some performance experiments with an LRU cache size of 20, and an mid-size index of 500,000 documents from wikipedia. Here are some test results: 50,000 AND queries with 3 terms each: old: 152 secs new (with LRU cache): 112 secs (26% faster) 50,000 OR queries with 3 terms each: old: 175 secs new (with LRU cache): 133 secs (24% faster) For bigger indexes this patch will probably have less impact, for smaller once more. I will attach a patch soon.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1201</id>
      <title>Add getIndexCommit method to IndexReader</title>
      <description>Spinoff from this thread: http://markmail.org/message/bojgqfgyxkkv4fyb I think it makes sense ask an IndexReader for the commit point it has open. This enables the use case described in the above thread, which is to create a deletion policy that is able to query all open readers for what commit points they are using, and prevent deletion of them.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1203</id>
      <title>[PATCH] Allow setting IndexReader to IndexSearcher</title>
      <description>As I've received no counter-arguments for my Lucene Java-User mailing list (see http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3Ca4d7dcf50803050918h68defe83v66e657debb76d3f3@mail.gmail.com%3E), I would like to propose adding a setter to set new instance of IndexReader to IndexSearcher. Why is this needed? The FAQ (http://wiki.apache.org/lucene-java/LuceneFAQ#head-48921635adf2c968f7936dc07d51dfb40d638b82) says: "Make sure you only open one IndexSearcher, and share it among all of the threads that are doing searches – this is safe, and it will minimize the number of files that are open concurently." So does the JavaDoc (http://lucene.apache.org/java/2_3_1/api/core/org/apache/lucene/search/IndexSearcher.html). In my application, I don't want to expose anything about IndexReader; all they need to know is Searcher - see my post to the mailing list how would I do this. However, if the index is updated, reopened reader cannot be set back to IndexSearcher, a new instance of IndexSearcher needs to be created (which contradicts FAQ and Javadoc). At the moment, the only way to go around this is to create a surrogate subclass of IndexSearcher and set new instance of IndexReader. A simple setter would just do the job.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1206</id>
      <title>Ability to store Reader / InputStream fields</title>
      <description>In some situations we would like to store the whole text, but the whole text won't always fit in memory so we can't create a String. Likewise for storing binary, it would sometimes be better if we didn't have to read into a byte[] up-front (even when it doesn't use much memory, it increases the number of copies made and adds burden to GC.) FieldsWriter currently writes the length at the start of the chunks though, so I don't know whether it would be possible to seek back and write the length after writing the data. It would also be useful to use this in conjunction with compression, both for Reader and InputStream types. And when retrieving the field, it should be possible to create a Reader without reading the entire String into memory up-front.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1207</id>
      <title>Allow spell check input to be part of the results</title>
      <description>As a threadshold marker, to see if the word seems to exist at all, or what not.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1209</id>
      <title>If setConfig(Config config) is called in resetInputs(), you can turn term vectors off and on by round</title>
      <description>I want to be able to run one benchmark that tests things using term vectors and not using term vectors. Currently this is not easy because you cannot specify term vectors per round. While you do have to create a new index per round, this automation is preferable to me in comparison to running two separate tests. If it doesn't affect anything else, it would be great to have setConfig(Config config) called in BasicDocMaker.resetInputs(). This would keep the term vector options up to date per round if you reset. Mark</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1212</id>
      <title>Basic refactoring of DocumentsWriter</title>
      <description>As a starting point for making DocumentsWriter more understandable, I've fixed its inner classes to be static, and then broke the classes out into separate sources, all in org.apache.lucene.index package.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1215</id>
      <title>Support of Unicode Collation</title>
      <description>New in java 6, we have java.text.Normalizer that supports Unicode Standard Annex #15 normalization. http://java.sun.com/javase/6/docs/api/java/text/Normalizer.html http://www.unicode.org/unicode/reports/tr15/ The normalization defined has four variants of C, D, KC, KD. Canonical Decomposition or Compatibility Decomposition will be normalize the representation of a String, and the search result will be improved. I'd like to submit a TokenFilter code supporting this feature!</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1216</id>
      <title>CharDelimiterTokenizer</title>
      <description>WhitespaceTokenizer is very useful for space separated languages, but my Japanese text is not always separated by a space. So, I created an alternative Tokenizer that we can specify the delimiter. The file submitted will be an improvement of the current WhitespaceTokenizer. I tried to extend it from CharTokenizer, but CharTokenizer has a limitation that a token can't be longer than 255 chars.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1217</id>
      <title>use isBinary cached variable instead of instanceof in Field</title>
      <description>Field class can hold three types of values, See: AbstractField.java protected Object fieldsData = null; currently, mainly RTTI (instanceof) is used to determine the type of the value stored in particular instance of the Field, but for binary value we have mixed RTTI and cached variable "boolean isBinary" This patch makes consistent use of cached variable isBinary. Benefit: consistent usage of method to determine run-time type for binary case (reduces chance to get out of sync on cached variable). It should be slightly faster as well. Thinking aloud: Would it not make sense to maintain type with some integer/byte"poor man's enum" (Interface with a couple of constants) code:java{ public static final interface Type{ public static final byte BOOLEAN = 0; public static final byte STRING = 1; public static final byte READER = 2; .... } } and use that instead of isBinary + instanceof?</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1218</id>
      <title>PassTokenizerFilter that pass text in a Token</title>
      <description>The PassTokenizer passes a text in a TokenStream that has a single token in its stream. This Tokenizer(attached) is very useful for debugging. You may test TokenFilter that will do sub-tokenization in a token. This Tokenizer is also useful for debugging the tokenization process dependent on the TokenFilter order. This Tokenizer is not suitable for processing a large text field.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1219</id>
      <title>support array/offset/ length setters for Field with binary data</title>
      <description>currently Field/Fieldable interface supports only compact, zero based byte arrays. This forces end users to create and copy content of new objects before passing them to Lucene as such fields are often of variable size. Depending on use case, this can bring far from negligible performance improvement. this approach extends Fieldable interface with 3 new methods getOffset(); gettLenght(); and getBinaryValue() (this only returns reference to the array)</description>
      <attachments/>
      <comments>23</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1227</id>
      <title>NGramTokenizer to handle more than 1024 chars</title>
      <description>Current NGramTokenizer can't handle character stream that is longer than 1024. This is too short for non-whitespace-separated languages. I created a patch for this issues.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1229</id>
      <title>NGramTokenFilter optimization in query phase</title>
      <description>I found that NGramTokenFilter-ed token stream could be optimized in query. A standard 1,2 NGramTokenFilter will generate a token stream from "abcde" as follows: a ab b bc c cd d de e When we index "abcde", we'll use all of the tokens. But when we query, we only need: ab cd de</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1231</id>
      <title>Column-stride fields (aka per-document Payloads)</title>
      <description>This new feature has been proposed and discussed here: http://markmail.org/search/?q=per-document+payloads#query:per-document%20payloads+page:1+mid:jq4g5myhlvidw3oc+state:results Currently it is possible in Lucene to store data as stored fields or as payloads. Stored fields provide good performance if you want to load all fields for one document, because this is an sequential I/O operation. If you however want to load the data from one field for a large number of documents, then stored fields perform quite badly, because lot's of I/O seeks might have to be performed. A better way to do this is using payloads. By creating a "special" posting list that has one posting with payload for each document you can "simulate" a column- stride field. The performance is significantly better compared to stored fields, however still not optimal. The reason is that for each document the freq value, which is in this particular case always 1, has to be decoded, also one position value, which is always 0, has to be loaded. As a solution we want to add real column-stride fields to Lucene. A possible format for the new data structure could look like this (CSD stands for column- stride data, once we decide for a final name for this feature we can change this): CSDList --&gt; FixedLengthList | &lt;VariableLengthList, SkipList&gt; FixedLengthList --&gt; &lt;Payload&gt;^SegSize VariableLengthList --&gt; &lt;DocDelta, PayloadLength?, Payload&gt; Payload --&gt; Byte^PayloadLength PayloadLength --&gt; VInt SkipList --&gt; see frq.file We distinguish here between the fixed length and the variable length cases. To allow flexibility, Lucene could automatically pick the "right" data structure. This could work like this: When the DocumentsWriter writes a segment it checks whether all values of a field have the same length. If yes, it stores them as FixedLengthList, if not, then as VariableLengthList. When the SegmentMerger merges two or more segments it checks if all segments have a FixedLengthList with the same length for a column-stride field. If not, it writes a VariableLengthList to the new segment. Once this feature is implemented, we should think about making the column- stride fields updateable, similar to the norms. This will be a very powerful feature that can for example be used for low-latency tagging of documents. Other use cases: replace norms allow to store boost values separately from norms as input for the FieldCache, thus providing significantly improved loading performance (see LUCENE-831) Things that need to be done here: decide for a name for this feature - I think "column-stride fields" was liked better than "per-document payloads" Design an API for this feature. We should keep in mind here that these fields are supposed to be updateable. Define datastructures. I would like to get this feature into 2.4. Feedback about the open questions is very welcome so that we can finalize the design soon and start implementing.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1232</id>
      <title>Use segments generation instead of version</title>
      <description>Right now the segments file stores generation, a long starting with 0 that increments by 1 with each commit, and version, a long starting with System.currentTimeMillis() that also increments by 1 with each commit. I think they are redundant so we can replace all methods/uses of version with generation instead. Spinoff from LUCENE-1228.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1233</id>
      <title>Fix Document.getFieldables and others to never return null</title>
      <description>Document.getFieldables (and other similar methods) returns null if there are no fields matching the name. We can avoid NPE in consumers of this API if instead we return an empty array. Spinoff from http://markmail.org/message/g2nzstmce4cnf3zj</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1235</id>
      <title>NGramTokenizer optimization in query phase</title>
      <description>As I described in LUCENE-1229, we can optimize token stream in query.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1236</id>
      <title>EdgeNGram* documentation improvement</title>
      <description>To clarify what "edge" means, I added some description. That edge means the beggining edge of a term or ending edge of a term.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1240</id>
      <title>TermsFilter: reuse TermDocs</title>
      <description>TermsFilter currently calls termDocs(Term) once per term in the TermsFilter. If we sort the terms it's filtering on, this can be optimised to call termDocs() once and then skip(Term) once per term, which should significantly speed up this filter.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1241</id>
      <title>0xffff char is not a string terminator</title>
      <description>Current trunk index.DocumentWriter uses "\uffff" as a string terminator, but it should not to be for some reasons. \uffff is not a terminator char itself and we can't handle a string that really contains \uffff. And also, we can calculate the end char position in a character sequence from the string length that we already know. However, I agree with the usage for assertion, that "\uffff" is placed after at the end of a string in a char sequence.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1243</id>
      <title>A few new benchmark tasks</title>
      <description>Some tasks that would be helpful to see added. Might want some expansion, but here are some basic ones I have been using: CommitIndexTask ReopenReaderTask SearchWithSortTask I do the sort in a similar way that the highlighting was done, but another method may be better. Just would be great to have sorting. Also, since there is no great field for sorting (reuters date always appears to be the same) I changed the id field from doc+id to just id. Again maybe not the best solution, but here I am to get the ball rolling</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1245</id>
      <title>MultiFieldQueryParser is not friendly for overriding getFieldQuery(String,String,int)</title>
      <description>LUCENE-1213 fixed an issue in MultiFieldQueryParser where the slop parameter wasn't being properly applied. Problem is, the fix which eventually got committed is calling super.getFieldQuery(String,String), bypassing any possibility of customising the query behaviour. This should be relatively simply fixable by modifying getFieldQuery(String,String,int) to, if field is null, recursively call getFieldQuery(String,String,int) instead of setting the slop itself. This gives subclasses which override either getFieldQuery method a chance to do something different.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1247</id>
      <title>Unnecessary assert in org.apache.lucene.index.DocumentsWriterThreadState.trimFields()</title>
      <description>In org.apache.lucene.index.DocumentsWriterThreadState.trimFields() is the following code: if (fp.lastGen == -1) { // This field was not seen since the previous // flush, so, free up its resources now // Unhash final int hashPos = fp.fieldInfo.name.hashCode() &amp; fieldDataHashMask; DocumentsWriterFieldData last = null; DocumentsWriterFieldData fp0 = fieldDataHash[hashPos]; while(fp0 != fp) { last = fp0; fp0 = fp0.next; } assert fp0 != null; The assert at the end is not necessary as fp0 cannot be null. The first line in the above code guarantees that fp is not null by the time the while loop is hit. The while loop is exited when fp0 and fp are equal. Since fp is not null then fp0 cannot be null when the while loop is exited, thus the assert is guaranteed to never occur. This was detected by FindBugs.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1248</id>
      <title>Unncessary creation of object in org.apache.lucene.analysis.WordlistLoader.getWordSet()</title>
      <description>Here's the function: public static HashSet getWordSet(File wordfile) throws IOException { HashSet result = new HashSet(); FileReader reader = null; try { reader = new FileReader(wordfile); result = getWordSet(reader); } finally { if (reader != null) reader.close(); } return result; } The creation of the new HashSet object in the declaration of "result" is unnecessary. Either "result" will be unconditionally set by the call to getWordSet() or an exception will occur. This was detected by FindBugs.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1256</id>
      <title>Changes.html formatting improvements</title>
      <description>Some improvements to the Changes.html generated by the changes2html.pl script via the 'changes-to-html' ant task: Simplified the Simple stylesheet (removed monospace font specification) and made it the default. The Fancy stylesheet is really hard for me to look at (yellow text on light blue background may provide high contrast with low eye strain, but IMHO it's ugly). Moved the monospace style from the Simple stylesheet to a new stylesheet named "Fixed Width" Fixed syntax errors in the Fancy stylesheet, so that it displays as intended. Added &lt;span style="attrib"&gt; to change attributions. In the Fancy and Simple stylesheets, change attributions are colored dark green. Now properly handling change attributions in CHANGES.txt that have trailing periods. Clicking on an anchor to expand its children now changes the document location to show the children. Hovering over anchors now causes a tooltip to be displayed - either "Click to expand" or "Click to collapse" - the tooltip changes appropriately after a collapse or expansion.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1257</id>
      <title>Port to Java5</title>
      <description>For my needs I've updated Lucene so that it uses Java 5 constructs. I know Java 5 migration had been planned for 2.1 someday in the past, but don't know when it is planned now. This patch against the trunk includes : most obvious generics usage (there are tons of usages of sets, ... Those which are commonly used have been generified) PriorityQueue generification replacement of indexed for loops with for each constructs removal of unnececessary unboxing The code is to my opinion much more readable with those features (you actually know what is stored in collections reading the code, without the need to lookup for field definitions everytime) and it simplifies many algorithms. Note that this patch also includes an interface for the Query class. This has been done for my company's needs for building custom Query classes which add some behaviour to the base Lucene queries. It prevents multiple unnnecessary casts. I know this introduction is not wanted by the team, but it really makes our developments easier to maintain. If you don't want to use this, replace all /Queriable/ calls with standard /Query/.</description>
      <attachments/>
      <comments>129</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>1258</id>
      <title>Increment position by default in StopFilter &amp; QueryParser -&gt; PhraseQuery</title>
      <description>Spinoff from here: https://issues.apache.org/jira/browse/LUCENE-1095 I think for 3.0 we should change the default so that: By default, StopFilter increments the positionIncrement whenever it skips stop words. Add option to revert back to old way. This is just toggling the boolean default. By default, when QueryParser adds terms to a PhraseQuery it should include the position reported by the analyzer. Add option to revert back to old way. I'm just opening this now, marking as 3.0 fix, to remind us all to actually fix it for 3.0.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1259</id>
      <title>Token.clone() copies termBuffer - unneccessary in most cases</title>
      <description>The method Token.clone() copies the termBuffer. This is OK for the clone()-method (it works according to what we expect from clone()). But in most cases the termBuffer is set directly after cloning. This is an unnecessary copy step we can avoid. This patch adds a new method called cloneWithoutTermBuffer().</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1260</id>
      <title>Norm codec strategy in Similarity</title>
      <description>The static span and resolution of the 8 bit norms codec might not fit with all applications. My use case requires that 100f-250f is discretized in 60 bags instead of the default.. 10?</description>
      <attachments/>
      <comments>44</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1267</id>
      <title>add numDocs() and maxDoc() methods to IndexWriter; deprecate docCount()</title>
      <description>Spinoff from here: http://mail-archives.apache.org/mod_mbox/lucene-java-user/200804.mbox/%3c405706.11550.qm@web65411.mail.ac4.yahoo.com%3e I think we should add maxDoc() and numDocs() methods to IndexWriter, and deprecate docCount() in favor of maxDoc(). To do this I think we should cache the deletion count of each segment in the segments file.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1268</id>
      <title>Changes.html should be visible to users for closed releases</title>
      <description>Changes.html is currently available only in the dev page, for trunk. See LUCENE-1157 for discussion on where exactly to expose this.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1272</id>
      <title>Support for boost factor in MoreLikeThis</title>
      <description>This is a patch I made to be able to boost the terms with a specific factor beside the relevancy returned by MoreLikeThis. This is helpful when having more then 1 MoreLikeThis in the query, so words in the field A (i.e. Title) can be boosted more than words in the field B (i.e. Description).</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1274</id>
      <title>Expose explicit 2-phase commit in IndexWriter</title>
      <description>Currently when IndexWriter commits, it does so with a two-phase commit, internally: first it prepares all the new index files, syncs them; then it writes a new segments_N file and syncs that, and only if that is successful does it remove any now un-referenced index files. However, these two phases are done privately, internal to the commit() method. But when Lucene is involved in a transaction with external resources (eg a database), it's very useful to explicitly break out the prepare phase from the commit phase. Spinoff from this thread: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200804.mbox/%3C16627610.post@talk.nabble.com%3E</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1275</id>
      <title>Expose Document Number</title>
      <description>Lucene maintains an internal document number, which this patch exposes using a mutator/accessor pair of methods. The field is set on document addition. This creates a unique way to refer to a document for editing and updating individual documents.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1278</id>
      <title>Add optional storing of document numbers in term dictionary</title>
      <description>Add optional storing of document numbers in term dictionary. String index field cache and range filter creation will be faster. Example read code: TermEnum termEnum = indexReader.terms(TermEnum.LOAD_DOCS); do { Term term = termEnum.term(); if (term == null || term.field() != field) break; int[] docs = termEnum.docs(); } while (termEnum.next()); Example write code: Document document = new Document(); document.add(new Field("tag", "dog", Field.Store.YES, Field.Index.UN_TOKENIZED, Field.Term.STORE_DOCS)); indexWriter.addDocument(document);</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1279</id>
      <title>RangeQuery and RangeFilter should use collation to check for range inclusion</title>
      <description>See this java-user discussion of problems caused by Unicode code-point comparison, instead of collation, in RangeQuery. RangeQuery could take in a Locale via a setter, which could be used with a java.text.Collator and/or CollationKey's, to handle ranges for languages which have alphabet orderings different from those in Unicode.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1283</id>
      <title>Factor out ByteSliceWriter from DocumentsWriterFieldData</title>
      <description>DocumentsWriter uses byte slices into shared byte[]'s to hold the growing postings data for many different terms in memory. This is probably the trickiest (most confusing) part of DocumentsWriter. Right now it's not cleanly factored out and not easy to separately test. In working on this issue: http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c126142c0805061426n1168421ya5594ef854fae5e4@mail.gmail.com%3e which eventually turned out to be a bug in Oracle JRE's JIT compiler, I factored out ByteSliceWriter and created a unit test to stress test the writing &amp; reading of byte slices. The test just randomly writes N streams interleaved into shared byte[]'s, then reads them back verifying the results are correct. I created the stress test to try to find any bugs in that code. The test ran fine (no bugs were found) but I think the refactoring is still very much worthwhile. I expected the changes to reduce indexing throughput, so I ran a test indexing first 200K Wikipedia docs using this alg: analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker docs.file=/Volumes/External/lucene/wiki.txt doc.stored = true doc.term.vector = true doc.add.log.step=2000 directory=FSDirectory autocommit=false compound=true ram.flush.mb=256 { "Rounds" ResetSystemErase { "BuildIndex" - CreateIndex { "AddDocs" AddDoc &gt; : 200000 - CloseIndex } NewRound } : 4 RepSumByPrefRound BuildIndex Ok trunk it produces these results: Operation round runCnt recsPerRun rec/s elapsedSec avgUsedMem avgTotalMem BuildIndex 0 1 200000 791.7 252.63 338,552,096 1,061,814,272 BuildIndex - - 1 - - 1 - - 200000 - - 793.1 - - 252.18 - 605,262,080 1,061,814,272 BuildIndex 2 1 200000 794.8 251.63 601,966,528 1,061,814,272 BuildIndex - - 3 - - 1 - - 200000 - - 782.5 - - 255.58 - 608,699,712 1,061,814,272 and with the patch: Operation round runCnt recsPerRun rec/s elapsedSec avgUsedMem avgTotalMem BuildIndex 0 1 200000 745.0 268.47 338,318,784 1,061,814,272 BuildIndex - - 1 - - 1 - - 200000 - - 792.7 - - 252.30 - 605,331,776 1,061,814,272 BuildIndex 2 1 200000 786.7 254.24 602,915,712 1,061,814,272 BuildIndex - - 3 - - 1 - - 200000 - - 795.3 - - 251.48 - 602,378,624 1,061,814,272 So it looks like the performance cost of this change is negligible (in the noise).</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1284</id>
      <title>Set of Java classes that allow the Lucene search engine to use morphological information developed for the Apertium open-source machine translation platform (http://www.apertium.org)</title>
      <description>Set of Java classes that allow the Lucene search engine to use morphological information developed for the Apertium open-source machine translation platform (http://www.apertium.org). Morphological information is used to index new documents and to process smarter queries in which morphological attributes can be used to specify query terms. The tool makes use of morphological analyzers and dictionaries developed for the open-source machine translation platform Apertium (http://apertium.org) and, optionally, the part-of-speech taggers developed for it. Currently there are morphological dictionaries available for Spanish, Catalan, Galician, Portuguese, Aranese, Romanian, French and English. In addition new dictionaries are being developed for Esperanto, Occitan, Basque, Swedish, Danish, Welsh, Polish and Italian, among others; we hope more language pairs to be added to the Apertium machine translation platform in the near future.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1286</id>
      <title>LargeDocHighlighter - another span highlighter optimized for large documents</title>
      <description>The existing Highlighter API is rich and well designed, but the approach taken is not very efficient for large documents. I believe that this is because the current Highlighter rebuilds the document by running through and scoring every every token in the tokenstream. With a break in the current API, an alternate approach can be taken: rebuild the document by running through the query terms by using their offsets. The benefit is clear - a large doc will have a large tokenstream, but a query will likely be very small in comparison. I expect this approach to be quite a bit faster for very large documents, while still supporting Phrase and Span queries. First rough patch to follow shortly.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1287</id>
      <title>Allow usage of HyphenationCompoundWordTokenFilter without dictionary</title>
      <description>We should allow to use the HyphenationCompoundWordTokenFilter without a dictionary. This produces a lot of "nonword" tokens but might be useful sometimes.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1288</id>
      <title>Add getVersion method to IndexCommit</title>
      <description>Returns the equivalent of IndexReader.getVersion for IndexCommit public abstract long getVersion();</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1289</id>
      <title>Make FieldDocSortedHitQueue and methods public, make FieldSortedHitQueue.fillFields public</title>
      <description>In implementing custom MultiSearcher like class, need public access.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1292</id>
      <title>Tag Index</title>
      <description>The problem the tag index solves is slow field cache loading and range queries, and reindexing an entire document to update fields that are not tokenized. The tag index holds untokenized terms with a docfreq of 1 in a term dictionary like index file. The file also stores the docs per term, similar to LUCENE-1278. The index also has a transaction log and in memory index for realtime updates to the tags. The transaction log is periodically merged into the existing tag term dictionary index file. The TagIndexReader extends IndexReader and is unified with a regular index by ParallelReader. There is a doc id to terms skip pointer file for the IndexReader.document method. This file contains a pointer for looking up the terms for a document. There is a higher level class that encapsulates writing a document with tag fields to IndexWriter and TagIndexWriter. This requires a hook into IndexWriter to coordinate doc ids and flushing segments to disk. The writer class could be as simple as: public class TagIndexWriter { public void add(Term term, DocIdSetIterator iterator) { } public void delete(Term term, DocIdSetIterator iterator) { } }</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1293</id>
      <title>Tweaks to PhraseQuery.explain()</title>
      <description>The explain() function in PhraseQuery.java is very clumzy and could use many optimizations. Perhaps it is only because it is intended to use while debugging? Here's an example: result.addDetail(fieldExpl); // combine them result.setValue(queryExpl.getValue() * fieldExpl.getValue()); if (queryExpl.getValue() == 1.0f) return fieldExpl; return result; } Can easily be tweaked and become: if (queryExpl.getValue() == 1.0f) { return fieldExpl; } result.addDetail(fieldExpl); // combine them result.setValue(queryExpl.getValue() * fieldExpl.getValue()); return result; } And thats really just for a start... Itamar.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1295</id>
      <title>Make retrieveTerms(int docNum) public in MoreLikeThis</title>
      <description>It would be useful if private PriorityQueue retrieveTerms(int docNum) throws IOException { were public, since it is similar in use to public PriorityQueue retrieveTerms(Reader r) throws IOException { It also seems useful to add public String [] retrieveInterestingTerms(int docNum) throws IOException{ to mirror the one that works on Reader.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1296</id>
      <title>Allow use of compact DocIdSet in CachingWrapperFilter</title>
      <description>Extends CachingWrapperFilter with a protected method to determine the DocIdSet to be cached.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1297</id>
      <title>Allow other string distance measures in spellchecker</title>
      <description>Updated spelling code to allow for other string distance measures to be used. Created StringDistance interface. Modified existing Levenshtein distance measure to implement interface (and renamed class). Verified that change to Levenshtein distance didn't impact runtime performance. Implemented Jaro/Winkler distance metric Modified SpellChecker to take distacne measure as in constructor or in set method and to use interface when calling.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1301</id>
      <title>Refactor DocumentsWriter</title>
      <description>I've been working on refactoring DocumentsWriter to make it more modular, so that adding new indexing functionality (like column-stride stored fields, LUCENE-1231) is just a matter of adding a plugin into the indexing chain. This is an initial step towards flexible indexing (but there is still alot more to do!). And it's very much still a work in progress – there are intemittant thread safety issues, I need to add tests cases and test/iterate on performance, many "nocommits", etc. This is a snapshot of my current state... The approach introduces "consumers" (abstract classes defining the interface) at different levels during indexing. EG DocConsumer consumes the whole document. DocFieldConsumer consumes separate fields, one at a time. InvertedDocConsumer consumes tokens produced by running each field through the analyzer. TermsHashConsumer writes its own bytes into in-memory posting lists stored in byte slices, indexed by term, etc. DocumentsWriter*.java is then much simpler: it only interacts with a DocConsumer and has no idea what that consumer is doing. Under that DocConsumer there is a whole "indexing chain" that does the real work: NormsWriter holds norms in memory and then flushes them to _X.nrm. FreqProxTermsWriter holds postings data in memory and then flushes to _X.frq/prx. StoredFieldsWriter flushes immediately to _X.fdx/fdt TermVectorsTermsWriter flushes immediately to _X.tvx/tvf/tvd DocumentsWriter still manages things like flushing a segment, closing doc stores, buffering &amp; applying deletes, freeing memory, aborting when necesary, etc. In this first step, everything is package-private, and, the indexing chain is hardwired (instantiated in DocumentsWriter) to the chain currently matching Lucene trunk. Over time we can open this up. There are no changes to the index file format. For the most part this is just a [large] refactoring, except for these two small actual changes: Improved concurrency with mixed large/small docs: previously the thread state would be tied up when docs finished indexing out-of-order. Now, it's not: instead I use a separate class to hold any pending state to flush to the doc stores, and immediately free up the thread state to index other docs. Buffered norms in memory now remain sparse, until flushed to the _X.nrm file. Previously we would "fill holes" in norms in memory, as we go, which could easily use way too much memory. Really this isn't a solution to the problem of sparse norms (LUCENE-830); it just delays that issue from causing memory blowup during indexing; memory use will still blowup during searching. I expect performance (indexing throughput) will be worse with this change. I'll profile &amp; iterate to minimize this, but I think we can accept some loss. I also plan to measure benefit of manually re-cycling RawPostingList instances from our own pool, vs letting GC recycle them.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1306</id>
      <title>CombinedNGramTokenFilter</title>
      <description>Alternative NGram filter that produce tokens with composite prefix and suffix markers. ts = new WhitespaceTokenizer(new StringReader("hello")); ts = new CombinedNGramTokenFilter(ts, 2, 2); assertNext(ts, "^h"); assertNext(ts, "he"); assertNext(ts, "el"); assertNext(ts, "ll"); assertNext(ts, "lo"); assertNext(ts, "o$"); assertNull(ts.next());</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1307</id>
      <title>Remove Contributions page</title>
      <description>&gt; On Fri, May 16, 2008 at 10:06 PM, Otis Gospodnetic &gt; &lt;otis_gospodnetic@yahoo.com&gt; wrote: &gt;&gt; Hola, &gt;&gt; &gt;&gt; Does anyone think the Contributions page should be removed? &gt;&gt; http://lucene.apache.org/java/2_3_2/contributions.html &gt;&gt; &gt;&gt; It looks sooooo outdated that I think it may give newcomers a bad &gt;&gt; impression of Lucene ("What, this is it for contributions?"). &gt;&gt; The only really valuable piece there is Luke, but Luke must be &gt;&gt; mentioned in a dozen places on the Wiki anyway. &gt;&gt; &gt;&gt; &gt;&gt; Should we remove the Contributions page? Yonik and Grant gave their +1s.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1308</id>
      <title>Remove String.intern() from Field.java to increase performance and lower contention</title>
      <description>Right now, *document.Field is interning all field names. While this makes sense because it lowers the overall memory consumption, the method intern() of String is know to be difficult to handle. 1) it is a native call and therefore slower than anything on the Java level 2) the String pool is part of the perm space and not of the general heap, so it's size is more restricted and needs extra VM params to be managed 3) Some VMs show GC problems with strings in the string pool Suggested solution is a WeakHashMap instead, that takes care of unifying the String instances and at the same time keeping the pool in the heap space and releasing the String when it is not longer needed. For extra performance in a concurrent environment, a ConcurrentHashMap-like implementation of a weak hashmap is recommended, because we mostly read from the pool. We saw a 10% improvement in throughout and response time of our application and the application is not only doing searches (we read a lot of documents from the result). So a single measurement test case could show even more improvement in single and concurrent usage. The Cache: /** Cache to replace the expensive String.intern() call with the java version */ private final static Map&lt;String, WeakReference&lt;String&gt;&gt; unifiedStringsCache = Collections.synchronizedMap(new WeakHashMap&lt;String, WeakReference&lt;String&gt;&gt;(109)); The access to it, instead of this.name = name.intern; // unify the strings, but do not use the expensive String.intern() version // which is not "weak enough", uses the perm space and is a native call String unifiedName = null; WeakReference&lt;String&gt; ref = unifiedStringsCache.get(name); if (ref != null) { unifiedName = ref.get(); } if (unifiedName == null) { unifiedStringsCache.put(name, new WeakReference(name)); unifiedName = name; } this.name = unifiedName; I guess it is sufficient to have mostly all fields names interned, so I skipped the additional synchronization around the access and take the risk that only 99.99% of all field names are interned.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1311</id>
      <title>Add ability to open prior commits to IndexReader</title>
      <description>If you use a customized DeletionPolicy, which keeps multiple commits around (instead of the default which is to only preserve the most recent commit), it's useful to be able to list all such commits and then open a reader against one of these commits. I've added this API to list commits: public static Collection IndexReader.listCommits(Directory) and these two new open methods to IndexReader to open a specific commit: public static IndexReader open(IndexCommit) public static IndexReader open(IndexCommit, IndexDeletionPolicy) Spinoff from here: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200806.mbox/%3c85d3c3b60806161735o207a3238sa2e6c415171a8019@mail.gmail.com%3e</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1313</id>
      <title>Near Realtime Search (using a built in RAMDirectory)</title>
      <description>Enable near realtime search in Lucene without external dependencies. When RAM NRT is enabled, the implementation adds a RAMDirectory to IndexWriter. Flushes go to the ramdir unless there is no available space. Merges are completed in the ram dir until there is no more available ram. IW.optimize and IW.commit flush the ramdir to the primary directory, all other operations try to keep segments in ram until there is no more space.</description>
      <attachments/>
      <comments>114</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1314</id>
      <title>IndexReader.clone</title>
      <description>Based on discussion http://www.nabble.com/IndexReader.reopen-issue-td18070256.html. The problem is reopen returns the same reader if there are no changes, so if docs are deleted from the new reader, they are also reflected in the previous reader which is not always desired behavior.</description>
      <attachments/>
      <comments>102</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1315</id>
      <title>Add setIndexReader in IndexSearcher</title>
      <description>Adds a setter for the "private IndexReader reader" member in IndexSearcher. Needed to in order to be able reload the reader underlying a remote searcher.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1317</id>
      <title>Add InstantiatedIndexWriter.addIndexes(IndexReader[] readers)</title>
      <description>Enable InstantiatedIndexWriter to have IndexReaders passed in like IndexWriter and merged into the index. Karl mentioned: bq: It's doable. The simplest solution I can think of is to reconstruct all the documents in one single enumeration of the source index and then add them to the writer. I'm however not certain this is the best way nor if InstantiatedIndexWriter is the place for the code. How would the documents be reconstructed without creating a lot of overhead? It seems like InstantiatedIndexWriter is the right place, given it is presumably more efficient to recreate all the IndexReaders and then commit?</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1319</id>
      <title>Allow user configurable buffersize for RAMDirectory</title>
      <description>Currently RAMDirectory via RAMOutputStream has a package protected value of 1024 as the buffer size for use in RAMFile. This issue proposes adding a single constructor to RAMDirectory allowing the user to specify the buffer size.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1320</id>
      <title>ShingleMatrixFilter, a three dimensional permutating shingle filter</title>
      <description>Backed by a column focused matrix that creates all permutations of shingle tokens in three dimensions. I.e. it handles multi token synonyms. Could for instance in some cases be used to replaces 0-slop phrase queries with something speedier. Token[][][]{ {{hello}, {greetings, and, salutations}}, {{world}, {earth}, {tellus}} } passes the following test with 2-3 grams: assertNext(ts, "hello_world"); assertNext(ts, "greetings_and"); assertNext(ts, "greetings_and_salutations"); assertNext(ts, "and_salutations"); assertNext(ts, "and_salutations_world"); assertNext(ts, "salutations_world"); assertNext(ts, "hello_earth"); assertNext(ts, "and_salutations_earth"); assertNext(ts, "salutations_earth"); assertNext(ts, "hello_tellus"); assertNext(ts, "and_salutations_tellus"); assertNext(ts, "salutations_tellus"); Contains more and less complex tests that demonstrate offsets, posincr, payload boosts calculation and construction of a matrix from a token stream. The matrix attempts to hog as little memory as possible by seeking no more than maximumShingleSize columns forward in the stream and clearing up unused resources (columns and unique token sets). Can still be optimized quite a bit though.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1322</id>
      <title>Remove synchronization in CompoundFileReader</title>
      <description>Currently there is what seems to be unnecessary synchronization in CompoundFileReader. This is solved by cloning the base IndexInput. Synchronization in low level IO classes creates lock contention on highly multi threaded Lucene installations, so much so that in many cases the CPU utilization never reaches the maximum without using something like ParallelMultiSearcher.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1325</id>
      <title>add IndexCommit.isOptimized method</title>
      <description>Spinoff from here: http://mail-archives.apache.org/mod_mbox/lucene-java-user/200807.mbox/%3C69de18140807010347s6269fea5r12c3212e0ec0a12a@mail.gmail.com%3E</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1329</id>
      <title>Remove synchronization in SegmentReader.isDeleted</title>
      <description>Removes SegmentReader.isDeleted synchronization by using a volatile deletedDocs variable on Java 1.5 platforms. On Java 1.4 platforms synchronization is limited to obtaining the deletedDocs reference.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1332</id>
      <title>Enable reader and binary fields in InstantiatedIndexWriter</title>
      <description>Currently InstantiatedIndexWriter does not support fields with a Reader or that are binary.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1333</id>
      <title>Token implementation needs improvements</title>
      <description>This was discussed in the thread (not sure which place is best to reference so here are two): http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200805.mbox/%3C21F67CC2-EBB4-48A0-894E-FBA4AECC0D50@gmail.com%3E or to see it all at once: http://www.gossamer-threads.com/lists/lucene/java-dev/62851 Issues: 1. JavaDoc is insufficient, leading one to read the code to figure out how to use the class. 2. Deprecations are incomplete. The constructors that take String as an argument and the methods that take and/or return String should all be deprecated. 3. The allocation policy is too aggressive. With large tokens the resulting buffer can be over-allocated. A less aggressive algorithm would be better. In the thread, the Python example is good as it is computationally simple. 4. The parts of the code that currently use Token's deprecated methods can be upgraded now rather than waiting for 3.0. As it stands, filter chains that alternate between char[] and String are sub-optimal. Currently, it is used in core by Query classes. The rest are in contrib, mostly in analyzers. 5. Some internal optimizations can be done with regard to char[] allocation. 6. TokenStream has next() and next(Token), next() should be deprecated, so that reuse is maximized and descendant classes should be rewritten to over-ride next(Token) 7. Tokens are often stored as a String in a Term. It would be good to add constructors that took a Token. This would simplify the use of the two together.</description>
      <attachments/>
      <comments>70</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1334</id>
      <title>Term improvement</title>
      <description>Term is designed for reuse of the supplied filter, to minimize intern(). One of the common use patterns is to create a Term with the txt field being an empty string. To simplify this pattern and to document it's usefulness, I suggest adding a constructor: public Term(String fld) with the obvious implementation and use it throughout core and contrib as a replacement.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1336</id>
      <title>Distributed Lucene using Hadoop RPC based RMI with dynamic classloading</title>
      <description>Hadoop RPC based RMI system for use with Lucene Searchable. Keeps the application logic on the client side with removing the need to deploy application logic to the Lucene servers. Removes the need to provision new code to potentially hundreds of servers for every application logic change. The use case is any deployment requiring Lucene on many servers. This system provides the added advantage of allowing custom Query and Filter classes (or other classes) to be defined on for example a development machine and executed on the server without deploying the custom classes to the servers first. This can save a lot of time and effort in provisioning, restarting processes. In the future this patch will include an IndexWriterService interface which will enable document indexing. This will allow subclasses of Analyzer to be dynamically loaded onto a server as documents are added by the client. Hadoop RPC is more scalable than Sun's RMI implementation because it uses non blocking sockets. Hadoop RPC is also far easier to understand and customize if needed as it is embodied in 2 main class files org.apache.hadoop.ipc.Client and org.apache.hadoop.ipc.Server. Features include automatic dynamic classloading. The dynamic classloading enables newly compiled client classes inheriting core objects such as Query or Filter to be used to query the server without first deploying the code to the server. Using RMI dynamic classloading is not used in practice because it is hard to setup, requiring placing the new code in jar files on a web server on the client. Then requires custom system properties to be setup as well as Java security manager configuration. The dynamic classloading in Hadoop RMI for Lucene uses RMI to load the classes. Custom serialization and deserialization manages the classes and the class versions on the server and client side. New class files are automatically detected and loaded using ClassLoader.getResourceAsStream and so this system does not require creating a JAR file. The use of the same networking system used for the remote method invocation is used for the loading classes over the network. This removes the necessity of a separate web server dedicated to the task and makes deployment a few lines of code.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1337</id>
      <title>[PATCH] improve searching under high concurrancy</title>
      <description>I was trying to load test my web server and kept running into a condition were the web server would become unresponsive even though the load was below one. Turns out Lucene has synchronization blocks around reading the index. It appears this was only necassary to synchronize access to a descriptor which contains a RandomAccessFile and information about the state of this file. My solution was to use a pool of descriptors so that they could be reused on subsequent reads. During periods of low contention only one or a few Descriptors will be created, but under heavy loads many Descriptors can be created to avoid synchronization. After creating and applying my patch, I was able to triple my searching throughput and fully utilize the resources, the CPU's becoming the new bottleneck. My patch modifies FSDirectory directly, but I'm not entirely sure that's the proper implementation. I'd like to help resolve this synchronization issue for other lucene users, so please let me know how I can help.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1339</id>
      <title>Add IndexReader.acquire() and release() methods using IndexReader's ref counting</title>
      <description>From: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3cPine.OSX.4.64.0807170752080.1708@c5850-a3-2-62-147-22-102.dial.proxad.net%3e I have a server where a bunch of threads are handling search requests. I have a another process that updates the index used by the search server and that asks the searcher server to reopen its index reader after the updates completed. When I reopen() the index reader, I also close the old one (if the reopen() yielded a new instance). This causes problems for the other threads that are currently in the middle of a search request. I'd like to propose the addition of two methods, acquire() and release() (attached to this bug report), that increment/decrement the ref count that IndexReader instances currently maintain for related purposes. That ref count prevents the index reader from being actually closed until it reaches zero. My server's search threads, thus acquiring and releasing the index reader can be sure that the index reader they're currently using is good until they're done with the current request, ie, until they release() it.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1340</id>
      <title>Make it posible not to include TF information in index</title>
      <description>Term Frequency is typically not needed for all fields, some CPU (reading one VInt less and one X&gt;&gt;&gt;1...) and IO can be spared by making pure boolen fields possible in Lucene. This topic has already been discussed and accepted as a part of Flexible Indexing... This issue tries to push things a bit faster forward as I have some concrete customer demands. benefits can be expected for fields that are typical candidates for Filters, enumerations, user rights, IDs or very short "texts", phone numbers, zip codes, names... Status: just passed standard test (compatibility), commited for early review, I have not tried new feature, missing some asserts and one two unit tests Complexity: simpler than expected can be used via omitTf() (who used omitNorms() will know where to find it</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1341</id>
      <title>BoostingNearQuery class (prototype)</title>
      <description>This patch implements term boosting for SpanNearQuery. Refer to: http://www.gossamer-threads.com/lists/lucene/java-user/62779 This patch works but probably needs more work. I don't like the use of 'instanceof', but I didn't want to touch Spans or TermSpans. Also, the payload code is mostly a copy of what's in BoostingTermQuery and could be common-sourced somewhere. Feel free to throw darts at it</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1343</id>
      <title>A replacement for AsciiFoldingFilter that does a more thorough job of removing diacritical marks or non-spacing modifiers.</title>
      <description>The ISOLatin1AccentFilter takes Unicode characters that have diacritical marks and replaces them with a version of that character with the diacritical mark removed. For example é becomes e. However another equally valid way of representing an accented character in Unicode is to have the unaccented character followed by a non-spacing modifier character (like this: é ) The ISOLatin1AccentFilter doesn't handle the accents in decomposed unicode characters at all. Additionally there are some instances where a word will contain what looks like an accented character, that is actually considered to be a separate unaccented character such as Ł but which to make searching easier you want to fold onto the latin1 lookalike version L . The UnicodeNormalizationFilter can filter out accents and diacritical marks whether they occur as composed characters or decomposed characters, it can also handle cases where as described above characters that look like they have diacritics (but don't) are to be folded onto the letter that they look like ( Ł -&gt; L )</description>
      <attachments/>
      <comments>28</comments>
      <commenters>12</commenters>
    </issue>
    <issue>
      <id>1344</id>
      <title>Make the Lucene jar an OSGi bundle</title>
      <description>In order to use Lucene in an OSGi environment, some additional headers are needed in the manifest of the jar. As Lucene has no dependency, it is pretty straight forward and it ill be easy to maintain I think.</description>
      <attachments/>
      <comments>67</comments>
      <commenters>13</commenters>
    </issue>
    <issue>
      <id>1346</id>
      <title>replace Vector with ArrayList in Queries</title>
      <description>Replace Vector with ArrayList in Queries. This can make a difference in heavily concurrent scenarios when Query objects are examined or compared (e.g. used as cache keys).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1351</id>
      <title>Add some ligatures (ff, fi, fl, ft, st) to ISOLatin1AccentFilter</title>
      <description>ISOLatin1AccentFilter removes common diacritics and some ligatures. This patch adds support for additional common ligatures: ff, fi, fl, ft, st.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1354</id>
      <title>Provide Programmatic Access to CheckIndex</title>
      <description>Would be nice to have programmatic access to the CheckIndex tool, so that it can be used in applications like Solr. See SOLR-566</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1356</id>
      <title>Allow easy extensions of TopDocCollector</title>
      <description>TopDocCollector's members and constructor are declared either private or package visible. It makes it hard to extend it as if you want to extend it you can reuse its hq and totatlHits members, but need to define your own. It also forces you to override getTotalHits() and topDocs(). By changing its members and constructor (the one that accepts a PQ) to protected, we allow users to extend it in order to get a different view of 'top docs' (like TopFieldCollector does), but still enjoy its getTotalHits() and topDocs() method implementations.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1360</id>
      <title>A Similarity class which has unique length norms for numTerms &lt;= 10</title>
      <description>A Similarity class which extends DefaultSimilarity and simply overrides lengthNorm. lengthNorm is implemented as a lookup for numTerms &lt;= 10, else as 1/sqrt(numTerms). This is to avoid term counts below 11 from having the same lengthNorm after stored as a single byte in the index. This is useful if your search is only on short fields such as titles or product descriptions. See mailing list discussion: http://www.nabble.com/How-to-boost-the-score-higher-in-case-user-query-matches-entire-field-value-than-just-some-words-within-a-field-td19079221.html</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1361</id>
      <title>QueryParser should have a setDateFormat(DateFormat) method</title>
      <description>Currently the only way to change the date format used by QueryParser.java is to override the getRangeQuery method. This seems a bit excessive to me. Since QueryParser isn't threadsafe (like DateFormat) I would suggest that a DateFormat field be introduced (protected DateFormat dateFormat) and a setter be introduced (public void setDateFormat(DateFormat format)) so that it's easier to customize the date format in queries. If there are good reasons against this (can't imagine, but who knows) why not introduce a protected 'DateFormat:createDateFormat())' method so that, again, it's easier for clients to override this logic.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1366</id>
      <title>Rename Field.Index.UN_TOKENIZED/TOKENIZED/NO_NORMS</title>
      <description>There is confusion about these current Field options and I think we should rename them, deprecating the old names in 2.4/2.9 and removing them in 3.0. How about this: TOKENIZED --&gt; ANALYZED UN_TOKENIZED --&gt; NOT_ANALYZED NO_NORMS --&gt; NOT_ANALYZED_NO_NORMS Should we also add ANALYZED_NO_NORMS? Spinoff from here: http://mail-archives.apache.org/mod_mbox/lucene-java-user/200808.mbox/%3C48a3076a.2679420a.1c53.ffffa5c4%40mx.google.com%3E</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1367</id>
      <title>Add a isDeleted method to IndexCommit</title>
      <description>I wish to add a IndexCommit.isDeleted() method. The use-case is that Solr will now support configurable IndexDeletionPolicy (SOLR-617). For the new replication (SOLR-561) to work, we need access to a list of IndexCommit instances which haven't been deleted yet. I can wrap the user specified IndexDeletionPolicy but since the IndexCommit does not have a isDeleted method, I may store a reference to an IndexCommit on which delete() has been called by the deletion policy. I can wrap the IndexCommit objects too just for having a isDeleted() method so a workaround exists. Not a big pain but if it can be managed on the lucene side easily, I'll appreciate it. It would save me from writing some delegate code.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1369</id>
      <title>Eliminate unnecessary uses of Hashtable and Vector</title>
      <description>Lucene uses Vector, Hashtable and Enumeration when it doesn't need to. Changing to ArrayList and HashMap may provide better performance. There are a few places Vector shows up in the API. IMHO, List should have been used for parameters and return values. There are a few distinct usages of these classes: internal but with ArrayList or HashMap would do as well. These can simply be replaced. internal and synchronization is required. Either leave as is or use a collections synchronization wrapper. As a parameter to a method where List or Map would do as well. For contrib, just replace. For core, deprecate current and add new method signature. Generated by JavaCC. (All *.jj files.) Nothing to be done here. As a base class. Not sure what to do here. (Only applies to SegmentInfos extends Vector, but it is not used in a safe manner in all places. Perhaps, implements List would be better.) As a return value from a package protected method, but synchronization is not used. Change return type. As a return value to a final method. Change to List or Map. In using a Vector the following iteration pattern is frequently used. for (int i = 0; i &lt; v.size(); i++) { Object o = v.elementAt; } This is an indication that synchronization is unimportant. The list could change during iteration.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1370</id>
      <title>Add ShingleFilter option to output unigrams if no shingles can be generated</title>
      <description>Currently if ShingleFilter.outputUnigrams==false and the underlying token stream is only one token long, then ShingleFilter.next() won't return any tokens. This patch provides a new option, outputUnigramIfNoNgrams; if this option is set and the underlying stream is only one token long, then ShingleFilter will return that token, regardless of the setting of outputUnigrams. My use case here is speeding up phrase queries. The technique is as follows: First, doing index-time analysis using ShingleFilter (using outputUnigrams==true), thereby expanding things as follows: "please divide this sentence into shingles" -&gt; "please", "please divide" "divide", "divide this" "this", "this sentence" "sentence", "sentence into" "into", "into shingles" "shingles" Second, do query-time analysis using ShingleFilter (using outputUnigrams==false and outputUnigramIfNoNgrams==true). If the user enters a phrase query, it will get tokenized in the following manner: "please divide this sentence into shingles" -&gt; "please divide" "divide this" "this sentence" "sentence into" "into shingles" By doing phrase queries with bigrams like this, I can gain a very considerable speedup. Without the outputUnigramIfNoNgrams option, then a single word query would tokenize like this: "please" -&gt; [no tokens] But thanks to outputUnigramIfNoNgrams, single words will now tokenize like this: "please" -&gt; "please" **** The patch also adds a little to the pre-outputUnigramIfNoNgrams option tests. **** I'm not sure if the patch in this state is useful to anyone else, but I thought I should throw it up here and try to find out.</description>
      <attachments/>
      <comments>27</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1372</id>
      <title>Proposal: introduce more sensible sorting when a doc has multiple values for a term</title>
      <description>At the moment, FieldCacheImpl has somewhat disconcerting values when sorting on a field for which multiple values exist for one document. For example, imagine a field "fruit" which is added to a document multiple times, with the values as follows: doc 1: {"apple"} doc 2: {"banana"} doc 3: {"apple", "banana"} doc 4: {"apple", "zebra"} if one sorts on the field "fruit", the loop in FieldCacheImpl.stringsIndexCache.createValue() (and similarly for the other methods in the various FieldCacheImpl caches) does the following: while (termDocs.next()) { retArray[termDocs.doc()] = t; } which means that we look over the terms in their natural order and, on each one, overwrite retArray[doc] with the value for each document with that term. Effectively, this overwriting means that a string sort in this circumstance will sort by the LAST term lexicographically, so the docs above will effecitvely be sorted as if they had the single values ("apple", "banana", "banana", "zebra") which is nonintuitive. To change this to sort on the first time in the TermEnum seems relatively trivial and low-overhead; while it's not perfect (it's not local-aware, for example) the behaviour seems much more sensible to me. Interested to see what people think. Patch to follow.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1375</id>
      <title>add IndexCommit.getTimestamp method</title>
      <description>Convenience method for getDirectory().fileModified(getSegmentsFileName()).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1377</id>
      <title>Add HTMLStripReader and WordDelimiterFilter from SOLR</title>
      <description>SOLR has two classes HTMLStripReader and WordDelimiterFilter which are very useful for a wide variety of use cases. It would be good to place them into core Lucene.</description>
      <attachments/>
      <comments>35</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1380</id>
      <title>Patch for ShingleFilter.enablePositions (or PositionFilter)</title>
      <description>Make it possible for all words and shingles to be placed at the same position, that is for all shingles (and unigrams if included) to be treated as synonyms of each other. Today the shingles generated are synonyms only to the first term in the shingle. For example the query "abcd efgh ijkl" results in: ("abcd" "abcd efgh" "abcd efgh ijkl") ("efgh" efgh ijkl") ("ijkl") where "abcd efgh" and "abcd efgh ijkl" are synonyms of "abcd", and "efgh ijkl" is a synonym of "efgh". There exists no way today to alter which token a particular shingle is a synonym for. This patch takes the first step in making it possible to make all shingles (and unigrams if included) synonyms of each other. See http://comments.gmane.org/gmane.comp.jakarta.lucene.user/34746 for mailing list thread.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1382</id>
      <title>Allow storing user data when IndexWriter.commit() is called</title>
      <description>Spinoff from here: http://www.mail-archive.com/java-user@lucene.apache.org/msg22303.html The idea is to allow optionally passing an opaque String commitUserData to the IndexWriter.commit method. This String would be stored in the segments_N file, and would be retrievable by an IndexReader. Applications could then use this to assign meaning to each commit. It would be nice to get this done for 2.4, but I don't think we should hold the release for it.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1387</id>
      <title>Add LocalLucene</title>
      <description>Local Lucene (Geo-search) has been donated to the Lucene project, per https://issues.apache.org/jira/browse/INCUBATOR-77. This issue is to handle the Lucene portion of integration. See http://lucene.markmail.org/message/orzro22sqdj3wows?q=LocalLucene</description>
      <attachments/>
      <comments>22</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>1388</id>
      <title>Add init method to CloseableThreadLocal</title>
      <description>Java ThreadLocal has an init method that allows subclasses to easily instantiate an initial value.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1390</id>
      <title>add ASCIIFoldingFilter and deprecate ISOLatin1AccentFilter</title>
      <description>The ISOLatin1AccentFilter is removing accents from accented characters in the ISO Latin 1 character set. It does what it does and there is no bug with it. It would be nicer, though, if there was a more comprehensive version of this code that included not just ISO-Latin-1 (ISO-8859-1) but the entire Latin 1 and Latin Extended A unicode blocks. See: http://en.wikipedia.org/wiki/Latin-1_Supplement_unicode_block See: http://en.wikipedia.org/wiki/Latin_Extended-A_unicode_block That way, all languages using roman characters are covered. A new class, ISOLatinAccentFilter is attached. It is intended to supercede ISOLatin1AccentFilter which should get deprecated.</description>
      <attachments/>
      <comments>41</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1392</id>
      <title>Some small javadocs/extra import fixes</title>
      <description>Two things that Uwe Schindler caught, plus fixes for javadoc warnings in core. I plan to commit to trunk &amp; 2.4.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1396</id>
      <title>Improve PhraseQuery.toString()</title>
      <description>PhraseQuery.toString() is overly simplistic, in that it doesn't correctly show phrases with gaps or overlapping terms. This may be misleading when presenting phrase queries built using complex analyzers and filters.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1398</id>
      <title>Add ReverseStringFilter</title>
      <description>add ReverseStringFilter and ReverseStringAnalyzer that can be used for backword much. For Example, "*ry", "*ing", "*ber".</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1400</id>
      <title>Add Apache RAT (Release Audit Tool) target to build.xml</title>
      <description>Apache RAT is a useful tool to check for common mistakes in our source code (eg missing copyright headers): http://incubator.apache.org/rat/ I'm just copying the patch Grant worked out for Solr (SOLR-762). I plan to commit to 2.4 &amp; 2.9.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1405</id>
      <title>Support for new Resources model in ant 1.7 in Lucene ant task.</title>
      <description>Ant Task for Lucene should use modern Resource model (not only FileSet child element). There is a patch with required changes. Supported by old (ant 1.6) and new (ant 1.7) resources model: &lt;index ....&gt; &lt;!-- Lucene Ant Task --&gt; &lt;fileset ... /&gt; &lt;/index&gt; Supported only by new (ant 1.7) resources model: &lt;index ....&gt; &lt;!-- Lucene Ant Task --&gt; &lt;filelist ... /&gt; &lt;/index&gt; &lt;index ....&gt; &lt;!-- Lucene Ant Task --&gt; &lt;userdefinied-filesource ... /&gt; &lt;/index&gt;</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1406</id>
      <title>new Arabic Analyzer (Apache license)</title>
      <description>I've noticed there is no Arabic analyzer for Lucene, most likely because Tim Buckwalter's morphological dictionary is GPL. However, it is not necessary to have full morphological analysis engine for a quality arabic search. This implementation implements the light-8s algorithm present in the following paper: http://ciir.cs.umass.edu/pubfiles/ir-249.pdf As you can see from the paper, improvement via this method over searching surface forms (as lucene currently does) is significant, with almost 100% improvement in average precision. While I personally don't think all the choices were the best, and some easily improvements are still possible, the major motivation for implementing it exactly the way it is presented in the paper is that the algorithm is TREC-tested, so the precision/recall improvements to lucene are already documented. For a stopword list, I used a list present at http://members.unine.ch/jacques.savoy/clef/index.html simply because the creator of this list documents the data as BSD-licensed. This implementation (Analyzer) consists of above mentioned stopword list plus two filters: ArabicNormalizationFilter: performs orthographic normalization (such as hamza seated on alif, alif maksura, teh marbuta, removal of harakat, tatweel, etc) ArabicStemFilter: performs arabic light stemming Both filters operate directly on termbuffer for maximum performance. There is no object creation in this Analyzer. There are no external dependencies. I've indexed about half a billion words of arabic text and tested against that. If there are any issues with this implementation I am willing to fix them. I use lucene on a daily basis and would like to give something back. Thanks.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1407</id>
      <title>Refactor Searchable to not have RMI Remote dependency</title>
      <description>Per http://lucene.markmail.org/message/fu34tuomnqejchfj?q=RemoteSearchable We should refactor Searchable slightly so that it doesn't extend the java.rmi.Remote marker interface. I believe the same could be achieved by just marking the RemoteSearchable and refactoring the RMI implementation out of core and into a contrib. If we do this, we should deprecate/denote it for 2.9 and then move it for 3.0</description>
      <attachments/>
      <comments>20</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1410</id>
      <title>PFOR implementation</title>
      <description>Implementation of Patched Frame of Reference.</description>
      <attachments/>
      <comments>101</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1411</id>
      <title>Enable IndexWriter to open an arbitrary commit point</title>
      <description>With a 2-phase commit involving multiple resources, each resource first does its prepareCommit and then if all are successful they each commit. If an exception or timeout/power loss is hit in any of the resources during prepareCommit or commit, all of the resources must then rollback. But, because IndexWriter always opens the most recent commit, getting Lucene to rollback after commit() has been called is not easy, unless you make Lucene the last resource to commit. A simple workaround is to simply remove the segments_N files of the newer commits but that's sort of a hassle. To fix this, we just need to add a ctor to IndexWriter that takes an IndexCommit. We recently added this for IndexReader (LUCENE-1311) as well. This ctor is definitely an "expert" method, and only makes sense if you have a custom DeletionPolicy that preserves more than just the most recent commit.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1413</id>
      <title>Creating PlainTextDictionary with UTF8 files</title>
      <description>Generate indexes from text files is good, but can't read utf8 files. It can easily made by adding the following code to PlainTextDictionary.java: public PlainTextDictionary(InputStream dictFile, String fileEncoding) throws UnsupportedEncodingException { in = new BufferedReader(new InputStreamReader(dictFile, fileEncoding)); }</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1417</id>
      <title>Allowing for distance measures that incorporate frequency/popularity for SuggestWord comparison</title>
      <description>Spelling suggestions are currently ordered first by a string edit distance measure, then by popularity/frequency. This limits the ability of popularity/frequency to affect suggestions. I think it would be better for the distance measure to accept popularity/frequency as an argument and provide a distance/score that incorporates any popularity/frequency considerations. I.e. change StringDistance.getDistance to accept an additional argument: frequency of the potential suggestion. The new SuggestWord.compareTo function would only order by score. We could achieve the existing behavior by adding a small inverse frequency value to the distances.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1419</id>
      <title>Expert API to specify indexing chain</title>
      <description>It would be nice to add an expert API to specify an indexing chain, so that we can make use of Mike's nice LUCENE-1301 feature. This patch simply adds a package-protected expert API to IndexWriter and DocumentsWriter. It adds a inner, abstract class to DocumentsWriter called IndexingChain, and a default implementation that is the currently used one. This might not be the final solution, but a nice way to play with different modules in the indexing chain. Could you take a look at the patch, Mike?</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1420</id>
      <title>Similarity.lengthNorm and positionIncrement=0</title>
      <description>Calculation of lengthNorm factor should in some cases take into account the number of tokens with positionIncrement=0. This should be made optional, to support two different scenarios: when analyzers insert artificially constructed tokens into TokenStream (e.g. ASCII-fied versions of accented terms, stemmed terms), and it's unlikely that users submit queries containing both versions of tokens: in this case lengthNorm calculation should ignore the tokens with positionIncrement=0. when analyzers insert synonyms, and it's likely that users may submit queries that contain multiple synonymous terms: in this case the lengthNorm should be calculated as it is now, i.e. it should take into account all terms no matter what is their positionIncrement. The default should be backward-compatible, i.e. it should count all tokens. (See also the discussion here: http://markmail.org/message/vfvmzrzhr6pya22h )</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1421</id>
      <title>Ability to group search results by field</title>
      <description>It would be awesome to group search results by specified field. Some functionality was provided for Apache Solr but I think it should be done in Core Lucene. There could be some useful information like total hits about collapsed data like total count and so on. Thanks, Artyom</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1422</id>
      <title>New TokenStream API</title>
      <description>This is a very early version of the new TokenStream API that we started to discuss here: http://www.gossamer-threads.com/lists/lucene/java-dev/66227 This implementation is a bit different from what I initially proposed in the thread above. I introduced a new class called AttributedToken, which contains the same termBuffer logic from Token. In addition it has a lazily-initialized map of Class&lt;? extends Attribute&gt; -&gt; Attribute. Attribute is also a new class in a new package, plus several implementations like PositionIncrementAttribute, PayloadAttribute, etc. Similar to my initial proposal is the prototypeToken() method which the consumer (e. g. DocumentsWriter) needs to call. The token is created by the tokenizer at the end of the chain and pushed through all filters to the end consumer. The tokenizer and also all filters can add Attributes to the token and can keep references to the actual types of the attributes that they need to read of modify. This way, when boolean nextToken() is called, no casting is necessary. I added a class called TestNewTokenStreamAPI which is not really a test case yet, but has a static demo() method, which demonstrates how to use the new API. The reason to not merge Token and TokenStream into one class is that we might have caching (or tee/sink) filters in the chain that might want to store cloned copies of the tokens in a cache. I added a new class NewCachingTokenStream that shows how such a class could work. I also implemented a deep clone method in AttributedToken and a copyFrom(AttributedToken) method, which is needed for the caching. Both methods have to iterate over the list of attributes. The Attribute subclasses itself also have a copyFrom(Attribute) method, which unfortunately has to down- cast to the actual type. I first thought that might be very inefficient, but it's not so bad. Well, if you add all Attributes to the AttributedToken that our old Token class had (like offsets, payload, posIncr), then the performance of the caching is somewhat slower (~40%). However, if you add less attributes, because not all might be needed, then the performance is even slightly faster than with the old API. Also the new API is flexible enough so that someone could implement a custom caching filter that knows all attributes the token can have, then the caching should be just as fast as with the old API. This patch is not nearly ready, there are lot's of things missing: unit tests change DocumentsWriter to use new API (in backwards-compatible fashion) patch is currently java 1.5; need to change before commiting to 2.9 all TokenStreams and -Filters should be changed to use new API javadocs incorrect or missing hashcode and equals methods missing in Attributes and AttributedToken I wanted to submit it already for brave people to give me early feedback before I spend more time working on this.</description>
      <attachments/>
      <comments>38</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1424</id>
      <title>Change all multi-term querys so that they extend MultiTermQuery and allow for a constant score mode</title>
      <description>Cleans up a bunch of code duplication, closer to how things should be - design wise, gives us constant score for all the multi term queries, and allows us at least the option of highlighting the constant score queries without much further work.</description>
      <attachments/>
      <comments>45</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1425</id>
      <title>Add ConstantScore highlighting support to SpanScorer</title>
      <description>Its actually easy enough to support the family of constantscore queries with the new SpanScorer. This will also remove the requirement that you rewrite queries against the main index before highlighting (in fact, if you do, the constantscore queries will not highlight).</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1426</id>
      <title>Next steps towards flexible indexing</title>
      <description>In working on LUCENE-1410 (PFOR compression) I tried to prototype switching the postings files to use PFOR instead of vInts for encoding. But it quickly became difficult. EG we currently mux the skip data into the .frq file, which messes up the int blocks. We inline payloads with positions which would also mess up the int blocks. Skipping offsets and TermInfo offsets hardwire the file pointers of frq &amp; prox files yet I need to change these to block + offset, etc. Separately this thread also started up, on how to customize how Lucene stores positional information in the index: http://www.gossamer-threads.com/lists/lucene/java-user/66264 So I decided to make a bit more progress towards "flexible indexing" by first modularizing/isolating the classes that actually write the index format. The idea is to capture the logic of each (terms, freq, positions/payloads) into separate interfaces and switch the flushing of a new segment as well as writing the segment during merging to use the same APIs.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1427</id>
      <title>QueryWrapperFilter should not do scoring</title>
      <description>The purpose of QueryWrapperFilter is to simply filter to include the docIDs that match the query. Its implementation is wasteful now because it computes scores for those matching docs even though the score is unused. We could fix this by getting a Scorer and iterating through the docs without asking for the score: Index: src/java/org/apache/lucene/search/QueryWrapperFilter.java =================================================================== --- src/java/org/apache/lucene/search/QueryWrapperFilter.java (revision 707060) +++ src/java/org/apache/lucene/search/QueryWrapperFilter.java (working copy) @@ -62,11 +62,9 @@ public DocIdSet getDocIdSet(IndexReader reader) throws IOException { final OpenBitSet bits = new OpenBitSet(reader.maxDoc()); - new IndexSearcher(reader).search(query, new HitCollector() { - public final void collect(int doc, float score) { - bits.set(doc); // set bit for hit - } - }); + final Scorer scorer = query.weight(new IndexSearcher(reader)).scorer(reader); + while(scorer.next()) + bits.set(scorer.doc()); return bits; } Maybe I'm missing something, but this seams like a simple win?</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1433</id>
      <title>Changes.html generation improvements</title>
      <description>Bug fixes for and improvements to changes2html.pl, which generates Changes.html from CHANGES.txt: When the current location has a fragment identifier, expand parent sections, so that the linked-to section is visible. Properly handle beginning-of-release comments that don't fall under a section heading (previously: some content in release "1.9 final" was invisible). Auto-linkify SOLR-XXX and INFRA-XXX JIRA issues (previously: only LUCENE-XXX issues). Auto-linkify Bugzilla bugs prefaced with "Issue" (previously: only "Bug" and "Patch"). Auto-linkify Bugzilla bugs in the form "bugs XXXXX and YYYYY". Auto-linkify issues that follow attributions.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1434</id>
      <title>IndexableBinaryStringTools: convert arbitrary byte sequences into Strings that can be used as index terms, and vice versa</title>
      <description>Provides support for converting byte sequences to Strings that can be used as index terms, and back again. The resulting Strings preserve the original byte sequences' sort order (assuming the bytes are interpreted as unsigned). The Strings are constructed using a Base 8000h encoding of the original binary data - each char of an encoded String represents a 15-bit chunk from the byte sequence. Base 8000h was chosen because it allows for all lower 15 bits of char to be used without restriction; the surrogate range [U+D800-U+DFFF] does not represent valid chars, and would require complicated handling to avoid them and allow use of char's high bit. This class is intended to serve as a mechanism to allow CollationKeys to serve as index terms.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1435</id>
      <title>CollationKeyFilter: convert tokens into CollationKeys encoded using IndexableBinaryStringTools</title>
      <description>Converts each token into its CollationKey using the provided collator, and then encodes the CollationKey with IndexableBinaryStringTools, to allow it to be stored as an index term. This will allow for efficient range searches and Sorts over fields that need collation for proper ordering.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1437</id>
      <title>Faster skipTo on SegmentTermEnum</title>
      <description>I've been trying to speed up function I have that involves calling TermEnum.skipTo a lot on a very large index with many terms. This patch avoids excessive object creation of Term objects as part of the default implementation in the TermEnum base class. My tests output with this patch was as follows: Took 1894 ms for 42304 calls to skipTo on index with 182693176 docs and unmodified Lucene 2.4 results were: Took 2438 ms for 42304 calls to skipTo on index with 182693176 docs The logic is based on the existing code in SegmentTermEnum. scanTo(Term term) and avoids the object construction code found in the TermEnum.skipTo code which repeatedly calls next() and term() to create Term objects. Anyone see any negative side effects in changing this?</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1440</id>
      <title>Add ability to run backwards-compatibility tests automatically</title>
      <description>This is an idea Doug mentioned on LUCENE-1422. This patch adds new targets to build.xml to automatically download the junit tests from a previous Lucene release and run them against the current core. Execute tests like this: ant -Dtag=lucene_2_4_0 test-tag It will create a new directory tags/lucene_2_4_0 and fetch the tests from the svn repository and run them.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1446</id>
      <title>Run 'test-tag' in nightly build</title>
      <description>Changes in this trivial patch: ant target 'nightly' now also depends on 'test-tag' adds property 'compatibility.tag' to common-build.xml that should always point to the last tagged release; its unit tests will be downloaded unless -Dtag="" is used to override 'download-tag' does not fail if the svn checkout wasn't successful; instead 'test-tag' checks if the specified tag is checked-out and available, if not it fails</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1447</id>
      <title>Improve payload error handling/reporting</title>
      <description>If you try to load a payload more than once you get the exception: IOException("Payload cannot be loaded more than once for the same term position."); You also get this exception if their is no payload to load, and its a bit confusing, as the message doesn't relate to the actual problem.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1458</id>
      <title>Further steps towards flexible indexing</title>
      <description>I attached a very rough checkpoint of my current patch, to get early feedback. All tests pass, though back compat tests don't pass due to changes to package-private APIs plus certain bugs in tests that happened to work (eg call TermPostions.nextPosition() too many times, which the new API asserts against). [Aside: I think, when we commit changes to package-private APIs such that back-compat tests don't pass, we could go back, make a branch on the back-compat tag, commit changes to the tests to use the new package private APIs on that branch, then fix nightly build to use the tip of that branch?o] There's still plenty to do before this is committable! This is a rather large change: Switches to a new more efficient terms dict format. This still uses tii/tis files, but the tii only stores term &amp; long offset (not a TermInfo). At seek points, tis encodes term &amp; freq/prox offsets absolutely instead of with deltas delta. Also, tis/tii are structured by field, so we don't have to record field number in every term. . On first 1 M docs of Wikipedia, tii file is 36% smaller (0.99 MB -&gt; 0.64 MB) and tis file is 9% smaller (75.5 MB -&gt; 68.5 MB). . RAM usage when loading terms dict index is significantly less since we only load an array of offsets and an array of String (no more TermInfo array). It should be faster to init too. . This part is basically done. Introduces modular reader codec that strongly decouples terms dict from docs/positions readers. EG there is no more TermInfo used when reading the new format. . There's nice symmetry now between reading &amp; writing in the codec chain – the current docs/prox format is captured in: FormatPostingsTermsDictWriter/Reader FormatPostingsDocsWriter/Reader (.frq file) and FormatPostingsPositionsWriter/Reader (.prx file). This part is basically done. Introduces a new "flex" API for iterating through the fields, terms, docs and positions: FieldProducer -&gt; TermsEnum -&gt; DocsEnum -&gt; PostingsEnum This replaces TermEnum/Docs/Positions. SegmentReader emulates the old API on top of the new API to keep back-compat. Next steps: Plug in new codecs (pulsing, pfor) to exercise the modularity / fix any hidden assumptions. Expose new API out of IndexReader, deprecate old API but emulate old API on top of new one, switch all core/contrib users to the new API. Maybe switch to AttributeSources as the base class for TermsEnum, DocsEnum, PostingsEnum – this would give readers API flexibility (not just index-file-format flexibility). EG if someone wanted to store payload at the term-doc level instead of term-doc-position level, you could just add a new attribute. Test performance &amp; iterate.</description>
      <attachments/>
      <comments>256</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>1461</id>
      <title>Cached filter for a single term field</title>
      <description>These classes implement inexpensive range filtering over a field containing a single term. They do this by building an integer array of term numbers (storing the term-&gt;number mapping in a TreeMap) and then implementing a fast integer comparison based DocSetIdIterator. This code is currently being used to do age range filtering, but could also be used to do other date filtering or in any application where there need to be multiple filters based on the same single term field. I have an untested implementation of single term filtering and have considered but not yet implemented term set filtering (useful for location based searches) as well. The code here is fairly rough; it works but lacks javadocs and toString() and hashCode() methods etc. I'm posting it here to discover if there is other interest in this feature; I don't mind fixing it up but would hate to go to the effort if it's not going to make it into Lucene.</description>
      <attachments/>
      <comments>70</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1466</id>
      <title>CharFilter - normalize characters before tokenizer</title>
      <description>This proposes to import CharFilter that has been introduced in Solr 1.4. Please see for the details: SOLR-822 http://www.nabble.com/Proposal-for-introducing-CharFilter-to20327007.html</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1469</id>
      <title>isValid should be invoked after analyze rather than before it so it can validate the output of analyze</title>
      <description>The Synonym map has a protected method String analyze(String word) designed for custom stemming. However, before analyze is invoked on a word, boolean isValid(String str) is used to validate the word - which causes the program to discard words that maybe useable by the custom analyze method. I think that isValid should be invoked after analyze rather than before it so it can validate the output of analyze and allow implemters to decide what is valid for the overridden analyze method. (In fact, if you look at code snippet below, isValid should really go after the empty string check) This is a two line change in org.apache.lucene.index.memory.SynonymMap /* Part B: ignore phrases (with spaces and hyphens) and non-alphabetic words, and let user customize word (e.g. do some stemming) */ if (!isValid(word)) continue; // ignore word = analyze(word); if (word == null || word.length() == 0) continue; // ignore</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1470</id>
      <title>Add TrieRangeFilter to contrib</title>
      <description>According to the thread in java-dev (http://www.gossamer-threads.com/lists/lucene/java-dev/67807 and http://www.gossamer-threads.com/lists/lucene/java-dev/67839), I want to include my fast numerical range query implementation into lucene contrib-queries. I implemented (based on RangeFilter) another approach for faster RangeQueries, based on longs stored in index in a special format. The idea behind this is to store the longs in different precision in index and partition the query range in such a way, that the outer boundaries are search using terms from the highest precision, but the center of the search Range with lower precision. The implementation stores the longs in 8 different precisions (using a class called TrieUtils). It also has support for Doubles, using the IEEE 754 floating-point "double format" bit layout with some bit mappings to make them binary sortable. The approach is used in rather big indexes, query times are even on low performance desktop computers &lt;&lt;100 ms for very big ranges on indexes with 500000 docs. I called this RangeQuery variant and format "TrieRangeRange" query because the idea looks like the well-known Trie structures (but it is not identical to real tries, but algorithms are related to it).</description>
      <attachments/>
      <comments>108</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>1471</id>
      <title>Faster MultiSearcher.search merge docs</title>
      <description>MultiSearcher.search places sorted search results from individual searchers into a PriorityQueue. This can be made to be more optimal by taking advantage of the fact that the results returned are already sorted. The proposed solution places the sub-searcher results iterator into a custom PriorityQueue that produces the sorted ScoreDocs.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1472</id>
      <title>DateTools.stringToDate() can cause lock contention under load</title>
      <description>Load testing our application (the JIRA Issue Tracker) has shown that threads spend a lot of time blocked in DateTools.stringToDate(). The stringToDate() method uses a singleton SimpleDateFormat object to parse the dates. Each call to SimpleDateFormat.parse() is synchronized because SimpleDateFormat is not thread safe.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1475</id>
      <title>Expose sub-IndexReaders from MultiReader or MultiSegmentReader</title>
      <description>MultiReader and MultiSegmentReader are package protected and do not expose the underlying sub-IndexReaders. A way to expose the sub-readers is to have an interface that an IndexReader may be cast to that exposes the underlying readers. This is for realtime indexing.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1476</id>
      <title>BitVector implement DocIdSet, IndexReader returns DocIdSet deleted docs</title>
      <description>Update BitVector to implement DocIdSet. Expose deleted docs DocIdSet from IndexReader.</description>
      <attachments/>
      <comments>101</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>1477</id>
      <title>Pluggable SegmentReader.deletedDocs</title>
      <description>Expose a set method in SegmentReader that allows setting the deletedDocs variable. For realtime indexing.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1478</id>
      <title>Missing possibility to supply custom FieldParser when sorting search results</title>
      <description>When implementing the new TrieRangeQuery for contrib (LUCENE-1470), I was confronted by the problem that the special trie-encoded values (which are longs in a special encoding) cannot be sorted by Searcher.search() and SortField. The problem is: If you use SortField.LONG, you get NumberFormatExceptions. The trie encoded values may be sorted using SortField.String (as the encoding is in such a way, that they are sortable as Strings), but this is very memory ineffective. ExtendedFieldCache gives the possibility to specify a custom LongParser when retrieving the cached values. But you cannot use this during searching, because there is no possibility to supply this custom LongParser to the SortField. I propose a change in the sort classes: Include a pointer to the parser instance to be used in SortField (if not given use the default). My idea is to create a SortField using a new constructor SortField(String field, int type, Object parser, boolean reverse) The parser is "object" because all current parsers have no super-interface. The ideal solution would be to have: SortField(String field, int type, FieldCache.Parser parser, boolean reverse) and FieldCache.Parser is a super-interface (just empty, more like a marker-interface) of all other parsers (like LongParser...). The sort implementation then must be changed to respect the given parser (if not NULL), else use the default FieldCache.getXXXX without parser.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1480</id>
      <title>Wrap messages output with a check of InfoStream != null</title>
      <description>I've found several places in the code where messages are output w/o first checking if infoStream != null. The result is that in most of the time, unnecessary strings are created but never output (because infoStream is not set). We should follow Java's logging best practices, where a log message is always output in the following format: if (logger.isLoggable(leve)) { logger.log(level, msg); } Log messages are usually created w/o paying too much attention to performance (such as string concatenation using '+' instead of StringBuffer). Therefore, at runtime it is important to avoid creating those messages, if they will be discarded eventually. I will add a method to IndexWriter messagesEnabled() and then use it wherever a call to iw.message() is made. Patch will follow</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1482</id>
      <title>Replace infoSteram by a logging framework (SLF4J)</title>
      <description>Lucene makes use of infoStream to output messages in its indexing code only. For debugging purposes, when the search application is run on the customer side, getting messages from other code flows, like search, query parsing, analysis etc can be extremely useful. There are two main problems with infoStream today: 1. It is owned by IndexWriter, so if I want to add logging capabilities to other classes I need to either expose an API or propagate infoStream to all classes (see for example DocumentsWriter, which receives its infoStream instance from IndexWriter). 2. I can either turn debugging on or off, for the entire code. Introducing a logging framework can allow each class to control its logging independently, and more importantly, allows the application to turn on logging for only specific areas in the code (i.e., org.apache.lucene.index.*). I've investigated SLF4J (stands for Simple Logging Facade for Java) which is, as it names states, a facade over different logging frameworks. As such, you can include the slf4j.jar in your application, and it recognizes at deploy time what is the actual logging framework you'd like to use. SLF4J comes with several adapters for Java logging, Log4j and others. If you know your application uses Java logging, simply drop slf4j.jar and slf4j-jdk14.jar in your classpath, and your logging statements will use Java logging underneath the covers. This makes the logging code very simple. For a class A the logger will be instantiated like this: public class A { private static final logger = LoggerFactory.getLogger(A.class); } And will later be used like this: public class A { private static final logger = LoggerFactory.getLogger(A.class); public void foo() { if (logger.isDebugEnabled()) { logger.debug("message"); } } } That's all ! Checking for isDebugEnabled is very quick, at least using the JDK14 adapter (but I assume it's fast also over other logging frameworks). The important thing is, every class controls its own logger. Not all classes have to output logging messages, and we can improve Lucene's logging gradually, w/o changing the API, by adding more logging messages to interesting classes. I will submit a patch shortly</description>
      <attachments/>
      <comments>22</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1483</id>
      <title>Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector</title>
      <description>This issue changes how an IndexSearcher searches over multiple segments. The current method of searching multiple segments is to use a MultiSegmentReader and treat all of the segments as one. This causes filters and FieldCaches to be keyed to the MultiReader and makes reopen expensive. If only a few segments change, the FieldCache is still loaded for all of them. This patch changes things by searching each individual segment one at a time, but sharing the HitCollector used across each segment. This allows FieldCaches and Filters to be keyed on individual SegmentReaders, making reopen much cheaper. FieldCache loading over multiple segments can be much faster as well - with the old method, all unique terms for every segment is enumerated against each segment - because of the likely logarithmic change in terms per segment, this can be very wasteful. Searching individual segments avoids this cost. The term/document statistics from the multireader are used to score results for each segment. When sorting, its more difficult to use a single HitCollector for each sub searcher. Ordinals are not comparable across segments. To account for this, a new field sort enabled HitCollector is introduced that is able to collect and sort across segments (because of its ability to compare ordinals across segments). This TopFieldCollector class will collect the values/ordinals for a given segment, and upon moving to the next segment, translate any ordinals/values so that they can be compared against the values for the new segment. This is done lazily. All and all, the switch seems to provide numerous performance benefits, in both sorted and non sorted search. We were seeing a good loss on indices with lots of segments (1000?) and certain queue sizes / queries, but the latest results seem to show thats been mostly taken care of (you shouldnt be using such a large queue on such a segmented index anyway). Introduces MultiReaderHitCollector - a HitCollector that can collect across multiple IndexReaders. Old HitCollectors are wrapped to support multiple IndexReaders. TopFieldCollector - a HitCollector that can compare values/ordinals across IndexReaders and sort on fields. FieldValueHitQueue - a Priority queue that is part of the TopFieldCollector implementation. FieldComparator - a new Comparator class that works across IndexReaders. Part of the TopFieldCollector implementation. FieldComparatorSource - new class to allow for custom Comparators. Alters IndexSearcher uses a single HitCollector to collect hits against each individual SegmentReader. All the other changes stem from this Deprecates TopFieldDocCollector FieldSortedHitQueue</description>
      <attachments/>
      <comments>319</comments>
      <commenters>14</commenters>
    </issue>
    <issue>
      <id>1484</id>
      <title>Remove SegmentReader.document synchronization</title>
      <description>This is probably the last synchronization issue in Lucene. It is the document method in SegmentReader. It is avoidable by using a threadlocal for FieldsReader.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1485</id>
      <title>Use OpenBitSet instead of BitVector in SegmentReader</title>
      <description>Tried out BitVector.get vs OpenBitSet.get here's the results which are about the same after running 25 times in milliseconds. It is assumed that implementing DocIdSetIterator in SegmentTermDocs will speed things up more. bit set size: 10,485,760 set bits count: 524,032 openbitset: 68 bitvector: 89 24% speed increase. I will implement a patch that adds the WriteableBitSet interface and make a subclass of OpenBitSet that is writeable to disk. We're working on an isSparse method for OpenBitSet.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1486</id>
      <title>Wildcards, ORs etc inside Phrase queries</title>
      <description>An extension to the default QueryParser that overrides the parsing of PhraseQueries to allow more complex syntax e.g. wildcards in phrase queries. The implementation feels a little hacky - this is arguably better handled in QueryParser itself. This works as a proof of concept for much of the query parser syntax. Examples from the Junit test include: checkMatches("\"j* smyth~\"", "1,2"); //wildcards and fuzzies are OK in phrases checkMatches("\"(jo* -john) smith\"", "2"); // boolean logic works checkMatches("\"jo* smith\"~2", "1,2,3"); // position logic works. checkBadQuery("\"jo* id:1 smith\""); //mixing fields in a phrase is bad checkBadQuery("\"jo* \"smith\" \""); //phrases inside phrases is bad checkBadQuery("\"jo* [sma TO smZ]\" \""); //range queries inside phrases not supported Code plus Junit test to follow...</description>
      <attachments/>
      <comments>99</comments>
      <commenters>18</commenters>
    </issue>
    <issue>
      <id>1487</id>
      <title>FieldCacheTermsFilter</title>
      <description>This is a companion to FieldCacheRangeFilter except it operates on a set of terms rather than a range. It works best when the set is comparatively large or the terms are comparatively common.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1488</id>
      <title>multilingual analyzer based on icu</title>
      <description>The standard analyzer in lucene is not exactly unicode-friendly with regards to breaking text into words, especially with respect to non-alphabetic scripts. This is because it is unaware of unicode bounds properties. I actually couldn't figure out how the Thai analyzer could possibly be working until i looked at the jflex rules and saw that codepoint range for most of the Thai block was added to the alphanum specification. defining the exact codepoint ranges like this for every language could help with the problem but you'd basically be reimplementing the bounds properties already stated in the unicode standard. in general it looks like this kind of behavior is bad in lucene for even latin, for instance, the analyzer will break words around accent marks in decomposed form. While most latin letter + accent combinations have composed forms in unicode, some do not. (this is also an issue for asciifoldingfilter i suppose). I've got a partially tested standardanalyzer that uses icu Rule-based BreakIterator instead of jflex. Using this method you can define word boundaries according to the unicode bounds properties. After getting it into some good shape i'd be happy to contribute it for contrib but I wonder if theres a better solution so that out of box lucene will be more friendly to non-ASCII text. Unfortunately it seems jflex does not support use of these properties such as [\p {Word_Break = Extend} ] so this is probably the major barrier. Thanks, Robert</description>
      <attachments/>
      <comments>35</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1492</id>
      <title>Allow readOnly OpenReader task</title>
      <description>I'd like to change OpenReader in contrib/benchmark to open a readOnly reader by default, and take readOnly optional param if for some reason a "writable IndexReader" becomes necessary in the future.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1493</id>
      <title>Enable setting hits queue size in Search*Task in contrib/benchmark</title>
      <description>In testing for LUCENE-1483, I'd like to try different collector queue sizes during benchmarking. But currently contrib/benchmark uses deprecated Hits with hardwired "top 100" queue size. I'll switch it to the TopDocs APIs.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1494</id>
      <title>masking field of span for cross searching across multiple fields (many-to-one style)</title>
      <description>This issue is to cover the changes required to do a search across multiple fields with the same name in a fashion similar to a many-to-one database. Below is my post on java-dev on the topic, which details the changes we need: — We have an interesting situation where we are effectively indexing two 'entities' in our system, which share a one-to-many relationship (imagine 'User' and 'Delivery Address' for demonstration purposes). At the moment, we index one Lucene Document per 'many' end, duplicating the 'one' end data, like so: userid: 1 userfirstname: fred addresscountry: au addressphone: 1234 userid: 1 userfirstname: fred addresscountry: nz addressphone: 5678 userid: 2 userfirstname: mary addresscountry: au addressphone: 5678 (note: 2 Documents indexed for user 1). This is somewhat annoying for us, because when we search in Lucene the results we want back (conceptually) are at the 'user' level, so we have to collapse the results by distinct user id, etc. etc (let alone that it blows out the size of our index enormously). So why do we do it? It would make more sense to use multiple fields: userid: 1 userfirstname: fred addresscountry: au addressphone: 1234 addresscountry: nz addressphone: 5678 userid: 2 userfirstname: mary addresscountry: au addressphone: 5678 But imagine the search "+addresscountry:au +addressphone:5678". We'd like this to match ONLY Mary, but of course it matches Fred also because he matches both those terms (just for different addresses). There are two aspects to the approach we've (more or less) got working but I'd like to run them past the group and see if they're worth trying to get them into Lucene proper (if so, I'll create a JIRA issue for them) 1) Use a modified SpanNearQuery. If we assume that country + phone will always be one token, we can rely on the fact that the positions of 'au' and '5678' in Fred's document will be different. SpanQuery q1 = new SpanTermQuery(new Term("addresscountry", "au")); SpanQuery q2 = new SpanTermQuery(new Term("addressphone", "5678")); SpanQuery snq = new SpanNearQuery(new SpanQuery[] {q1, q2} , 0, false); the slop of 0 means that we'll only return those where the two terms are in the same position in their respective fields. This works brilliantly, BUT requires a change to SpanNearQuery's constructor (which checks that all the clauses are against the same field). Are people amenable to perhaps adding another constructor to SNQ which doesn't do the check, or subclassing it to do the same (give it a protected non-checking constructor for the subclass to call)? 2) (snipped ... see LUCENE-1626 for second idea)</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1495</id>
      <title>Allow TaskSequence to run for certain time</title>
      <description>To help the perf testing for LUCENE-1483, I added simple ability to specify a fixed run time (seconds) for a task sequence, eg: { "XSearchWithSort" SearchWithSort(doctitle:string) &gt; : 2.7s iterates on that subtask until 2.7 seconds have elapsed, and then sets the repetition count to how many iterations were done. This is useful when you are running searches whose runtime may vary drastically.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1497</id>
      <title>Minor changes to SimpleHTMLFormatter</title>
      <description>I'd like to make few minor changes to SimpleHTMLFormatter. 1. Define DEFAULT_PRE_TAG and DEFAULT_POST_TAG and use them in the default constructor. This will not trigger String lookups by the JVM whenever the highlighter is instantiated. 2. Create the StringBuffer in highlightTerm with the right number of characters from the beginning. Even though StringBuffer's default constructor allocates 16 chars, which will probably be enough for most highlighted terms (pre + post tags are 7 chars, which leaves 9 chars for terms), I think it's better to allocate SB with the right # of chars in advance, to avoid char[] allocations in the middle.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1499</id>
      <title>Minor refactoring to IndexFileNameFilter</title>
      <description>IndexFileNameFilter looks like it's designed to be a singleton, however its constructor is public and its singleton member is package visible. The proposed patch changes the constructor and member to private. Since it already has a static getFilter() method, and no code in Lucene references those two, I don't think it creates any problems from an API perspective.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1501</id>
      <title>Phonetic filters</title>
      <description>Metaphone, double metaphone, soundex and refined soundex filters using commons codec API.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1503</id>
      <title>refactor spatial contrib "Filter" "Query" classes</title>
      <description>From erik's comments in LUCENE-1387 DistanceQuery is awkwardly named. It's not an (extends) Query.... it's a POJO with helpers. Maybe DistanceQueryFactory? (but it creates a Filter also) CartesianPolyFilter is not a Filter (but CartesianShapeFilter is)</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1504</id>
      <title>Contrib-Spatial should use DocSet API rather then deprecated BitSet API</title>
      <description>Contrib-Spatial should be rewritten to use the new DocIdSet Filter API with OpenBitSets instead of j.u.BitSets. FilteredDocIdSet can be used to replace (I)SerialChainFilter.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1505</id>
      <title>Change contrib/spatial to use trie's NumericUtils, and remove NumberUtils</title>
      <description>Currently spatial contrib includes a copy of NumberUtils from solr (otherwise it would depend on solr) Once LUCENE-1496 is sorted out, this copy should be removed.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1506</id>
      <title>Adding FilteredDocIdSet and FilteredDocIdSetIterator</title>
      <description>Adding 2 convenience classes: FilteredDocIdSet and FilteredDocIDSetIterator.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1507</id>
      <title>adding EmptyDocIdSet/Iterator</title>
      <description>Adding convenience classes for EmptyDocIdSet and EmptyDocIdSetIterator</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1508</id>
      <title>CartesianTierPlotter fieldPrefix should be configurable</title>
      <description>CartesianTierPlotter field prefix is currrently hardcoded to "_localTier" – this should be configurable</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1512</id>
      <title>Incorporate GeoHash in contrib/spatial</title>
      <description>Based on comments from Yonik and Ryan in SOLR-773 GeoHash provides the ability to store latitude / longitude values in a single field consistent hash field. Which elements the need to maintain 2 field caches for latitude / longitude fields, reducing the size of an index and the amount of memory needed for a spatial search.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1513</id>
      <title>fastss fuzzyquery</title>
      <description>code for doing fuzzyqueries with fastssWC algorithm. FuzzyIndexer: given a lucene field, it enumerates all terms and creates an auxiliary offline index for fuzzy queries. FastFuzzyQuery: similar to fuzzy query except it queries the auxiliary index to retrieve a candidate list. this list is then verified with levenstein algorithm. sorry but the code is a bit messy... what I'm actually using is very different from this so its pretty much untested. but at least you can see whats going on or fix it up.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1514</id>
      <title>ShingleMatrixFilter eaily throws StackOverFlow as the complexity of a matrix grows</title>
      <description>ShingleMatrixFilter#next makes a recursive function invocation when the current permutation iterator is exhausted or if the current state of the permutation iterator already has produced an identical shingle. In a not too complex matrix this will require a gigabyte sized stack per thread. My solution is to avoid the recursive invocation by refactoring like this: public Token next(final Token reusableToken) throws IOException { assert reusableToken != null; if (matrix == null) { matrix = new Matrix(); // fill matrix with maximumShingleSize columns while (matrix.columns.size() &lt; maximumShingleSize &amp;&amp; readColumn()) { // this loop looks ugly } } // this loop exists in order to avoid recursive calls to the next method // as the complexity of a large matrix // then would require a multi gigabyte sized stack. Token token; do { token = produceNextToken(reusableToken); } while (token == request_next_token); return token; } private static final Token request_next_token = new Token(); /** * This method exists in order to avoid reursive calls to the method * as the complexity of a fairlt small matrix then easily would require * a gigabyte sized stack per thread. * * @param reusableToken * @return null if exhausted, instance request_next_token if one more call is required for an answer, or instance parameter resuableToken. * @throws IOException */ private Token produceNextToken(final Token reusableToken) throws IOException {</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1515</id>
      <title>Improved(?) Swedish snowball stemmer</title>
      <description>Snowball stemmer for Swedish lacks support for '-an' and '-ans' related suffix stripping, ending up with non compatible stems for example "klocka", "klockor", "klockornas", "klockAN", "klockANS". Complete list of new suffix stripping rules: {pre} 'an' 'anen' 'anens' 'anare' 'aner' 'anerna' 'anernas' 'ans' 'ansen' 'ansens' 'anser' 'ansera' 'anserar' 'anserna' 'ansernas' 'iera' (delete){pre} The problem is all the exceptions (e.g. svans|svan, finans|fin, nyans|ny) and this is an attempt at solving that problem. The rules and exceptions are based on the SAOL entries suffixed with 'an' and 'ans'. There a few known problematic stemming rules but seems to work quite a bit better than the current SwedishStemmer. It would not be a bad idea to check all of SAOL entries in order to make sure the integrity of the rules. My Snowball syntax skills are rather limited so I'm certain the code could be optimized quite a bit. The code is released under BSD and not ASL. I've been posting a bit in the Snowball forum and privatly to Martin Porter himself but never got any response so now I post it here instead in hope for some momentum.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1516</id>
      <title>Integrate IndexReader with IndexWriter</title>
      <description>The current problem is an IndexReader and IndexWriter cannot be open at the same time and perform updates as they both require a write lock to the index. While methods such as IW.deleteDocuments enables deleting from IW, methods such as IR.deleteDocument(int doc) and norms updating are not available from IW. This limits the capabilities of performing updates to the index dynamically or in realtime without closing the IW and opening an IR, deleting or updating norms, flushing, then opening the IW again, a process which can be detrimental to realtime updates. This patch will expose an IndexWriter.getReader method that returns the currently flushed state of the index as a class that implements IndexReader. The new IR implementation will differ from existing IR implementations such as MultiSegmentReader in that flushing will synchronize updates with IW in part by sharing the write lock. All methods of IR will be usable including reopen and clone.</description>
      <attachments/>
      <comments>91</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1517</id>
      <title>Change superclass of TrieRangeQuery</title>
      <description>This patch changes the superclass of TrieRangeQuery to ConstantScoreQuery. The current implementation is using rewrite() and was copied from early RangeQueries. But this is not needed, the TrieRangeQuery can easily subclassed from ConstantScoreQuery. If LUCENE-1345 is solved, the whole TrieRangeQuery can be removed, as TrieRangeFilter can be added to BooleanQueries. The whole TrieRangeQuery class is just a convenience class for easier usage of the trie contrib.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1518</id>
      <title>Merge Query and Filter classes</title>
      <description>This issue presents a patch, that merges Queries and Filters in a way, that the new Filter class extends Query. This would make it possible, to use every filter as a query. The new abstract filter class would contain all methods of ConstantScoreQuery, deprecate ConstantScoreQuery. If somebody implements the Filter's getDocIdSet()/bits() methods he has nothing more to do, he could just use the filter as a normal query. I do not want to completely convert Filters to ConstantScoreQueries. The idea is to combine Queries and Filters in such a way, that every Filter can automatically be used at all places where a Query can be used (e.g. also alone a search query without any other constraint). For that, the abstract Query methods must be implemented and return a "default" weight for Filters which is the current ConstantScore Logic. If the filter is used as a real filter (where the API wants a Filter), the getDocIdSet part could be directly used, the weight is useless (as it is currently, too). The constant score default implementation is only used when the Filter is used as a Query (e.g. as direct parameter to Searcher.search()). For the special case of BooleanQueries combining Filters and Queries the idea is, to optimize the BooleanQuery logic in such a way, that it detects if a BooleanClause is a Filter (using instanceof) and then directly uses the Filter API and not take the burden of the ConstantScoreQuery (see LUCENE-1345). Here some ideas how to implement Searcher.search() with Query and Filter: User runs Searcher.search() using a Filter as the only parameter. As every Filter is also a ConstantScoreQuery, the query can be executed and returns score 1.0 for all matching documents. User runs Searcher.search() using a Query as the only parameter: No change, all is the same as before User runs Searcher.search() using a BooleanQuery as parameter: If the BooleanQuery does not contain a Query that is subclass of Filter (the new Filter) everything as usual. If the BooleanQuery only contains exactly one Filter and nothing else the Filter is used as a constant score query. If BooleanQuery contains clauses with Queries and Filters the new algorithm could be used: The queries are executed and the results filtered with the filters. For the user this has the main advantage: That he can construct his query using a simplified API without thinking about Filters oder Queries, you can just combine clauses together. The scorer/weight logic then identifies the cases to use the filter or the query weight API. Just like the query optimizer of a RDB.</description>
      <attachments/>
      <comments>45</comments>
      <commenters>14</commenters>
    </issue>
    <issue>
      <id>1522</id>
      <title>another highlighter</title>
      <description>I've written this highlighter for my project to support bi-gram token stream (general token stream (e.g. WhitespaceTokenizer) also supported. see test code in patch). The idea was inherited from my previous project with my colleague and LUCENE-644. This approach needs highlight fields to be TermVector.WITH_POSITIONS_OFFSETS, but is fast and can support N-grams. This depends on LUCENE-1448 to get refined term offsets. usage: TopDocs docs = searcher.search( query, 10 ); Highlighter h = new Highlighter(); FieldQuery fq = h.getFieldQuery( query ); for( ScoreDoc scoreDoc : docs.scoreDocs ){ // fieldName="content", fragCharSize=100, numFragments=3 String[] fragments = h.getBestFragments( fq, reader, scoreDoc.doc, "content", 100, 3 ); if( fragments != null ){ for( String fragment : fragments ) System.out.println( fragment ); } } features: fast for large docs supports not only whitespace-based token stream, but also "fixed size" N-gram (e.g. (2,2), not (1,3)) (can solve LUCENE-1489) supports PhraseQuery, phrase-unit highlighting with slops q="w1 w2" &lt;b&gt;w1 w2&lt;/b&gt; --------------- q="w1 w2"~1 &lt;b&gt;w1&lt;/b&gt; w3 &lt;b&gt;w2&lt;/b&gt; w3 &lt;b&gt;w1 w2&lt;/b&gt; highlight fields need to be TermVector.WITH_POSITIONS_OFFSETS easy to apply patch due to independent package (contrib/highlighter2) uses Java 1.5 looks query boost to score fragments (currently doesn't see idf, but it should be possible) pluggable FragListBuilder pluggable FragmentsBuilder to do: term positions can be unnecessary when phraseHighlight==false collects performance numbers</description>
      <attachments/>
      <comments>56</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>1523</id>
      <title>isOpen needs to be accessible by subclasses of Directory</title>
      <description>The Directory abstract class has a member variable named isOpen which is package accessible. The usage of the variable is such that it should be readable and must be writable (in order to implement close()) by any concrete implementation of directory. Because of the current accessibility of this variable is is not possible to create a Directory implementation that is not also in the org.apache.lucene.store. I propose that either the isOpen variable either needs to be declared protected or that there should be getter/setter methods that are protected.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1526</id>
      <title>For near real-time search, use paged copy-on-write BitVector impl</title>
      <description>SegmentReader currently uses a BitVector to represent deleted docs. When performing rapid clone (see LUCENE-1314) and delete operations, performing a copy on write of the BitVector can become costly because the entire underlying byte array must be created and copied. A way to make this clone delete process faster is to implement tombstones, a term coined by Marvin Humphrey. Tombstones represent new deletions plus the incremental deletions from previously reopened readers in the current reader. The proposed implementation of tombstones is to accumulate deletions into an int array represented as a DocIdSet. With LUCENE-1476, SegmentTermDocs iterates over deleted docs using a DocIdSet rather than accessing the BitVector by calling get. This allows a BitVector and a set of tombstones to by ANDed together as the current reader's delete docs. A tombstone merge policy needs to be defined to determine when to merge tombstone DocIdSets into a new deleted docs BitVector as too many tombstones would eventually be detrimental to performance. A probable implementation will merge tombstones based on the number of tombstones and the total number of documents in the tombstones. The merge policy may be set in the clone/reopen methods or on the IndexReader.</description>
      <attachments/>
      <comments>66</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1528</id>
      <title>Add support for Ideographic Space to the queryparser - also know as fullwith space and wide-space</title>
      <description>The Ideographic Space is a space character that is as wide as a normal CJK character cell. It is also known as wide-space or fullwith space.This type of space is used in CJK languages. This patch adds support for the wide space, making the queryparser component more friendly to queries that contain CJK text. Reference: 'http://en.wikipedia.org/wiki/Space_(punctuation)' - see Table of spaces, char U+3000. I also added a new testcase that fails before the patch. After the patch is applied all junits pass.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1529</id>
      <title>back-compat tests ("ant test-tag") should test JAR drop-in-ability</title>
      <description>We now test back-compat with "ant test-tag", which is very useful for catching breaks in back compat before committing. However, that currently checks out "src/test" sources and then compiles them against the trunk JAR, and runs the tests. Whereas our back compat policy: http://wiki.apache.org/lucene-java/BackwardsCompatibility states that no recompilation is required on upgrading to a new JAR. Ie you should be able to drop in the new JAR in place of your old one and things should work fine. So... we should fix "ant test-tag" to: Do full checkout of core sources &amp; tests from the back-compat-tag Compile the JAR from the back-compat sources Compile the tests against that back-compat JAR Swap in the trunk JAR Run the tests</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1530</id>
      <title>Support inclusive/exclusive for TrieRangeQuery/-Filter, remove default trie variant setters/getters</title>
      <description>TrieRangeQuery/Filter is missing one thing: Ranges that have exclusive bounds. For TrieRangeQuery this may not be important for ranges on long or Date (==long) values (because [1..5] is the same like ]0..6[ or ]0..5]). This is not so simple for doubles because you must add/substract 1 from the trie encoded unsigned long. To be conform with the other range queries, I will submit a patch that has two additional boolean parameters in the ctors to support inclusive/exclusive ranges for both ends. Internally it will be implemented using TrieUtils.incrementTrieCoded/decrementTrieCoded() but makes life simplier for double ranges (a simple exclusive replacement for the floating point range [0.0..1.0] is not possible without having the underlying unsigned long). In December, when trie contrib was included (LUCENE-1470), 3 trie variants were supplied by TrieUtils. For new APIs a statically configureable default Trie variant does not conform to an API we want in Lucene (currently we want to deprecate all these static setters/getters). The important thing: It does not make code shorter or easier to understand, its more error prone. Before release of 2.9 it is a good time to remove the default trie variant and always force the parameter in TrieRangeQuery/Filter. It is better to choose the variant in the application and do not automatically manage it. As Lucene 2.9 was not yet released, I will change the ctors and not preserve the old ones.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1531</id>
      <title>contrib/xml-query-parser, BoostingTermQuery support</title>
      <description>I'm not 100% on this patch. BooleanTermQuery is a part of the spans family, but I generally use that class as a replacement for TermQuery. Thus in the DTD I have stated that it can be a part of the root queries as well as a part of a span. However, SpanFooQueries xml elements are named &lt;SpanFoo/&gt; rather than &lt;SpanFooQuery/&gt;, I have however chosen to call it &lt;BoostingTermQuery/&gt;. It would be possible to set it up so it would be parsed as &lt;SpanBoostingTerm/&gt; when inside of a &lt;SpanSomething&gt;, but I just find that confusing.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1532</id>
      <title>File based spellcheck with doc frequencies supplied</title>
      <description>The file-based spellchecker treats all words in the dictionary as equally valid, so it can suggest a very obscure word rather than a more common word which is equally close to the misspelled word that was entered. It would be very useful to have the option of supplying an integer with each word which indicates its commonness. I.e. the integer could be the document frequency in some index or set of indexes. I've implemented a modification to the spellcheck API to support this by defining a DocFrequencyInfo interface for obtaining the doc frequency of a word, and a class which implements the interface by looking up the frequency in an index. So Lucene users can provide alternative implementations of DocFrequencyInfo. I could submit this as a patch if there is interest. Alternatively, it might be better to just extend the spellcheck API to have a way to supply the frequencies when you create a PlainTextDictionary, but that would mean storing the frequencies somewhere when building the spellcheck index, and I'm not sure how best to do that.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1533</id>
      <title>Deleted documents as a Filter or top level Query</title>
      <description>In exploring alternative and perhaps faster ways to implement the deleted documents functionality, the idea of filtering the deleted documents at a higher level came up. This system would save on checking the deleted docs BitVector of each doc read from the posting list by SegmentTermDocs. This is equivalent to an AND NOT deleted docs query. If the patch improves the speed of indexes with delete documents, many core unit tests will need to change, or alternatively the functionality provided by this patch can be an IndexReader option. I'm thinking the first implementation will be a Filter in IndexSearcher.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1536</id>
      <title>if a filter can support random access API, we should use it</title>
      <description>I ran some performance tests, comparing applying a filter via random-access API instead of current trunk's iterator API. This was inspired by LUCENE-1476, where we realized deletions should really be implemented just like a filter, but then in testing found that switching deletions to iterator was a very sizable performance hit. Some notes on the test: Index is first 2M docs of Wikipedia. Test machine is Mac OS X 10.5.6, quad core Intel CPU, 6 GB RAM, java 1.6.0_07-b06-153. I test across multiple queries. 1-X means an OR query, eg 1-4 means 1 OR 2 OR 3 OR 4, whereas +1-4 is an AND query, ie 1 AND 2 AND 3 AND 4. "u s" means "united states" (phrase search). I test with multiple filter densities (0, 1, 2, 5, 10, 25, 75, 90, 95, 98, 99, 99.99999 (filter is non-null but all bits are set), 100 (filter=null, control)). Method high means I use random-access filter API in IndexSearcher's main loop. Method low means I use random-access filter API down in SegmentTermDocs (just like deleted docs today). Baseline (QPS) is current trunk, where filter is applied as iterator up "high" (ie in IndexSearcher's search loop).</description>
      <attachments/>
      <comments>211</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>1537</id>
      <title>InstantiatedIndexReader.clone</title>
      <description>This patch will implement IndexReader.clone for InstantiatedIndexReader.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1539</id>
      <title>Improve Benchmark</title>
      <description>Benchmark can be improved by incorporating recent suggestions posted on java-dev. M. McCandless' Python scripts that execute multiple rounds of tests can either be incorporated into the codebase or converted to Java.</description>
      <attachments/>
      <comments>36</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1540</id>
      <title>Improvements to contrib.benchmark for TREC collections</title>
      <description>The benchmarking utilities for TREC test collections (http://trec.nist.gov) are quite limited and do not support some of the variations in format of older TREC collections. I have been doing some benchmarking work with Lucene and have had to modify the package to support: Older TREC document formats, which the current parser fails on due to missing document headers. Variations in query format - newlines after &lt;title&gt; tag causing the query parser to get confused. Ability to detect and read in uncompressed text collections Storage of document numbers by default without storing full text. I can submit a patch if there is interest, although I will probably want to write unit tests for the new functionality first.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1541</id>
      <title>Trie range - make trie range indexing more flexible</title>
      <description>In the current trie range implementation, a single precision step is specified. With a large precision step (say 8), a value is indexed in fewer terms (8) but the number of terms for a range can be large. With a small precision step (say 2), the number of terms for a range is smaller but a value is indexed in more terms (32). We want to add an option that different precision steps can be set for different precisions. An expert can use this option to keep the number of terms for a range small and at the same time index a value in a small number of terms. See the discussion in LUCENE-1470 that results in this issue.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1543</id>
      <title>Field specified norms in MatchAllDocumentsScorer</title>
      <description>This patch allows for optionally setting a field to use for norms factoring when scoring a MatchingAllDocumentsQuery. From the test case: . RAMDirectory dir = new RAMDirectory(); IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED); iw.setMaxBufferedDocs(2); // force multi-segment addDoc("one", iw, 1f); addDoc("two", iw, 20f); addDoc("three four", iw, 300f); iw.close(); IndexReader ir = IndexReader.open(dir); IndexSearcher is = new IndexSearcher(ir); ScoreDoc[] hits; // assert with norms scoring turned off hits = is.search(new MatchAllDocsQuery(), null, 1000).scoreDocs; assertEquals(3, hits.length); assertEquals("one", ir.document(hits[0].doc).get("key")); assertEquals("two", ir.document(hits[1].doc).get("key")); assertEquals("three four", ir.document(hits[2].doc).get("key")); // assert with norms scoring turned on MatchAllDocsQuery normsQuery = new MatchAllDocsQuery("key"); assertEquals(3, hits.length); // is.explain(normsQuery, hits[0].doc); hits = is.search(normsQuery, null, 1000).scoreDocs; assertEquals("three four", ir.document(hits[0].doc).get("key")); assertEquals("two", ir.document(hits[1].doc).get("key")); assertEquals("one", ir.document(hits[2].doc).get("key"));</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1546</id>
      <title>Add IndexReader.flush(commitUserData)</title>
      <description>IndexWriter offers a commit(String commitUserData) method. IndexReader can commit as well using the flush/close methods and so needs an analogous method that accepts commitUserData.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1549</id>
      <title>Strengthen CheckIndex a bit</title>
      <description>A few small improvements to CheckIndex to detect possible "docs out of order" cases.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1550</id>
      <title>Add N-Gram String Matching for Spell Checking</title>
      <description>N-Gram version of edit distance based on paper by Grzegorz Kondrak, "N-gram similarity and distance". Proceedings of the Twelfth International Conference on String Processing and Information Retrieval (SPIRE 2005), pp. 115-126, Buenos Aires, Argentina, November 2005. http://www.cs.ualberta.ca/~kondrak/papers/spire05.pdf</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1551</id>
      <title>Add reopen(IndexCommit) methods to IndexReader</title>
      <description>Add reopen(IndexCommit) methods to IndexReader to be able to reopen an index on any previously saved commit points with all advantages of LUCENE-1483. Similar to open(IndexCommit) &amp; company available in 2.4.0.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1557</id>
      <title>Make constant-score rewrite the default for multi-term queries</title>
      <description>For queries that expand to multiple terms (PrefixQuery, RangeQuery, FuzzyQuery, WildcardQuery), the default now is to rewrite to a BooleanQuery, which scales poorly, and can hit the dreaded TooManyClauses (ungraceful degradation). Except for FuzzyQuery (which we should fix with this issue), they all support setConstantScoreRewrite, which scales better. In 3.0 we should make constantScoreRewrite the default, and leave an option to turn it off. This is a spinoff from LUCENE-998.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1558</id>
      <title>Make IndexReader/Searcher ctors readOnly=true by default</title>
      <description>Another "change the defaults" in 3.0. Right now you get a read/write reader from IndexReader.open and new IndexSearcher(...), and reserving the right to write causes thread contention (on isDeleted). In 3.0 let's make readOnly reader the default, but still allow opening a read/write IndexReader.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1560</id>
      <title>maxDocBytesToAnalyze should be required arg up front</title>
      <description>We recently changed IndexWriter to require you to specify up-front MaxFieldLength, on creation, so that you are aware of this dangerous "loses stuff" setting. Too many developers had fallen into the trap of "how come my search can't find this document...". I think we should do the same with "maxDocBytesToAnalyze" with highlighter? Spinoff from this thread: http://www.nabble.com/Lucene-Highlighting-and-Dynamic-Summaries-p22385887.html</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1561</id>
      <title>Maybe rename Field.omitTf, and strengthen the javadocs</title>
      <description>Spinoff from here: http://www.nabble.com/search-problem-when-indexed-using-Field.setOmitTf()-td22456141.html Maybe rename omitTf to something like omitTermPositions, and make it clear what queries will silently fail to work as a result.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1565</id>
      <title>CLONE -Add meta keywords to HTMLParser</title>
      <description>It would be good if the HTMLParser could give us the keywords specified in the meta tags, so that we can index them. In HTMLParser.jj: void addMetaTag() { metaTags.setProperty(currentMetaTag, currentMetaContent); currentMetaTag = null; currentMetaContent = null; return; } One way to do it: void addMetaTag() throws IOException { metaTags.setProperty(currentMetaTag, currentMetaContent); if (currentMetaTag.equalsIgnoreCase("keywords")) { pipeOut.write(currentMetaContent); } currentMetaTag = null; currentMetaContent = null; return; }</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1567</id>
      <title>New flexible query parser</title>
      <description>From "New flexible query parser" thread by Micheal Busch in my team at IBM we have used a different query parser than Lucene's in our products for quite a while. Recently we spent a significant amount of time in refactoring the code and designing a very generic architecture, so that this query parser can be easily used for different products with varying query syntaxes. This work was originally driven by Andreas Neumann (who, however, left our team); most of the code was written by Luis Alves, who has been a bit active in Lucene in the past, and Adriano Campos, who joined our team at IBM half a year ago. Adriano is Apache committer and PMC member on the Tuscany project and getting familiar with Lucene now too. We think this code is much more flexible and extensible than the current Lucene query parser, and would therefore like to contribute it to Lucene. I'd like to give a very brief architecture overview here, Adriano and Luis can then answer more detailed questions as they're much more familiar with the code than I am. The goal was it to separate syntax and semantics of a query. E.g. 'a AND b', '+a +b', 'AND(a,b)' could be different syntaxes for the same query. We distinguish the semantics of the different query components, e.g. whether and how to tokenize/lemmatize/normalize the different terms or which Query objects to create for the terms. We wanted to be able to write a parser with a new syntax, while reusing the underlying semantics, as quickly as possible. In fact, Adriano is currently working on a 100% Lucene-syntax compatible implementation to make it easy for people who are using Lucene's query parser to switch. The query parser has three layers and its core is what we call the QueryNodeTree. It is a tree that initially represents the syntax of the original query, e.g. for 'a AND b': AND / \ A B The three layers are: 1. QueryParser 2. QueryNodeProcessor 3. QueryBuilder 1. The upper layer is the parsing layer which simply transforms the query text string into a QueryNodeTree. Currently our implementations of this layer use javacc. 2. The query node processors do most of the work. It is in fact a configurable chain of processors. Each processors can walk the tree and modify nodes or even the tree's structure. That makes it possible to e.g. do query optimization before the query is executed or to tokenize terms. 3. The third layer is also a configurable chain of builders, which transform the QueryNodeTree into Lucene Query objects. Furthermore the query parser uses flexible configuration objects, which are based on AttributeSource/Attribute. It also uses message classes that allow to attach resource bundles. This makes it possible to translate messages, which is an important feature of a query parser. This design allows us to develop different query syntaxes very quickly. Adriano wrote the Lucene-compatible syntax in a matter of hours, and the underlying processors and builders in a few days. We now have a 100% compatible Lucene query parser, which means the syntax is identical and all query parser test cases pass on the new one too using a wrapper. Recent posts show that there is demand for query syntax improvements, e.g improved range query syntax or operator precedence. There are already different QP implementations in Lucene+contrib, however I think we did not keep them all up to date and in sync. This is not too surprising, because usually when fixes and changes are made to the main query parser, people don't make the corresponding changes in the contrib parsers. (I'm guilty here too) With this new architecture it will be much easier to maintain different query syntaxes, as the actual code for the first layer is not very much. All syntaxes would benefit from patches and improvements we make to the underlying layers, which will make supporting different syntaxes much more manageable.</description>
      <attachments/>
      <comments>119</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>1570</id>
      <title>QueryParser.setAllowLeadingWildcard could provide finer granularity</title>
      <description>It's great that Lucene now allows support for leading wildcards to be turned on. However, leading wildcard searches are more expensive, so it would be useful to be able to turn it on only for certain search fields. I'm specifically thinking of wiki searches where it may be too expensive to allow leading wildcards in the 'content:' field, but it would still be very useful to be able to selectively turn on support for 'path:' and perhaps other fields such as 'title:'. Would this be possible?</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1574</id>
      <title>PooledSegmentReader, pools SegmentReader underlying byte arrays</title>
      <description>PooledSegmentReader pools the underlying byte arrays of deleted docs and norms for realtime search. It is designed for use with IndexReader.clone which can create many copies of byte arrays, which are of the same length for a given segment. When pooled they can be reused which could save on memory. Do we want to benchmark the memory usage comparison of PooledSegmentReader vs GC? Many times GC is enough for these smaller objects.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1575</id>
      <title>Refactoring Lucene collectors (HitCollector and extensions)</title>
      <description>This issue is a result of a recent discussion we've had on the mailing list. You can read the thread here. We have agreed to do the following refactoring: Rename MultiReaderHitCollector to Collector, with the purpose that it will be the base class for all Collector implementations. Deprecate HitCollector in favor of the new Collector. Introduce new methods in IndexSearcher that accept Collector, and deprecate those that accept HitCollector. Create a final class HitCollectorWrapper, and use it in the deprecated methods in IndexSearcher, wrapping the given HitCollector. HitCollectorWrapper will be marked deprecated, so we can remove it in 3.0, when we remove HitCollector. It will remove any instanceof checks that currently exist in IndexSearcher code. Create a new (abstract) TopDocsCollector, which will: Leave collect and setNextReader unimplemented. Introduce protected members PriorityQueue and totalHits. Introduce a single protected constructor which accepts a PriorityQueue. Implement topDocs() and getTotalHits() using the PQ and totalHits members. These can be used as-are by extending classes, as well as be overridden. Introduce a new topDocs(start, howMany) method which will be used a convenience method when implementing a search application which allows paging through search results. It will also attempt to improve the memory allocation, by allocating a ScoreDoc[] of the requested size only. Change TopScoreDocCollector to extend TopDocsCollector, use the topDocs() and getTotalHits() implementations as they are from TopDocsCollector. The class will also be made final. Change TopFieldCollector to extend TopDocsCollector, and make the class final. Also implement topDocs(start, howMany). Change TopFieldDocCollector (deprecated) to extend TopDocsCollector, instead of TopScoreDocCollector. Implement topDocs(start, howMany) Review other places where HitCollector is used, such as in Scorer, deprecate those places and use Collector instead. Additionally, the following proposal was made w.r.t. decoupling score from collect(): Change collect to accecpt only a doc Id (unbased). Introduce a setScorer(Scorer) method. If during collect the implementation needs the score, it can call scorer.score(). If we do this, then we need to review all places in the code where collect(doc, score) is called, and assert whether Scorer can be passed. Also this raises few questions: What if during collect() Scorer is null? (i.e., not set) - is it even possible? I noticed that many (if not all) of the collect() implementations discard the document if its score is not greater than 0. Doesn't it mean that score is needed in collect() always? Open issues: The name for Collector TopDocsCollector was mentioned on the thread as TopResultsCollector, but that was when we thought to call Colletor ResultsColletor. Since we decided (so far) on Collector, I think TopDocsCollector makes sense, because of its TopDocs output. Decoupling score from collect(). I will post a patch a bit later, as this is expected to be a very large patch. I will split it into 2: (1) code patch (2) test cases (moving to use Collector instead of HitCollector, as well as testing the new topDocs(start, howMany) method. There might be even a 3rd patch which handles the setScorer thing in Collector (maybe even a different issue?)</description>
      <attachments/>
      <comments>130</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1577</id>
      <title>Benchmark of different in RAM realtime techniques</title>
      <description>A place to post code that benchmarks the differences in the speed of indexing and searching using different realtime techniques.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1578</id>
      <title>InstantiatedIndex supports non-optimized IndexReaders</title>
      <description>InstantiatedIndex does not currently support non-optimized IndexReaders.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1581</id>
      <title>LowerCaseFilter should be able to be configured to use a specific locale.</title>
      <description>//Since I am a .Net programmer, Sample codes will be in c# but I don't think that it would be a problem to understand them. // Assume an input text like "İ" and and analyzer like below public class SomeAnalyzer : Analyzer { public override TokenStream TokenStream(string fieldName, System.IO.TextReader reader) { TokenStream t = new SomeTokenizer(reader); t = new Lucene.Net.Analysis.ASCIIFoldingFilter(t); t = new LowerCaseFilter(t); return t; } } ASCIIFoldingFilter will return "I" and after, LowerCaseFilter will return "i" (if locale is "en-US") or "ı' if(locale is "tr-TR") (that means,this token should be input to another instance of ASCIIFoldingFilter) So, calling LowerCaseFilter before ASCIIFoldingFilter would be a solution, but a better approach can be adding a new constructor to LowerCaseFilter and forcing it to use a specific locale. public sealed class LowerCaseFilter : TokenFilter { /* +++ */System.Globalization.CultureInfo CultureInfo = System.Globalization.CultureInfo.CurrentCulture; public LowerCaseFilter(TokenStream in) : base(in) { } /* +++ */ public LowerCaseFilter(TokenStream in, System.Globalization.CultureInfo CultureInfo) : base(in) /* +++ */ { /* +++ */ this.CultureInfo = CultureInfo; /* +++ */ } public override Token Next(Token result) { result = Input.Next(result); if (result != null) { char[] buffer = result.TermBuffer(); int length = result.termLength; for (int i = 0; i &lt; length; i++) /* +++ */ buffer[i] = System.Char.ToLower(buffer[i],CultureInfo); return result; } else return null; } } DIGY</description>
      <attachments/>
      <comments>23</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1582</id>
      <title>Make TrieRange completely independent from Document/Field with TokenStream of prefix encoded values</title>
      <description>TrieRange has currently the following problem: To add a field, that uses a trie encoding, you can manually add each term to the index or use a helper method from TrieUtils. The helper method has the problem, that it uses a fixed field configuration TrieUtils currently creates per default a helper field containing the lower precision terms to enable sorting (limitation of one term/document for sorting) trieCodeLong/Int() creates unnecessarily String[] and char[] arrays that is heavy for GC, if you index lot of numeric values. Also a lot of char[] to String copying is involved. This issue should improve this: trieCodeLong/Int() returns a TokenStream. During encoding, all char[] arrays are reused by Token API, additional String[] arrays for the encoded result are not created, instead the TokenStream enumerates the trie values. Trie fields can be added to Documents during indexing using the standard API: new Field(name,TokenStream,...), so no extra util method needed. By using token filters, one could also add payload and so and customize everything. The drawback is: Sorting would not work anymore. To enable sorting, a (sub-)issue can extend the FieldCache to stop iterating the terms, as soon as a lower precision one is enumerated by TermEnum. I will create a "hack" patch for TrieUtils-use only, that uses a non-checked Exceptionin the Parser to stop iteration. With LUCENE-831, a more generic API for this type can be used (custom parser/iterator implementation for FieldCache). I will attach the field cache patch (with the temporary solution, until FieldCache is reimplemented) as a separate patch file, or maybe open another issue for it.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1584</id>
      <title>Callback for intercepting merging segments in IndexWriter</title>
      <description>For things like merging field caches or bitsets, it's useful to know which segments were merged to create a new segment.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1585</id>
      <title>Allow to control how payloads are merged</title>
      <description>Lucene handles backwards-compatibility of its data structures by converting them from the old into the new formats during segment merging. Payloads are simply byte arrays in which users can store arbitrary data. Applications that use payloads might want to convert the format of their payloads in a similar fashion. Otherwise it's not easily possible to ever change the encoding of a payload without reindexing. So I propose to introduce a PayloadMerger class that the SegmentMerger invokes to merge the payloads from multiple segments. Users can then implement their own PayloadMerger to convert payloads from an old into a new format. In the future we need this kind of flexibility also for column-stride fields (LUCENE-1231) and flexible indexing codecs. In addition to that it would be nice if users could store version information in the segments file. E.g. they could store "in segment _2 the term a:b uses payloads of format x.y".</description>
      <attachments/>
      <comments>38</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1586</id>
      <title>add IndexReader.getUniqueTermCount</title>
      <description>Simple API to return number of unique terms (across all fields). Spinoff from here: http://www.lucidimagination.com/search/document/536b22e017be3e27/term_limit</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1588</id>
      <title>Update Spatial Lucene sort to use FieldComparatorSource</title>
      <description>Update distance sorting to use FieldComparator sorting as opposed to SortComparator</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1589</id>
      <title>IndexWriter.addIndexesNoOptimize(IndexReader... readers)</title>
      <description>Similar to IndexWriter.addIndexesNoOptimize(Directory[] dirs) but for IndexReaders. This will be used to flush cloned ram indexes to disk for near realtime indexing.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1591</id>
      <title>Enable bzip compression in benchmark</title>
      <description>bzip compression can aid the benchmark package by not requiring extracting bzip files (such as enwiki) in order to index them. The plan is to add a config parameter bzip.compression=true/false and in the relevant tasks either decompress the input file or compress the output file using the bzip streams. It will add a dependency on ant.jar which contains two classes similar to GZIPOutputStream and GZIPInputStream which compress/decompress files using the bzip algorithm. bzip is known to be superior in its compression performance to the gzip algorithm (~20% better compression), although it does the compression/decompression a bit slower. I wil post a patch which adds this parameter and implement it in LineDocMaker, EnwikiDocMaker and WriteLineDoc task. Maybe even add the capability to DocMaker or some of the super classes, so it can be inherited by all sub-classes.</description>
      <attachments/>
      <comments>78</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1592</id>
      <title>fix or deprecate TermsEnum.skipTo</title>
      <description>This method is a trap: it looks legitimate but it has hideously poor performance (simple linear scan implemented in the TermsEnum base class since none of the concrete impls override it with a more efficient implementation). The least we should do for 2.9 is deprecate the method with a strong warning about its performance. See here for background: http://www.lucidimagination.com/search/document/77dc4f8e893d3cf3/possible_terminfosreader_speedup And, here for historical context: http://www.lucidimagination.com/search/document/88f1b95b404ebf16/remove_termenum_skipto_term_target</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1593</id>
      <title>Optimizations to TopScoreDocCollector and TopFieldCollector</title>
      <description>This is a spin-off of LUCENE-1575 and proposes to optimize TSDC and TFC code to remove unnecessary checks. The plan is: Ensure that IndexSearcher returns segements in increasing doc Id order, instead of numDocs(). Change TSDC and TFC's code to not use the doc id as a tie breaker. New docs will always have larger ids and therefore cannot compete. Pre-populate HitQueue with sentinel values in TSDC (score = Float.NEG_INF) and remove the check if reusableSD == null. Also move to use "changing top" and then call adjustTop(), in case we update the queue. some methods in Sort explicitly add SortField.FIELD_DOC as a "tie breaker" for the last SortField. But, doing so should not be necessary (since we already break ties by docID), and is in fact less efficient (once the above optimization is in). Investigate PQ - can we deprecate insert() and have only insertWithOverflow()? Add a addDummyObjects method which will populate the queue without "arranging" it, just store the objects in the array (this can be used to pre-populate sentinel values)? I will post a patch as well as some perf measurements as soon as I have them.</description>
      <attachments/>
      <comments>101</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1594</id>
      <title>Use source code specialization to maximize search performance</title>
      <description>Towards eeking absolute best search performance, and after seeing the Java ghosts in LUCENE-1575, I decided to build a simple prototype source code specializer for Lucene's searches. The idea is to write dynamic Java code, specialized to run a very specific query context (eg TermQuery, collecting top N by field, no filter, no deletions), compile that Java code, and run it. Here're the performance gains when compared to trunk: Query Sort Filt Deletes Scoring Hits QPS (base) QPS (new) % 1 Date (long) no no Track,Max 2561886 6.8 10.6 55.9% 1 Date (long) no 5% Track,Max 2433472 6.3 10.5 66.7% 1 Date (long) 25% no Track,Max 640022 5.2 9.9 90.4% 1 Date (long) 25% 5% Track,Max 607949 5.3 10.3 94.3% 1 Date (long) 10% no Track,Max 256300 6.7 12.3 83.6% 1 Date (long) 10% 5% Track,Max 243317 6.6 12.6 90.9% 1 Relevance no no Track,Max 2561886 11.2 17.3 54.5% 1 Relevance no 5% Track,Max 2433472 10.1 15.7 55.4% 1 Relevance 25% no Track,Max 640022 6.1 14.1 131.1% 1 Relevance 25% 5% Track,Max 607949 6.2 14.4 132.3% 1 Relevance 10% no Track,Max 256300 7.7 15.6 102.6% 1 Relevance 10% 5% Track,Max 243317 7.6 15.9 109.2% 1 Title (string) no no Track,Max 2561886 7.8 12.5 60.3% 1 Title (string) no 5% Track,Max 2433472 7.5 11.1 48.0% 1 Title (string) 25% no Track,Max 640022 5.7 11.2 96.5% 1 Title (string) 25% 5% Track,Max 607949 5.5 11.3 105.5% 1 Title (string) 10% no Track,Max 256300 7.0 12.7 81.4% 1 Title (string) 10% 5% Track,Max 243317 6.7 13.2 97.0% Those tests were run on a 19M doc wikipedia index (splitting each Wikipedia doc @ ~1024 chars), on Linux, Java 1.6.0_10 But: it only works with TermQuery for now; it's just a start. It should be easy for others to run this test: apply patch cd contrib/benchmark run python -u bench.py -delindex &lt;/path/to/index/with/deletes&gt; -nodelindex &lt;/path/to/index/without/deletes&gt; (You can leave off one of -delindex or -nodelindex and it'll skip those tests). For each test, bench.py generates a single Java source file that runs that one query; you can open contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/FastSearchTask.java to see it. I'll attach an example. It writes "results.txt", in Jira table format, which you should be able to copy/paste back here. The specializer uses pretty much every search speedup I can think of – the ones from LUCENE-1575 (to score or not, to maxScore or not), the ones suggested in the spinoff LUCENE-1593 (pre-fill w/ sentinels, don't use docID for tie breaking), LUCENE-1536 (random access filters). It bypasses TermDocs and interacts directly with the IndexInput, and with BitVector for deletions. It directly folds in the collector, if possible. A filter if used must be random access, and is assumed to pre-multiply-in the deleted docs. Current status: I only handle TermQuery. I'd like to add others over time... It can collect by score, or single field (with the 3 scoring options in LUCENE-1575). It can't do reverse field sort nor multi-field sort now. The auto-gen code (gen.py) is rather hideous. It could use some serious refactoring, etc.; I think we could get it to the point where each Query can gen its own specialized code, maybe. It also needs to be eventually ported to Java. The script runs old, then new, then checks that the topN results are identical, and aborts if not. So I'm pretty sure the specialized code is working correctly, for the cases I'm testing. The patch includes a few small changes to core, mostly to open up package protected APIs so I can access stuff I think this is an interesting effort for several reasons: It gives us a best-case upper bound performance we can expect from Lucene's normal search classes (minus algorithmic improvements eg PFOR) because it makes life as easy as possible on the compiler/JRE to convert to assembly. We can spin out optimization ideas from this back into the core (eg LUCENE-1593 already has one example), and prioritize. EG I think given these results, optimizing for filters that support random-access API is important. As we fold speedups back into core, the gains from specialization will naturally decrease. Eventually (maybe, eg as a future "experimental" module) this can be used in production as a simple "search wrapper". Ie, for a given query, the specializer is checked. If the query "matches" what the specializer can handle, then the specialized code is run; else we fallback to Lucene core. Likely one would pre-compile the space of all specializations, or we could compile java-on-the-fly (eg what a JSP source does when it's changed) but I'm not sure how costly/portable that is.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1595</id>
      <title>Split DocMaker into ContentSource and DocMaker</title>
      <description>This issue proposes some refactoring to the benchmark package. Today, DocMaker has two roles: collecting documents from a collection and preparing a Document object. These two should actually be split up to ContentSource and DocMaker, which will use a ContentSource instance. ContentSource will implement all the methods of DocMaker, like getNextDocData, raw size in bytes tracking etc. This can actually fit well w/ 1591, by having a basic ContentSource that offers input stream services, and wraps a file (for example) with a bzip or gzip streams etc. DocMaker will implement the makeDocument methods, reusing DocState etc. The idea is that collecting the Enwiki documents, for example, should be the same whether I create documents using DocState, add payloads or index additional metadata. Same goes for Trec and Reuters collections, as well as LineDocMaker. In fact, if one inspects EnwikiDocMaker and LineDocMaker closely, they are 99% the same and 99% different. Most of their differences lie in the way they read the data, while most of the similarity lies in the way they create documents (using DocState). That led to a somehwat bizzare extension of LineDocMaker by EnwikiDocMaker (just the reuse of DocState). Also, other DocMakers do not use that DocState today, something they could have gotten for free with this refactoring proposed. So by having a EnwikiContentSource, ReutersContentSource and others (TREC, Line, Simple), I can write several DocMakers, such as DocStateMaker, ConfigurableDocMaker (one which accpets all kinds of config options) and custom DocMakers (payload, facets, sorting), passing to them a ContentSource instance and reuse the same DocMaking algorithm with many content sources, as well as the same ContentSource algorithm with many DocMaker implementations. This will also give us the opportunity to perf test content sources alone (i.e., compare bzip, gzip and regular input streams), w/o the overhead of creating a Document object. I've already done so in my code environment (I extend the benchmark package for my application's purposes) and I like the flexibility I have. I think this can be a nice contribution to the benchmark package, which can result in some code cleanup as well.</description>
      <attachments/>
      <comments>41</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1596</id>
      <title>optimize MultiTermEnum/MultiTermDocs</title>
      <description>Optimize MultiTermEnum and MultiTermDocs to avoid seeks on TermDocs that don't match the term.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1597</id>
      <title>New Document and Field API</title>
      <description>This is a super rough prototype of how a new document API could look like. It's basically what I came up with during a long flight across the Atlantic It is not integrated with anything yet (like IndexWriter, DocumentsWriter, etc.) and heavily uses Java 1.5 features, such as generics and annotations. The general idea sounds similar to what Marvin is doing in KS, which I found out by reading Mike's comments on LUCENE-831, I haven't looked at the KS API myself yet. Main ideas: separate a field's value from its configuration; therefore this patch introduces two classes: FieldDescriptor and FieldValue I was thinking that in most cases the documents people add to a Lucene index look alike, i.e. they contain mostly the same fields with the same settings. Yet, for every field instance the DocumentsWriter checks the settings and calls the right consumers, which themselves check settings and return true or false, indicating whether or not they want to do something with that field or not. So I was thinking we could design the document API similar to the Class&lt;-&gt;Object concept of OO-languages. There a class is a blueprint (as everyone knows ), and an object is one instance of it. So in this patch I introduced a class called DocumentDescriptor, which contains all FieldDescriptors with the field settings. This descriptor is given to the consumer (IndexWriter) once in the constructor. Then the Document "instances" are created and added via addDocument(). A Document instance allows adding "variable fields" in addition to the "fixed fields" the DocumentDescriptor contains. For these fields the consumers have to check the field settings for every document instance (like with the old document API). This is for maintaining Lucene's flexibility that everyone loves. Disregard the changes to AttributeSource for now. The code that's worth looking at is contained in a new package "newdoc". Again, this is not a "real" patch, but rather a demo of how a new API could roughly work.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1600</id>
      <title>Reduce usage of String.intern(), performance is terrible</title>
      <description>I profiled a simple MatchAllDocsQuery() against ~1.5 million documents (8 fields of short text, Field.Store.YES,Field.Index.NOT_ANALYZED_NO_NORMS), then retrieved all documents via searcher.doc(i, fs). String.intern() showed up as a top hotspot (see attached screenshot), so i implemented a small optimization to not intern() for every new Field(), instead forcing the intern in the FieldInfos class and adding a optional "internName" constructor to Field. This reduced execution time for searching and iterating through all documents by 35%. Results were similar for -server and -client. TRUNK (2.9) w/out patch: matched 1435563 in 8884 ms/search TRUNK (2.9) w/patch: matched 1435563 in 5786 ms/search</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1601</id>
      <title>Handle 3.0 specific TODOs, following LUCENE-1575</title>
      <description>In LUCENE-1575 we've changed the HitCollector API to Collector, as well as decoupled score from collect(). Also, we've allowed TopFieldCollector to not track document scores and maxScore. For back-compat reasons we kept defaults to true (i.e., track scores), but should remove them in 3.0. This issue will handle all the 3.0 targeted changes.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1602</id>
      <title>Rewrite TrieRange to use MultiTermQuery</title>
      <description>Issue for discussion here: http://www.lucidimagination.com/search/document/46a548a79ae9c809/move_trierange_to_core_module_and_integration_issues This patch is a rewrite of TrieRange using MultiTermQuery like all other core queries. This should make TrieRange identical in functionality to core range queries.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1603</id>
      <title>Changes for TrieRange in FilteredTermEnum and MultiTermQuery improvement</title>
      <description>This is a patch, that is needed for the MultiTermQuery-rewrite of TrieRange (LUCENE-1602): Make the private members protected, to have access to them from the very special TrieRangeTermEnum Fix a small inconsistency (docFreq() now only returns a value, if a valid term is existing) Improvement of MultiTermFilter.getDocIdSet to return DocIdSet.EMPTY_DOCIDSET, if the TermEnum is empty (less memory usage) and faster. Add the getLastNumberOfTerms() to MultiTermQuery for statistics on different multi term queries and how may terms they affect, using this new functionality, the improvement of TrieRange can be shown (extract from test case there, 10000 docs index, long values): [junit] Average number of terms during random search on 'field8': [junit] Trie query: 244.2 [junit] Classical query: 3136.94 [junit] Average number of terms during random search on 'field4': [junit] Trie query: 38.3 [junit] Classical query: 3018.68 [junit] Average number of terms during random search on 'field2': [junit] Trie query: 18.04 [junit] Classical query: 3539.42 All core tests pass.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1604</id>
      <title>Stop creating huge arrays to represent the absense of field norms</title>
      <description>Creating and keeping around huge arrays that hold a constant value is very inefficient both from a heap usage standpoint and from a localility of reference standpoint. It would be much more efficient to use null to represent a missing norms table.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1605</id>
      <title>Add subset method to BitVector</title>
      <description>Recently I needed the ability to efficiently compute subsets of a BitVector. The method is: public BitVector subset(int start, int end) where "start" is the starting index, inclusive and "end" is the ending index, exclusive. Attached is a patch including the subset method as well as relevant unit tests.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1606</id>
      <title>Automaton Query/Filter (scalable regex)</title>
      <description>Attached is a patch for an AutomatonQuery/Filter (name can change if its not suitable). Whereas the out-of-box contrib RegexQuery is nice, I have some very large indexes (100M+ unique tokens) where queries are quite slow, 2 minutes, etc. Additionally all of the existing RegexQuery implementations in Lucene are really slow if there is no constant prefix. This implementation does not depend upon constant prefix, and runs the same query in 640ms. Some use cases I envision: 1. lexicography/etc on large text corpora 2. looking for things such as urls where the prefix is not constant (http:// or ftp://) The Filter uses the BRICS package (http://www.brics.dk/automaton/) to convert regular expressions into a DFA. Then, the filter "enumerates" terms in a special way, by using the underlying state machine. Here is my short description from the comments: The algorithm here is pretty basic. Enumerate terms but instead of a binary accept/reject do: 1. Look at the portion that is OK (did not enter a reject state in the DFA) 2. Generate the next possible String and seek to that. the Query simply wraps the filter with ConstantScoreQuery. I did not include the automaton.jar inside the patch but it can be downloaded from http://www.brics.dk/automaton/ and is BSD-licensed.</description>
      <attachments/>
      <comments>224</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>1607</id>
      <title>String.intern() faster alternative</title>
      <description>By using our own interned string pool on top of default, String.intern() can be greatly optimized. On my setup (java 6) this alternative runs ~15.8x faster for already interned strings, and ~2.2x faster for 'new String(interned)' For java 5 and 4 speedup is lower, but still considerable.</description>
      <attachments/>
      <comments>52</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1608</id>
      <title>CustomScoreQuery should support arbitrary Queries</title>
      <description>CustomScoreQuery only allows the secondary queries to be of type ValueSourceQuery instead of allowing them to be any type of Query. As a result, what you can do with CustomScoreQuery is pretty limited. It would be nice to extend CustomScoreQuery to allow arbitrary Query objects. Most of the code should stay about the same, though a little more care would need to be taken in CustomScorer.score() to use 0.0 when the sub-scorer does not produce a score for the current document.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1609</id>
      <title>Eliminate synchronization contention on initial index reading in TermInfosReader ensureIndexIsRead</title>
      <description>synchronized method ensureIndexIsRead in TermInfosReader causes contention under heavy load Simple to reproduce: e.g. Under Solr, with all caches turned off, do a simple range search e.g. id:[0 TO 999999] on even a small index (in my case 28K docs) and under a load/stress test application, and later, examining the Thread dump (kill -3) , many threads are blocked on 'waiting for monitor entry' to this method. Rather than using Double-Checked Locking which is known to have issues, this implementation uses a state pattern, where only one thread can move the object from IndexNotRead state to IndexRead, and in doing so alters the objects behavior, i.e. once the index is loaded, the index nolonger needs a synchronized method. In my particular test, this uncreased throughput at least 30 times.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1610</id>
      <title>Preserve whitespace in &lt;code&gt; sections in the Changes.html generated from CHANGES.txt by changes2html.pl</title>
      <description>The Trunk section of CHANGES.txt sports use of a new feature: &lt;code&gt; sections, for the two mentions of LUCENE-1575. This looks fine in the text rendering, but looks crappy in the HTML version, since changes2html.pl escapes HTML metacharacters to appear as-is in the HTML rendering, but the newlines in the code are converted to a single space. I think this should be fixed by modifying changes2html.pl to convert &lt;code&gt; and &lt;/code&gt; into (unescaped) &lt;code&gt;&lt;pre&gt; and &lt;/pre&gt;&lt;/code&gt;, respectively, since just passing through &lt;code&gt; and &lt;/code&gt;, without &lt;/?pre&gt;, while changing the font to monospaced (nice), still collapses whitespace (not nice). See the java-dev thread that spawned this issue here: http://www.nabble.com/CHANGES.txt-td23102627.html</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1612</id>
      <title>expose lastDocId in the posting from the TermEnum API</title>
      <description>We currently have on the TermEnum api: docFreq() which gives the number docs in the posting. It would be good to also have the max docid in the posting. That information is useful when construction a custom DocIdSet, .e.g determine sparseness of the doc list to decide whether or not to use a BitSet. I have written a patch to do this, the problem with it is the TermInfosWriter encodes values in VInt/VLong, there is very little flexibility to add in lastDocId while making the index backward compatible. (If simple int is used for say, docFreq, a bit can be used to flag reading of a new piece of information) output.writeVInt(ti.docFreq); // write doc freq output.writeVLong(ti.freqPointer - lastTi.freqPointer); // write pointers output.writeVLong(ti.proxPointer - lastTi.proxPointer); Anyway, patch is attached with:TestSegmentTermEnum modified to test this. TestBackwardsCompatibility fails due to reasons described above.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1614</id>
      <title>Add next() and skipTo() variants to DocIdSetIterator that return the current doc, instead of boolean</title>
      <description>See http://www.nabble.com/Another-possible-optimization---now-in-DocIdSetIterator-p23223319.html for the full discussion. The basic idea is to add variants to those two methods that return the current doc they are at, to save successive calls to doc(). If there are no more docs, return -1. A summary of what was discussed so far: Deprecate those two methods. Add nextDoc() and skipToDoc(int) that return doc, with default impl in DISI (calls next() and skipTo() respectively, and will be changed to abstract in 3.0). I actually would like to propose an alternative to the names: advance() and advance(int) - the first advances by one, the second advances to target. Wherever these are used, do something like '(doc = advance()) &gt;= 0' instead of comparing to -1 for improved performance. I will post a patch shortly</description>
      <attachments/>
      <comments>144</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>1615</id>
      <title>deprecated method used in fieldsReader / setOmitTf()</title>
      <description>setOmitTf(boolean) is deprecated and should not be used by core classes. One place where it appears is FieldsReader , this patch fixes it. It was necessary to change Fieldable to AbstractField at two places, only local variables.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1616</id>
      <title>add one setter for start and end offset to OffsetAttribute</title>
      <description>add OffsetAttribute. setOffset(startOffset, endOffset); trivial change, no JUnit needed Changed CharTokenizer to use it</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1617</id>
      <title>Add "testpackage" to common-build.xml</title>
      <description>One can define "testcase" to execute just one test class, which is convenient. However, I didn't notice any equivalent for testing a whole package. I find it convenient to be able to test packages rather than test cases because often it is not so clear which test class to run. Following patch allows one to "ant test -Dtestpackage=search" (for example) and run all tests under the */search/* packages in core, contrib and tags, or do "ant test-core -Dtestpackage=search" and execute similarly just for core, or do "ant test-core -Dtestpacakge=lucene/search/function" and run all the tests under */lucene/search/function/* (just in case there is another o.a.l.something.search.function package out there which we want to exclude.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1618</id>
      <title>Allow setting the IndexWriter docstore to be a different directory</title>
      <description>Add an IndexWriter.setDocStoreDirectory method that allows doc stores to be placed in a different directory than the IW default dir.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1619</id>
      <title>TermAttribute.termLength() optimization</title>
      <description>public int termLength() { initTermBuffer(); // This patch removes this method call return termLength; } I see no reason to initTermBuffer() in termLength()... all tests pass, but I could be wrong?</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1621</id>
      <title>deprecate term and getTerm in MultiTermQuery</title>
      <description>This means moving getTerm and term up to sub classes as appropriate and reimplementing equals, hashcode as appropriate in sub classes.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1622</id>
      <title>Multi-word synonym filter (synonym expansion at indexing time).</title>
      <description>It would be useful to have a filter that provides support for indexing-time synonym expansion, especially for multi-word synonyms (with multi-word matching for original tokens). The problem is not trivial, as observed on the mailing list. The problems I was able to identify (mentioned in the unit tests as well): if multi-word synonyms are indexed together with the original token stream (at overlapping positions), then a query for a partial synonym sequence (e.g., "big" in the synonym "big apple" for "new york city") causes the document to match; there are problems with highlighting the original document when synonym is matched (see unit tests for an example), if the synonym is of different length than the original sequence of tokens to be matched, then phrase queries spanning the synonym and the original sequence boundary won't be found. Example "big apple" synonym for "new york city". A phrase query "big apple restaurants" won't match "new york city restaurants". I am posting the patch that implements phrase synonyms as a token filter. This is not necessarily intended for immediate inclusion, but may provide a basis for many people to experiment and adjust to their own scenarios.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1624</id>
      <title>Don't commit an empty segments_N when IW is opened with create=true</title>
      <description>If IW is opened with create=true, it forcefully commits an empty segments_N. But really it should not: if autoCommit is false, nothing should be committed until commit or close is explicitly called. Spinoff from http://www.nabble.com/no-segments*-file-found:-files:-Error-on-opening-index-td23219520.html</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1626</id>
      <title>getPositionIncrementGap(String fieldname, int currentPos)</title>
      <description>This issue is to cover the changes required to do a search across multiple fields with the same name in a fashion similar to a many-to-one database. Below is my post on java-dev on the topic, which details the changes we need: (sniped to only second idea ... see LUCENE-1494 for background and first idea) 2) It gets slightly more complicated in the case of variable-length terms. For example, imagine if we had an 'address' field ('123 Smith St') which will result in (1 to n) tokens; slop 0 in a SpanNearQuery won't work here, of course. One thing we've toyed with is the idea of using getPositionIncrementGap – if we knew that 'address' would be, at most, 20 tokens, we might use a position increment gap of 100, and make the slop factor 50; this works fine for the simple case (yay!), but with a great many addresses-per-user starts to get more complicated, as the gap counts from the last term (so the position sequence for a single value field might be 0, 100, 200, but for the address field it might be 0, 1, 2, 3, 103, 104, 105, 106, 206, 207... so it's going to get out of sync). The simplest option here seems to be changing (or supplementing) public int getPositionIncrementGap(String fieldname) to public int getPositionIncrementGap(String fieldname, int currentPos) so that we can override that to round up to the nearest 100 (or whatever) based on currentPos. The default implementation could just delegate to getPositionIncrementGap().</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1628</id>
      <title>Persian Analyzer</title>
      <description>A simple persian analyzer. i measured trec scores with the benchmark package below against http://ece.ut.ac.ir/DBRG/Hamshahri/ : SimpleAnalyzer: SUMMARY Search Seconds: 0.012 DocName Seconds: 0.020 Num Points: 981.015 Num Good Points: 33.738 Max Good Points: 36.185 Average Precision: 0.374 MRR: 0.667 Recall: 0.905 Precision At 1: 0.585 Precision At 2: 0.531 Precision At 3: 0.513 Precision At 4: 0.496 Precision At 5: 0.486 Precision At 6: 0.487 Precision At 7: 0.479 Precision At 8: 0.465 Precision At 9: 0.458 Precision At 10: 0.460 Precision At 11: 0.453 Precision At 12: 0.453 Precision At 13: 0.445 Precision At 14: 0.438 Precision At 15: 0.438 Precision At 16: 0.438 Precision At 17: 0.429 Precision At 18: 0.429 Precision At 19: 0.419 Precision At 20: 0.415 PersianAnalyzer: SUMMARY Search Seconds: 0.004 DocName Seconds: 0.011 Num Points: 987.692 Num Good Points: 36.123 Max Good Points: 36.185 Average Precision: 0.481 MRR: 0.833 Recall: 0.998 Precision At 1: 0.754 Precision At 2: 0.715 Precision At 3: 0.646 Precision At 4: 0.646 Precision At 5: 0.631 Precision At 6: 0.621 Precision At 7: 0.593 Precision At 8: 0.577 Precision At 9: 0.573 Precision At 10: 0.566 Precision At 11: 0.572 Precision At 12: 0.562 Precision At 13: 0.554 Precision At 14: 0.549 Precision At 15: 0.542 Precision At 16: 0.538 Precision At 17: 0.533 Precision At 18: 0.527 Precision At 19: 0.525 Precision At 20: 0.518</description>
      <attachments/>
      <comments>24</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1629</id>
      <title>contrib intelligent Analyzer for Chinese</title>
      <description>I wrote a Analyzer for apache lucene for analyzing sentences in Chinese language. it's called "imdict-chinese-analyzer", the project on google code is here: http://code.google.com/p/imdict-chinese-analyzer/ In Chinese, "我是中国人"(I am Chinese), should be tokenized as "我"(I) "是"(am) "中国人"(Chinese), not "我" "是中" "国人". So the analyzer must handle each sentence properly, or there will be mis-understandings everywhere in the index constructed by Lucene, and the accuracy of the search engine will be affected seriously! Although there are two analyzer packages in apache repository which can handle Chinese: ChineseAnalyzer and CJKAnalyzer, they take each character or every two adjoining characters as a single word, this is obviously not true in reality, also this strategy will increase the index size and hurt the performance baddly. The algorithm of imdict-chinese-analyzer is based on Hidden Markov Model (HMM), so it can tokenize chinese sentence in a really intelligent way. Tokenizaion accuracy of this model is above 90% according to the paper "HHMM-based Chinese Lexical analyzer ICTCLAL" while other analyzer's is about 60%. As imdict-chinese-analyzer is a really fast and intelligent. I want to contribute it to the apache lucene repository.</description>
      <attachments/>
      <comments>64</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1630</id>
      <title>Mating Collector and Scorer on doc Id orderness</title>
      <description>This is a spin off of LUCENE-1593. This issue proposes to expose appropriate API on Scorer and Collector such that one can create an optimized Collector based on a given Scorer's doc-id orderness and vice versa. Copied from LUCENE-1593, here is the list of changes: Deprecate Weight and create QueryWeight (abstract class) with a new scorer(reader, scoreDocsInOrder), replacing the current scorer(reader) method. QueryWeight implements Weight, while score(reader) calls score(reader, false /* out-of-order */) and scorer(reader, scoreDocsInOrder) is defined abstract. Also add QueryWeightWrapper to wrap a given Weight implementation. This one will also be deprecated, as well as package-private. Add to Query variants of createWeight and weight which return QueryWeight. For now, I prefer to add a default impl which wraps the Weight variant instead of overriding in all Query extensions, and in 3.0 when we remove the Weight variants - override in all extending classes. Add to Scorer isOutOfOrder with a default to false, and override in BS to true. Modify BooleanWeight to extend QueryWeight and implement the new scorer method to return BS2 or BS based on the number of required scorers and setAllowOutOfOrder. Add to Collector an abstract acceptsDocsOutOfOrder which returns true/false. Use it in IndexSearcher.search methods, that accept a Collector, in order to create the appropriate Scorer, using the new QueryWeight. Provide a static create method to TFC and TSDC which accept this as an argument and creates the proper instance. Wherever we create a Collector (TSDC or TFC), always ask for out-of-order Scorer and check on the resulting Scorer isOutOfOrder(), so that we can create the optimized Collector instance. Modify IndexSearcher to use all of the above logic. The only class I'm worried about, and would like to verify with you, is Searchable. If we want to deprecate all the search methods on IndexSearcher, Searcher and Searchable which accept Weight and add new ones which accept QueryWeight, we must do the following: Deprecate Searchable in favor of Searcher. Add to Searcher the new QueryWeight variants. Here we have two choices: (1) break back-compat and add them as abstract (like we've done with the new Collector method) or (2) add them with a default impl to call the Weight versions, documenting these will become abstract in 3.0. Have Searcher extend UnicastRemoteObject and have RemoteSearchable extend Searcher. That's the part I'm a little bit worried about - Searchable implements java.rmi.Remote, which means there could be an implementation out there which implements Searchable and extends something different than UnicastRemoteObject, like Activeable. I think there is very small chance this has actually happened, but would like to confirm with you guys first. Add a deprecated, package-private, SearchableWrapper which extends Searcher and delegates all calls to the Searchable member. Deprecate all uses of Searchable and add Searcher instead, defaulting the old ones to use SearchableWrapper. Make all the necessary changes to IndexSearcher, MultiSearcher etc. regarding overriding these new methods. One other optimization that was discussed in LUCENE-1593 is to expose a topScorer() API (on Weight) which returns a Scorer that its score(Collector) will be called, and additionally add a start() method to DISI. That will allow Scorers to initialize either on start() or score(Collector). This was proposed mainly because of BS and BS2 which check if they are initialized in every call to next(), skipTo() and score(). Personally I prefer to see that in a separate issue, following that one (as it might add methods to QueryWeight).</description>
      <attachments/>
      <comments>64</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1632</id>
      <title>boolean docid set iterator improvement</title>
      <description>This was first brought up in Lucene-1345. But Lucene-1345 conversation has digressed. As per suggested, creating a separate issue to track. Added perf comparisons with boolean set iterators with current scorers See patch System: Ubunto, java version "1.6.0_11" Intel core2 Duo 2.44ghz new milliseconds=470 new milliseconds=534 new milliseconds=450 new milliseconds=443 new milliseconds=444 new milliseconds=445 new milliseconds=449 new milliseconds=441 new milliseconds=444 new milliseconds=445 new total milliseconds=4565 old milliseconds=529 old milliseconds=491 old milliseconds=428 old milliseconds=549 old milliseconds=427 old milliseconds=424 old milliseconds=420 old milliseconds=424 old milliseconds=423 old milliseconds=422 old total milliseconds=4537 New/Old Time 4565/4537 (100.61715%) OrDocIdSetIterator milliseconds=1138 OrDocIdSetIterator milliseconds=1106 OrDocIdSetIterator milliseconds=1065 OrDocIdSetIterator milliseconds=1066 OrDocIdSetIterator milliseconds=1065 OrDocIdSetIterator milliseconds=1067 OrDocIdSetIterator milliseconds=1072 OrDocIdSetIterator milliseconds=1118 OrDocIdSetIterator milliseconds=1065 OrDocIdSetIterator milliseconds=1069 OrDocIdSetIterator total milliseconds=10831 DisjunctionMaxScorer milliseconds=1914 DisjunctionMaxScorer milliseconds=1981 DisjunctionMaxScorer milliseconds=1861 DisjunctionMaxScorer milliseconds=1893 DisjunctionMaxScorer milliseconds=1886 DisjunctionMaxScorer milliseconds=1885 DisjunctionMaxScorer milliseconds=1887 DisjunctionMaxScorer milliseconds=1889 DisjunctionMaxScorer milliseconds=1891 DisjunctionMaxScorer milliseconds=1888 DisjunctionMaxScorer total milliseconds=18975 Or/DisjunctionMax Time 10831/18975 (57.080368%) OrDocIdSetIterator milliseconds=1079 OrDocIdSetIterator milliseconds=1075 OrDocIdSetIterator milliseconds=1076 OrDocIdSetIterator milliseconds=1093 OrDocIdSetIterator milliseconds=1077 OrDocIdSetIterator milliseconds=1074 OrDocIdSetIterator milliseconds=1078 OrDocIdSetIterator milliseconds=1075 OrDocIdSetIterator milliseconds=1074 OrDocIdSetIterator milliseconds=1074 OrDocIdSetIterator total milliseconds=10775 DisjunctionSumScorer milliseconds=1398 DisjunctionSumScorer milliseconds=1322 DisjunctionSumScorer milliseconds=1320 DisjunctionSumScorer milliseconds=1305 DisjunctionSumScorer milliseconds=1304 DisjunctionSumScorer milliseconds=1301 DisjunctionSumScorer milliseconds=1304 DisjunctionSumScorer milliseconds=1300 DisjunctionSumScorer milliseconds=1301 DisjunctionSumScorer milliseconds=1317 DisjunctionSumScorer total milliseconds=13172 Or/DisjunctionSum Time 10775/13172 (81.80231%) AndDocIdSetIterator milliseconds=330 AndDocIdSetIterator milliseconds=336 AndDocIdSetIterator milliseconds=298 AndDocIdSetIterator milliseconds=299 AndDocIdSetIterator milliseconds=310 AndDocIdSetIterator milliseconds=298 AndDocIdSetIterator milliseconds=298 AndDocIdSetIterator milliseconds=334 AndDocIdSetIterator milliseconds=298 AndDocIdSetIterator milliseconds=299 AndDocIdSetIterator total milliseconds=3100 ConjunctionScorer milliseconds=332 ConjunctionScorer milliseconds=307 ConjunctionScorer milliseconds=302 ConjunctionScorer milliseconds=350 ConjunctionScorer milliseconds=300 ConjunctionScorer milliseconds=304 ConjunctionScorer milliseconds=305 ConjunctionScorer milliseconds=303 ConjunctionScorer milliseconds=303 ConjunctionScorer milliseconds=299 ConjunctionScorer total milliseconds=3105 And/Conjunction Time 3100/3105 (99.83897%) main contributors to the patch: Anmol Bhasin &amp; Yasuhiro Matsuda</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1634</id>
      <title>LogMergePolicy should use the number of deleted docs when deciding which segments to merge</title>
      <description>I found that IndexWriter.optimize(int) method does not pick up large segments with a lot of deletes even when most of the docs are deleted. And the existence of such segments affected the query performance significantly. I created an index with 1 million docs, then went over all docs and updated a few thousand at a time. I ran optimize(20) occasionally. What saw were large segments with most of docs deleted. Although these segments did not have valid docs they remained in the directory for a very long time until more segments with comparable or bigger sizes were created. This is because LogMergePolicy.findMergeForOptimize uses the size of segments but does not take the number of deleted documents into consideration when it decides which segments to merge. So, a simple fix is to use the delete count to calibrate the segment size. I can create a patch for this.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1637</id>
      <title>Getting an IndexReader from a committed IndexWriter</title>
      <description>I just had a look at the job done in IndexWriter in order to get an IndexReader with all the current ongoing changes done using the IndexWriter. This feature is very useful, and I was wondering if another feature, which (I think) is simple to implement (compared to the previous one) might make sense. Many times, an application opens an IndexWriter, does whatever changes it does, and then commits the changes. It would be nice to get an IndexReader (read only one is fine) that corresponds to the committed (or even closed) IndexWriter. This will allow for a cache of IndexReader that is already used to be updated with a fresh IndexReader, without the need to reopen one (which should be slower than opening one based on the IndexWriter information). The main difference is the fact that the mentioned IndexReader could still be reopened without the need to throw an AlreadyClosedException. More information can be found here: http://www.nabble.com/Getting-an-IndexReader-from-a-committed-IndexWriter-td23551978.html</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1641</id>
      <title>Correct spatial and trie documentation links in JavaDocs and site</title>
      <description>After updating myself in the site docs, I have some changes to the site and javadocs of Lucene 2.9: Add spatial contrib to javadocs Add trie package to the contrib/queries package Both changes prevent these pacakges from a apearing in core's pacakge list on the javadocs/all homepage. I also adjusted the documentation page to reflect the changes. I will commit the attached patch, if nobody objects.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1643</id>
      <title>use reusable collation keys in ICUCollationFilter</title>
      <description>ICUCollationFilter need not create a new CollationKey object for each token. In ICU there is a mechanism to use a reusable key.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1644</id>
      <title>Enable MultiTermQuery's constant score mode to also use BooleanQuery under the hood</title>
      <description>When MultiTermQuery is used (via one of its subclasses, eg WildcardQuery, PrefixQuery, FuzzyQuery, etc.), you can ask it to use "constant score mode", which pre-builds a filter and then wraps that filter as a ConstantScoreQuery. If you don't set that, it instead builds a [potentially massive] BooleanQuery with one SHOULD clause per term. There are some limitations of this approach: The scores returned by the BooleanQuery are often quite meaningless to the app, so, one should be able to use a BooleanQuery yet get constant scores back. (Though I vaguely remember at least one example someone raised where the scores were useful...). The resulting BooleanQuery can easily have too many clauses, throwing an extremely confusing exception to newish users. It'd be better to have the freedom to pick "build filter up front" vs "build massive BooleanQuery", when constant scoring is enabled, because they have different performance tradeoffs. In constant score mode, an OpenBitSet is always used, yet for sparse bit sets this does not give good performance. I think we could address these issues by giving BooleanQuery a constant score mode, then empower MultiTermQuery (when in constant score mode) to pick &amp; choose whether to use BooleanQuery vs up-front filter, and finally empower MultiTermQuery to pick the best (sparse vs dense) bit set impl.</description>
      <attachments/>
      <comments>33</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1646</id>
      <title>QueryParser throws new exceptions even if custom parsing logic threw a better one</title>
      <description>We have subclassed QueryParser and have various custom fields. When these fields contain invalid values, we throw a subclass of ParseException which has a more useful message (and also a localised message.) Problem is, Lucene's QueryParser is doing this: catch (ParseException tme) { // rethrow to include the original query: throw new ParseException("Cannot parse '" +query+ "': " + tme.getMessage()); } Thus, our nice and useful ParseException is thrown away, replaced by one with no information about what's actually wrong with the query (it does append getMessage() but that isn't localised. And it also throws away the underlying cause for the exception.) I am about to patch our copy to simply remove these four lines; the caller knows what the query string was (they have to have a copy of it because they are passing it in!) so having it in the error message itself is not useful. Furthermore, when the query string is very big, what the user wants to know is not that the whole query was bad, but which part of it was bad.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1650</id>
      <title>Small fix in CustomScoreQuery JavaDoc</title>
      <description>I have fixed the javadoc for "Modified Score" formular in CustomScoreQuery. - Patch attached: customScoreQuery_JavaDoc.patch I'm quite curious why the method: public float customScore(int doc, float subQueryScore, float valSrcScores[]) calls public float customScore(int doc, float subQueryScore, float valSrcScore]) only in 2 of the 3 cases which makes the choice to override either one of the customScore methods dependent on the number of ValueSourceQuery passed to the constructor. I figure it would be more consistent if it would call the latter in all 3 cases. I also attached a patch which proposes a fix for that issue. The patch does also include the JavaDoc issue mentioned above. customScoreQuery_CodeChange+JavaDoc.patch</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1652</id>
      <title>Enhancements to Scorers following the changes to DocIdSetIterator</title>
      <description>In LUCENE-1614, we changed the semantics of DocIdSetIterator's methods to return a sentinel NO_MORE_DOCS (= Integer.MAX_VALUE) when the iterator has exhausted. Due to backward compatibility issues, we couldn't implement that semantics in doc(). Therefore this issue, which can be introduced in 3.0 only will: Implement the new semantics in all extending classes, such that doc() will return NO_MORE_DOCS when the iterator has exhausted. Change BooleanScorer to take advantage of that by removing sub.done from SubScorer and operate under the assumption that NO_MORE_DOCS is larger than any doc ID (Integer.MAX_VALUE). Change ConjunctionScorer to operate under the same assumptions and remove 'more'. Change ReqExclScorer to not rely on reqScorer in doc(), since the latter may be null. Make more changes to ConjunctionScorer's init() and remove 'firstTime' to improve the performance of nextDoc(), score(), advance(). Add start()/finish() to DISI? A snippet from LUCENE-1614 regarding the change in BooleanScorer int doc = sub.done ? -1 : scorer.doc(); while (!sub.done &amp;&amp; doc &lt; end) { sub.collector.collect(doc); doc = scorer.nextDoc(); sub.done = doc &lt; 0; } To this: int doc = scorer.doc(); while (doc &lt; end) { sub.collector.collect(doc); doc = scorer.nextDoc(); } And in ConjunctionScorer, change this: while (more &amp;&amp; (firstScorer=scorers[first]).doc() &lt; (lastDoc=lastScorer.doc())) { more = firstScorer.advance(lastDoc) &gt;= 0; lastScorer = firstScorer; first = (first == (scorers.length-1)) ? 0 : first+1; } return more; To this: while ((firstScorer=scorers[first]).doc() &lt; (lastDoc=lastScorer.doc())) { firstScorer.advance(lastDoc); lastScorer = firstScorer; first = (first == (scorers.length-1)) ? 0 : first+1; } return lastDoc != DOC_SENTINEL;</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1653</id>
      <title>Change DateTools to not create a Calendar in every call to dateToString or timeToString</title>
      <description>DateTools creates a Calendar instance on every call to dateToString and timeToString. Specifically: timeToString calls Calendar.getInstance on every call. dateToString calls timeToString(date.getTime()), which then instantiates a new Date(). I think we should change the order of the calls, or not have each call the other. round(), which is called from timeToString (after creating a Calendar instance) creates another Calendar instance ... Seems that if we synchronize the methods and create the Calendar instance once (static), it should solve it.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1654</id>
      <title>Include diagnostics per-segment when writing a new segment</title>
      <description>It would be very helpful if each segment in an index included diagnostic information, such as the current version of Lucene. EG, in LUCENE-1474 this would be very helpful to see if certain segments were written under 2.4.0. We can start with just the current version. We could also consider making this extensible, so you could provide your own arbitrary diagnostics, but SegmentInfo/s is not public so I think such an API would be "one-way" in that you'd have to use CheckIndex to check on it later. Or we could wait on such extensibility until we provide some consistent way to access per-segment details in the index.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1656</id>
      <title>When sorting by field, IndexSearcher should not compute scores by default</title>
      <description>In 2.9 we've added the ability to turn off scoring (maxScore &amp; trackScores, separately) when sorting by field. I expect most apps don't use the scores when sorting by field, and there's a sizable performance gain when scoring is off, so I think for 2.9 we should not score by default, and add show in CHANGES how to enable scoring if you rely on it. If there are no objections, I'll commit that change in a day or two (it's trivial).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1657</id>
      <title>Make "boolean readOnly" a required arg to IndexReader.open</title>
      <description>Most apps don't need read/write IndexReader, and, a readOnly IndexReader has better concurrent performance. I'd love to simply default readOnly to true, and you'd have to specify "false" if you want a read/write reader (I think that's the natural default), but I think that'd break too many back-compat cases. So the workaround is to make the parameter explicit, in 2.9. I think even for IndexSearcher's methods that open an IndexReader under the hood, we should also make the parameter explicit.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1658</id>
      <title>Absorb NIOFSDirectory into FSDirectory</title>
      <description>I think whether one uses java.io.* vs java.nio.* or eventually java.nio2.*, or some other means, is an under-the-hood implementation detail of FSDirectory and doesn't merit a whole separate class. I think FSDirectory should be the core class one uses when one's index is in the filesystem. So, I'd like to deprecate NIOFSDirectory, absorbing it into FSDirectory, and add a setting "useNIO" to FSDirectory. It should default to "true" for non-Windows OSs, because it gives far better concurrent performance on all platforms but Windows (due to known Sun JRE issue http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734).</description>
      <attachments/>
      <comments>71</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1659</id>
      <title>Make readOnly setting to IndexReader.open / IndexSearcher ctor explicit</title>
      <description>Most apps don't need writable IndexReader, and, a readOnly IndexReader has better concurrent performance. I'd love to simply default readOnly to true, and you'd have to specify "false" if you want a read/write reader (I think that's the natural default), but that'd likely break too many back-compat cases. So the workaround is to make the parameter explicit, in 2.9. I think even for IndexSearcher's methods that open an IndexReader under the hood, we should also make the parameter explicit.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1660</id>
      <title>Make StopFilter.enablePositionIncrements explicit</title>
      <description>I think the default for this should be true, ie, do not lose information when filtering (preserve the positions of the original tokens). But, we can't change this without breaking back-compat. So, as workaround, we should make the parameter explicit so one must decide up front.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1661</id>
      <title>Change visibility of getComparator method in SortField from protected to public</title>
      <description>Hi, Currently I'm using SortField for the creation of FieldComparators, but I ran into an issue. I cannot invoke SortField.getComparator(...) directly from my code, which forces me to use a workaround. (subclass SortField and override the getComparator method with visiblity public) I'm proposing to make this method public. Currently I do not see any problems changing the visibility to public, I do not know if there are any (and the reason why this method is currently protected) I think that this is a cleaner solution then the workaround I used and also other developers can benefit from it. I will also attach a patch to this issue based on the code in the trunk (26th of May). place). Please let me know your thoughts about this. Cheers, Martijn</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1662</id>
      <title>consolidate FieldCache and ExtendedFieldCache instances</title>
      <description>It's confusing and error prone having two instances of FieldCache... FieldCache .DEFAULT and ExtendedFieldCache .EXT_DEFAULT. Accidentally use the wrong one and you silently double the memory usage for that field. Since ExtendedFieldCache extends FieldCache, there's no reason not to share the same instance across both.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1663</id>
      <title>Documentation bug. The 2.4.1 query parser syntax wiki page says it is for 1.9</title>
      <description>This page: http://lucene.apache.org/java/2_4_1/queryparsersyntax.html says this: .bq This page provides the Query Parser syntax in Lucene 1.9. If you are using a different version of Lucene, please consult the copy of docs/queryparsersyntax.html that was distributed with the version you are using. This is misleading on a doc page for 2.4.1</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1665</id>
      <title>Remove SortField.AUTO</title>
      <description>I'd like to remove SortField.AUTO... it's dangerous for Lucene to guess the type of your field, based on the first term it encounters. It can easily be wrong, and, whether it's wrong or right could suddenly change as you index different documents. It unexepctedly binds SortField to needing an IndexReader to do the guessing. It's caused various problems in the past (most recently, for me on LUCENE-1656) as we fix other issues/make improvements. I'd prefer that users of Lucene's field sort be explicit about the type that Lucene should cast the field to. Someday, if we have optional strong[er] typing of Lucene's fields, such type information would already be known. But in the meantime, I think users should be explicit.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1667</id>
      <title>ConcurrentMergeScheduler use a thread pool (per directory)</title>
      <description>Modifiy ConcurrentMergeScheduler to use a thread pool per merge target directory. Add settings for the thread pool. For use with LUCENE-1313. We may want to wait to implement this in 3.0 when we can reuse ThreadPoolExecutor in Java 1.5.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1670</id>
      <title>Cosmetic JavaDoc updates</title>
      <description>I've taken the liberty of making a few cosmetic updates to various JavaDocs: MergePolicy (minor cosmetic change) LogMergePolicy (minor cosmetic change) IndexWriter (major cleanup in class description, changed anchors to JavaDoc links [now works in Eclipse], no content change) Attached diff from SVN r780545. I would appreciate if whomever goes over this can let me know if my issue parameter choices were correct (yeah, blame my OCD), and if there's a more practical/convenient way to send these in, please let me know</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1671</id>
      <title>FSDirectory internally caches and clones FSIndexInput</title>
      <description>The patch will fix this small problem where if FSDirectory.openInput is called, a new unnecessary file descriptors is opened (whereas an IndexInput.clone would work).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1673</id>
      <title>Move TrieRange to core</title>
      <description>TrieRange was iterated many times and seems stable now (LUCENE-1470, LUCENE-1582, LUCENE-1602). There is lots of user interest, Solr added it to its default FieldTypes (SOLR-940) and if possible I want to move it to core before release of 2.9. Before this can be done, there are some things to think about: There are now classes called LongTrieRangeQuery, IntTrieRangeQuery, how should they be called in core? I would suggest to leave it as it is. On the other hand, if this keeps our only numeric query implementation, we could call it LongRangeQuery, IntRangeQuery or NumericRangeQuery (see below, here are problems). Same for the TokenStreams and Filters. Maybe the pairs of classes for indexing and searching should be moved into one class: NumericTokenStream, NumericRangeQuery, NumericRangeFilter. The problem here: ctors must be able to pass int, long, double, float as range parameters. For the end user, mixing these 4 types in one class is hard to handle. If somebody forgets to add a L to a long, it suddenly instantiates a int version of range query, hitting no results and so on. Same with other types. Maybe accept java.lang.Number as parameter (because nullable for half-open bounds) and one enum for the type. TrieUtils move into o.a.l.util? or document or? Move TokenStreams into o.a.l.analysis, ShiftAttribute into o.a.l.analysis.tokenattributes? Somewhere else? If we rename the classes, should Solr stay with Trie (because there are different impls)? Maybe add a subclass of AbstractField, that automatically creates these TokenStreams and omits norms/tf per default for easier addition to Document instances?</description>
      <attachments/>
      <comments>41</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1674</id>
      <title>Add IndexReaderFactory to IndexWriter</title>
      <description>With LUCENE-1516, IndexWriter.getReader, we take over the instantiating of IndexReaders which prevents users who have implemented custom IndexReader subclasses from using them. The patch will create an IndexWriter.setReaderFactory method and a IndexReaderFactory class that allows custom creation of the internal readers created by IndexWriter.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1675</id>
      <title>Add a link to the release archive</title>
      <description>It would be nice if the Releases page contained a link to the release archive at http://archive.apache.org/dist/lucene/java/.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1676</id>
      <title>New Token filter for adding payloads "in-stream"</title>
      <description>This TokenFilter is able to split a token based on a delimiter and use one part as the token and the other part as a payload. This allows someone to include payloads inline with tokens (presumably setup by a pipeline ahead of time). An example is apropos. Given a | delimiter, we could have a stream that looks like: The quick|JJ red|JJ fox|NN jumped|VB over the lazy|JJ brown|JJ dogs|NN In this case, this would produce tokens and payloads (assuming whitespace tokenization): Token: the Payload: null Token: quick Payload: JJ Token: red Pay: JJ. and so on. This patch will also support pluggable encoders for the payloads, so it can convert from the character array to byte arrays as appropriate.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1679</id>
      <title>Make WildcardTermEnum#difference() non-final</title>
      <description>The method WildcardTermEnum#difference() is declared final. I found it very useful to subclass WildcardTermEnum to implement different scoring for exact vs. partial matches. The change is rather trivial (attached) but I guess it could make life easier for a couple of users. I attached two patches: one which contains the single change to make difference() non-final (WildcardTermEnum.patch) one which does also contain some minor cleanup of WildcardTermEnum. I removed unnecessary member initialization and made those final. ( WildcardTermEnum_cleanup.patch) Thanks simon</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1680</id>
      <title>Make prefixLength accessible to PrefixTermEnum subclasses</title>
      <description>PrefixTermEnum#difference() offers a way to influence scoring based on the difference between the prefix Term and a term in the enumeration. To effectively use this facility the length of the prefix should be accessible to subclasses. Currently the prefix term is private to PrefixTermEnum. I added a getter for the prefix length and made PrefixTermEnum#endEnum(), PrefixTermEnum#termCompare() final for consistency with other TermEnum subclasses. Patch is attached. Simon</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1682</id>
      <title>unit tests should use private directories</title>
      <description>This only affects our unit tests... I run "ant test" and "ant test-tag" concurrently, but some tests have false failures (eg TestPayloads) because they use a fixed test directory in the filesystem for testing. I've added a simple method to _TestUtil to get a temp dir, and switched over those tests that I've hit false failures on.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1683</id>
      <title>RegexQuery matches terms the input regex doesn't actually match</title>
      <description>I was writing some unit tests for our own wrapper around the Lucene regex classes, and got tripped up by something interesting. The regex "cat." will match "cats" but also anything with "cat" and 1+ following letters (e.g. "cathy", "catcher", ...) It is as if there is an implicit .* always added to the end of the regex. Here's a unit test for the behaviour I would expect myself: @Test public void testNecessity() throws Exception { File dir = new File(new File(System.getProperty("java.io.tmpdir")), "index"); IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), true); try { Document doc = new Document(); doc.add(new Field("field", "cat cats cathy", Field.Store.YES, Field.Index.TOKENIZED)); writer.addDocument(doc); } finally { writer.close(); } IndexReader reader = IndexReader.open(dir); try { TermEnum terms = new RegexQuery(new Term("field", "cat.")).getEnum(reader); assertEquals("Wrong term", "cats", terms.term()); assertFalse("Should have only been one term", terms.next()); } finally { reader.close(); } } This test fails on the term check with terms.term() equal to "cathy". Our workaround is to mangle the query like this: String fixed = String.format("(?:%s)$", original);</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1684</id>
      <title>Add matchVersion to StandardAnalyzer</title>
      <description>I think we should add a matchVersion arg to StandardAnalyzer. This allows us to fix bugs (for new users) while keeping precise back compat (for users who upgrade). We've discussed this on java-dev, but I'd like to now make it concrete (patch attached). I think it actually works very well, and is a simple tool to help us carry out our back-compat policy. I coded up an example with StandardAnalyzer: The ctor now takes a required arg (Version matchVersion). You pass Version.LUCENE_CURRENT to always get lates &amp; greatest, or eg Version.LUCENE_24 to match 2.4's bugs/settings/behavior. StandardAalyzer conditionalizes the "replace invalid acronym" and "enable position increment in StopFilter" based on matchVersion. It also prevents creating zillions of ctors, over time, as we need to change settings in the class. EG StandardAnalyzer now has 2 settings that are version dependent, and there's at least another 2 issues open on fixing some more of its bugs. The migration is also very clean: we'd only add this to classes on an "as needed" basis. On the first release that adds the arg, the default remains back compatible with the prior release. Then, going forward, we are free to fix issues on that class and conditionalize by matchVersion. The javadoc at the top of StandardAnalyzer clearly calls out what version specific behavior is done: * &lt;p&gt;You must specify the required {@link Version} * compatibility when creating StandardAnalyzer: * &lt;ul&gt; * &lt;li&gt; As of 2.9, StopFilter preserves position * increments by default * &lt;li&gt; As of 2.9, Tokens incorrectly idenfied as acronyms * are corrected (see &lt;a href="https://issues.apache.org/jira/browse/LUCENE-1068"&gt;LUCENE-1608&lt;/a&gt; * &lt;/ul&gt; *</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1685</id>
      <title>Make the Highlighter use SpanScorer by default</title>
      <description>I've always thought this made sense, but frankly, it took me a year to get the SpanScorer included with Lucene at all, so I was pretty much ready to move on after I it got in, rather than push for it as a default. I think it makes sense as the default in Solr as well, and I mentioned that back when it was put in, but alas, its an option there as well. The Highlighter package has no back compat req, but custom has been conservative - one reason I havn't pushed for this change before. Might be best to actually make the switch in 3? I could go either way - as is, I know a bunch of people use it, but I'm betting its the large minority. It has never been listed in a changes entry and its not in LIA 1, so you pretty much have to stumble upon it, and figure out what its for. I'll point out again that its just as fast as the standard scorer for any clause of a query that is not position sensitive. Position sensitive query clauses will obviously be somewhat slower to highlight, but that is because they will be highlighted correctly rather than ignoring position.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1686</id>
      <title>Remove Unnecessary NULL check in FindSegmentsFile - cleanup</title>
      <description>FindSegmentsFile accesses the member "directory" in line 579 while performing a null check in 592. The null check is unnecessary as if directory is null line 579 would throw a NPE. I removed the null check and made the member "directory" final. In addition I added a null check in the constructor as If the value is null we should catch it asap.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1687</id>
      <title>Remove ExtendedFieldCache by rolling functionality into FieldCache</title>
      <description>It is silly that we have ExtendedFieldCache. It is a workaround to our supposed back compatibility problem. This patch will merge the ExtendedFieldCache interface into FieldCache, thereby breaking back compatibility, but creating a much simpler API for FieldCache.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1688</id>
      <title>Deprecating StopAnalyzer ENGLISH_STOP_WORDS - General replacement with an immutable Set</title>
      <description>StopAnalyzer and StandartAnalyzer are using the static final array ENGLISH_STOP_WORDS by default in various places. Internally this array is converted into a mutable set which looks kind of weird to me. I think the way to go is to deprecate all use of the static final array and replace it with an immutable implementation of CharArraySet. Inside an analyzer it does not make sense to have a mutable set anyway and we could prevent set creation each time an analyzer is created. In the case of an immutable set we won't have multithreading issues either. in essence we get rid of a fair bit of "converting string array to set" code, do not have a PUBLIC static reference to an array (which is mutable) and reduce the overhead of analyzer creation. let me know what you think and I create a patch for it. simon</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1689</id>
      <title>supplementary character handling</title>
      <description>for Java 5. Java 5 is based on unicode 4, which means variable-width encoding. supplementary character support should be fixed for code that works with char/char[] For example: StandardAnalyzer, SimpleAnalyzer, StopAnalyzer, etc should at least be changed so they don't actually remove suppl characters, or modified to look for surrogates and behave correctly. LowercaseFilter should be modified to lowercase suppl. characters correctly. CharTokenizer should either be deprecated or changed so that isTokenChar() and normalize() use int. in all of these cases code should remain optimized for the BMP case, and suppl characters should be the exception, but still work.</description>
      <attachments/>
      <comments>46</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1690</id>
      <title>Morelikethis queries are very slow compared to other search types</title>
      <description>The MoreLikeThis object performs term frequency lookups for every query. From my testing that's what seems to take up the majority of time for MoreLikeThis searches. For some (I'd venture many) applications it's not necessary for term statistics to be looked up every time. A fairly naive opt-in caching mechanism tied to the life of the MoreLikeThis object would allow applications to cache term statistics for the duration that suits them. I've got this working in my test code. I'll put together a patch file when I get a minute. From my testing this can improve performance by a factor of around 10.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1691</id>
      <title>An index copied over another index can result in corruption</title>
      <description>After restoring an older backup of an index over the top of a newer version of the index, attempts to open the index can result in CorruptIndexExceptions, such as: Caused by: org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _ed: fieldsReader shows 1137 but segmentInfo shows 1389 at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:362) at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:306) at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:228) at org.apache.lucene.index.MultiSegmentReader.&lt;init&gt;(MultiSegmentReader.java:55) at org.apache.lucene.index.ReadOnlyMultiSegmentReader.&lt;init&gt;(ReadOnlyMultiSegmentReader.java:27) at org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:102) at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:653) at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:115) at org.apache.lucene.index.IndexReader.open(IndexReader.java:316) at org.apache.lucene.index.IndexReader.open(IndexReader.java:237) The apparent cause is the strategy of taking the maximum of the ID in the segments.gen file, and the IDs of the apparently valid segment files (See lines 523-593 here), and using this as the current generation of the index. This will include "stale" segments that existed before the backup was restored.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1693</id>
      <title>AttributeSource/TokenStream API improvements</title>
      <description>This patch makes the following improvements to AttributeSource and TokenStream/Filter: introduces interfaces for all Attributes. The corresponding implementations have the postfix 'Impl', e.g. TermAttribute and TermAttributeImpl. AttributeSource now has a factory for creating the Attribute instances; the default implementation looks for implementing classes with the postfix 'Impl'. Token now implements all 6 TokenAttribute interfaces. new method added to AttributeSource: addAttributeImpl(AttributeImpl). Using reflection it walks up in the class hierarchy of the passed in object and finds all interfaces that the class or superclasses implement and that extend the Attribute interface. It then adds the interface-&gt;instance mappings to the attribute map for each of the found interfaces. removes the set/getUseNewAPI() methods (including the standard ones). Instead it is now enough to only implement the new API, if one old TokenStream implements still the old API (next()/next(Token)), it is wrapped automatically. The delegation path is determined via reflection (the patch determines, which of the three methods was overridden). Token is no longer deprecated, instead it implements all 6 standard token interfaces (see above). The wrapper for next() and next(Token) uses this, to automatically map all attribute interfaces to one TokenWrapper instance (implementing all 6 interfaces), that contains a Token instance. next() and next(Token) exchange the inner Token instance as needed. For the new incrementToken(), only one TokenWrapper instance is visible, delegating to the currect reusable Token. This API also preserves custom Token subclasses, that maybe created by very special token streams (see example in Backwards-Test). AttributeImpl now has a default implementation of toString that uses reflection to print out the values of the attributes in a default formatting. This makes it a bit easier to implement AttributeImpl, because toString() was declared abstract before. Cloning is now done much more efficiently in captureState. The method figures out which unique AttributeImpl instances are contained as values in the attributes map, because those are the ones that need to be cloned. It creates a single linked list that supports deep cloning (in the inner class AttributeSource.State). AttributeSource keeps track of when this state changes, i.e. whenever new attributes are added to the AttributeSource. Only in that case will captureState recompute the state, otherwise it will simply clone the precomputed state and return the clone. restoreState(AttributeSource.State) walks the linked list and uses the copyTo() method of AttributeImpl to copy all values over into the attribute that the source stream (e.g. SinkTokenizer) uses. Tee- and SinkTokenizer were deprecated, because they use Token instances for caching. This is not compatible to the new API using AttributeSource.State objects. You can still use the old deprecated ones, but new features provided by new Attribute types may get lost in the chain. A replacement is a new TeeSinkTokenFilter, which has a factory to create new Sink instances, that have compatible attributes. Sink instances created by one Tee can also be added to another Tee, as long as the attribute implementations are compatible (it is not possible to add a sink from a tee using one Token instance to a tee using the six separate attribute impls). In this case UOE is thrown. The cloning performance can be greatly improved if not multiple AttributeImpl instances are used in one TokenStream. A user can e.g. simply add a Token instance to the stream instead of the individual attributes. Or the user could implement a subclass of AttributeImpl that implements exactly the Attribute interfaces needed. I think this should be considered an expert API (addAttributeImpl), as this manual optimization is only needed if cloning performance is crucial. I ran some quick performance tests using Tee/Sink tokenizers (which do cloning) and the performance was roughly 20% faster with the new API. I'll run some more performance tests and post more numbers then. Note also that when we add serialization to the Attributes, e.g. for supporting storing serialized TokenStreams in the index, then the serialization should benefit even significantly more from the new API than cloning. This issue contains one backwards-compatibility break: TokenStreams/Filters/Tokenizers should normally be final (see LUCENE-1753 for the explaination). Some of these core classes are not final and so one could override the next() or next(Token) methods. In this case, the backwards-wrapper would automatically use incrementToken(), because it is implemented, so the overridden method is never called. To prevent users from errors not visible during compilation or testing (the streams just behave wrong), this patch makes all implementation methods final (next(), next(Token), incrementToken()), whenever the class itsself is not final. This is a BW break, but users will clearly see, that they have done something unsupoorted and should better create a custom TokenFilter with their additional implementation (instead of extending a core implementation). For further changing contrib token streams the following procedere should be used: rewrite and replace next(Token)/next() implementations by new API if the class is final, no next(Token)/next() methods needed (must be removed!!!) if the class is non-final add the following methods to the class: /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should * not be overridden. Delegates to the backwards compatibility layer. */ public final Token next(final Token reusableToken) throws java.io.IOException { return super.next(reusableToken); } /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should * not be overridden. Delegates to the backwards compatibility layer. */ public final Token next() throws java.io.IOException { return super.next(); } Also the incrementToken() method must be final in this case (and the new method end() of LUCENE-1448)</description>
      <attachments/>
      <comments>172</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1694</id>
      <title>Query#mergeBooleanQueries argument should be of type BooleanQuery[] instead of Query[]</title>
      <description>The method #mergeBooleanQueries accepts Query[] and casts elements to BooleanQuery without checking. This will guarantee a ClassCastException if it is not a boolean query. We should enforce this by changing the signature. This won't really break back compat. as it only works with instances of BooleanQuery.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1696</id>
      <title>Added New Token API impl for ASCIIFoldingFilter</title>
      <description>I added an implementation of incrementToken to ASCIIFoldingFilter.java and extended the existing testcase for it. I will attach the patch shortly. Beside this improvement I would like to start up a small discussion about this filter. ASCIIFoldingFitler is meant to be a replacement for ISOLatin1AccentFilter which is quite nice as it covers a superset of the latter. I have used this filter quite often but never on a as it is basis. In the most cases this filter does the correct thing (replace a special char with its ascii correspondent) but in some cases like for German umlaut it does not return the expected result. A german umlaut like 'ä' does not translate to a but rather to 'ae'. I would like to change this but I'n not 100% sure if that is expected by all users of that filter. Another way of doing it would be to make it configurable with a flag. This would not affect performance as we only check if such a umlaut char is found. Further it would be really helpful if that filter could "inject" the original/unmodified token with the same position increment into the token stream on demand. I think its a valid use-case to index the modified and unmodified token. For instance, the german word "süd" would be folded to "sud". In a query q:(süd) the filter would also fold to sud and therefore find sud which has a totally different meaning. Folding works quite well but for special cases would could add those options to make users life easier. The latter could be done in a subclass while the umlaut problem should be fixed in the base class. simon</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1697</id>
      <title>MoreLikeThis should use the new Token API</title>
      <description>The MoreLikeThis functionality needs to be converted to use the new TokenStream API. See also LUCENE-1695.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1699</id>
      <title>Field tokenStream should be usable with stored fields.</title>
      <description>Field.tokenStream should be usable for indexing even for stored values. Useful for many types of pre-analyzed values (text/numbers, etc) http://search.lucidimagination.com/search/document/902bad4eae20bdb8/field_tokenstreamvalue</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1701</id>
      <title>Add NumericField, make plain text numeric parsers public in FieldCache, move trie parsers to FieldCache</title>
      <description>In discussions about LUCENE-1673, Mike &amp; me wanted to add a new NumericField to o.a.l.document specific for easy indexing. An alternative would be to add a NumericUtils.newXxxField() factory, that creates a preconfigured Field instance with norms and tf off, optionally a stored text (LUCENE-1699) and the TokenStream already initialized. On the other hand NumericUtils.newXxxSortField could be moved to NumericSortField. I and Yonik tend to use the factory for both, Mike tends to create the new classes. Also the parsers for string-formatted numerics are not public in FieldCache. As the new SortField API (LUCENE-1478) makes it possible to support a parser in SortField instantiation, it would be good to have the static parsers in FieldCache public available. SortField would init its member variable to them (instead of NULL), so making code a lot easier (FieldComparator has this ugly null checks when retrieving values from the cache). Moving the Trie parsers also as static instances into FieldCache would make the code cleaner and we would be able to hide the "hack" StopFillCacheException by making it private to FieldCache (currently its public because NumericUtils is in o.a.l.util).</description>
      <attachments/>
      <comments>54</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1703</id>
      <title>Add a waitForMerges() method to IndexWriter</title>
      <description>It would be very useful to have a waitForMerges() method on the IndexWriter. Right now, the only way i can see to achieve this is to call IndexWriter.close() ideally, there would be a method on the IndexWriter to wait for merges without actually closing the index. This would make it so that background merges (or optimize) can be waited for without closing the IndexWriter, and then reopening a new IndexWriter the close() reopen IndexWriter method can be problematic if the close() fails as the write lock won't be released this could then result in the following sequence: close() - fails force unlock the write lock (per close() documentation) new IndexWriter() (acquires write lock) finalize() on old IndexWriter releases the write lock Index is now not locked, and another IndexWriter pointing to the same directory could be opened If you don't force unlock the write lock, opening a new IndexWriter will fail until garbage collection calls finalize() the old IndexWriter If the waitForMerges() method is available, i would likely never need to close() the IndexWriter until right before the process being shutdown, so this issue would not occur (worst case scenario, the waitForMerges() fails)</description>
      <attachments/>
      <comments>27</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1704</id>
      <title>org.apache.lucene.ant.HtmlDocument added Tidy config file passthrough availability</title>
      <description>Parsing HTML documents using the org.apache.lucene.ant.HtmlDocument.Document method resulted in many error messages such as this: line 152 column 725 - Error: &lt;as-html&gt; is not recognized! This document has errors that must be fixed before using HTML Tidy to generate a tidied up version. The solution is to configure Tidy to accept these abnormal tags by adding the tag name to the "new-inline-tags" option in the Tidy config file (or the command line which does not make sense in this context), like so: new-inline-tags: as-html Tidy needs to know where the configuration file is, so a new constructor and Document method can be added. Here is the code: /** * Constructs an &lt;code&gt;HtmlDocument&lt;/code&gt; from a {@link * java.io.File}. * *@param file the &lt;code&gt;File&lt;/code&gt; containing the * HTML to parse *@param tidyConfigFile the &lt;code&gt;String&lt;/code&gt; containing * the full path to the Tidy config file *@exception IOException if an I/O exception occurs */ public HtmlDocument(File file, String tidyConfigFile) throws IOException { Tidy tidy = new Tidy(); tidy.setConfigurationFromFile(tidyConfigFile); tidy.setQuiet(true); tidy.setShowWarnings(false); org.w3c.dom.Document root = tidy.parseDOM(new FileInputStream(file), null); rawDoc = root.getDocumentElement(); } /** * Creates a Lucene &lt;code&gt;Document&lt;/code&gt; from a {@link * java.io.File}. * *@param file *@param tidyConfigFile the full path to the Tidy config file *@exception IOException */ public static org.apache.lucene.document.Document Document(File file, String tidyConfigFile) throws IOException { HtmlDocument htmlDoc = new HtmlDocument(file, tidyConfigFile); org.apache.lucene.document.Document luceneDoc = new org.apache.lucene.document.Document(); luceneDoc.add(new Field("title", htmlDoc.getTitle(), Field.Store.YES, Field.Index.ANALYZED)); luceneDoc.add(new Field("contents", htmlDoc.getBody(), Field.Store.YES, Field.Index.ANALYZED)); String contents = null; BufferedReader br = new BufferedReader(new FileReader(file)); StringWriter sw = new StringWriter(); String line = br.readLine(); while (line != null) { sw.write(line); line = br.readLine(); } br.close(); contents = sw.toString(); sw.close(); luceneDoc.add(new Field("rawcontents", contents, Field.Store.YES, Field.Index.NO)); return luceneDoc; } I am using this now and it is working fine. The configuration file is being passed to Tidy and now I am able to index thousands of HTML pages with no more Tidy tag errors.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1706</id>
      <title>Site search powered by Lucene/Solr</title>
      <description>For a number of years now, the Lucene community has been criticized for not eating our own "dog food" when it comes to search. My company has built and hosts a site search (http://www.lucidimagination.com/search) that is powered by Apache Solr and Lucene and we'd like to donate it's use to the Lucene community. Additionally, it allows one to search all of the Lucene content from a single place, including web, wiki, JIRA and mail archives. See also http://www.lucidimagination.com/search/document/bf22a570bf9385c7/search_on_lucene_apache_org You can see it live on Mahout, Tika and Solr Lucid has a fault tolerant setup with replication and fail over as well as monitoring services in place. We are committed to maintaining and expanding the search capabilities on the site. The following patch adds a skin to the Forrest site that enables the Lucene site to search Lucene only content using Lucene/Solr. When a search is submitted, it automatically selects the Lucene facet such that only Lucene content is searched. From there, users can then narrow/broaden their search criteria. I plan on committing in a 3 or 4 days.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1707</id>
      <title>Don't use ensureOpen() excessively in IndexReader and IndexWriter</title>
      <description>A spin off from here: http://www.nabble.com/Excessive-use-of-ensureOpen()-td24127806.html. We should stop calling this method when it's not necessary for any internal Lucene code. Currently, this code seems to hurt properly written apps, unnecessarily. Will post a patch soon</description>
      <attachments/>
      <comments>29</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1708</id>
      <title>Improve the use of isDeleted in the indexing code</title>
      <description>A spin off from here: http://www.nabble.com/Some-thoughts-around-the-use-of-reader.isDeleted-and-hasDeletions-td23931216.html. Two changes: Optimize SegmentMerger work when a reader has no deletions. IndexReader.document() will no longer check if the document is deleted. Will post a patch shortly</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1709</id>
      <title>Parallelize Tests</title>
      <description>The Lucene tests can be parallelized to make for a faster testing system. This task from ANT can be used: http://ant.apache.org/manual/CoreTasks/parallel.html Previous discussion: http://www.gossamer-threads.com/lists/lucene/java-dev/69669 Notes from Mike M.: I'd love to see a clean solution here (the tests are embarrassingly parallelizable, and we all have machines with good concurrency these days)... I have a rather hacked up solution now, that uses "-Dtestpackage=XXX" to split the tests up. Ideally I would be able to say "use N threads" and it'd do the right thing... like the -j flag to make.</description>
      <attachments/>
      <comments>37</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>1710</id>
      <title>Add byte/short to NumericUtils, NumericField and NumericRangeQuery</title>
      <description>Although NumericRangeQuery will not profit much from trie-encoding short/byte fields (byte fields with e.g. precisionStep 8 would only create one precision), it may be good to have these two data types available with NumericField to be generally able to store them in prefix-encoded form in index. This is important for loading them into FieldCache where they require much less memory.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1711</id>
      <title>Field meta-data</title>
      <description>Allow user defined meta-data per Field. This would be stored by FieldInfos.write. Not sure about how to merge different values. The actual typed value should be Map&lt;String,String&gt; available from Field. The functionality can be used for a variety of purposes including trie, schemas, CSF, etc.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1712</id>
      <title>Set default precisionStep for NumericField and NumericRangeFilter</title>
      <description>This is a spinoff from LUCENE-1701. A user using Numeric* should not need to understand what's "under the hood" in order to do their indexing &amp; searching. They should be able to simply: doc.add(new NumericField("price", 15.50); And have a decent default precisionStep selected for them. Actually, if we add ctors to NumericField for each of the supported types (so the above code works), we can set the default per-type. I think we should do that? 4 for int and 6 for long was proposed as good defaults. The default need not be "perfect", as advanced users can always optimize their precisionStep, and for users experiencing slow RangeQuery performance, NumericRangeQuery with any of the defaults we are discussing will be much faster.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1713</id>
      <title>Rename RangeQuery -&gt; TermRangeQuery</title>
      <description>Since we now have NumericRangeQuery (LUCENE-1701) we should rename RangeQuery to TextRangeQuery to make it clear that TextRangeQuery (TermRangeQuery? StringRangeQuery) is based entirely on text comparison. And, existing users on upgrading to 2.9 and using RangeQuery for [slow] numeric searching would realize they now have a good option for numeric range searching.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1716</id>
      <title>Adding norms, properties indexing and writer.infoStream support to benchmark</title>
      <description>I would like to add the following support in benchmark: Ability to specify whether norms should be stored in the index. Ability to specify whether norms should be stored for the body field (assuming norms are usually stored for that field in real life applications, make it explicit) Ability to specify an infoStream for IndexWriter Ability to specify whether to index the properties returned on DocData (for content sources like TREC, these may include arbitrary &lt;meta&gt; tags, which we may not want to index). Patch to come shortly.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1719</id>
      <title>Add javadoc notes about ICUCollationKeyFilter's advantages over CollationKeyFilter</title>
      <description>contrib/collation's ICUCollationKeyFilter, which uses ICU4J collation, is faster than CollationKeyFilter, the JVM-provided java.text.Collator implementation in the same package. The javadocs of these classes should be modified to add a note to this effect. My curiosity was piqued by Robert Muir's comment on LUCENE-1581, in which he states that ICUCollationKeyFilter is up to 30x faster than CollationKeyFilter. I timed the operation of these two classes, with Sun JVM versions 1.4.2/32-bit, 1.5.0/32- and 64-bit, and 1.6.0/64-bit, using 90k word lists of 4 languages (taken from the corresponding Debian wordlist packages and truncated to the first 90k words after a fixed random shuffling), using Collators at the default strength, on a Windows Vista 64-bit machine. I used an analysis pipeline consisting of WhitespaceTokenizer chained to the collation key filter, so to isolate the time taken by the collation key filters, I also timed WhitespaceTokenizer operating alone for each combination. The rightmost column represents the performance advantage of the ICU4J implemtation (ICU) over the java.text.Collator implementation (JVM), after discounting the WhitespaceTokenizer time (WST): (JVM-ICU) / (ICU-WST). The best times out of 5 runs for each combination, in milliseconds, are as follows: Sun JVM Language java.text ICU4J WhitespaceTokenizer ICU4J Improvement 1.4.2_17 (32 bit) English 522 212 13 156% 1.4.2_17 (32 bit) French 716 243 14 207% 1.4.2_17 (32 bit) German 669 264 16 163% 1.4.2_17 (32 bit) Ukranian 931 474 25 102% 1.5.0_15 (32 bit) English 604 176 16 268% 1.5.0_15 (32 bit) French 817 209 17 317% 1.5.0_15 (32 bit) German 799 225 20 280% 1.5.0_15 (32 bit) Ukranian 1029 436 26 145% 1.5.0_15 (64 bit) English 431 89 10 433% 1.5.0_15 (64 bit) French 562 112 11 446% 1.5.0_15 (64 bit) German 567 116 13 438% 1.5.0_15 (64 bit) Ukranian 734 281 21 174% 1.6.0_13 (64 bit) English 162 81 9 113% 1.6.0_13 (64 bit) French 192 92 10 122% 1.6.0_13 (64 bit) German 204 99 14 124% 1.6.0_13 (64 bit) Ukranian 273 202 21 39%</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1720</id>
      <title>TimeLimitedIndexReader and associated utility class</title>
      <description>An alternative to TimeLimitedCollector that has the following advantages: 1) Any reader activity can be time-limited rather than just single searches e.g. the document retrieve phase. 2) Times out faster (i.e. runaway queries such as fuzzies detected quickly before last "collect" stage of query processing) Uses new utility timeout class that is independent of IndexReader. Initial contribution includes a performance test class but not had time as yet to work up a formal Junit test. TimeLimitedIndexReader is coded as JDK1.5 but can easily be undone.</description>
      <attachments/>
      <comments>62</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1721</id>
      <title>IndexWriter to allow deletion by doc ids</title>
      <description>It would be great if IndexWriter would allow for deletion by doc ids as well. It makes sense for cases where a "query" has been executed beforehand, and later, that query needs to be applied in order to delete the matched documents. More information here: http://www.nabble.com/Delete-by-docId-in-IndexWriter-td24239930.html</description>
      <attachments/>
      <comments>21</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1722</id>
      <title>SmartChineseAnalyzer javadoc improvement</title>
      <description>Chinese -&gt; English, and corrections to match reality (removes several javadoc warnings)</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1724</id>
      <title>Analysis package calls Java 1.5 API</title>
      <description>I found compile errors when I tried to compile trunk with 1.4 JVM. org.apache.lucene.analysis.NormalizeCharMap org.apache.lucene.analysis.MappingCharFilter uses Character.valueOf() which has been added in 1.5. I added a CharacterCache (+ testcase) with a valueOf method as a replacement for that quite useful method. org.apache.lucene.analysis.BaseTokenTestCase uses StringBuilder instead of the synchronized version StringBuffer (available in 1.4) I will attach a patch shortly.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1726</id>
      <title>IndexWriter.readerPool create new segmentReader outside of sync block</title>
      <description>I think we will want to do something like what field cache does with CreationPlaceholder for IndexWriter.readerPool. Otherwise we have the (I think somewhat problematic) issue of all other readerPool.get* methods waiting for an SR to warm. It would be good to implement this for 2.9.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1728</id>
      <title>Move SmartChineseAnalyzer &amp; resources to own contrib project</title>
      <description>SmartChineseAnalyzer depends on a large dictionary that causes the analyzer jar to grow up to 3MB. The dictionary is quite big compared to all the other resouces / class files contained in that jar. Having a separate analyzer-cn contrib project enables footprint-sensitive users (e.g. using lucene on a mobile phone) to include analyzer.jar without getting into trouble with disk space. Moving SmartChineseAnalyzer to a separate project could also include a small refactoring as Robert mentioned in LUCENE-1722 several classes should be package protected, members and classes could be final, commented syserr and logging code should be removed etc. I set this issue target to 2.9 - if we can not make it until then feel free to move it to 3.0</description>
      <attachments/>
      <comments>32</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1731</id>
      <title>Allow ConstantScoreQuery to use custom rewrite method if using for highlighting</title>
      <description>I'd like to submit a patch for ConstantScoreQuery which simply contains a setter method to state whether it is being used for highlighting or not. If it is being used for highlighting, then the rewrite method can take each of the terms in the filter and create a BooleanQuery to return (if the number of terms in the filter are less than 1024), otherwise it simply uses the old rewrite method. This allows you to highlight upto 1024 terms when using a ConstantScoreQuery, which since it is a filter, will currently not be highlighted. The idea for this came from Mark Millers article "Bringing the Highlighter back to Wildcard Queries in Solr 1.4", I would just like to make it available in core lucene</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1732</id>
      <title>Multi-threaded Spatial Search</title>
      <description>The attached patch is a large refactoring of the spatial search contrib. The primary contribution is the creation of the ThreadedDistanceFilter, which uses an ExecutorService to filter the documents in multiple threads. As a result of doing the filtering in multiple threads, the time taken to filter 1.2 million documents has been reduced from nearly 3s, to between 500-800ms. As part of this work, the DistanceQueryBuilder has been replaced by the SpatialFilter, a Lucene Filter, some unused functionality has been removed, and the package hierarchy has changed. Consequently this patch breaks backwards compatibility with the existing spatial search contrib. Also during the process of making these changes, abstractions have been added so that the one implementation of the ThreadedDistanceFilter can work with lat/long and geohash data formats, and so that precise but costly arc distance calculations can be replaced by less precise but much more efficient flat plane calculations if needed. This patch will be used in an upcoming patch for Solr which will improve Solr's support for spatial search.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1736</id>
      <title>DateTools.java general improvements</title>
      <description>Applying the attached patch shows the improvements to DateTools.java that I think should be done. All logic that does anything at all is moved to instance methods of the inner class Resolution. I argue this is more object-oriented. 1. In cases where Resolution is an argument to the method, I can simply invoke the appropriate call on the Resolution object. Formerly there was a big branch if/else. 2. Instead of "synchronized" being used seemingly everywhere, synchronized is used to sync on the object that is not threadsafe, be it a DateFormat or Calendar instance. 3. Since different DateFormat and Calendar instances are created per-Resolution, there is now less lock contention since threads using different resolutions will not use the same locks. 4. The old implementation of timeToString rounded the time before formatting it. That's unnecessary since the format only includes the resolution desired. 5. round() now uses a switch statement that benefits from fall-through (no break). Another debatable improvement that could be made is putting the resolution instances into an array indexed by format length. This would mean I could remove the switch in lookupResolutionByLength() and avoid the length constants there. Maybe that would be a bit too over-engineered when the switch is fine.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1737</id>
      <title>Always use bulk-copy when merging stored fields and term vectors</title>
      <description>Lucene has nice optimizations in place during merging of stored fields (LUCENE-1043) and term vectors (LUCENE-1120) whereby the bytes are bulk copied to the new segmetn. This is much faster than decoding &amp; rewriting one document at a time. However the optimization is rather brittle: it relies on the mapping of field name to number to be the same ("congruent") for the segment being merged. Unfortunately, the field mapping will be congruent only if the app adds the same fields in precisely the same order to each document. I think we should fix IndexWriter to assign the same field number for a given field that has been assigned in the past. Ie, when writing a new segment, we pre-seed the field numbers based on past segments. All other aspects of FieldInfo would remain fully dynamic.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1738</id>
      <title>IndexWriter.addIndexes without syncing</title>
      <description>When LUCENE-1313 is completed, it would be good to have a way to replicate segments from one IndexWriter to another. Callback on successful flush (maybe for other events as well?) Ability to access files for a segment (which would presumably be read from the IW ramdir), then copy them to a temporary serialized ramdir (or equivalent as ramdir uses extra space in blocks, whereas we'll already know the size of the files before we write them). On the receiving end, we may be able to use addIndexesNoOptimize(Directory[]), however this would entail each directory having an extraneous segment_N file for each replicated update (so we may want another format). It will rely on having a new public version of SegmentInfo.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1740</id>
      <title>Lucli: Command to change the Analyzer</title>
      <description>Currently, Lucli is hardcoded to use StandardAnalyzer. The provided patch introduces a command "analyzer" to specify a different Analyzer class. If something fails, StandardAnalyzer is the fall-back.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1741</id>
      <title>Make MMapDirectory.MAX_BBUF user configureable to support chunking the index files in smaller parts</title>
      <description>This is a followup for java-user thred: http://www.lucidimagination.com/search/document/9ba9137bb5d8cb78/oom_with_2_9#9bf3b5b8f3b1fb9b It is easy to implement, just add a setter method for this parameter to MMapDir.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1742</id>
      <title>Wrap SegmentInfos in public class</title>
      <description>Wrap SegmentInfos in a public class so that subclasses of MergePolicy do not need to be in the org.apache.lucene.index package.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1743</id>
      <title>MMapDirectory should only mmap large files, small files should be opened using SimpleFS/NIOFS</title>
      <description>This is a followup to LUCENE-1741: Javadocs state (in FileChannel#map): "For most operating systems, mapping a file into memory is more expensive than reading or writing a few tens of kilobytes of data via the usual read and write methods. From the standpoint of performance it is generally only worth mapping relatively large files into memory." MMapDirectory should get a user-configureable size parameter that is a lower limit for mmapping files. All files with a size&lt;limit should be opened using a conventional IndexInput from SimpleFS or NIO (another configuration option for the fallback?).</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1745</id>
      <title>Add ability to specify compilation/matching flags to RegexCapabiltiies implementations</title>
      <description>The Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression. While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not. Therefore, this improvement request is to add the capability to provide those modification flags to either implementation. I've developed a working implementation that makes minor additions to the existing code. The default constructor is explicitly defined with no arguments, and then a new constructor with an additional "int flags" argument is provided. This provides complete backwards compatibility. For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as FLAGS_XXX static fields. These are pass through to the underlying implementation. They are re-defined to avoid bleeding the actual implementation classes into the caller namespace. Proposed changes: For the JavaUtilRegexCapabilities.java, the following is the changes made. private int flags = 0; // Define the optional flags from Pattern that can be used. // Do this here to keep Pattern contained within this class. public final int FLAG_CANON_EQ = Pattern.CANON_EQ; public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE; public final int FLAG_COMMENTS = Pattern.COMMENTS; public final int FLAG_DOTALL = Pattern.DOTALL; public final int FLAG_LITERAL = Pattern.LITERAL; public final int FLAG_MULTILINE = Pattern.MULTILINE; public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE; public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES; /** Default constructor that uses java.util.regex.Pattern with its default flags. */ public JavaUtilRegexCapabilities() { this.flags = 0; } /** Constructor that allows for the modification of the flags that the java.util.regex.Pattern will use to compile the regular expression. This gives the user the ability to fine-tune how the regular expression to match the functionlity that they need. The {@link java.util.regex.Pattern Pattern} class supports specifying these fields via the regular expression text itself, but this gives the caller another option to modify the behavior. Useful in cases where the regular expression text cannot be modified, or if doing so is undesired. @flags The flags that are ORed together. */ public JavaUtilRegexCapabilities(int flags) { this.flags = flags; } public void compile(String pattern) { this.pattern = Pattern.compile(pattern, this.flags); } For the JakartaRegexpCapabilties.java, the following is changed: private int flags = RE.MATCH_NORMAL; /** * Flag to specify normal, case-sensitive matching behaviour. This is the default. */ public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL; /** * Flag to specify that matching should be case-independent (folded) */ public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT; /** * Contructs a RegexCapabilities with the default MATCH_NORMAL match style. */ public JakartaRegexpCapabilities() {} /** * Constructs a RegexCapabilities with the provided match flags. * Multiple flags should be ORed together. * * @param flags The matching style */ public JakartaRegexpCapabilities(int flags) { this.flags = flags; } public void compile(String pattern) { regexp = new RE(pattern, this.flags); }</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1746</id>
      <title>Improve ParallelMultiSearcher</title>
      <description>As we're going to Java5, we can use the java.util.concurrent thread pool. The thread pool size can default to the number of processors. We can optimize usage of readers where small segments are searched sequentially, larger segments are searched on in parallel Need a plan for how Collector.setNextReader works when parallelized (i.e. where do we add synchronization without creating a bottleneck?)</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1747</id>
      <title>Contrib/Spatial needs code cleanup before release</title>
      <description>I had a brief look at the spatial sources and found that there are quite a couple of warnings, main methods, loggers, immutable classes not having final members, unused variables, unused methodes etc. Once mike has commited https://issues.apache.org/jira/browse/LUCENE-1505 I will start cleaning this up a bit. It seem that there are not many unit test in this project either I might open an issue for 3.0 / 3.1 later though.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1749</id>
      <title>FieldCache introspection API</title>
      <description>FieldCache should expose an Expert level API for runtime introspection of the FieldCache to provide info about what is in the FieldCache at any given moment. We should also provide utility methods for sanity checking that the FieldCache doesn't contain anything "odd"... entries for the same reader/field with different types/parsers entries for the same field/type/parser in a reader and it's subreader(s) etc...</description>
      <attachments/>
      <comments>79</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1750</id>
      <title>Create a MergePolicy that limits the maximum size of it's segments</title>
      <description>Basically I'm trying to create largish 2-4GB shards using LogByteSizeMergePolicy, however I've found in the attached unit test segments that exceed maxMergeMB. The goal is for segments to be merged up to 2GB, then all merging to that segment stops, and then another 2GB segment is created. This helps when replicating in Solr where if a single optimized 60GB segment is created, the machine stops working due to IO and CPU starvation.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1754</id>
      <title>Get rid of NonMatchingScorer from BooleanScorer2</title>
      <description>Over in LUCENE-1614 Mike has made a comment about removing NonMatchinScorer from BS2, and return null in BooleanWeight.scorer(). I've checked and this can be easily done, so I'm going to post a patch shortly. For reference: https://issues.apache.org/jira/browse/LUCENE-1614?focusedCommentId=12715064&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12715064. I've marked the issue as 2.9 just because it's small, and kind of related to all the search enhancements done for 2.9.</description>
      <attachments/>
      <comments>40</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1758</id>
      <title>improve arabic analyzer: light8 -&gt; light10</title>
      <description>Someone mentioned on the java user list that the arabic analysis was not as good as they would like. This patch adds the لل- prefix (light10 algorithm versus light8 algorithm). In the light10 paper, this improves precision from .390 to .413 They mention this is not statistically significant, but it makes linguistic sense and at least has been shown not to hurt. In the future, I hope openrelevance will allow us to try some more approaches.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1762</id>
      <title>Slightly more readable code in Token/TermAttributeImpl</title>
      <description>No big deal. growTermBuffer(int newSize) was using correct, but slightly hard to follow code. the method was returning null as a hint that the current termBuffer has enough space to the upstream code or reallocated buffer. this patch simplifies logic making this method to only reallocate buffer, nothing more. It reduces number of if(null) checks in a few methods and reduces amount of code. all tests pass. This also adds tests for the new basic attribute impls (copies of the Token tests).</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1763</id>
      <title>MergePolicy should require an IndexWriter upon construction</title>
      <description>MergePolicy does not require an IW upon construction, but requires one to be passed as method arg to various methods. This gives the impression as if a single MP instance can be shared across various IW instances, which is not true for all MPs (if at all). In addition, LogMergePolicy uses the IW instance passed to these methods incosistently, and is currently exposed to potential NPEs. This issue will change MP to require an IW instance, however for back-compat reasons the following changes will be made: A new MP ctor w/ IW as arg will be introduced. Additionally, for back-compat a default ctor will also be declared which will assign null to the member IW. Methods that require IW will be deprecated, and new ones will be declared. For back-compat, the new ones will not be made abstract, but will throw UOE, with a comment that they will become abstract in 3.0. All current MP impls will move to use the member instance. The code which calls MP methods will continue to use the deprecated methods, passing an IW even that it won't be necessary --&gt; this is strictly for back-compat. In 3.0, we'll remove the deprecated default ctor and methods, and change the code to not call the IW method variants anymore. I hope that I didn't leave anything out. I'm sure I'll find out when I work on the patch .</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1766</id>
      <title>Add Thread-Safety note to IndexWriter JavaDoc</title>
      <description>IndexWriter Javadocs should contain a note about thread-safety. This is already mentioned on the wiki FAQ page but such an essential information should be part of the module documentation too.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1767</id>
      <title>Add sizeof to OpenBitSet</title>
      <description>Adding a sizeof method to OpenBitSet will facilitate estimating RAM usage when many OBS' are cached (such as Solr).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1768</id>
      <title>NumericRange support for new query parser</title>
      <description>It would be good to specify some type of "schema" for the query parser in future, to automatically create NumericRangeQuery for different numeric types? It would then be possible to index a numeric value (double,float,long,int) using NumericField and then the query parser knows, which type of field this is and so it correctly creates a NumericRangeQuery for strings like "[1.567..*]" or "(1.787..19.5]". There is currently no way to extract if a field is numeric from the index, so the user will have to configure the FieldConfig objects in the ConfigHandler. But if this is done, it will not be that difficult to implement the rest. The only difference between the current handling of RangeQuery is then the instantiation of the correct Query type and conversion of the entered numeric values (simple Number.valueOf(...) cast of the user entered numbers). Evenerything else is identical, NumericRangeQuery also supports the MTQ rewrite modes (as it is a MTQ). Another thing is a change in Date semantics. There are some strange flags in the current parser that tells it how to handle dates.</description>
      <attachments/>
      <comments>131</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>1778</id>
      <title>Add log.step support per task</title>
      <description>Following LUCENE-1774, this will add support for log.step per task name, rather than a single log.step setting for all tasks. The .alg file will support: log.step - for all tasks. log.step.&lt;Task Class Name&gt; - for a specific task. For example, log.step.AddDoc, or log.step.DeleteDoc I will post the patch soon</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1779</id>
      <title>Remove unused "numSlotsFull" from FieldComparator.setNextReader</title>
      <description>This param is a relic from older optimizations that we've since turned off, and it's quite confusing. I don't think we need it, and we haven't released the API yet so we're free to remove it now.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1782</id>
      <title>Rename OriginalQueryParserHelper</title>
      <description>We should rename the new QueryParser so it's clearer that it's Lucene's default QueryParser, going forward, and not just a temporary "bridge" to a future new QueryParser. How about we rename oal.queryParser.original --&gt; oal.queryParser.standard (can't use "default": it's a Java keyword)? Then, leave the OriginalQueryParserHelper under that package, but simply rename it to QueryParser? This way if we create other sub-packages in the future, eg ComplexPhraseQueryParser, they too can have a QueryParser class under them, to make it clear that's the "top" class you use to parse queries.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1784</id>
      <title>Make BooleanWeight and DisjunctionMaxWeight protected</title>
      <description>Currently, BooleanWeight is private, yet it has 2 protected members (similarity, weights) which are unaccessible from custom code i have some use cases where it would be very useful to crawl a BooleanWeight to get at the sub Weight objects however, since BooleanWeight is private, i have no way of doing this If BooleanWeight is protected, then i can subclass BooleanQuery to hook in and wrap BooleanWeight with a subclass to facilitate this walking of the Weight objects Would also want DisjunctionMaxWeight to be protected, along with its "weights" member Would be even better if these Weights were made public with accessors to their sub "weights" objects (then no subclassing would be necessary on my part) this should be really trivial and would be great if it can get into 2.9 more generally, it would be nice if all Weight classes were public with nice accessors to relevant "sub weights"/etc so custom code can get its hooks in where and when desired</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1785</id>
      <title>Simple FieldCache merging</title>
      <description>We'll merge the field caches in RAM as the SegmentReader's are merged in IndexWriter (the first cut will work in conjunction with IW.getReader). There will be an optional callback to determine which fields to merge.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1787</id>
      <title>Standard Tokenizer doesn't recognise I.B.M as Acronym, it requires it ends with a dot i.e I.B.M.</title>
      <description>Standard Tokenzizer doesn't recognise I.B.M it requires it end with a dot i.e I.B.M. This is particulary problematic if I.B.M is added tot the index, with the StandardAnalyser it will get added as IBM , a search for I.B.M will not match because I.B.M will be left as is, I would expect a match in this scenario I think it could be fixed by modifying the grammar ACRONYM_DEP in StandardTokenizerImpl.jflex so that it also supports {ALPHANUM} ("." {ALPHANUM} )+ dot only required between each character, (I'm not familiar with jflex syntax )</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1789</id>
      <title>getDocValues should provide a MultiReader DocValues abstraction</title>
      <description>When scoring a ValueSourceQuery, the scoring code calls ValueSource.getValues(reader) on each leaf level subreader – so DocValue instances are backed by the individual FieldCache entries of the subreaders – but if Client code were to inadvertently called getValues() on a MultiReader (or DirectoryReader) they would wind up using the "outer" FieldCache. Since getValues(IndexReader) returns DocValues, we have an advantage here that we don't have with FieldCache API (which is required to provide direct array access). getValues(IndexReader) could be implimented so that IF some a caller inadvertently passes in a reader with non-null subReaders, getValues could generate a DocValues instance for each of the subReaders, and then wrap them in a composite "MultiDocValues".</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1790</id>
      <title>Add Boosting Function Term Query and Some Payload Query refactorings</title>
      <description>Similar to the BoostingTermQuery, the BoostingFunctionTermQuery is a SpanTermQuery, but the difference is the payload score for a doc is not the average of all the payloads, but applies a function to them instead. BoostingTermQuery becomes a BoostingFunctionTermQuery with an AveragePayloadFunction applied to it. Also add marker interface to indicate PayloadQuery types. Refactor Similarity.scorePayload to also take in the doc id.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1793</id>
      <title>remove custom encoding support in Greek/Russian Analyzers</title>
      <description>The Greek and Russian analyzers support custom encodings such as KOI-8, they define things like Lowercase and tokenization for these. I think that analyzers should support unicode and that conversion/handling of other charsets belongs somewhere else. I would like to deprecate/remove the support for these other encodings.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1794</id>
      <title>implement reusableTokenStream for all contrib analyzers</title>
      <description>most contrib analyzers do not have an impl for reusableTokenStream regardless of how expensive the back compat reflection is for indexing speed, I think we should do this to mitigate any performance costs. hey, overall it might even be an improvement! the back compat code for non-final analyzers is already in place so this is easy money in my opinion.</description>
      <attachments/>
      <comments>58</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1796</id>
      <title>Speed up repeated TokenStream init</title>
      <description>by caching isMethodOverridden results</description>
      <attachments/>
      <comments>43</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1798</id>
      <title>FieldCacheSanityChecker called directly by FieldCache.get*</title>
      <description>As suggested by McCandless in LUCENE-1749, we can make FieldCacheImpl a client of the FieldCacheSanityChecker and have it sanity check itself each time it creates a new cache entry, and log a warning if it thinks there is a problem. (although we'd probably only want to do this if the caller has set some sort of infoStream/warningStream type property on the FieldCache object.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1799</id>
      <title>Unicode compression</title>
      <description>In lucene-1793, there is the off-topic suggestion to provide compression of Unicode data. The motivation was a custom encoding in a Russian analyzer. The original supposition was that it provided a more compact index. This led to the comment that a different or compressed encoding would be a generally useful feature. BOCU-1 was suggested as a possibility. This is a patented algorithm by IBM with an implementation in ICU. If Lucene provide it's own implementation a freely avIlable, royalty-free license would need to be obtained. SCSU is another Unicode compression algorithm that could be used. An advantage of these methods is that they work on the whole of Unicode. If that is not needed an encoding such as iso8859-1 (or whatever covers the input) could be used.</description>
      <attachments/>
      <comments>71</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1800</id>
      <title>QueryParser should use reusable token streams</title>
      <description>Just like indexing, the query parser should use reusable token streams</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1806</id>
      <title>Add args to test-macro</title>
      <description>Add passing args to JUnit. (Like Solr and mainly for debugging).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1807</id>
      <title>Add convenient constructor to PerFieldAnalyzerWrapper for Dependency Injection</title>
      <description>It would be good if PerFieldAnalyzerWrapper had a constructor which took in an analyzer map, rather than having to repeatedly call addAnalyzer – this would make it much easier/cleaner to use this class in e.g. Spring XML configurations. Relatively trivial change, patch to be attached.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1808</id>
      <title>make Query.createWeight public (or add back Query.createQueryWeight())</title>
      <description>Now that the QueryWeight class has been removed, the public QueryWeight createQueryWeight() method on Query was also removed i have cases where i want to create a weight for a sub query (outside of the org.apache.lucene.search package) and i don't want the weight normalized (think BooleanQuery outside of the o.a.l.search package) in order to do this, i have to create a static Utils class inside o.a.l.search, pass in the Query and searcher, and have the static method call the protected createWeight method this should not be necessary This could be fixed in one of 2 ways: 1. make createWeight() public on Query (breaks back compat) 2. add the following method: public Weight createQueryWeight(Searcher searcher) throws IOException { return createWeight(searcher); } createWeight(Searcher) should then be deprectated in favor of the publicly accessible method</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1812</id>
      <title>Static index pruning by in-document term frequency (Carmel pruning)</title>
      <description>This module provides tools to produce a subset of input indexes by removing postings data for those terms where their in-document frequency is below a specified threshold. The net effect of this processing is a much smaller index that for common types of queries returns nearly identical top-N results as compared with the original index, but with increased performance. Optionally, stored values and term vectors can also be removed. This functionality is largely independent, so it can be used without term pruning (when term freq. threshold is set to 1). As the threshold value increases, the total size of the index decreases, search performance increases, and recall decreases (i.e. search quality deteriorates). NOTE: especially phrase recall deteriorates significantly at higher threshold values. Primary purpose of this class is to produce small first-tier indexes that fit completely in RAM, and store these indexes using IndexWriter.addIndexes(IndexReader[]). Usually the performance of this class will not be sufficient to use the resulting index view for on-the-fly pruning and searching. NOTE: If the input index is optimized (i.e. doesn't contain deletions) then the index produced via IndexWriter.addIndexes(IndexReader[]) will preserve internal document id-s so that they are in sync with the original index. This means that all other auxiliary information not necessary for first-tier processing, such as some stored fields, can also be removed, to be quickly retrieved on-demand from the original index using the same internal document id. Threshold values can be specified globally (for terms in all fields) using defaultThreshold parameter, and can be overriden using per-field or per-term values supplied in a thresholds map. Keys in this map are either field names, or terms in field:text format. The precedence of these values is the following: first a per-term threshold is used if present, then per-field threshold if present, and finally the default threshold. A command-line tool (PruningTool) is provided for convenience. At this moment it doesn't support all functionality available through API.</description>
      <attachments/>
      <comments>43</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1813</id>
      <title>Add option to ReverseStringFilter to mark reversed tokens</title>
      <description>This patch implements additional functionality in the filter to "mark" reversed tokens with a special marker character (Unicode 0001). This is useful when indexing both straight and reversed tokens (e.g. to implement efficient leading wildcards search).</description>
      <attachments/>
      <comments>31</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1820</id>
      <title>WildcardQueryNode to expose the positions of the wildcard characters, for easier use in processors and builders</title>
      <description>Change the WildcardQueryNode to expose the positions of the wildcard characters. This would allow the AllowLeadingWildcardProcessor not to have to knowledge about the wildcard chars * and ? and avoid double check again.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1822</id>
      <title>FastVectorHighlighter: SimpleFragListBuilder hard-coded 6 char margin is too naive</title>
      <description>The new FastVectorHighlighter performs extremely well, however I've found in testing that the window of text chosen per fragment is often very poor, as it is hard coded in SimpleFragListBuilder to always select starting 6 characters to the left of the first phrase match in a fragment. When selecting long fragments, this often means that there is barely any context before the highlighted word, and lots after; even worse, when highlighting a phrase at the end of a short text the beginning is cut off, even though the entire phrase would fit in the specified fragCharSize. For example, highlighting "Punishment" in "Crime and Punishment" returns "e and &lt;b&gt;Punishment&lt;/b&gt;" no matter what fragCharSize is specified. I am going to attach a patch that improves the text window selection by recalculating the starting margin once all phrases in the fragment have been identified - this way if a single word is matched in a fragment, it will appear in the middle of the highlight, instead of 6 characters from the beginning. This way one can also guarantee that the entirety of short texts are represented in a fragment by specifying a large enough fragCharSize.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1823</id>
      <title>QueryParser with new features for Lucene 3</title>
      <description>I'd like to have a new QueryParser implementation in Lucene 3.1, ideally based on the new QP framework in contrib. It should share as much code as possible with the current StandardQueryParser implementation for easy maintainability. Wish list (feel free to extend): 1. Operator precedence: Support operator precedence for boolean operators 2. Opaque terms: Ability to plugin an external parser for certain syntax extensions, e.g. XML query terms 3. Improved RangeQuery syntax: Use more intuitive &lt;=, =, &gt;= instead of [] and {} 4. Support for trierange queries: See LUCENE-1768 5. Complex phrases: See LUCENE-1486 6. ANY operator: E.g. (a b c d) ANY 3 should match if 3 of the 4 terms occur in the same document 7. New syntax for Span queries: I think the surround parser supports this? 8. Escaped wildcards: See LUCENE-588</description>
      <attachments/>
      <comments>34</comments>
      <commenters>14</commenters>
    </issue>
    <issue>
      <id>1824</id>
      <title>FastVectorHighlighter truncates words at beginning and end of fragments</title>
      <description>FastVectorHighlighter does not take word boundaries into consideration when building fragments, so that in most cases the first and last word of a fragment are truncated. This makes the highlights less legible than they should be. I will attach a patch to BaseFragmentBuilder that resolves this by expanding the start and end boundaries of the fragment to the first whitespace character on either side of the fragment, or the beginning or end of the source text, whichever comes first. This significantly improves legibility, at the cost of returning a slightly larger number of characters than specified for the fragment size.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1825</id>
      <title>Incorrect usage of AttributeSource.addAttribute/getAttribute leads to failures when onlyUseNewAPI=true</title>
      <description>when seting "use only new API" for TokenStream, i received the following exception: [junit] Caused by: java.lang.IllegalArgumentException: This AttributeSource does not have the attribute 'interface org.apache.lucene.analysis.tokenattributes.TermAttribute'. [junit] at org.apache.lucene.util.AttributeSource.getAttribute(AttributeSource.java:249) [junit] at org.apache.lucene.index.TermsHashPerField.start(TermsHashPerField.java:252) [junit] at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:145) [junit] at org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:244) [junit] at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:772) [junit] at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:755) [junit] at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:2613) However, i can't actually see the culprit that caused this exception suggest that the IllegalArgumentException include "getClass().getName()" in order to be able to identify which TokenStream implementation actually caused this</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1826</id>
      <title>All Tokenizer implementations should have constructors that take AttributeSource and AttributeFactory</title>
      <description>I have a TokenStream implementation that joins together multiple sub TokenStreams (i then do additional filtering on top of this, so i can't just have the indexer do the merging) in 2.4, this worked fine. once one sub stream was exhausted, i just started using the next stream however, in 2.9, this is very difficult, and requires copying Term buffers for every token being aggregated however, if all the sub TokenStreams share the same AttributeSource, and my "concat" TokenStream shares the same AttributeSource, this goes back to being very simple (and very efficient) So for example, i would like to see the following constructor added to StandardTokenizer: public StandardTokenizer(AttributeSource source, Reader input, boolean replaceInvalidAcronym) { super(source); ... } would likewise want similar constructors added to all Tokenizer sub classes provided by lucene</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1827</id>
      <title>Make the Payload Boosting Queries consistent</title>
      <description>BoostingFunctionTermQuery should be consistent with BoostingNearQuery - Renaming to PayloadNearQuery and PayloadTermQuery</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1829</id>
      <title>'ant javacc' in root project should also properly create contrib/queryparser Java files</title>
      <description>'ant javacc' in the project root doesn't run javacc in contrib/queryparser 'ant javacc' in contrib/queryparser does not properly create the Java files. What still needs to be done by hand is (partly!) described in contrib/queryparser/README.javacc. I think this process should be automated. Patch provided.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1833</id>
      <title>When we move to java 1.5 in 3.0 we should replace all Interger, Long, etc construction with .valueOf</title>
      <description>-128 to 128 are guaranteed to be cached and using valueOf in that case is 3.5 times faster than using contructor</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1839</id>
      <title>Scorer.explain is deprecated but abstract, should have impl that throws UnsupportedOperationException</title>
      <description>Suggest having Scorer implement explain to throw UnsupportedOperationException right now, i have to implement this method (because its abstract), and javac yells at me for overriding a deprecated method if the following implementation is in Scorer, i can remove my "empty" implementations of explain from my Scorers /** Returns an explanation of the score for a document. * &lt;br&gt;When this method is used, the {@link #next()}, {@link #skipTo(int)} and * {@link #score(HitCollector)} methods should not be used. * @param doc The document number for the explanation. * * @deprecated Please use {@link IndexSearcher#explain} * or {@link Weight#explain} instead. */ public Explanation explain(int doc) throws IOException { throw new UnsupportedOperationException(); } best i figure, this shouldn't break back compat (people already have to recompile anyway) (2.9 definitely not binary compatible with 2.4)</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1840</id>
      <title>QueryUtils should check that equals properly handles null</title>
      <description>Its part of the equals contract, but many classes currently violate</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1841</id>
      <title>Provide Summary Information on the Files in the Lucene index</title>
      <description>I find myself often having to remember, by file extension, what is in a particular index file. The information is all contained in the File Formats, but not summarized. This patch provides a simple table that describes the extensions and provides links to the relevant section.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1843</id>
      <title>Convert some tests to new TokenStream API, better support of cross-impl AttributeImpl.copyTo()</title>
      <description>This patch converts some remaining tests to the new TokenStream API and non-deprecated classes. This patch also enhances AttributeImpl.copyTo() of Token and TokenWrapper to also support copying e.g. TermAttributeImpl into Token. The target impl must only support all interfaces but must not be of the same type. Token and TokenWrapper use optimized coping without casting to 6 interfaces where possible. Maybe the special tokenizers in contrib (shingle matrix and so on using tokens to cache may be enhanced by that). Also Yonik's request for optimized copying of states between incompatible AttributeSources may be enhanced by that (possibly a new issue).</description>
      <attachments/>
      <comments>9</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1844</id>
      <title>Speed up junit tests</title>
      <description>As Lucene grows, so does the number of JUnit tests. This is obviously a good thing, but it comes with longer and longer test times. Now that we also run back compat tests in a standard test run, this problem is essentially doubled. There are some ways this may get better, including running parallel tests. You will need the hardware to fully take advantage, but it should be a nice gain. There is already an issue for this, and Junit 4.6, 4.7 have the beginnings of something we might be able to count on soon. 4.6 was buggy, and 4.7 still doesn't come with nice ant integration. Parallel tests will come though. Beyond parallel testing, I think we also need to concentrate on keeping our tests lean. We don't want to sacrifice coverage or quality, but I'm sure there is plenty of fat to skim. I've started making a list of some of the longer tests - I think with some work we can make our tests much faster - and then with parallelization, I think we could see some really great gains.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1845</id>
      <title>if the build fails to download JARs for contrib/db, just skip its tests</title>
      <description>Every so often our nightly build fails because contrib/db is unable to download the necessary BDB JARs from http://downloads.osafoundation.org. I think in such cases we should simply skip contrib/db's tests, if it's the nightly build that's running, since it's a false positive failure.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1848</id>
      <title>Remove references to older versions of Lucene in "per-release" documentation</title>
      <description>Some of the documentation that is "per release" contains references to older versions, which is often confusing. This is most noticeable in the file formats docs, but there might be other places too.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1851</id>
      <title>'ant javacc' in root project should also properly create contrib/surround Java files</title>
      <description>For consistency after LUCENE-1829 which did the same for contrib/queryparser</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1853</id>
      <title>SubPhraseQuery for matching and scoring sub phrase matches.</title>
      <description>The goal is to give more control via configuration when searching using user entered queries against multiple fields where sub phrases have special significance. For a query like "homes in new york with swimming pool", if a document's field matches only "new york" it should get scored and it should get scored higher than two separate matches "new" and "york". Also, a 3 word sub phrase match must gets scored considerably higher than a 2 word sub phrase match. (boost factor should be configurable) Using shingles for this use case, means each field of each document needs to be indexed as shingles of all (1..N)-grams as well as the query. (Please correct me if I am wrong.) The query could also support ignoring of idf and/or field norms, (so that factors outside the document don't influence scoring) consider only the longest match (for example match on "new york" is scored and considered rather than "new" furniture and "york" city) ignore duplicates ("new york" appearing twice or thrice does not make any difference) This kind of query could be combined with DisMax query. For example, something like solr's dismax request handler can be made to use this query where we run a user query as it is against all fields and configure each field with above configurations. I have also attached a patch with comments and test cases in case, my description is not clear enough. Would appreciate alternatives or feedback. Example Usage: &lt;code&gt; // sub phrase config SubPhraseQuery.SubPhraseConfig conf = new SubPhraseQuery.SubPhraseConfig(); conf.ignoreIdf = true; conf.ignoreFieldNorms = true; conf.matchOnlyLongest = true; conf.ignoreDuplicates = true; conf.phraseBoost = 2; // phrase query as usual SubPhraseQuery pq = new SubPhraseQuery(); pq.add(new Term("f", term)); pq.add(new Term("f", term)); pq.setSubPhraseConf(conf); Hits hits = searcher.search(pq); &lt;/code&gt;</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1854</id>
      <title>build.xml's tar task should use longfile="gnu"</title>
      <description>The default (used now) is the same, but we get all those nasty false warnings filling the screen.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1855</id>
      <title>Change AttributeSource API to use generics</title>
      <description>The AttributeSource API will be easier to use with JDK 1.5 generics. Uwe, if you started working on a patch for this already feel free to assign this to you.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1857</id>
      <title>Change NumericRangeQuery to generics (for the numeric type)</title>
      <description>NumericRangeQuery/Filter can use generics for more type-safety: NumericRangeQuery&lt;T extends Number&gt;</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1860</id>
      <title>switch MultiTermQuery to "constant score auto" rewrite by default</title>
      <description>Right now it defaults to scoring BooleanQuery, and that's inconsistent w/ QueryParser which does constant score auto. The new multi-term queries already set this default, so the only core queries this will impact are PrefixQuery and WildcardQuery. FuzzyQuery, which has its own rewrite to BooleanQuery, will keep doing so.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1869</id>
      <title>when checking tvx/fdx size mismatch, also include whether the file exists</title>
      <description>IndexWriter checks, during flush and during merge, that the size of the index file for stored fields (.fdx) and term vectors (.tvx) matches how many bytes it has just written. This originally was added for LUCENE-1282, ie, as a safety to catch the nasty "off by 1" JRE hotspot bug that would otherwise silently corrupt the index. However, this check also seems to catch a different case, where the size of the file is zero. The most recent example is LUCENE-1521. I'd like to improve the message in the exception to include whether or not the file exists, to help understand why users are sometimes hitting this exception. My best theory at this point is something external is removing the file out from under the IndexWriter.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1872</id>
      <title>Improve javadocs for Numeric*</title>
      <description>I'm working on improving Numeric* javadocs.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1876</id>
      <title>Some contrib packages are missing a package.html</title>
      <description>Dunno if we will get to this one this release, but a few contribs don't have a package.html (or a good overview that would work as a replacement) - I don't think this is hugely important, but I think it is important - you should be able to easily and quickly read a quick overview for each contrib I think. So far I have identified collation and spatial.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1877</id>
      <title>Use NativeFSLockFactory as default for new API (direct ctors &amp; FSDir.open)</title>
      <description>A user requested we add a note in IndexWriter alerting the availability of NativeFSLockFactory (allowing you to avoid retaining locks on abnormal jvm exit). Seems reasonable to me - we want users to be able to easily stumble upon this class. The below code looks like a good spot to add a note - could also improve whats there a bit - opening an IndexWriter does not necessarily create a lock file - that would depend on the LockFactory used. &lt;p&gt;Opening an &lt;code&gt;IndexWriter&lt;/code&gt; creates a lock file for the directory in use. Trying to open another &lt;code&gt;IndexWriter&lt;/code&gt; on the same directory will lead to a {@link LockObtainFailedException}. The {@link LockObtainFailedException} is also thrown if an IndexReader on the same directory is used to delete documents from the index.&lt;/p&gt; Anyone remember why NativeFSLockFactory is not the default over SimpleFSLockFactory?</description>
      <attachments/>
      <comments>46</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1879</id>
      <title>Parallel incremental indexing</title>
      <description>A new feature that allows building parallel indexes and keeping them in sync on a docID level, independent of the choice of the MergePolicy/MergeScheduler. Find details on the wiki page for this feature: http://wiki.apache.org/lucene-java/ParallelIncrementalIndexing Discussion on java-dev: http://markmail.org/thread/ql3oxzkob7aqf3jd</description>
      <attachments/>
      <comments>21</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1880</id>
      <title>Make contrib/collation/(ICU)CollationKeyAnalyzer constructors public</title>
      <description>In contrib/collation, the constructors for CollationKeyAnalyzer and ICUCollationKeyAnalyzer are package private, and so are effectively unusable.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1883</id>
      <title>Fix typos in CHANGES.txt and contrib/CHANGES.txt prior to 2.9 release</title>
      <description>I noticed a few typos in CHANGES.txt and contrib/CHANGES.txt. (Once they make it past a release, they're set in stone...) Will attach a patch shortly.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1887</id>
      <title>o.a.l.messages should be moved to core</title>
      <description>contrib/queryParser contains an org.apache.lucene.messages package containing some generallized code that (claims in it's javadocs) is not specific to the queryParser. If this is truely general purpose code, it should probably be moved out of hte queryParser contrib – either into it's own contrib, or into the core (it's very small) EDIT: alternate suggestion to rename package to fall under the o.a.l.queryParser namespace retracted due to comments in favor of (eventually) promoting to it's own contrib</description>
      <attachments/>
      <comments>26</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1888</id>
      <title>Provide Option to Store Payloads on the Term Vector</title>
      <description>Would be nice to have the option to access the payloads in a document-centric way by adding them to the Term Vectors. Naturally, this makes the Term Vectors bigger, but it may be just what one needs.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1891</id>
      <title>Spatial uses java util logging that causes needless minor work (multiple string concat, a method call) due to not checking log level</title>
      <description>Not sure there should be logging here - just used in two spots and looks more for debug - but if its going to be there, should check for isFineEnabled.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1896</id>
      <title>Modify confusing javadoc for queryNorm</title>
      <description>See http://markmail.org/message/arai6silfiktwcer The javadoc confuses me as well.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1898</id>
      <title>Decide if we should remove lines numbers from latest Changes</title>
      <description>As Lucene dev has grown, a new issue has arisen - many times, new changes invalidate old changes. A proper changes file should just list the changes from the last version, not document the dev life of the issues. Keeping changes in proper order now requires a lot of renumbering sometimes. The numbers have no real meaning and could be added to more rich versions (such as the html version) automatically if desired. I think an * makes a good replacement myself. The issues already have ids that are stable, rather than the current, decorational numbers which are subject to change over a dev cycle. I think we should replace the numbers with an asterix for the 2.9 section and going forward (ie 4. becomes *). If we don't get consensus very quickly, this issue won't block.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>1904</id>
      <title>move wordnet based synonym code out of contrib/memory and into contrib/wordnet (or somewhere else)</title>
      <description>see LUCENE-387 ... some synonym related code has been living in contrib/memory for a very long time ... it should be refactored out.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1908</id>
      <title>Similarity javadocs for scoring function to relate more tightly to scoring models in effect</title>
      <description>See discussion in the related issue.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1910</id>
      <title>Extension to MoreLikeThis to use tag information</title>
      <description>I would like to contribute a class based on the MoreLikeThis class in contrib/queries that generates a query based on the tags associated with a document. The class assumes that documents are tagged with a set of tags (which are stored in the index in a seperate Field). The class determines the top document terms associated with a given tag using the information gain metric. While generating a MoreLikeThis query for a document the tags associated with document are used to determine the terms in the query. This class is useful for finding similar documents to a document that does not have many relevant terms but was tagged.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1914</id>
      <title>allow for custom segment files</title>
      <description>Create a plugin framework where one can provide some sort of callback to add to a custom segment file, given a doc and provide some sort of merge logic. This is in light of the flexible indexing effort.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1917</id>
      <title>ShingleFilter include words</title>
      <description>By default ShingleFilter creates shingles (i.e. combines tokens into a single token) from all tokens. For the purposes of for example, indexing stop words as shingles, however not creating shingles out of every word, we can supply an include words CharArraySet to ShingleFilter that determines the tokens to shingle. This is similar to Nutch CommonGrams and SOLR-908. SOLR-908 does not utilize the new token attribute API, and I figured this functionality is more suitable being a part of Lucene.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1920</id>
      <title>Make MultiReader.isOptimized() return a useful result</title>
      <description>The MultiReader statically returns FALSE on any isOptimized() call. This makes it unusable as the source index for an InstantiatedIndex, which checks initially if the source index is optimized and aborts if not. The attached patch iterates all sub readers and returns TRUE if all of them are optimized.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1922</id>
      <title>exposing the ability to get the number of unique term count per field</title>
      <description>Add an api to get the number of unique term count given a field name, e.g.: IndexReader.getUniqueTermCount(String field) This issue has a dependency on LUCENE-1458</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1924</id>
      <title>BalancedSegmentMergePolicy, contributed from the Zoie project for realtime indexing</title>
      <description>Written by Yasuhiro Matsuda for Zoie realtime indexing system used to handle high update rates to avoid large segment merges. Detailed write-up is at: http://code.google.com/p/zoie/wiki/ZoieMergePolicy</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1925</id>
      <title>In IndexSearcher class, make subReader and docCount arrays protected so sub classes can access them</title>
      <description>Please make these two member variables protected so subclasses can access them, e.g.: protected IndexReader[] subReaders; protected int[] docStarts; Thanks</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1931</id>
      <title>no hits query - query object that returns no hits</title>
      <description>Query implementation that return no hits.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1933</id>
      <title>Provide an convenience AttributeFactory that implements all default attributes with Token</title>
      <description>I found some places in contrib tests, where the Token.class was added using addAttributeImpl(). The problem here is, that you cannot be sure, that the attribute is really added and you may fail later (because you only update your local instance). The tests in contrib will partially fail with 3.0 without backwards layer (because the backwards layer uses Token/TokenWrapper internally and copyTo() will work. The correct way to achieve this is using an AttributeFactory. The AttributeFactory is currently private in SingleTokenTokenStream. I want to move it to Token.java as a static class / static member. In this case the tests can be rewritten. I also want to mark addAttributeImpl() as EXPERT, because you must really know whats happening and what are the traps.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1934</id>
      <title>Rework (Float)LatLng implementation and distance calculation</title>
      <description>Clean up of the code and normalisation of the distance calculation to standard</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1937</id>
      <title>Add more methods to manipulate QueryNodeProcessorPipeline elements</title>
      <description>QueryNodeProcessorPipeline allows the user to define a list of processors to process a query tree. However, it's not very flexible when the user wants to extend/modify an already created pipeline, because it only provides an add method, which only allows the user to append a new processor to the pipeline. So, I propose to add new methods to manipulate the processor in a pipeline. I think the methods should not consider an index position when modifying the pipeline, hence the index position in a pipeline does not mean anything, a processor has a meaning when it's after or before another processor. Therefore, I suggest the methods should always consider another processor when inserting/modifying the pipeline. For example, insertAfter(processor, newProcessor), which will insert the "newProcessor" after the "processor".</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1938</id>
      <title>Precedence query parser using the contrib/queryparser framework</title>
      <description>Extend the current StandardQueryParser on contrib so it supports boolean precedence</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>1943</id>
      <title>ChineseFilter is inefficient</title>
      <description>trivial patch to use CharArraySet, so it can use termBuffer() instead of term()</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1945</id>
      <title>Make all classes that have a close() methods instanceof Closeable (Java 1.5)</title>
      <description>This should be simple.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1950</id>
      <title>Remove autoCommit from IndexWriter</title>
      <description>IndexWriter's autoCommit is deprecated; in 3.0 it will be hardwired to false.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1951</id>
      <title>wildcardquery rewrite improvements</title>
      <description>wildcardquery has logic to rewrite to termquery if there is no wildcard character, but it needs to pass along the boost if it does this if the user asked for a 'constant score' rewriteMethod, it should rewrite to a constant score query for consistency. additionally, if the query is really a prefixquery, it would be nice to rewrite to prefix query. both will enumerate the same number of terms, but prefixquery has a simpler comparison function.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1952</id>
      <title>@since tags missing from Javadocs</title>
      <description>It would be useful to be able to see at which version classes/methods have been added by adding the @since javadoc tag. I use quite an old version of Lucene that is integrated into the CMS I use, and often find that they features I need to use are not supported in the version I have.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1959</id>
      <title>Index Splitter</title>
      <description>If an index has multiple segments, this tool allows splitting those segments into separate directories.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>1962</id>
      <title>Persian Arabic Analyzer cleanup</title>
      <description>While browsing through the code I found some places for minor improvements in the new Arabic / Persian Analyzer code. prevent default stopwords from being loaded each time a default constructor is called replace if blocks with a single switch marking private members final where needed changed protected visibility to final in final class.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1963</id>
      <title>ArabicAnalyzer: Lowercase before Stopfilter</title>
      <description>ArabicAnalyzer lowercases text in case you have some non-Arabic text around. It also allows you to set a custom stopword list (you might augment the Arabic list with some English ones, for example). In this case its helpful for these non-Arabic stopwords, to lowercase before stopfilter.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>1965</id>
      <title>Lazy Atomic Loading Stopwords in SmartCN</title>
      <description>The default constructor in SmartChineseAnalyzer loads the default (jar embedded) stopwords each time the constructor is invoked. This should be atomically loaded only once in an unmodifiable set.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1966</id>
      <title>Arabic Analyzer: Stopwords list needs enhancement</title>
      <description>The provided Arabic stopwords list needs some enhancements (e.g. it contains a lot of words that not stopwords, and some cleanup) . patch will be provided with this issue.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1967</id>
      <title>make it easier to access default stopwords for language analyzers</title>
      <description>DM Smith made the following comment: (sometimes it is hard to dig out the stop set from the analyzers) Looking around, some of these analyzers have very different ways of storing the default list. One idea is to consider generalizing something like what Simon did with LUCENE-1965, LUCENE-1962, and having all stopwords lists stored as .txt files in resources folder. /** * Returns an unmodifiable instance of the default stop-words set. * @return an unmodifiable instance of the default stop-words set. */ public static Set&lt;String&gt; getDefaultStopSet()</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1969</id>
      <title>adding kamikaze to lucene contrib</title>
      <description>Adding kamikaze to lucene contrib</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>1981</id>
      <title>Allow access to entries in the field cache</title>
      <description>If the data required is already in the field cache, it seems unnecessary to go to the disk for it, if the data is already in RAM. We have a case where we need one field from a large number (500 -1000) of scattered documents in a fairly large index (50-100m docs), and seek time to collect the data from disk is prohibitive, so we'd like to grab the data from the cache, instead.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1984</id>
      <title>DisjunctionMaxQuery - Type safety</title>
      <description>DisjunctionMaxQuery code has containers that are not type-safe . The comments indicate type-safety though. Better to express in the API and the internals the explicit type as opposed to type-less containers. Patch attached. Comments / backward compatibility concerns welcome.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1985</id>
      <title>DisjunctionMaxQuery - Iterator code to for ( A a : container ) construct</title>
      <description>For better readability - converting the Iterable&lt;T&gt; to for ( A a : container ) constructs that is more intuitive to read.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1988</id>
      <title>CharacterCache - references deleted</title>
      <description>CharacterCache is deprecated by Character.valueOf(c) . Hence the latter is chosen over the former.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1990</id>
      <title>Add unsigned packed int impls in oal.util</title>
      <description>There are various places in Lucene that could take advantage of an efficient packed unsigned int/long impl. EG the terms dict index in the standard codec in LUCENE-1458 could subsantially reduce it's RAM usage. FieldCache.StringIndex could as well. And I think "load into RAM" codecs like the one in TestExternalCodecs could use this too. I'm picturing something very basic like: interface PackedUnsignedLongs { long get(long index); void set(long index, long value); } Plus maybe an iterator for getting and maybe also for setting. If it helps, most of the usages of this inside Lucene will be "write once" so eg the set could make that an assumption/requirement. And a factory somewhere: PackedUnsignedLongs create(int count, long maxValue); I think we should simply autogen the code (we can start from the autogen code in LUCENE-1410), or, if there is an good existing impl that has a compatible license that'd be great. I don't have time near-term to do this... so if anyone has the itch, please jump!</description>
      <attachments/>
      <comments>73</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1991</id>
      <title>Similarity#score deprecated method - javadoc reference + SimilarityDelegator</title>
      <description>Old method public float scorePayload(String fieldName, byte [] payload, int offset, int length) has been deprecated by - public float scorePayload(int docId, String fieldName, int start, int end, byte [] payload, int offset, int length) References in PayLoadNearQuery (javadoc) changed. Also - SimilarityDelegator overrides the new method as opposed to the (deprecated) old one.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>1993</id>
      <title>MoreLikeThis - allow to exclude terms that appear in too many documents (patch included)</title>
      <description>The MoreLikeThis class allows to generate a likeness query based on a given document. So far, it is impossible to suppress words from the likeness query, that appear in almost all documents, making it necessary to use extensive lists of stop words. Therefore I suggest to allow excluding words for which a certain absolute document count or a certain percentage of documents is exceeded. Depending on the corpus of text, words that appear in more than 50 or even 70% of documents can usually be considered insignificant for classifying a document.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1997</id>
      <title>Explore performance of multi-PQ vs single-PQ sorting API</title>
      <description>Spinoff from recent "lucene 2.9 sorting algorithm" thread on java-dev, where a simpler (non-segment-based) comparator API is proposed that gathers results into multiple PQs (one per segment) and then merges them in the end. I started from John's multi-PQ code and worked it into contrib/benchmark so that we could run perf tests. Then I generified the Python script I use for running search benchmarks (in contrib/benchmark/sortBench.py). The script first creates indexes with 1M docs (based on SortableSingleDocSource, and based on wikipedia, if available). Then it runs various combinations: Index with 20 balanced segments vs index with the "normal" log segment size Queries with different numbers of hits (only for wikipedia index) Different top N Different sorts (by title, for wikipedia, and by random string, random int, and country for the random index) For each test, 7 search rounds are run and the best QPS is kept. The script runs singlePQ then multiPQ, and records the resulting best QPS for each and produces table (in Jira format) as output.</description>
      <attachments/>
      <comments>102</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>1998</id>
      <title>Use Java 5 enums</title>
      <description>Replace the use of o.a.l.util.Parameter with Java 5 enums, deprecating Parameter. Replace other custom enum patterns with Java 5 enums.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>1999</id>
      <title>Match spotter for all query types</title>
      <description>Related to LUCENE-1929 and the current inability to highlight NumericRangeQuery, spatial, cached term filters and other exotica. This patch provides the ability to wrap any Query objects and record match info as flags encoded in the overall document score. Using this approach it would be possible to understand (and therefore highlight) which fields matched clauses in a query. The match encoding approach loses some precision in scores as noted here: http://tinyurl.com/ykt8nx7 Avoiding these precision issues would require a change to Lucene core to record docId, score AND a matchFlag byte in ScoreDoc objects and collector APIs. This may be something we should consider.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2006</id>
      <title>Optimization for FieldDocSortedHitQueue</title>
      <description>When updating core for generics, I found the following as a optimization of FieldDocSortedHitQueue: All FieldDoc values are Compareables (also the score or docid, if they appear as SortField in a MultiSearcher or ParallelMultiSearcher). The code of lessThan seems very ineffective, as it has a big switch statement on the SortField type, then casts the value to the underlying numeric type Object, calls Number.xxxValue() &amp; co for it and then compares manually. As j.l.Number is itself Comparable, I see no reason to do this. Just call compareTo on the Comparable interface and we are happy. The big deal is that it prevents casting and the two method calls xxxValue(), as Number.compareTo works more efficient internally. The only special cases are String sort, where the Locale may be used and the score sorting which is backwards. But these are two if statements instead of the whole switch. I had not tested it now for performance, but in my opinion it should be faster for MultiSearchers. All tests still pass (because they should).</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2007</id>
      <title>Add DocsetQuery to turn a DocIdSet into a query</title>
      <description>Added a class DocsetQuery that can be constructed from a DocIdSet.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2008</id>
      <title>TokenStream/Tokenizer/TokenFilter/Token javadoc improvements</title>
      <description>Some of the javadoc for the new TokenStream/Tokenizer/TokenFilter/Token APIs had javadoc errors. To the best of my knowledge, I corrected these and refined the copy a bit.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2009</id>
      <title>task.mem should be set to use jvmargs that pin the min and max heap size</title>
      <description>Currently, task.mem sets the java ant task param maxmemory - there is no equivalent minmemory though. jvmargs should be used instead, and xms,xmx pinned to task.mem - otherwise, results are affected as the JVM resizes the heap.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2010</id>
      <title>Remove segments with all documents deleted in commit/flush/close of IndexWriter instead of waiting until a merge occurs.</title>
      <description>I do not know if this is a bug in 2.9.0, but it seems that segments with all documents deleted are not automatically removed: 4 of 14: name=_dlo docCount=5 compound=true hasProx=true numFiles=2 size (MB)=0.059 diagnostics = {java.version=1.5.0_21, lucene.version=2.9.0 817268P - 2009-09-21 10:25:09, os=SunOS, os.arch=amd64, java.vendor=Sun Microsystems Inc., os.version=5.10, source=flush} has deletions [delFileName=_dlo_1.del] test: open reader.........OK [5 deleted docs] test: fields..............OK [136 fields] test: field norms.........OK [136 fields] test: terms, freq, prox...OK [1698 terms; 4236 terms/docs pairs; 0 tokens] test: stored fields.......OK [0 total field count; avg ? fields per doc] test: term vectors........OK [0 total vector count; avg ? term/freq vector fields per doc] Shouldn't such segments not be removed automatically during the next commit/close of IndexWriter? Mike McCandless: Lucene doesn't actually short-circuit this case, ie, if every single doc in a given segment has been deleted, it will still merge it [away] like normal, rather than simply dropping it immediately from the index, which I agree would be a simple optimization. Can you open a new issue? I would think IW can drop such a segment immediately (ie not wait for a merge or optimize) on flushing new deletes.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2015</id>
      <title>ASCIIFoldingFilter: expose folding logic + small improvements to ISOLatin1AccentFilter</title>
      <description>This patch adds a couple of non-ascii chars to ISOLatin1AccentFilter (namely: left &amp; right single quotation marks, en dash, em dash) which we very frequently encounter in our projects. I know that this class is now deprecated; this improvement is for legacy code that hasn't migrated yet. It also enables easy access to the ascii folding technique use in ASCIIFoldingFilter for potential re-use in non-Lucene-related code.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2017</id>
      <title>CloseableThreadLocal is now obsolete</title>
      <description>Since Lucene 3 depends on Java 5, we can use ThreadLocal#remove() to take care or resource management.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2018</id>
      <title>Reconsider boolean max clause exception</title>
      <description>Now that we have smarter multi-term queries, I think its time to reconsider the boolean max clause setting. It made more sense before, because you could hit it more unaware when the multi-term queries got huge - now its more likely that if it happens its because a user built the boolean themselves. And no duh thousands more boolean clauses means slower perf and more resources needed. We don't throw an exception when you try to use a ton of resources in a thousand other ways. The current setting also suffers from the static hell argument - especially when you consider something like Solr's multicore feature - you can have different settings for this in different cores, and the last one is going to win. Its ugly. Yes, that could be addressed better in Solr as well - but I still think it should be less ugly in Lucene as well. I'd like to consider either doing away with it, or raising it by quite a bit at the least. Or an alternative better solution. Right now, it aint so great.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2019</id>
      <title>map unicode process-internal codepoints to replacement character</title>
      <description>A spinoff from LUCENE-2016. There are several process-internal codepoints in unicode, we should not store these in the index. Instead they should be mapped to replacement character (U+FFFD), so they can be used process-internally. An example of this is how Lucene Java currently uses U+FFFF process-internally, it can't be in the index or will cause problems.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2021</id>
      <title>French elision filter should use CharArraySet</title>
      <description>French elision filter creates new strings, lowercases them, etc just to check against a Set&lt;String&gt;. trivial patch to use chararrayset instead.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2023</id>
      <title>Improve performance of SmartChineseAnalyzer</title>
      <description>I've noticed SmartChineseAnalyzer is a bit slow, compared to say CJKAnalyzer on chinese text. This patch improves the internal hhmm implementation. Time to index my chinese corpus is 75% of the previous time.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2025</id>
      <title>Ability to turn off the store for an index</title>
      <description>It would be really good in combination with parallel indexing if the Lucene store could be turned off entirely for an index. The reason is that part of the store is the FieldIndex (.fdx file), which contains an 8 bytes pointer for each document in a segment, even if a document does not contain any stored fields. With parallel indexing we will want to rewrite certain parallel indexes to update them, and if such an update affects only a small number of documents it will be a waste if you have to write the .fdx file every time. So in the case where you only want to update a data structure in the inverted index it makes sense to separate your index into multiple parallel indexes, where the ones you want to update don't contain any stored fields. It'd be also great to not only allow turning off the store but to make it customizable, similarly to what flexible indexing wants to achieve regarding the inverted index. As a start I'd be happy with the ability to simply turn off the store and to add more flexibility later.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2026</id>
      <title>Refactoring of IndexWriter</title>
      <description>I've been thinking for a while about refactoring the IndexWriter into two main components. One could be called a SegmentWriter and as the name says its job would be to write one particular index segment. The default one just as today will provide methods to add documents and flushes when its buffer is full. Other SegmentWriter implementations would do things like e.g. appending or copying external segments [what addIndexes*() currently does]. The second component's job would it be to manage writing the segments file and merging/deleting segments. It would know about DeletionPolicy, MergePolicy and MergeScheduler. Ideally it would provide hooks that allow users to manage external data structures and keep them in sync with Lucene's data during segment merges. API wise there are things we have to figure out, such as where the updateDocument() method would fit in, because its deletion part affects all segments, whereas the new document is only being added to the new segment. Of course these should be lower level APIs for things like parallel indexing and related use cases. That's why we should still provide easy to use APIs like today for people who don't need to care about per-segment ops during indexing. So the current IndexWriter could probably keeps most of its APIs and delegate to the new classes.</description>
      <attachments/>
      <comments>39</comments>
      <commenters>13</commenters>
    </issue>
    <issue>
      <id>2027</id>
      <title>Deprecate Directory.touchFile</title>
      <description>Lucene doesn't use this method, and, FindBugs reports that FSDirectory's impl shouldn't swallow the returned result from File.setLastModified.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2030</id>
      <title>CachingSpanFilter synchronizing on a none final protected object</title>
      <description>CachingSpanFilter and CachingWrapperFilter expose their internal cache via a protected member which is lazily instantiated in the getDocSetId method. The current code yields the chance to double instantiate the cache and internally synchronizes on a protected none final member. My first guess is that this member was exposed for testing purposes so it should rather be changed to package private. This patch breaks backwards compat while I guess the cleanup is kind of worth breaking it.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2033</id>
      <title>exposed MultiTermDocs and MultiTermPositions from package protected to public</title>
      <description>making these classes public can help classes that extends MultiReader.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2034</id>
      <title>Massive Code Duplication in Contrib Analyzers - unifly the analyzer ctors</title>
      <description>Due to the variouse tokenStream APIs we had in lucene analyzer subclasses need to implement at least one of the methodes returning a tokenStream. When you look at the code it appears to be almost identical if both are implemented in the same analyzer. Each analyzer defnes the same inner class (SavedStreams) which is unnecessary. In contrib almost every analyzer uses stopwords and each of them creates his own way of loading them or defines a large number of ctors to load stopwords from a file, set, arrays etc.. those ctors should be removed / deprecated and eventually removed.</description>
      <attachments/>
      <comments>51</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2037</id>
      <title>Allow Junit4 tests in our environment.</title>
      <description>Now that we're dropping Java 1.4 compatibility for 3.0, we can incorporate Junit4 in testing. Junit3 and junit4 tests can coexist, so no tests should have to be rewritten. We should start this for the 3.1 release so we can get a clean 3.0 out smoothly. It's probably worthwhile to convert a small set of tests as an exemplar.</description>
      <attachments/>
      <comments>38</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2039</id>
      <title>Regex support and beyond in JavaCC QueryParser</title>
      <description>Since the early days the standard query parser was limited to the queries living in core, adding other queries or extending the parser in any way always forced people to change the grammar file and regenerate. Even if you change the grammar you have to be extremely careful how you modify the parser so that other parts of the standard parser are affected by customisation changes. Eventually you had to live with all the limitation the current parser has like tokenizing on whitespaces before a tokenizer / analyzer has the chance to look at the tokens. I was thinking about how to overcome the limitation and add regex support to the query parser without introducing any dependency to core. I added a new special character that basically prevents the parser from interpreting any of the characters enclosed in the new special characters. I choose the forward slash '/' as the delimiter so that everything in between two forward slashes is basically escaped and ignored by the parser. All chars embedded within forward slashes are treated as one token even if it contains other special chars like * []?{} or whitespaces. This token is subsequently passed to a pluggable "parser extension" with builds a query from the embedded string. I do not interpret the embedded string in any way but leave all the subsequent work to the parser extension. Such an extension could be another full featured query parser itself or simply a ctor call for regex query. The interface remains quiet simple but makes the parser extendible in an easy way compared to modifying the javaCC sources. The downsides of this patch is clearly that I introduce a new special char into the syntax but I guess that would not be that much of a deal as it is reflected in the escape method though. It would truly be nice to have more than once extension an have this even more flexible so treat this patch as a kickoff though. Another way of solving the problem with RegexQuery would be to move the JDK version of regex into the core and simply have another method like: protected Query newRegexQuery(Term t) { ... } which I would like better as it would be more consistent with the idea of the query parser to be a very strict and defined parser. I will upload a patch in a second which implements the extension based approach I guess I will add a second patch with regex in core soon too.</description>
      <attachments/>
      <comments>50</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>2040</id>
      <title>PhraseQuery weight uses IDFExplanation.getIdf() method instead of Similarity.idf() directly</title>
      <description>It should use Similarity's idf() method instead.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2041</id>
      <title>Complete parallelizaton of ParallelMultiSearcher</title>
      <description>ParallelMultiSearcher is parallel only for the method signatures of 'search'. Part of a query process calls the method docFreq(). There was a TODO comment to parallelize this. Parallelizing this method actually increases the performance of a query on multiple indexes, especially remotely.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2042</id>
      <title>Allow controllable printing of the hits</title>
      <description>Adds "print.hits.field" property to the alg. If set, then the hits retrieved by Search* tasks are printed, along with the value of the specified field, for each doc.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2043</id>
      <title>Fix CommitIndexTask to also commit IndexReader changes</title>
      <description>I'm setting up a benchmark for LUCENE-1458, and one limitation I hit is that the CommitIndexTask doesn't commit pending changes in the IndexReader (eg via DeleteByPercent), using a named commit point.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2044</id>
      <title>Allow random seed to be set in DeleteByPercentTask</title>
      <description>Need this to make index identical on multiple runs.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2047</id>
      <title>IndexWriter should immediately resolve deleted docs to docID in near-real-time mode</title>
      <description>Spinoff from LUCENE-1526. When deleteDocuments(Term) is called, we currently always buffer the Term and only later, when it's time to flush deletes, resolve to docIDs. This is necessary because we don't in general hold SegmentReaders open. But, when IndexWriter is in NRT mode, we pool the readers, and so deleting in the foreground is possible. It's also beneficial, in that in can reduce the turnaround time when reopening a new NRT reader by taking this resolution off the reopen path. And if multiple threads are used to do the deletion, then we gain concurrency, vs reopen which is not concurrent when flushing the deletes.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2048</id>
      <title>Omit positions but keep termFreq</title>
      <description>it would be useful to have an option to discard positional information but still keep the term frequency - currently setOmitTermFreqAndPositions discards both. Even though position-dependent queries wouldn't work in such case, still any other queries would work fine and we would get the right scoring.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2050</id>
      <title>Improve contrib/benchmark for testing near-real-time search performance</title>
      <description>It's not easy to test NRT performance right now w/ contrib/benchmark. I've made some initial fixes to improve this: Added new '&amp;', that can follow any task within a serial sequence, to "background" the task (just like a shell). The test runs in the BG, and then at the end of all serial tasks, any still running BG tasks are stopped &amp; joined. Added WaitTask that simply waits; useful for controlling how long the BG'd tasks get to run. Added RollbackIndex task, which is real handy for using a given index for an NRT test, doing a bunch of updates, then reverting it all so your next run uses the same starting index Fixed the existing NearRealTimeReaderTask to simply periodically open the new reader (previously it was also running a fixed search), and removed its own threading (since &amp; can do that now). It periodically wakes up, opens the new reader, and swaps it into the PerfRunData, at the schedule you specify. I switched all usage of PerfRunData's get/setIndexReader APIs to use ref counting. With these changes you can now make some very simple but powerful algs, eg: OpenIndex { NearRealtimeReader(0.5) &amp; # Warm Search { "Index1" AddDoc &gt; : * : 100/sec &amp; [ { "Search" Search &gt; : * ] : 4 &amp; Wait(30.0) } CloseReader RollbackIndex RepSumByName This alg first opens the IndexWriter, then spawns the BG thread to reopen the NRT reader twice per second, does one warming Search (in the FG), spans a new thread to index documents at the rate of 100 per second, then spawns 4 search threads that do as many searches as they can. We then wait for 30 seconds, then stop all the threads, revert the index, and report. The patch is a work in progress – it generally works, but there're a few nocommits, and, we may want to improve reporting (though I think that's a separate issue).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2051</id>
      <title>Contrib Analyzer Setters should be deprecated and replace with ctor arguments</title>
      <description>Some analyzers in contrib provide setters for stopword / stem exclusion sets / hashtables etc. Those setters should be deprecated as they yield unexpected behaviour. The way they work is they set the reusable token stream instance to null in a thread local cache which only affects the tokenstream in the current thread. Analyzers itself should be immutable except of the threadlocal. will attach a patch soon.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2052</id>
      <title>Scan method signatures and add varargs where possible</title>
      <description>I changed a lot of signatures, but there may be more. The important ones like MultiReader and MultiSearcher are already done. This applies also to contrib. Varargs are no backwards break, they stay arrays as before.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2053</id>
      <title>When thread is interrupted we should throw a clear exception</title>
      <description>This is the 3.0 followon from LUCENE-1573. We should throw a dedicated exception, not just RuntimeException. Recent discussion from java-dev "Thread.interrupt()" subject: http://www.lucidimagination.com/search/document/8423f9f0b085034e/thread_interrupt</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2056</id>
      <title>Should NIOFSDir use direct ByteBuffers?</title>
      <description>I'm trying to test NRT performance, and noticed when I dump the thread stacks that the darned threads often seem to be in java.nio.Bits.copyToByteArray(Native Method)... so I wondered whether we could/should use direct ByteBuffers, and whether that would gain performance in general. We currently just use our own byte[] buffer via BufferedIndexInput. It's hard to test since it's likely platform specific, but if it does result in gains it could be an easy win.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2058</id>
      <title>benchmark pkg: specify trec_eval submission output from the command line</title>
      <description>the QueryDriver for the trec benchmark currently requires 4 command line arguments. the third argument is ignored (i typically populate this with "bogus") Instead, allow the third argument to specify the submission.txt file for trec_eval. while I am here, add a usage() documenting what the arguments to this driver program do.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2059</id>
      <title>benchmark pkg: allow TrecContentSource not to change the docname</title>
      <description>TrecContentSource currently appends 'iteration number' to the docname field. Example: if the original docname is DOC0001 then it will be indexed as DOC0001_0 this presents a problem for relevance testing, because when judging results, the expected docname will never be present. This patch adds an option to disable this behavior, defaulting to the existing behavior (which is to append the iteration number).</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2060</id>
      <title>CMS should default its maxThreadCount to 1 (not 3)</title>
      <description>From rough experience, I think the current default of 3 is too large. I think we get the most bang for the buck going from 0 to 1. I think this will especially impact optimize on an index with many segments – in this case the MergePolicy happily exposes concurrency (multiple pending merges), and CMS will happily launch 3 threads to carry that out.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2062</id>
      <title>Bulgarian Analyzer</title>
      <description>someone asked about bulgarian analysis on solr-user today... http://www.lucidimagination.com/search/document/e1e7a5636edb1db2/non_english_languages I was surprised we did not have anything. This analyzer implements the algorithm specified here, http://members.unine.ch/jacques.savoy/Papers/BUIR.pdf In the measurements there, this improves MAP approx 34%</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2063</id>
      <title>Use thread pool in ConcurrentMergeScheduler</title>
      <description>Currently it looks like CMS creates a new thread object for each merge, which may not be expensive anymore on Java5+ JVMs, however we can fairly simply implement the Java5 thread pooling. Also I'm thinking we may be interested in using thread pools for other tasks in IndexWriter (such as LUCENE-2047 performing deletes in the background).</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2064</id>
      <title>Highlighter should support all MultiTermQuery subclasses without casts</title>
      <description>In order to support MultiTermQuery subclasses the Highlighter component applies instanceof checks for concrete classes from the lucene core. This prevents classes like RegexQuery in contrib from being supported. Introducing dependencies on other contribs is not feasible just for being supported by the highlighter. While the instanceof checks and subsequent casts might hopefully go somehow away in the future but for supporting more multterm queries I have a alternative approach using a fake IndexReader that uses a RewriteMethod to force the MTQ to pass the field name to the given reader without doing any real work. It is easier to explain once you see the patch - I will upload shortly.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2065</id>
      <title>Java 5 port phase II</title>
      <description>LUCENE-1257 addresses the public API changes ( generics , mainly ) and other j.u.c. package changes related to the API . The changes are frozen and closed for 3.0 . This would be a placeholder JIRA for 3.0+ version to address the pending changes ( tests for generics etc.) and any other internal API changes as necessary.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2067</id>
      <title>Czech Stemmer</title>
      <description>Currently, the CzechAnalyzer is merely stopwords, and there isn't a czech stemmer in snowball. This patch implements the light stemming algorithm described in: http://portal.acm.org/citation.cfm?id=1598600 In their measurements, it improves MAP 42% The analyzer does not use this stemmer if LUCENE_VERSION &lt;= 3.0, for back compat.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2069</id>
      <title>fix LowerCaseFilter for unicode 4.0</title>
      <description>lowercase suppl. characters correctly. this only fixes the filter, the LowerCaseTokenizer is part of a more complex issue (CharTokenizer)</description>
      <attachments/>
      <comments>28</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2070</id>
      <title>document LengthFilter wrt Unicode 4.0</title>
      <description>LengthFilter calculates its min/max length from TermAttribute.termLength() This is not characters, but instead UTF-16 code units. In my opinion this should not be changed, merely documented. If we changed it, it would have an adverse performance impact because we would have to actually calculate Character.codePointCount() on the text. If you feel strongly otherwise, fixing it to count codepoints would be a trivial patch, but I'd rather not hurt performance. I admit I don't fully understand all the use cases for this filter.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2071</id>
      <title>Allow updating of IndexWriter SegmentReaders</title>
      <description>This discussion kind of started in LUCENE-2047. Basically, we'll allow users to perform delete document, and norms updates on SegmentReaders that are handled by IndexWriter.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2072</id>
      <title>Upgrade contrib/regex to jakarta-regex 1.5</title>
      <description>contrib/regex uses jakarta regex 1.4 while 1.5 is out for a while and make the package private workaround obsolete. We should upgrade the lib.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2075</id>
      <title>Share the Term -&gt; TermInfo cache across threads</title>
      <description>Right now each thread creates its own (thread private) SimpleLRUCache, holding up to 1024 terms. This is rather wasteful, since if there are a high number of threads that come through Lucene, you're multiplying the RAM usage. You're also cutting way back on likelihood of a cache hit (except the known multiple times we lookup a term within-query, which uses one thread). In NRT search we open new SegmentReaders (on tiny segments) often which each thread must then spend CPU/RAM creating &amp; populating. Now that we are on 1.5 we can use java.util.concurrent.*, eg ConcurrentHashMap. One simple approach could be a double-barrel LRU cache, using 2 maps (primary, secondary). You check the cache by first checking primary; if that's a miss, you check secondary and if you get a hit you promote it to primary. Once primary is full you clear secondary and swap them. Or... any other suggested approach?</description>
      <attachments/>
      <comments>86</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>2077</id>
      <title>changes-to-html: better handling of bulleted lists in CHANGES.txt</title>
      <description>bulleted lists should be rendered as such in output HTML</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2078</id>
      <title>Remove dependencies on specific field names or prefixes for field names (i.e. tierPrefix)</title>
      <description>Currently, the spatial contrib makes a lot of assumptions about what field names are when these are simply not needed. By doing so, it prevents re-use in other applications that have setup their fields differently.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2079</id>
      <title>Further improvements to contrib/benchmark for testing NRT</title>
      <description>Some small changes: Allow specifying a priority for BG threads, after the "&amp;" character; priority increment is + or - int that's added to main thread's priority to set child thread's. For my NRT tests I make the reopen thread +2, the indexing threads +1, and leave searching threads at their default. Added test case NearRealTimeReopenTask now reports @ the end the full array of msec of each reopen latency Added optional breakout of counts by time steps. If you set log.time.step.msec to eg 1000 then reported counts for serial task sequence is broken out by 1 second windows. EG you can use this to measure slowdown over time.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2081</id>
      <title>CartesianShapeFilter improvements</title>
      <description>The CartesiahShapeFilter could use some improvements. For starters, if we make sure the boxIds are sorted in index order, this should reduce the cost of seeks. I think we should also replace the logging with a similar approach to Lucene's output stream. We also can do Term creation a tad bit more efficiently too.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2082</id>
      <title>Performance improvement for merging posting lists</title>
      <description>A while ago I had an idea about how to improve the merge performance for posting lists. This is currently by far the most expensive part of segment merging due to all the VInt de-/encoding. Not sure if an idea for improving this was already mentioned in the past? So the basic idea is it to perform a raw copy of as much posting data as possible. The reason why this is difficult is that we have to remove deleted documents. But often the fraction of deleted docs in a segment is rather low (&lt;10%?), so it's likely that there are quite long consecutive sections without any deletions. To find these sections we could use the skip lists. Basically at any point during the merge we would find the skip entry before the next deleted doc. All entries to this point can be copied without de-/encoding of the VInts. Then for the section that has deleted docs we perform the "normal" way of merging to remove the deletes. Then we check again with the skip lists if we can raw copy the next section. To make this work there are a few different necessary changes: 1) Currently the multilevel skiplist reader/writer can only deal with fixed-size skips (16 on the lowest level). It would be an easy change to allow variable-size skips, but then the MultiLevelSkipListReader can't return numSkippedDocs anymore, which SegmentTermDocs needs -&gt; change 2) 2) Store the last docID in which a term occurred in the term dictionary. This would also be beneficial for other use cases. By doing that the SegmentTermDocs#next(), #read() and #skipTo() know when the end of the postinglist is reached. Currently they have to track the df, which is why after a skip it's important to take the numSkippedDocs into account. 3) Change the merging algorithm according to my description above. It's important to create a new skiplist entry at the beginning of every block that is copied in raw mode, because its next skip entry's values are deltas from the beginning of the block. Also the very first posting, and that one only, needs to be decoded/encoded to make sure that the payload length is explicitly written (i.e. must not depend on the previous length). Also such a skip entry has to be created at the beginning of each source segment's posting list. With change 2) we don't have to worry about the positions of the skip entries. And having a few extra skip entries in merged segments won't hurt much. If a segment has no deletions at all this will avoid any decoding/encoding of VInts (best case). I think it will also work great for segments with a rather low amount of deletions. We should probably then have a threshold: if the number of deletes exceeds this threshold we should fall back to old style merging. I haven't implemented any of this, so there might be complications I haven't thought about. Please let me know if you can think of reasons why this wouldn't work or if you think more changes are necessary. I will probably not have time to work on this soon, but I wanted to open this issue to not forget about it . Anyone should feel free to take this! Btw: I think the flex-indexing branch would be a great place to try this out as a new codec. This would also be good to figure out what APIs are needed to make merging fully flexible as well.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2083</id>
      <title>Use ReadWriteLock in IndexWriter</title>
      <description>Doing a small patch to make sure things don't break in a big way, we'll use RRWL replacing some of the global synchronized locks in IndexWriter. We'll read lock during operations that for example delete from a segment, and gwrite lock when we're changing the main segment infos collection (i.e. we're swapping in new segments after a merge, or flushing a new segment). I want to implement this, see if any tests break. Spin off from LUCENE-2047.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2084</id>
      <title>remove Byte/CharBuffer wrapping for collation key generation</title>
      <description>We can remove the overhead of ByteBuffer and CharBuffer wrapping in CollationKeyFilter and ICUCollationKeyFilter. this patch moves the logic in IndexableBinaryStringTools into char[],int,int and byte[],int,int based methods, with the previous Byte/CharBuffer methods delegating to these. Previously, the Byte/CharBuffer methods required a backing array anyway.</description>
      <attachments/>
      <comments>32</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2086</id>
      <title>When resolving deletes, IW should resolve in term sort order</title>
      <description>See java-dev thread "IndexWriter.updateDocument performance improvement".</description>
      <attachments/>
      <comments>22</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2087</id>
      <title>Remove recursion in NumericRangeTermEnum</title>
      <description>The current FilteredTermEnum in NRQ uses setEnum() which itsself calls next(). This may lead to a recursion that can overflow stack, if the index is empty and a large range with low precStep is used. With 64 bit numbers and precStep == 1 there may be 127 recursions, as each sub-range would hit no term on empty index and the setEnum call would then call next() which itsself calls setEnum again. This leads to recursion depth of 256. Attached is a patch that converts to iterative approach. setEnum is now unused and throws UOE (like endEnum()).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2089</id>
      <title>explore using automaton for fuzzyquery</title>
      <description>we can optimize fuzzyquery by using AutomatonTermsEnum. The idea is to speed up the core FuzzyQuery in similar fashion to Wildcard and Regex speedups, maintaining all backwards compatibility. The advantages are: we can seek to terms that are useful, instead of brute-forcing the entire terms dict we can determine matches faster, as true/false from a DFA is array lookup, don't even need to run levenshtein. We build Levenshtein DFAs in linear time with respect to the length of the word: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652 To implement support for 'prefix' length, we simply concatenate two DFAs, which doesn't require us to do NFA-&gt;DFA conversion, as the prefix portion is a singleton. the concatenation is also constant time with respect to the size of the fuzzy DFA, it only need examine its start state. with this algorithm, parametric tables are precomputed so that DFAs can be constructed very quickly. if the required number of edits is too large (we don't have a table for it), we use "dumb mode" at first (no seeking, no DFA, just brute force like now). As the priority queue fills up during enumeration, the similarity score required to be a competitive term increases, so, the enum gets faster and faster as this happens. This is because terms in core FuzzyQuery are sorted by boost value, then by term (in lexicographic order). For a large term dictionary with a low minimal similarity, you will fill the pq very quickly since you will match many terms. This not only provides a mechanism to switch to more efficient DFAs (edit distance of 2 -&gt; edit distance of 1 -&gt; edit distance of 0) during enumeration, but also to switch from "dumb mode" to "smart mode". With this design, we can add more DFAs at any time by adding additional tables. The tradeoff is the tables get rather large, so for very high K, we would start to increase the size of Lucene's jar file. The idea is we don't have include large tables for very high K, by using the 'competitive boost' attribute of the priority queue. For more information, see http://en.wikipedia.org/wiki/Levenshtein_automaton</description>
      <attachments/>
      <comments>120</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>2090</id>
      <title>convert automaton to char[] based processing and TermRef / TermsEnum api</title>
      <description>The automaton processing is currently done with String, mostly because TermEnum is based on String. it is easy to change the processing to work with char[], since behind the scenes this is used anyway. in general I think we should make sure char[] based processing is exposed in the automaton pkg anyway, for things like pattern-based tokenizers and such.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2091</id>
      <title>Add BM25 Scoring to Lucene</title>
      <description>http://nlp.uned.es/~jperezi/Lucene-BM25/ describes an implementation of Okapi-BM25 scoring in the Lucene framework, as an alternative to the standard Lucene scoring (which is a version of mixed boolean/TFIDF). I have refactored this a bit, added unit tests and improved the runtime somewhat. I would like to contribute the code to Lucene under contrib.</description>
      <attachments/>
      <comments>41</comments>
      <commenters>13</commenters>
    </issue>
    <issue>
      <id>2093</id>
      <title>Use query-private scope instead of shared Term-&gt;TermInfo cache</title>
      <description>Spinoff of LUCENE-2075. We currently use a shared terms cache so multiple resolves of the same term within execution of a single query save CPU. But this ties up a good amount of long term RAM... So, it might be better to instead create a "query private scope", where places in Lucene like the terms dict could store &amp; retrieve results. The scope would be private to each running query, and would be GCable as soon as the query completes. Then we've have perfect within query hit rate...</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2096</id>
      <title>Investigate parallelizing Ant junit tests</title>
      <description>Ant Contrib has a "ForEach" construct that may speed up running all of the Junit tests by parallelizing them with a configurable number of threads. I envision this in several stages. First, see if ForEach works for us with hard-coded lists, distribute this for testing then make the changes "for real". I intend to hard-code the list for the first pass, ordered by the time they take. This won't do for check-in, but will give us a fast proof-of-concept. This approach will be most useful for multi-core machines. In particular, we need to see whether the parallel tasks are isolated enough from each other to prevent mutual interference. All this assumes the fragmentary reference I found is still available...</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2098</id>
      <title>make BaseCharFilter more efficient in performance</title>
      <description>Performance degradation in Solr 1.4 was reported. See: http://www.lucidimagination.com/search/document/43c4bdaf5c9ec98d/html_stripping_slower_in_solr_1_4 The inefficiency has been pointed out in BaseCharFilter javadoc by Mike: NOTE: This class is not particularly efficient. For example, a new class instance is created for every call to addOffCorrectMap(int, int), which is then appended to a private list.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2099</id>
      <title>Revise PositionIncrement in StopFilter / QueryParser</title>
      <description>Based on the discussion in LUCENE-2094 the way PositionIncrement in StopFiter and QueryParser should be revised and at least extensively documented. This issue is a follow-up for the discussion in LUCENE-2094.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2100</id>
      <title>Make contrib analyzers final</title>
      <description>The analyzers in contrib/analyzers should all be marked final. None of the Analyzers should ever be subclassed - users should build their own analyzers if a different combination of filters and Tokenizers is desired.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2101</id>
      <title>Default Stopwords should use specific Version in CharArraySet construtor</title>
      <description>LUCENE-2094 added a version to the constructor of CharArraySet. The default sets in *Analyzer uses Version.LUCENE_CURRENT which currently does not do any harm. Yet, in the future changes to CharArraySet depending on the version could potentially cause index corruption if those default version are not changed. To make sure such a corruption can not happen the default sets should use a specific Version (Version.LUCENE_31)</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2102</id>
      <title>LowerCaseFilter for Turkish language</title>
      <description>java.lang.Character.toLowerCase() converts 'I' to 'i' however in Turkish alphabet lowercase of 'I' is not 'i'. It is LATIN SMALL LETTER DOTLESS I.</description>
      <attachments/>
      <comments>51</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2109</id>
      <title>Make DocsEnum subclass of DocIdSetIterator</title>
      <description>Spinoff from LUCENE-1458: One thing I came along long time ago, but now with a new API it get's interesting again: DocsEnum should extend DocIdSetIterator, that would make it simplier to use and implement e.g. in MatchAllDocQuery.Scorer, FieldCacheRangeFilter and so on. You could e.g. write a filter for all documents that simply returns the docs enumeration from IndexReader. So it should be an abstract class that extends DocIdSetIterator. It has the same methods, only some methods must be a little bit renamed. The problem is, because java does not support multiple inheritace, we cannot also extends attributesource Would DocIdSetIterator be an interface it would work (this is one of the cases where interfaces for really simple patterns can be used, like iterators). The problem with multiple inheritance could be solved by an additional method attributes() that creates a new AttributeSource on first access then (because constructing an AttributeSource is costly). The same applies for the other *Enums, it should be separated for lazy init. DocsEnum could look like this: public abstract class DocsEnum extends DocIdSetIterator { private AttributeSource atts = null; public int freq() public DontKnowClassName positions() public final AttributeSource attributes() { if (atts==null) atts=new AttributeSource(); return atts; } ...default impl of the bulk access using the abstract methods from DocIdSetIterator }</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2110</id>
      <title>Refactoring of FilteredTermsEnum and MultiTermQuery</title>
      <description>FilteredTermsEnum is confusing as it is initially positioned to the first term. It should instead work like an uninitialized TermsEnum for a field before the first call to next() or seek(). FilteredTermsEnums cannot implement seek() as eg. NRQ or Automaton are not able to support this. Seeking is also not needed for MTQ at all, so seek can just throw UOE. This issue changes some of the internal behaviour of MTQ and FilteredTermsEnum to allow also seeking in NRQ and Automaton (see comments below).</description>
      <attachments/>
      <comments>32</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2111</id>
      <title>Wrapup flexible indexing</title>
      <description>Spinoff from LUCENE-1458. The flex branch is in fairly good shape – all tests pass, initial search performance testing looks good, it survived several visits from the Unicode policeman But it still has a number of nocommits, could use some more scrutiny especially on the "emulate old API on flex index" and vice/versa code paths, and still needs some more performance testing. I'll do these under this issue, and we should open separate issues for other self contained fixes. The end is in sight!</description>
      <attachments/>
      <comments>58</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2114</id>
      <title>Improve org.apache.lucene.search.Filter Documentation and Tests to reflect per segment readers</title>
      <description>Filter Javadoc does not mention that the Reader passed to getDocIDSet(Reader) could be on a per-segment basis. This caused confusion on the users-list – see http://lucene.markmail.org/message/6knz2mkqbpxjz5po?q=date:200912+list:org.apache.lucene.java-user&amp;page=1 We should improve the javadoc and also add a testcase that reflects filtering on a per-segment basis.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2115</id>
      <title>Port to Generics - test cases in contrib</title>
      <description>LUCENE-1257 in Lucene 3.0 addressed porting to generics across public api-s . LUCENE-2065 addressed across src/test . This would be a placeholder JIRA for any remaining pending generic conversions across the code base. Please keep it open after commiting and we can close it when we are near a 3.1 release , so that this could be a placeholder ticket.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2116</id>
      <title>Add link to irc channel #lucene on the website</title>
      <description>We should add a link to #lucene IRC channel on chat.freenode.org.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2122</id>
      <title>Use JUnit4 capabilites for more thorough Locale testing for classes deriving from LocalizedTestCase</title>
      <description>Use the @Parameterized capabilities of Junit4 to allow more extensive testing of Locales.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2125</id>
      <title>Ability to store and retrieve attributes in the inverted index</title>
      <description>Now that we have the cool attribute-based TokenStream API and also the great new flexible indexing features, the next logical step is to allow storing the attributes inline in the posting lists. Currently this is only supported for the PayloadAttribute. The flex search APIs already provide an AttributeSource, so there will be a very clean and performant symmetry. It should be seamlessly possible for the user to define a new attribute, add it to the TokenStream, and then retrieve it from the flex search APIs. What I'm planning to do is to add additional methods to the token attributes (e.g. by adding a new class TokenAttributeImpl, which extends AttributeImpl and is the super class of all impls in o.a.l.a.tokenattributes): void serialize(DataOutput) void deserialize(DataInput) boolean storeInIndex() The indexer will only call the serialize method of an TokenAttributeImpl in case its storeInIndex() returns true. The big advantage here is the ease-of-use: A user can implement in one place everything necessary to add the attribute to the index. Btw: I'd like to introduce DataOutput and DataInput as super classes of IndexOutput and IndexInput. They will contain methods like readByte(), readVInt(), etc., but methods such as close(), getFilePointer() etc. will stay in the super classes. Currently the payload concept is hardcoded in TermsHashPerField and FreqProxTermsWriterPerField. These classes take care of copying the contents of the PayloadAttribute over into the intermediate in-memory postinglist representation and reading it again. Ideally these classes should not know about specific attributes, but only call serialze() on those attributes that shall be stored in the posting list. We also need to change the PositionsEnum and PositionsConsumer APIs to deal with attributes instead of payloads. I think the new codecs should all support storing attributes. Only the preflex one should be hardcoded to only take the PayloadAttribute into account. We'll possibly need another extension point that allows us to influence compression across multiple postings. Today we use the length-compression trick for the payloads: if the previous payload had the same length as the current one, we don't store the length explicitly again, but only set a bit in the shifted position VInt. Since often all payloads of one posting list have the same length, this results in effective compression. Now an advanced user might want to implement a similar encoding, where it's not enough to just control serialization of a single value, but where e.g. the previous position can be taken into account to decide how to encode a value. I'm not sure yet how this extension point should look like. Maybe the flex APIs are actually already sufficient. One major goal of this feature is performance: It ought to be more efficient to e.g. define an attribute that writes and reads a single VInt than storing that VInt as a payload. The payload has the overhead of converting the data into a byte array first. An attribute on the other hand should be able to call 'int value = dataInput.readVInt();' directly without the byte[] indirection. After this part is done I'd like to use a very similar approach for column-stride fields.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2126</id>
      <title>Split up IndexInput and IndexOutput into DataInput and DataOutput</title>
      <description>I'd like to introduce the two new classes DataInput and DataOutput that contain all methods from IndexInput and IndexOutput that actually decode or encode data, such as readByte()/writeByte(), readVInt()/writeVInt(). Methods like getFilePointer(), seek(), close(), etc., which are not related to data encoding, but to files as input/output source stay in IndexInput/IndexOutput. This patch also changes ByteSliceReader/ByteSliceWriter to extend DataInput/DataOutput. Previously ByteSliceReader implemented the methods that stay in IndexInput by throwing RuntimeExceptions. See also LUCENE-2125. All tests pass.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2127</id>
      <title>Improved large result handling</title>
      <description>Per http://search.lucidimagination.com/search/document/350c54fc90d257ed/lots_of_results#fbb84bd297d15dd5, it would be nice to offer some other Collectors that are better at handling really large number of results. This could be implemented in a variety of ways via Collectors. For instance, we could have a raw collector that does no sorting and just returns the ScoreDocs, or we could do as Mike suggests and have Collectors that have heuristics about memory tradeoffs and only heapify when appropriate.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2128</id>
      <title>Further parallelizaton of ParallelMultiSearcher</title>
      <description>When calling search(Query, Filter, int) on a ParallelMultiSearcher, the createWeights function of MultiSearcher is called, and sequentially calls docFreqs() on every sub-searcher. This can take a significant amount of time when there are lots of remote sub-searchers.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2130</id>
      <title>Investigate Rewriting Constant Scoring MultiTermQueries per segment</title>
      <description>This issue is likely not to go anywhere, but I thought we might explore it. The only idea I have come up with is fairly ugly, and unless something better comes up, this is not likely to happen. But if we could rewrite constant score multi-term queries per segment, MTQ's with auto (when the heuristic doesnt cut over to constant filter), or constant boolean rewrite could enum terms against a single segment and then apply a boolean query against each segment with just the terms that are known to be in that segment. This also allows you to avoid DirectoryReaders MultiTermEnum and its PQ. (See Roberts comment below). No biggie, not likely, but what the heck. So the ugly way to do it is to add a property to query's and weights - lateCnstRewrite or something, that defaults to false. MTQ would return true if its in a constant score mode. On the top level rewrite, if this is detected, an empty ConstantScoreQuery is made, and its Weight is turned to lateCnstRewrite and it keeps a ref to the original MTQ query. It also gets its boost set to the MTQ's boost. Then when we are searching per segment, if the Weight is lateCnstRewrite, we grab the orig query and actually do the rewrite against the subreader and grab the actual constantscore weight. It works I think - but its a little ugly. Not sure its worth the baggage for the win - but perhaps the objective can be met in another way.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2133</id>
      <title>[PATCH] IndexCache: Refactoring of FieldCache, FieldComparator, SortField</title>
      <description>Hi all, up to the current version Lucene contains a conceptual flaw, that is the FieldCache. The FieldCache is a singleton which is supposed to cache certain information for every IndexReader that is currently open The FieldCache is flawed because it is incorrect to assume that: 1. one IndexReader instance equals one index. In fact, there can be many clones (of SegmentReader) or decorators (FilterIndexReader) which all access the very same data. 2. the cache information remains valid for the lifetime of an IndexReader. In fact, some IndexReaders may be reopen()'ed and thus they may contain completely different information. 3. all IndexReaders need the same type of cache. In fact, because of the limitations imposed by the singleton construct there was no implementation other than FieldCacheImpl. Furthermore, FieldCacheImpl and FieldComparator are bloated by several static inner-classes that could be moved to package level. There have been a few attempts to improve FieldCache, namely LUCENE-831, LUCENE-1579 and LUCENE-1749, but the overall situation remains the same: There is a central registry for assigning Caches to IndexReader instances. I now propose the following: 1. Obsolete FieldCache and FieldCacheKey and provide index-specific, extensible cache instances ("IndexCache"). IndexCaches provide common caching functionality for all IndexReaders and may be extended (for example, SegmentReader would have a SegmentReaderIndexCache and store different data than a regular IndexCache) 2. Add the index-specific field cache (IndexFieldCache) to the IndexCache. IndexFieldCache is an interface just like FieldCache and may support different implementations. 3. The IndexCache instances may be flushed/closed by the associated IndexReaders whenever necessary. 4. Obsolete FieldCacheSanityChecker because no more "insanities" are expected (or at least, they do not impact the overall performance) 5. Refactor FieldCacheImpl and the related classes (FieldComparator, SortField) I have provided an patch which takes care of all these issues. It passes all JUnit tests. The patch is quite large, admittedly, but the change required several modifications and some more to preserve backwards-compatibility. Backwards-compatibility is preserved by moving some of the updated functionality in the package org.apache.lucene.search.fields (field comparators and parsers, SortField) while adding wrapper instances and keeping old code in org.apache.lucene.search. In detail and besides the above mentioned improvements, the following is provided: 1. An IndexCache specific for SegmentReaders. The two ThreadLocals are moved from SegmentReader to SegmentReaderIndexCache. 2. A housekeeping improvement to CloseableThreadLocal. Now delegates the close() method to all registered instances by calling an onClose() method with the threads' instances. 3. Analyzer.close now may throw an IOException (this already is covered by java.io.Closeable). 4. A change to Collector: allow IndexCache instead of IndexReader being passed to setNextReader() 5. SortField's numeric types have been replaced by direct assignments of FieldComparatorSource. This removes the "switch" statements and the possibility to throw IllegalArgumentExceptions because of unsupported type values. The following classes have been deprecated and replaced by new classes in org.apache.lucene.search.fields: FieldCacheRangeFilter (=&gt; IndexFieldCacheRangeFilter) FieldCacheTermsFilter (=&gt; IndexFieldCacheTermsFilter) FieldCache (=&gt; IndexFieldCache) FieldCacheImpl (=&gt; IndexFieldCacheImpl) all classes in FieldCacheImpl (=&gt; several package-level classes) all subclasses of FieldComparator (=&gt; several package-level classes) Final notes: The patch would be simpler if no backwards compatibility was necessary. The Lucene community has to decide which classes/methods can immediately be removed, which ones later, which not at all. Whenever new classes depend on the old ones, an appropriate notice exists in the javadocs. The patch introduces a new, deprecated class IndexFieldCacheSanityChecker.java which is just there for testing purposes, to show that no sanity checks are necessary any longer. This class may be removed at any time. I expect that the patch does not impact performance. On the contrary, as the patch removes a few unnecessary checks we might even see a slight speedup. No benchmarking has been done so far, though. I have tried to preserve the existing functionality wherever possible and to focus on the class/method structure only. We certainly may improve the caches' behavior, but this out of scope for this patch. The refactoring finally makes the high duplication of code visible: For all supported atomic types (byte, double, float, int, long, short) three classes each are required: *Cache, *Comparator and *Parser. I think that further simplification might be possible (maybe using Java generics?), but I guess the current patch is large enough for now. Cheers, Christian</description>
      <attachments/>
      <comments>47</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2136</id>
      <title>MultiReader should not use PQ for its Term/sEnum if it has only 1 reader</title>
      <description>Related to LUCENE-2130.... Even though we've switched to segment-based searching, there are still times when the Term/sEnum is used against the top-level reader. I think Solr does this, and from LUCENE-2130, certain rewrite modes of MTQ will do this as well. Currently, on an optimized index, MTQ is still using a PQ to present the terms, which is silly because this just adds a sizable amount of overhead. In such cases we should simply delecate to the single segment. Note that the single segment can have deletions, and we should still delegate. Ie, the index need not be optimized, just have a single segment.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2137</id>
      <title>Replace SegmentReader.Ref with AtomicInteger</title>
      <description>I think the patch should be applied to backcompat tag in its entirety.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2138</id>
      <title>Allow custom index readers when using IndexWriter.getReader</title>
      <description>This is needed for backwards compatible support with Solr, and is a spin-off from SOLR-1606.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2139</id>
      <title>Cleanup and Improvement of Spatial Contrib</title>
      <description>The current spatial contrib can be improved by adding documentation, tests, removing unused classes and code, repackaging the classes and improving the performance of the distance filtering. The latter will incorporate the multi-threaded functionality introduced in LUCENE-1732. Other improvements involve adding better support for different distance units, different distance calculators and different data formats (whether it be lat/long fields, geohashes, or something else in the future). Patch to be added soon.</description>
      <attachments/>
      <comments>35</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2140</id>
      <title>TopTermsScoringBooleanQueryRewrite minscore</title>
      <description>when using the TopTermsScoringBooleanQueryRewrite (LUCENE-2123), it would be nice if MultiTermQuery could set an attribute specifying the minimum required score once the Priority Queue is filled. This way, FilteredTermsEnums could adjust their behavior accordingly based on the minimal score needed to actually be a useful term (i.e. not just pass thru the pq) An example is FuzzyTermsEnum: at some point the bottom of the priority queue contains words with edit distance of 1 and enumerating any further terms is simply a waste of time. This is because terms are compared by score, then termtext. So in this case FuzzyTermsEnum could simply seek to the exact match, then end. This behavior could be also generalized for all n, for a different impl of fuzzyquery where it is only looking in the term dictionary for words within edit distance of n' which is the lowest scoring term in the pq (they adjust their behavior during enumeration of the terms depending upon this attribute). Other FilteredTermsEnums could make use of this minimal score in their own way, to drive the most efficient behavior so that they do not waste time enumerating useless terms.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2141</id>
      <title>Make String and StringIndex in field cache more RAM efficient</title>
      <description>Once flex has landed and LUCENE-1990 is done, we should improve the RAM efficiency of String and StringIndex. The text data can be stored in native UTF8 (saves decode when loading), and as byte[] blocks (saves GC load and high RAM overhead of individual strings). And with packed unsigned ints we can save alot for cases that don't have that many unique string values.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2147</id>
      <title>Improve Spatial Utility like classes</title>
      <description>DistanceUnits can be improved by giving functionality to the enum, such as being able to convert between different units, and adding tests. GeoHashUtils can be improved through some code tidying, documentation, and tests. SpatialConstants allows us to move all constants, such as the radii and circumferences of Earth, to a single consistent location that we can then use throughout the contrib. This also allows us to improve the transparency of calculations done in the contrib, as users of the contrib can easily see the values being used. Currently this issues does not migrate classes to use these constants, that will happen in issues related to the appropriate classes.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2148</id>
      <title>Improve Spatial Point2D and Rectangle Classes</title>
      <description>The Point2D and Rectangle classes have alot of duplicate, redundant and used functionality. This issue cleans them both up and simplifies the functionality they provide. Subsequent to this, Eclipse and LineSegment, which depend on Point2D, are not used anywhere in the contrib, therefore rather than trying to update them to use the improved Point2D, they will be removed.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2149</id>
      <title>Simplify Spatial LatLng and LLRect classes</title>
      <description>Currently in the contrib there is FloatLatLng, and FixedLatLng, which both extend LatLng. The reason for this separation is not clear and is not needed in the current functionality. The functionality that is used can be collapsed into LatLng, which can be made a concrete class. Internally LatLng can benefit from the improvements suggested in LUCENE-1934. LLRect, which uses LatLng, can also be simplified by removing the unused functionality, and using the new LatLng class. All classes can be improved through documentation, some method renaming, and general code tidy up.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2150</id>
      <title>Build should enable unchecked warnings in javac</title>
      <description>Just have to uncomment this: &lt;!-- for generics in Java 1.5: --&gt; &lt;!--&lt;compilerarg line="-Xlint:unchecked"/&gt;--&gt; in common-build.xml. Test &amp; core are clean, but contrib still has many warnings. Either we fix contrib with this issue, or, conditionalize this (anyone anty who can do this?) so contrib is off until we can fix it.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2151</id>
      <title>Abstract the distance calculation process in the Spatial contrib</title>
      <description>The spatial contrib shouldn't tie users to one particular way of calculating distances. Wikipedia lists multiple different formulas for the great-circle distance calculation, and there are alternatives to that as well. In a situation where many documents have the same points, it would be useful to be able to cache some calculated values as well (currently this is sort of handled in the filtering process itself). This issue addresses this by abstracting away the distance calculator, allowing the user to provide the implementation of choice. It would then be possible to swap in different distance calculation strategies without altering the distance filtering process itself.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2152</id>
      <title>Abstract Spatial distance filtering process and supported field formats</title>
      <description>Currently the second stage of the filtering process in the spatial contrib involves calculating the exact distance for the remaining documents, and filtering out those that fall out of the search radius. Currently this is done through the 2 impls of DistanceFilter, LatLngDistanceFilter and GeoHashDistanceFilter. The main difference between these 2 impls is the format of data they support, the former supporting lat/lngs being stored in 2 distinct fields, while the latter supports geohashed lat/lngs through the GeoHashUtils. This difference should be abstracted out so that the distance filtering process is data format agnostic. The second issue is that the distance filtering algorithm can be considerably optimized by using multiple-threads. Therefore it makes sense to have an abstraction of DistanceFilter which has different implementations, one being a multi-threaded implementation and the other being a blank implementation that can be used when no distance filtering is to occur.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2156</id>
      <title>use AtomicInteger/Boolean to track IR.refCount and IW.closed</title>
      <description>Less costly than synchronized methods we have now...</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2157</id>
      <title>DelimitedPayloadTokenFilter copies the bufer over itsself. Instead it should only set the length. Also optimize logic.</title>
      <description>This is a small improvement I found when looking around. It is also a bad idea to copy a array over itsself. All tests pass, will commit later!</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2159</id>
      <title>Tool to expand the index for perf/stress testing.</title>
      <description>Sometimes it is useful to take a small-ish index and expand it into a large index with K segments for perf/stress testing. This tool does that. See attached class.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2160</id>
      <title>Tool to rename a field</title>
      <description>We found it useful to be able to rename a field. It can save a lot of reindexing time/cost when being used in conjunction with ParallelReader to update partially a field.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2161</id>
      <title>Some concurrency improvements for NRT</title>
      <description>Some concurrency improvements for NRT I found &amp; fixed some silly thread bottlenecks that affect NRT: Multi/DirectoryReader.numDocs is synchronized, I think so only 1 thread computes numDocs if it's -1. I removed this sync, and made numDocs volatile, instead. Yes, multiple threads may compute the numDocs for the first time, but I think that's harmless? Fixed BitVector's ctor to set count to 0 on creating a new BV, and clone to copy the count over; this saves CPU computing the count unecessarily. Also strengthened assertions done in SR, testing the delete docs count. I also found an annoying thread bottleneck that happens, due to CMS. Whenever CMS hits the max running merges (default changed from 3 to 1 recently), and the merge policy now wants to launch another merge, it forces the incoming thread to wait until one of the BG threads finishes. This is a basic crude throttling mechanism – you force the mutators (whoever is causing new segments to appear) to stop, so that merging can catch up. Unfortunately, when stressing NRT, that thread is the one that's opening a new NRT reader. So, the first serious problem happens when you call .reopen() on your NRT reader – this call simply forwards to IW.getReader if the reader was an NRT reader. But, because DirectoryReader.doReopen is synchronized, this had the horrible effect of holding the monitor lock on your main IR. In my test, this blocked all searches (since each search uses incRef/decRef, still sync'd until LUCENE-2156, at least). I fixed this by making doReopen only sync'd on this if it's not simply forwarding to getWriter. So that's a good step forward. This prevents searches from being blocked while trying to reopen to a new NRT. However... it doesn't fix the problem that when an immense merge is off and running, opening an NRT reader could hit a tremendous delay because CMS blocks it. The BalancedSegmentMergePolicy should help here... by avoiding such immense merges. But, I think we should also pursue an improvement to CMS. EG, if it has 2 merges running, where one is huge and one is tiny, it ought to increase thread priority of the tiny one. I think with such a change we could increase the max thread count again, to prevent this starvation. I'll open a separate issue....</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2162</id>
      <title>ExtendableQueryParser should allow extensions to access the toplevel parser settings/ properties</title>
      <description>Based on the latest discussions in LUCENE-2039 this issue will expose the toplevel parser via the ExtensionQuery so that ExtensionParsers can access properties like getAllowLeadingWildcards() from the top level parser.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2163</id>
      <title>Remove synchronized from DirReader.reopen/clone</title>
      <description>Spinoff from LUCENE-2161, where the fact that DirReader.reopen is sync'd was dangerous in the context of NRT (could block all searches against that reader when CMS was throttling). So, with LUCENE-2161, we're removing the synchronization when it's an NRT reader that you're reopening. But... why should we sync even for a "normal" reopen? There are various sync'd methods on IndexReader/DirReader (we are reducing that, with LUCENE-2161 and also LUCENE-2156), but, in general it doesn't seem like "normal" reopen really needs to be sync'd. Performing a reopen shouldn't incur any chance of blocking a search...</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2164</id>
      <title>Make CMS smarter about thread priorities</title>
      <description>Spinoff from LUCENE-2161... The hard throttling CMS does (blocking the incoming thread that wants to launch a new merge) can be devastating when it strikes during NRT reopen. It can easily happen if a huge merge is off and running, but then a tiny merge is needed to clean up recently created segments due to frequent reopens. I think a small change to CMS, whereby it assigns a higher thread priority to tiny merges than big merges, should allow us to increase the max merge thread count again, and greatly reduce the chance that NRT's reopen would hit this.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2167</id>
      <title>Implement StandardTokenizer with the UAX#29 Standard</title>
      <description>It would be really nice for StandardTokenizer to adhere straight to the standard as much as we can with jflex. Then its name would actually make sense. Such a transition would involve renaming the old StandardTokenizer to EuropeanTokenizer, as its javadoc claims: This should be a good tokenizer for most European-language documents The new StandardTokenizer could then say This should be a good tokenizer for most languages. All the english/euro-centric stuff like the acronym/company/apostrophe stuff can stay with that EuropeanTokenizer, and it could be used by the european analyzers.</description>
      <attachments/>
      <comments>138</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>2169</id>
      <title>Speedup of CharArraySet#copy if a CharArraySet instance is passed to copy.</title>
      <description>the copy method should use the entries array itself to copy the set internally instead of iterating over all values. this would speedup copying even small set</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2171</id>
      <title>Over synchronization for read-only index readers in SegmentTermDocs</title>
      <description>In SegmentTermDocs constructor (from 2.9.1) 46 protected SegmentTermDocs(SegmentReader parent) { 47 this.parent = parent; 48 this.freqStream = (IndexInput) parent.core.freqStream.clone(); 49 synchronized (parent) { 50 this.deletedDocs = parent.deletedDocs; 51 } 52 this.skipInterval = parent.core.getTermsReader().getSkipInterval(); 53 this.maxSkipLevels = parent.core.getTermsReader().getMaxSkipLevels(); 54 } The synchronization on "parent" for accessing deletedDocs is unnecessary on readonly indexes. If that access was moved into the SegmentReader then it could be protected there by default and overridden in ReadonlySegmentReader.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2173</id>
      <title>Simplify and tidy Cartesian Tier Code in Spatial</title>
      <description>The Cartesian Tier filtering code in the spatial code can be simplified, tidied and generally improved. Improvements include removing default field name support which isn't the responsibility of the code, adding javadoc, making method names more intuitive and trying to make the complex code in CartesianPolyFilterBuilder more understandable. Few deprecations have to occur as part of this work, but some public methods in CartesianPolyFilterBuilder will be made private where possible so future improvements of this class can occur.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2174</id>
      <title>Add new SpatialFilter and DistanceFieldComparatorSource to Spatial</title>
      <description>The current DistanceQueryBuilder and DistanceFieldComparatorSource in Spatial are based on the old filtering process, most of which has been deprecated in previous issues. These will be replaced by a new SpatialFilter class, which is a proper Lucene filter, and a new DistanceFieldComparatorSource which will be relocated and will use the new DistanceFilter interface.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2175</id>
      <title>Deprecate remaining unused classes in Spatial</title>
      <description>The major changes to spatial have rendered a few other classes in the module unnecessary. This issue deprecates these classes so they can be removed later on.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2176</id>
      <title>Pass Lucli the index to use as a command line argument</title>
      <description>I made a patch to let you tell Lucli which index to use by passing it in as a command line argument. I made the change off of the 3.0 branch, wasn't sure where the best place to do it was.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2178</id>
      <title>Benchmark contrib should allow multiple locations in ext.classpath</title>
      <description>When ant run-task is invoked with the -Dbenchmark.ext.classpath=... option, only a single location may be specified. If a classpath with more than one location is specified, none of the locations is put on the classpath for the invoked JVM.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2179</id>
      <title>CharArraySet.clear()</title>
      <description>I needed CharArraySet.clear() for something I was working on in Solr in a tokenstream. instead I ended up using CharArrayMap&lt;Boolean&gt; because it supported .clear() it would be better to use a set though, currently it will throw UOE for .clear() because AbstractSet will call iterator.remove() which throws UOE. In Solr, the very similar CharArrayMap.clear() looks like this: @Override public void clear() { count = 0; Arrays.fill(keys,null); Arrays.fill(values,null); } I think we can do a similar thing as long as we throw UOE for the UnmodifiableCharArraySet will submit a patch later tonight (unless someone is bored and has nothing better to do)</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2181</id>
      <title>benchmark for collation</title>
      <description>Steven Rowe attached a contrib/benchmark-based benchmark for collation (both jdk and icu) under LUCENE-2084, along with some instructions to run it... I think it would be a nice if we could turn this into a committable patch and add it to benchmark.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2183</id>
      <title>Supplementary Character Handling in CharTokenizer</title>
      <description>CharTokenizer is an abstract base class for all Tokenizers operating on a character level. Yet, those tokenizers still use char primitives instead of int codepoints. CharTokenizer should operate on codepoints and preserve bw compatibility.</description>
      <attachments/>
      <comments>31</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2186</id>
      <title>First cut at column-stride fields (index values storage)</title>
      <description>I created an initial basic impl for storing "index values" (ie column-stride value storage). This is still a work in progress... but the approach looks compelling. I'm posting my current status/patch here to get feedback/iterate, etc. The code is standalone now, and lives under new package oal.index.values (plus some util changes, refactorings) – I have yet to integrate into Lucene so eg you can mark that a given Field's value should be stored into the index values, sorting will use these values instead of field cache, etc. It handles 3 types of values: Six variants of byte[] per doc, all combinations of fixed vs variable length, and stored either "straight" (good for eg a "title" field), "deref" (good when many docs share the same value, but you won't do any sorting) or "sorted". Integers (variable bit precision used as necessary, ie this can store byte/short/int/long, and all precisions in between) Floats (4 or 8 byte precision) String fields are stored as the UTF8 byte[]. This patch adds a BytesRef, which does the same thing as flex's TermRef (we should merge them). This patch also adds basic initial impl of PackedInts (LUCENE-1990); we can swap that out if/when we get a better impl. This storage is dense (like field cache), so it's appropriate when the field occurs in all/most docs. It's just like field cache, except the reading API is a get() method invocation, per document. Next step is to do basic integration with Lucene, and then compare sort performance of this vs field cache. For the "sort by String value" case, I think RAM usage &amp; GC load of this index values API should be much better than field caache, since it does not create object per document (instead shares big long[] and byte[] across all docs), and because the values are stored in RAM as their UTF8 bytes. There are abstract Writer/Reader classes. The current reader impls are entirely RAM resident (like field cache), but the API is (I think) agnostic, ie, one could make an MMAP impl instead. I think this is the first baby step towards LUCENE-1231. Ie, it cannot yet update values, and the reading API is fully random-access by docID (like field cache), not like a posting list, though I do think we should add an iterator() api (to return flex's DocsEnum) – eg I think this would be a good way to track avg doc/field length for BM25/lnu.ltc scoring.</description>
      <attachments/>
      <comments>37</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>2187</id>
      <title>improve lucene's similarity algorithm defaults</title>
      <description>First things first: I am not an IR guy. The goal of this issue is to make 'surgical' tweaks to lucene's formula to bring its performance up to that of more modern algorithms such as BM25. In my opinion, the concept of having some 'flexible' scoring with good speed across the board is an interesting goal, but not practical in the short term. Instead here I propose incorporating some work similar to lnu.ltc and friends, but slightly different. I noticed this seems to be in line with that paper published before about the trec million queries track... Here is what I propose in pseudocode (overriding DefaultSimilarity): @Override public float tf(float freq) { return 1 + (float) Math.log(freq); } @Override public float lengthNorm(String fieldName, int numTerms) { return (float) (1 / ((1 - slope) * pivot + slope * numTerms)); } Where slope is a constant (I used 0.25 for all relevance evaluations: the goal is to have a better default), and pivot is the average field length. Obviously we shouldnt make the user provide this but instead have the system provide it. These two pieces do not improve lucene much independently, but together they are competitive with BM25 scoring with the test collections I have run so far. The idea here is that this logarithmic tf normalization is independent of the tf / mean TF that you see in some of these algorithms, in fact I implemented lnu.ltc with cosine pivoted length normalization and log(tf)/log(mean TF) stuff and it did not fare as well as this method, and this is simpler, we do not need to calculate this mean TF at all. The BM25-like "binary" pivot here works better on the test collections I have run, but of course only with the tf modification. I am uploading a document with results from 3 test collections (Persian, Hindi, and Indonesian). I will test at least 3 more languages... yes including English... across more collections and upload those results also, but i need to process these corpora to run the tests with the benchmark package, so this will take some time (maybe weeks) so, please rip it apart with scoring theory etc, but keep in mind 2 of these 3 test collections are in the openrelevance svn, so if you think you have a great idea, don't hesitate to test it and upload results, this is what it is for. also keep in mind again I am not a scoring or IR guy, the only thing i can really bring to the table here is the willingness to do a lot of relevance testing!</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2188</id>
      <title>A handy utility class for tracking deprecated overridden methods</title>
      <description>This issue provides a new handy utility class that keeps track of overridden deprecated methods in non-final sub classes. This class can be used in new deprecations. See the javadocs for an example.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2189</id>
      <title>Simple9 (de)compression</title>
      <description>Simple9 is an alternative for VInt.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2191</id>
      <title>rename Tokenizer.reset(Reader) to Tokenizer.setReader(Reader)</title>
      <description>in TokenStream there is a reset() method, but the method in Tokenizer used to set a new Reader is called reset(Reader). in my opinion this name overloading creates a lot of confusion, and we see things like reset(Reader) calling reset() even in StandardTokenizer... So I think this would be some work to fulfill all the backwards compatibility, but worth it because when you look at the existing reset(Reader) and reset() code in various tokenizers, or the javadocs for Tokenizer, its pretty confusing and inconsistent.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2194</id>
      <title>improve efficiency of snowballfilter</title>
      <description>snowball stemming currently creates 2 new strings and 1 new stringbuilder for every word. all of this is unnecessary, so don't do it.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2195</id>
      <title>Speedup CharArraySet if set is empty</title>
      <description>CharArraySet#contains(...) always creates a HashCode of the String, Char[] or CharSequence even if the set is empty. contains should return false if set it empty</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2196</id>
      <title>Spellchecker should implement java.io.Closable</title>
      <description>As the most of the lucene classes implement Closable (IndexWriter) Spellchecker should do too.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2198</id>
      <title>support protected words in Stemming TokenFilters</title>
      <description>This is from LUCENE-1515 I propose that all stemming TokenFilters have an 'exclusion set' that bypasses any stemming for words in this set. Some stemming tokenfilters have this, some do not. This would be one way for Karl to implement his new swedish stemmer (as a text file of ignore words). Additionally, it would remove duplication between lucene and solr, as they reimplement snowballfilter since it does not have this functionality. Finally, I think this is a pretty common use case, where people want to ignore things like proper nouns in the stemming. As an alternative design I considered a case where we generalized this to CharArrayMap (and ignoring words would mean mapping them to themselves), which would also provide a mechanism to override the stemming algorithm. But I think this is too expert, could be its own filter, and the only example of this i can find is in the Dutch stemmer. So I think we should just provide ignore with CharArraySet, but if you feel otherwise please comment.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2200</id>
      <title>Several final classes have non-overriding protected members</title>
      <description>Protected member access in final classes, except where a protected method overrides a superclass's protected method, makes little sense. The attached patch converts final classes' protected access on fields to private, removes two final classes' unused protected constructors, and converts one final class's protected final method to private.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2201</id>
      <title>more performance improvements for snowball</title>
      <description>i took a more serious look at snowball after LUCENE-2194. This gives greatly improved performance, but note it has some minor breaks to snowball internals: Among.s becomes a char[] instead of a string SnowballProgram.current becomes a char[] instead of a StringBuilder SnowballProgram.eq_s(int, String) becomes eq_s(int, CharSequence), so that eq_v(StringBuilder) doesnt need to create an extra string. same as the above with eq_s_b and eq_v_b replace_s(int, int, String) becomes replace_s(int, int, CharSequence), so that StringBuilder-based slice and insertion methods don't need to create an extra string. all of these "breaks" imho are only theoretical, the problem is just that pretty much everything is public or protected in the snowball internals. the performance improvement here depends heavily upon the snowball language in use, but its way more significant than LUCENE-2194.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2204</id>
      <title>FastVectorHighlighter: some classes and members should be publicly accessible to implement FragmentsBuilder</title>
      <description>I intended to design custom FragmentsBuilder can be written and pluggable, though, when I tried to write it out of the FVH package, it came out that some classes and members should be publicly accessible.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2205</id>
      <title>Rework of the TermInfosReader class to remove the Terms[], TermInfos[], and the index pointer long[] and create a more memory efficient data structure.</title>
      <description>Basically packing those three arrays into a byte array with an int array as an index offset. The performance benefits are stagering on my test index (of size 6.2 GB, with ~1,000,000 documents and ~175,000,000 terms), the memory needed to load the terminfos into memory were reduced to 17% of there original size. From 291.5 MB to 49.7 MB. The random access speed has been made better by 1-2%, load time of the segments are ~40% faster as well, and full GC's on my JVM were made 7 times faster. I have already performed the work and am offering this code as a patch. Currently all test in the trunk pass with this new code enabled. I did write a system property switch to allow for the original implementation to be used as well. -Dorg.apache.lucene.index.TermInfosReader=default or small I have also written a blog about this patch here is the link. http://www.nearinfinity.com/blogs/aaron_mccurry/my_first_lucene_patch.html</description>
      <attachments/>
      <comments>51</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2206</id>
      <title>integrate snowball stopword lists</title>
      <description>The snowball project creates stopword lists as well as stemmers, example: http://svn.tartarus.org/snowball/trunk/website/algorithms/english/stop.txt?view=markup This patch includes the following: snowball stopword lists for 13 languages in contrib/snowball/resources all stoplists are unmodified, only added license header and converted each one from whatever encoding it was in to UTF-8 added getSnowballWordSet to WordListLoader, this is because the format of these files is very different, for example it supports multiple words per line and embedded comments. I did not add any changes to SnowballAnalyzer to actually automatically use these lists yet, i would like us to discuss this in a future issue proposing integrating snowball with contrib/analyzers.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2213</id>
      <title>Small improvements to ArrayUtil.getNextSize</title>
      <description>Spinoff from java-dev thread "Dynamic array reallocation algorithms" started on Jan 12, 2010. Here's what I did: Keep the +3 for small sizes Added 2nd arg = number of bytes per element. Round up to 4 or 8 byte boundary (if it's 32 or 64 bit JRE respectively) Still grow by 1/8th If 0 is passed in, return 0 back I also had to remove some asserts in tests that were checking the actual values returned by this method – I don't think we should test that (it's an impl. detail).</description>
      <attachments/>
      <comments>27</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>2215</id>
      <title>paging collector</title>
      <description>http://issues.apache.org/jira/browse/LUCENE-2127?focusedCommentId=12796898&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12796898 Somebody assign this to Aaron McCurry and we'll see if we can get enough votes on this issue to convince him to upload his patch.</description>
      <attachments/>
      <comments>44</comments>
      <commenters>12</commenters>
    </issue>
    <issue>
      <id>2217</id>
      <title>Remaining reallocation should use ArrayUtil.getNextSize()</title>
      <description>See recent discussion on ArrayUtils.getNextSize().</description>
      <attachments/>
      <comments>18</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2218</id>
      <title>ShingleFilter improvements</title>
      <description>ShingleFilter should allow configuration of minimum shingle size (in addition to maximum shingle size), so that it's possible to (e.g.) output only trigrams instead of bigrams mixed with trigrams. The token separator used in composing shingles should be configurable too.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2223</id>
      <title>ShingleFilter benchmark</title>
      <description>Spawned from LUCENE-2218: a benchmark for ShingleFilter, along with a new task to instantiate (non-default-constructor) ShingleAnalyzerWrapper: NewShingleAnalyzerTask. The included shingle.alg runs ShingleAnalyzerWrapper, wrapping the default StandardAnalyzer, with 4 different configurations over 10,000 Reuters documents each. To allow ShingleFilter timings to be isolated from the rest of the pipeline, StandardAnalyzer is also run over the same set of Reuters documents. This set of 5 runs is then run 5 times. The patch includes two perl scripts, the first to output JIRA table formatted timing information, with the minimum elapsed time for each of the 4 ShingleAnalyzerWrapper runs and the StandardAnalyzer run, and the second to compare two runs' JIRA output, producing another JIRA table showing % improvement.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2228</id>
      <title>AES Encrypted Directory</title>
      <description>Provides an encryption solution for Lucene indexes, using the AES encryption algorithm. You must have the JCE Unlimited Strength Jurisdiction Policy Files 6 Release Candidate which you can get from java.sun.com.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2230</id>
      <title>Lucene Fuzzy Search: BK-Tree can improve performance 3-20 times.</title>
      <description>W. Burkhard and R. Keller. Some approaches to best-match file searching, CACM, 1973 http://portal.acm.org/citation.cfm?doid=362003.362025 I was inspired by http://blog.notdot.net/2007/4/Damn-Cool-Algorithms-Part-1-BK-Trees (Nick Johnson, Google). Additionally, simplified algorythm at http://www.catalysoft.com/articles/StrikeAMatch.html seems to be much more logically correct than Levenstein distance, and it is 3-5 times faster (isolated tests). Big list od distance implementations: http://www.dcs.shef.ac.uk/~sam/stringmetrics.htm</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2232</id>
      <title>Use VShort to encode positions</title>
      <description>Improve decoding speed for typical case of two bytes for a delta position at the cost of increasing the size of the proximity file.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2234</id>
      <title>Hindi Analyzer</title>
      <description>An analyzer for hindi. below are MAP values on the FIRE 2008 test collection. QE means expansion with morelikethis, all defaults, on top 5 docs. setup T T(QE) TD TD(QE) TDN TDN(QE) words only 0.1646 0.1979 0.2241 0.2513 0.2468 0.2735 HindiAnalyzer 0.2875 0.3071 0.3387 0.3791* 0.3837 0.3810 improvement 74.67% 55.18% 51.14% 50.86% 55.47% 39.31% TD was the official measurement, highest score for this collection in FIRE 2008 was 0.3487: http://www.isical.ac.in/~fire/paper/mcnamee-jhu-fire2008.pdf needs a bit of cleanup and more tests</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2236</id>
      <title>Similarity can only be set per index, but I may want to adjust scoring behaviour at a field level</title>
      <description>Similarity can only be set per index, but I may want to adjust scoring behaviour at a field level, to faciliate this could we pass make field name available to all score methods. Currently it is only passed to some such as lengthNorm() but not others such as tf()</description>
      <attachments/>
      <comments>18</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2237</id>
      <title>allow TimeLimitedCollector timer thread to be shutdown</title>
      <description>When using Tomcat 6 and Solr 1.3 (with Lucene 2.4) we found that if we caused Tomcat to reload our .war files a number of times, we would eventually see PermGen memory errors where the JVM' s GC reported that all "permanent generation" memory had been consumed and none could be freed. This turns out to be a fairly common issue when using Tomcat's autoDeploy feature (or similar features of other application servers). See, for example: http://ampedandwired.dreamhosters.com/2008/05/09/causes-of-java-permgen-memory-leaks/ http://cornelcreanga.com/2009/02/how-to-prevent-memory-leaks-when-reloading-web-applications/ http://www.samaxes.com/2007/10/classloader-leaks-and-permgen-space/ http://blogs.sun.com/fkieviet/entry/how_to_fix_the_dreaded My understanding of the issue is that when reloading a webapp, Tomcat starts by releasing all of its references to the ClassLoader used to load the previous version of the application. Then it creates a new ClassLoader which reloads the application. The old ClassLoader and old version of the app are left to the garbage collector to be cleaned up. However, if the app itself hold references to the ClassLoader, the GC may not be able to ascertain that the ClassLoader is truly unused, in which case, it and the entire old version of app remain in memory. If one causes a sufficient number of app reloads, eventually PermGen space is exhausted. The particular issue we had with Solr and Lucene was that Lucene's TimeLimitedCollector creates a thread which is not shut down anywhere; this in turn seems to prevent Tomcat from unloading the ClassLoader. To solve this I applied a minor patch to TimeLimitedCollector which adds a flag variable controlling the timer thread's loop and some methods to set it so the thread will exit. The stopThread() method can then be called by an application such as Solr from a class registered as a servlet context listener; when the server is unloading the application the listener will execute and in turn stop the timer thread. My testing during multiple reloads of solr.war with and without these patches indicates that without them, we consistently get PermGen errors, and with them, once the PermGen is nearly exhausted (which may take a lot of reloads, e.g., 10-15!), the GC is able to free space and no PermGen errors occur.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2243</id>
      <title>FastVectorHighlighter: support DisjunctionMaxQuery</title>
      <description>Add DisjunctionMaxQuery support in FVH.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2252</id>
      <title>stored field retrieve slow</title>
      <description>IndexReader.document() on a stored field is rather slow. Did a simple multi-threaded test and profiled it: 40+% time is spent in getting the offset from the index file 30+% time is spent in reading the count (e.g. number of fields to load) Although I ran it on my lap top where the disk isn't that great, but still seems to be much room in improvement, e.g. load field index file into memory (for a 5M doc index, the extra memory footprint is 20MB, peanuts comparing to other stuff being loaded) A related note, are there plans to have custom segments as part of flexible indexing feature?</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2254</id>
      <title>Support more queries (other than just title) in Trec quality pkg</title>
      <description>Now that we can properly parse descriptions and narratives from TREC queries (LUCENE-2210), it would be nice to allow the user to easily run quality evaluations on more than just "Title" This patch adds an optional commandline argument to QueryDriver (the default is Title as before), where you can specify something like: T: Title-only D: Description-only N: Narrative-only TD: Title + Description, TDN: Title+Description+Narrative, DN: Description+Narrative The SimpleQQParser has an additional constructor that simply accepts a String[] of these fields, forming a booleanquery across all of them.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2257</id>
      <title>relax the per-segment max unique term limit</title>
      <description>Lucene can't handle more than 2.1B (limit of signed 32 bit int) unique terms in a single segment. But I think we can improve this to termIndexInterval (default 128) * 2.1B. There is one place (internal API only) where Lucene uses an int but should use a long.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2258</id>
      <title>Remove "synchonized" from FuzzyTermEnum#similarity(final String target)</title>
      <description>The similarity method in FuzzyTermEnum is synchronized which is stupid because of: TermEnums are the iterator pattern and so are single-thread per definition The method is private, so nobody could ever create a fake FuzzyTermEnum just to have this method and use it multithreaded. The method is not static and has no static fields - so instances do not affect each other The root of this comes from LUCENE-296, but was never reviewd and simply committed. The argument for making it synchronized is wrong.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2259</id>
      <title>add IndexWriter.removeUnferencedFiles, so apps can more immediately delete index files when readers are closed</title>
      <description>This has come up several times on the user's list. On Windows, which prevents deletion of still-open files, IndexWriter cannot remove files that are in-use by open IndexReaders. This is fine, and IndexWriter periodically retries the delete, but it doesn't retry very often (only on open, on flushing a new segment, and on committing a merge). So it lacks immediacy. With this expert method, apps that want faster deletion can call this method.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2261</id>
      <title>configurable MultiTermQuery TopTermsScoringBooleanRewrite pq size</title>
      <description>MultiTermQuery has a TopTermsScoringBooleanRewrite, that uses a priority queue to expand the query to the top-N terms. currently N is hardcoded at BooleanQuery.getMaxClauseCount(), but it would be nice to be able to set this for top-N MultiTermQueries: e.g. expand a fuzzy query to at most only the 50 closest terms. at a glance it seems one way would be to expose TopTermsScoringBooleanRewrite (it is private right now) and add a ctor to it, so a MultiTermQuery can instantiate one with its own limit.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2262</id>
      <title>QueryParser should now allow leading '?' wildcards</title>
      <description>QueryParser currently throws an exception if a wildcard term begins with the '?' operator. The current documentation describes why this is: When set, * or ? are allowed as the first character of a PrefixQuery and WildcardQuery. Note that this can produce very slow queries on big indexes. In the flexible indexing branch, wildcard queries with leading '?' operator are no longer slow on big indexes (they do not enumerate terms in linear fashion). Thus, it no longer makes sense to throw a ParseException for a leading '?' So, users should be able to perform a query of "?foo" and no longer get a ParseException from the QueryParser. For the flexible indexing branch, wildcard queries of 'foo?', '?foo', 'f?oo', etc are all the same from a performance perspective.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2265</id>
      <title>improve automaton performance by running on byte[]</title>
      <description>Currently, when enumerating terms, automaton must convert entire terms from flex's native utf-8 byte[] to char[] first, then step each char thru the state machine. we can make this more efficient, by allowing the state machine to run on byte[], so it can return true/false faster.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2267</id>
      <title>Add solr's artifact signing scripts into lucene's build.xml/common-build.xml</title>
      <description>Solr has nice artifact signing scripts in its common-build.xml and build.xml. For me as release manager of 3.0 it would have be good to have them also when building lucene artifacts. I will investigate how to add them to src artifacts and maven artifacts</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2271</id>
      <title>Function queries producing scores of -inf or NaN (e.g. 1/x) return incorrect results with TopScoreDocCollector</title>
      <description>This is a foolowup to LUCENE-2270, where a part of this problem was fixed (boost = 0 leading to NaN scores, which is also un-intuitive), but in general, function queries in Solr can create these invalid scores easily. In previous version of Lucene these scores ordered correct (except NaN, which mixes up results), but never invalid document ids are returned (like Integer.MAX_VALUE). The problem is: TopScoreDocCollector pre-fills the HitQueue with sentinel ScoreDocs with a score of -inf and a doc id of Integer.MAX_VALUE. For the HQ to work, this sentinel must be smaller than all posible values, which is not the case: -inf is equal and the document is not inserted into the HQ, as not competitive, but the HQ is not yet full, so the sentinel values keep in the HQ and result is the Integer.MAX_VALUE docs. This problem is solveable (and only affects the Ordered collector) by chaning the exit condition to: if (score &lt;= pqTop.score &amp;&amp; pqTop.doc != Integer.MAX_VALUE) { // Since docs are returned in-order (i.e., increasing doc Id), a document // with equal score to pqTop.score cannot compete since HitQueue favors // documents with lower doc Ids. Therefore reject those docs too. return; } The NaN case can be fixed in the same way, but then has another problem: all comparisons with NaN result in false (none of these is true): x &lt; NaN, x &gt; NaN, NaN == NaN. This leads to the fact that HQ's lessThan always returns false, leading to unexspected ordering in the PQ and sometimes the sentinel values do not stay at the top of the queue. A later hit then overrides the top of the queue but leaves the incorrect sentinels unchanged -&gt; invalid results. This can be fixed in two ways in HQ: Force all sentinels to the top: protected final boolean lessThan(ScoreDoc hitA, ScoreDoc hitB) { if (hitA.doc == Integer.MAX_VALUE) return true; if (hitB.doc == Integer.MAX_VALUE) return false; if (hitA.score == hitB.score) return hitA.doc &gt; hitB.doc; else return hitA.score &lt; hitB.score; } or alternatively have a defined order for NaN (Float.compare sorts them after +inf): protected final boolean lessThan(ScoreDoc hitA, ScoreDoc hitB) { if (hitA.score == hitB.score) return hitA.doc &gt; hitB.doc; else return Float.compare(hitA.score, hitB.score) &lt; 0; } The problem with both solutions is, that we have now more comparisons per hit and the use of sentinels is questionable. I would like to remove the sentinels and use the old pre 2.9 code for comparing and using PQ.add() when a competitive hit arrives. The order of NaN would be unspecified. To fix the order of NaN, it would be better to replace all score comparisons by Float.compare() [also in FieldComparator]. I would like to delay 2.9.2 and 3.0.1 until this problem is discussed and solved.</description>
      <attachments/>
      <comments>35</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2275</id>
      <title>DocumentsWriter.applyDeletes should not create TermDocs or IndexSearcher if not needed</title>
      <description>DocumentsWriter.applyDeletes(IndexReader, int) always creates TermDocs and IndexSearcher, even if there were no deletes by Term or by Query. The attached patch wraps those creations w/ checks on whether there were any deletes by these two. Additionally, the searcher wasn't closed in a finally block, so I fixed that as well. I'll attach a patch shortly.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2279</id>
      <title>eliminate pathological performance on StopFilter when using a Set&lt;String&gt; instead of CharArraySet</title>
      <description>passing a Set&lt;Srtring&gt; to a StopFilter instead of a CharArraySet results in a very slow filter. this is because for each document, Analyzer.tokenStream() is called, which ends up calling the StopFilter (if used). And if a regular Set&lt;String&gt; is used in the StopFilter all the elements of the set are copied to a CharArraySet, as we can see in it's ctor: public StopFilter(boolean enablePositionIncrements, TokenStream input, Set stopWords, boolean ignoreCase) { super(input); if (stopWords instanceof CharArraySet) { this.stopWords = (CharArraySet)stopWords; } else { this.stopWords = new CharArraySet(stopWords.size(), ignoreCase); this.stopWords.addAll(stopWords); } this.enablePositionIncrements = enablePositionIncrements; init(); } i feel we should make the StopFilter signature specific, as in specifying CharArraySet vs Set, and there should be a JavaDoc warning on using the other variants of the StopFilter as they all result in a copy for each invocation of Analyzer.tokenStream().</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2281</id>
      <title>Add doBeforeFlush to IndexWriter</title>
      <description>IndexWriter has doAfterFlush which can be overridden by extensions in order to perform operations after flush has been called. Since flush is final, one can only override doAfterFlush. This issue will handle two things: Make doAfterFlush protected, instead of package-private, to allow for easier extendability of IW. Add doBeforeFlush which will be called by flush before it starts, to allow extensions to perform any operations before flush begings. Will post a patch shortly. BTW, any chance to get it out in 3.0.1?</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2282</id>
      <title>Expose IndexFileNames as public, and make use of its methods in the code</title>
      <description>IndexFileNames is useful for applications that extend Lucene, an in particular those who extend Directory or IndexWriter. It provides useful constants and methods to query whether a certain file is a core Lucene file or not. In addition, IndexFileNames should be used by Lucene's code to generate segment file names, or query whether a certain file matches a certain extension. I'll post the patch shortly.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2285</id>
      <title>Code cleanup from all sorts of (trivial) warnings</title>
      <description>I would like to do some code cleanup and remove all sorts of trivial warnings, like unnecessary casts, problems w/ javadocs, unused variables, redundant null checks, unnecessary semicolon etc. These are all very trivial and should not pose any problem. I'll create another issue for getting rid of deprecated code usage, like LuceneTestCase and all sorts of deprecated constructors. That's also trivial because it only affects Lucene code, but it's a different type of change. Another issue I'd like to create is about introducing more generics in the code, where it's missing today - not changing existing API. There are many places in the code like that. So, with you permission, I'll start with the trivial ones first, and then move on to the others.</description>
      <attachments/>
      <comments>58</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2286</id>
      <title>enable DefaultSimilarity.setDiscountOverlaps by default</title>
      <description>I think we should enable setDiscountOverlaps in DefaultSimilarity by default. If you are using synonyms or commongrams or a number of other 0-posInc-term-injecting methods, these currently screw up your length normalization. These terms have a position increment of zero, so they shouldnt count towards the length of the document. I've done relevance tests with persian showing the difference is significant, and i think its a big trap to anyone using synonyms, etc: your relevance can actually get worse if you don't flip this boolean flag.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2287</id>
      <title>Unexpected terms are highlighted within nested SpanQuery instances</title>
      <description>I haven't yet been able to resolve why I'm seeing spurious highlighting in nested SpanQuery instances. Briefly, the issue is illustrated by the second instance of "Lucene" being highlighted in the test below, when it doesn't satisfy the inner span. There's been some discussion about this on the java-dev list, and I'm opening this issue now because I have made some initial progress on this. This new test, added to the HighlighterTest class in lucene_2_9_1, illustrates this: /* Ref: http://www.lucidimagination.com/blog/2009/07/18/the-spanquery/ */ public void testHighlightingNestedSpans2() throws Exception { String theText = "The Lucene was made by Doug Cutting and Lucene great Hadoop was"; // Problem //String theText = "The Lucene was made by Doug Cutting and the great Hadoop was"; // Works okay String fieldName = "SOME_FIELD_NAME"; SpanNearQuery spanNear = new SpanNearQuery(new SpanQuery[] { new SpanTermQuery(new Term(fieldName, "lucene")), new SpanTermQuery(new Term(fieldName, "doug")) } , 5, true); Query query = new SpanNearQuery(new SpanQuery[] { spanNear, new SpanTermQuery(new Term(fieldName, "hadoop")) } , 4, true); String expected = "The &lt;B&gt;Lucene&lt;/B&gt; was made by &lt;B&gt;Doug&lt;/B&gt; Cutting and Lucene great &lt;B&gt;Hadoop&lt;/B&gt; was"; //String expected = "The &lt;B&gt;Lucene&lt;/B&gt; was made by &lt;B&gt;Doug&lt;/B&gt; Cutting and the great &lt;B&gt;Hadoop&lt;/B&gt; was"; String observed = highlightField(query, fieldName, theText); System.out.println("Expected: \"" + expected + "\n" + "Observed: \"" + observed); assertEquals("Why is that second instance of the term \"Lucene\" highlighted?", expected, observed); } Is this an issue that's arisen before? I've been reading through the source to QueryScorer, WeightedSpanTerm, WeightedSpanTermExtractor, Spans, and NearSpansOrdered, but haven't found the solution yet. Initially, I thought that the extractWeightedSpanTerms method in WeightedSpanTermExtractor should be called on each clause of a SpanNearQuery or SpanOrQuery, but that didn't get me too far.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2288</id>
      <title>Create EMPTY_ARGS constsant in SnowballProgram instead of allocating new Object[0]</title>
      <description>Instead of allocating new Object[0] create a proper constant in SnowballProgram. The same (for new Class[0]) is created in Among, although it's less critical because Among is called from static initializers ... Patch will follow shortly.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2289</id>
      <title>Calls to SegmentInfos.message should be wrapped w/ infoStream != null checks</title>
      <description>To avoid the expensive message creation (which involves the '+' operator on strings, calls to message should be wrapped w/ infoStream != null check, rather than inside message(). I'll attach a patch which does that.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2290</id>
      <title>Remove unnecessary String concatenation in IndexWriter</title>
      <description>I've noticed a couple of places in IndexWriter where a boolean string is created by bool + "", or integer by int + "". There are some places (in setDiagonstics) where a string is concatenated with an empty String ... The patch uses Boolean.toString and Integer.toString, as well as remove the unnecessary str + "".</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2291</id>
      <title>nighly build should regen all gen'd sources</title>
      <description>Just opening an issue to track this (idea was spun out of LUCENE-2285)... I don't have time to work on it now... We have a number of sources that are generated (eg using tools like JFlex, JavaCC. I think we should strive to have the nightly build recreate these files, if possible. This would help us catch mistakes more quickly, eg where we accidentally make a fix to the generated file. We'd have to get jflex/javacc/etc. installed onto lucene.zones to do this (not sure if they are already).</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2292</id>
      <title>ByteBuffer Directory - allowing to store the index outside the heap</title>
      <description>A byte buffer based directory with the benefit of being able to create direct byte buffer thus storing the index outside the JVM heap.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2294</id>
      <title>Create IndexWriterConfiguration and store all of IW configuration there</title>
      <description>I would like to factor out of all IW configuration parameters into a single configuration class, which I propose to name IndexWriterConfiguration (or IndexWriterConfig). I want to store there almost everything besides the Directory, and to reduce all the ctors down to one: IndexWriter(Directory, IndexWriterConfiguration). What I was thinking of storing there are the following parameters: All of ctors parameters, except for Directory. The different setters where it makes sense. For example I still think infoStream should be set on IW directly. I'm thinking that IWC should expose everything in a setter/getter methods, and defaults to whatever IW defaults today. Except for Analyzer which will need to be defined in the ctor of IWC and won't have a setter. I am not sure why MaxFieldLength is required in all IW ctors, yet IW declares a DEFAULT (which is an int and not MaxFieldLength). Do we still think that 10000 should be the default? Why not default to UNLIMITED and otherwise let the application decide what LIMITED means for it? I would like to make MFL optional on IWC and default to something, and I hope that default will be UNLIMITED. We can document that on IWC, so that if anyone chooses to move to the new API, he should be aware of that ... I plan to deprecate all the ctors and getters/setters and replace them by: One ctor as described above getIndexWriterConfiguration, or simply getConfig, which can then be queried for the setting of interest. About the setters, I think maybe we can just introduce a setConfig method which will override everything that is overridable today, except for Analyzer. So someone could do iw.getConfig().setSomething(); iw.setConfig(newConfig); The setters on IWC can return an IWC to allow chaining set calls ... so the above will turn into iw.setConfig(iw.getConfig().setSomething1().setSomething2()); BTW, this is needed for Parallel Indexing (see LUCENE-1879), but I think it will greatly simplify IW's API. I'll start to work on a patch.</description>
      <attachments/>
      <comments>57</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2295</id>
      <title>Create a MaxFieldLengthAnalyzer to wrap any other Analyzer and provide the same functionality as MaxFieldLength provided on IndexWriter</title>
      <description>A spinoff from LUCENE-2294. Instead of asking the user to specify on IndexWriter his requested MFL limit, we can get rid of this setting entirely by providing an Analyzer which will wrap any other Analyzer and its TokenStream with a TokenFilter that keeps track of the number of tokens produced and stop when the limit has reached. This will remove any count tracking in IW's indexing, which is done even if I specified UNLIMITED for MFL. Let's try to do it for 3.1.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2296</id>
      <title>Include UML diagrams in javadocs</title>
      <description>As Lucene source becomes more and more complex it would be helpful to provide a visual overview of Lucene classes in a form of UML class diagrams. These can be created automatically during the build process, using ApiViz (http://code.google.com/p/apiviz/).</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2297</id>
      <title>IndexWriter should let you optionally enable reader pooling</title>
      <description>For apps using a large index and frequently need to commit and resolve deletes, the cost of opening the SegmentReaders on demand for every commit can be prohibitive. We an already pool readers (NRT does so), but, we only turn it on if NRT readers are in use. We should allow separate control. We should do this after LUCENE-2294.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2298</id>
      <title>Polish Analyzer</title>
      <description>Andrzej Bialecki has written a Polish stemmer and provided stemming tables for it under Apache License. You can read more about it here: http://www.getopt.org/stempel/ In reality, the stemmer is general code and we could use it for more languages too perhaps.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2301</id>
      <title>search for &amp; fix all "TODO 4.0" comments before releasing 4.0</title>
      <description>Let's try to use the specific string?: TODO 4.0 to mark any place where we must do something for 4.0?</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2302</id>
      <title>Replacement for TermAttribute+Impl with extended capabilities (byte[] support, CharSequence, Appendable)</title>
      <description>For flexible indexing terms can be simple byte[] arrays, while the current TermAttribute only supports char[]. This is fine for plain text, but e.g NumericTokenStream should directly work on the byte[] array. Also TermAttribute lacks of some interfaces that would make it simplier for users to work with them: Appendable and CharSequence I propose to create a new interface "CharTermAttribute" with a clean new API that concentrates on CharSequence and Appendable. The implementation class will simply support the old and new interface working on the same term buffer. DEFAULT_ATTRIBUTE_FACTORY will take care of this. So if somebody adds a TermAttribute, he will get an implementation class that can be also used as CharTermAttribute. As both attributes create the same impl instance both calls to addAttribute are equal. So a TokenFilter that adds CharTermAttribute to the source will work with the same instance as the Tokenizer that requested the (deprecated) TermAttribute. To also support byte[] only terms like Collation or NumericField needs, a separate getter-only interface will be added, that returns a reusable BytesRef, e.g. BytesRefGetterAttribute. The default implementation class will also support this interface. For backwards compatibility with old self-made-TermAttribute implementations, the indexer will check with hasAttribute(), if the BytesRef getter interface is there and if not will wrap a old-style TermAttribute (a deprecated wrapper class will be provided): new BytesRefGetterAttributeWrapper(TermAttribute), that is used by the indexer then.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2304</id>
      <title>FuzzyLikeThisQuery should set MaxNonCompetitiveBoost for faster speed</title>
      <description>FuzzyLikeThisQuery uses FuzzyTermsEnum directly, and maintains a priority queue for its purposes. Just like TopTermsRewrite method, it should set the MaxNonCompetitiveBoost attribute, so that FuzzyTermsEnum can run faster. Its already tracking the minScore, just not updating the attribute. This would be especially nice as it appears to have nice defaults already (pq size of 50)</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2305</id>
      <title>Introduce Version in more places long before 4.0</title>
      <description>We need to introduce Version in as many places as we can (wherever it makes sense of course), and preferably long before 4.0 (or shall I say 3.9?) is out. That way, we can have a bunch of deprecated API now, that will be gone in 4.0, rather than doing it one class at a time and never finish . The purpose is to introduce Version wherever it is mandatory now, and also in places where we think it might be useful in the future (like most of our Analyzers, configured classes and configuration classes). I marked this issue for 3.1, though I don't expect it to end in 3.1. I still think it will be done one step at a time, perhaps for cluster of classes together. But on the other hand I don't want to mark it for 4.0.0 because that needs to be resolved much sooner. So if I had a 3.9 version defined, I'd mark it for 3.9. We can do several commits in one issue right? So this one can live for a while in JIRA, while we gradually convert more and more classes. The first candidate is InstantiatedIndexWriter which probably should take an IndexWriterConfig. While I converted the code to use IWC, I've noticed Instantiated defaults its maxFieldLength to the current default (10,000) which is deprecated. I couldn't change it for back-compat reasons. But we can upgrade it to accept IWC, and set to unlimited if the version is onOrAfter 3.1, otherwise stay w/ the deprecated default. if it's acceptable to have several commits in one issue, I can start w/ Instantiated, post a patch and then we can continue to more classes.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2306</id>
      <title>contrib/xml-query-parser: NumericRangeQuery and -Filter support</title>
      <description>Create a FilterBuilder for NumericRangeFilter so that it may be used with the XML query parser.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2308</id>
      <title>Separately specify a field's type</title>
      <description>This came up from dicussions on IRC. I'm summarizing here... Today when you make a Field to add to a document you can set things index or not, stored or not, analyzed or not, details like omitTfAP, omitNorms, index term vectors (separately controlling offsets/positions), etc. I think we should factor these out into a new class (FieldType?). Then you could re-use this FieldType instance across multiple fields. The Field instance would still hold the actual value. We could then do per-field analyzers by adding a setAnalyzer on the FieldType, instead of the separate PerFieldAnalzyerWrapper (likewise for per-field codecs (with flex), where we now have PerFieldCodecWrapper). This would NOT be a schema! It's just refactoring what we already specify today. EG it's not serialized into the index. This has been discussed before, and I know Michael Busch opened a more ambitious (I think?) issue. I think this is a good first baby step. We could consider a hierarchy of FIeldType (NumericFieldType, etc.) but maybe hold off on that for starters...</description>
      <attachments/>
      <comments>200</comments>
      <commenters>14</commenters>
    </issue>
    <issue>
      <id>2309</id>
      <title>Fully decouple IndexWriter from analyzers</title>
      <description>IndexWriter only needs an AttributeSource to do indexing. Yet, today, it interacts with Field instances, holds a private analyzers, invokes analyzer.reusableTokenStream, has to deal with a wide variety (it's not analyzed; it is analyzed but it's a Reader, String; it's pre-analyzed). I'd like to have IW only interact with attr sources that already arrived with the fields. This would be a powerful decoupling – it means others are free to make their own attr sources. They need not even use any of Lucene's analysis impls; eg they can integrate to other things like OpenPipeline. Or make something completely custom. LUCENE-2302 is already a big step towards this: it makes IW agnostic about which attr is "the term", and only requires that it provide a BytesRef (for flex). Then I think LUCENE-2308 would get us most of the remaining way – ie, if the FieldType knows the analyzer to use, then we could simply create a getAttrSource() method (say) on it and move all the logic IW has today onto there. (We'd still need existing IW code for back-compat).</description>
      <attachments/>
      <comments>63</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>2311</id>
      <title>Pass potent SR to IRWarmer.warm(), and also call warm() for new segments</title>
      <description>Currently warm() receives a SegmentReader without terms index and docstores. It would be arguably more useful for the app to receive a fully loaded reader, so it can actually fire up some caches. If the warmer is undefined on IW, we probably leave things as they are. It is also arguably more concise and clear to call warm() on all newly created segments, so there is a single point of warming readers in NRT context, and every subreader coming from getReader is guaranteed to be warmed up -&gt; you don't have to introduce even more mess in your code by rechecking it.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2312</id>
      <title>Search on IndexWriter's RAM Buffer</title>
      <description>In order to offer user's near realtime search, without incurring an indexing performance penalty, we can implement search on IndexWriter's RAM buffer. This is the buffer that is filled in RAM as documents are indexed. Currently the RAM buffer is flushed to the underlying directory (usually disk) before being made searchable. Todays Lucene based NRT systems must incur the cost of merging segments, which can slow indexing. Michael Busch has good suggestions regarding how to handle deletes using max doc ids. https://issues.apache.org/jira/browse/LUCENE-2293?focusedCommentId=12841923&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12841923 The area that isn't fully fleshed out is the terms dictionary, which needs to be sorted prior to queries executing. Currently IW implements a specialized hash table. Michael B has a suggestion here: https://issues.apache.org/jira/browse/LUCENE-2293?focusedCommentId=12841915&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12841915</description>
      <attachments/>
      <comments>98</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2313</id>
      <title>Add VERBOSE to LuceneTestCase and LuceneTestCaseJ4</title>
      <description>component-build.xml allows to define tests.verbose as a system property when running tests. Both LuceneTestCase and LuceneTestCaseJ4 don't read that property. It will be useful for overriding tests to access one place for this setting (I believe currently some tests do it on their own). Then (as a separate issue) we can move all tests that don't check the parameter to only print if VERBOSE is true. I will post a patch soon.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2314</id>
      <title>Add AttributeSource.copyTo(AttributeSource)</title>
      <description>One problem with AttributeSource at the moment is the missing "insight" into AttributeSource.State. If you want to create TokenStreams that inspect cpatured states, you have no chance. Making the contents of State public is a bad idea, as it does not help for inspecting (its a linked list, so you have to iterate). AttributeSource currently contains a cloneAttributes() call, which returns a new AttrubuteSource with all current attributes cloned. This is the (more expensive) captureState. The problem is that you cannot copy back the cloned AS (which is the restoreState). To use this behaviour (by the way, ShingleMatrix can use it), one can alternatively use cloneAttributes and copyTo. You can easily change the cloned attributes and store them in lists and copy them back. The only problem is lower performance of these calls (as State is a very optimized class). One use case could be: AttributeSource state = cloneAttributes(); // .... do something ... state.getAttribute(TermAttribute.class).setTermBuffer(foobar); // ... more work state.copyTo(this);</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2316</id>
      <title>Define clear semantics for Directory.fileLength</title>
      <description>On this thread: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201003.mbox/%3C126142c1003121525v24499625u1589bbef4c0792e7@mail.gmail.com%3E it was mentioned that Directory's fileLength behavior is not consistent between Directory implementations if the given file name does not exist. FSDirectory returns a 0 length while RAMDirectory throws FNFE. The problem is that the semantics of fileLength() are not defined. As proposed in the thread, we'll define the following semantics: Returns the length of the file denoted by &lt;code&gt;name&lt;/code&gt; if the file exists. The return value may be anything between 0 and Long.MAX_VALUE. Throws FileNotFoundException if the file does not exist. Note that you can call dir.fileExists(name) if you are not sure whether the file exists or not. For backwards we'll create a new method w/ clear semantics. Something like: /** * @deprecated the method will become abstract when #fileLength(name) has been removed. */ public long getFileLength(String name) throws IOException { long len = fileLength(name); if (len == 0 &amp;&amp; !fileExists(name)) { throw new FileNotFoundException(name); } return len; } The first line just calls the current impl. If it throws exception for a non-existing file, we're ok. The second line verifies whether a 0 length is for an existing file or not and throws an exception appropriately.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2317</id>
      <title>allow separate control of whether docTermFreq and positions are indexed</title>
      <description>[Spinoff of LUCENE-2308... we keep spinning things off... I feel like we live inside a particle accelerator] Right now Lucene indexes the docTermFreq and positions into the postings, by default. You can use omitTFAP to turn them both off, which if you also omit norms gives you "match only" scoring. But, really, they ought to be separately controllable – one may want to include docTermFreq but not positions, to get full scoring for non-positional phrases. Probably we should wait until LUCENE-2308 is done, and make the API change on *FieldType.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2319</id>
      <title>IndexReader # doCommit - typo nit about v3.0 in trunk</title>
      <description>Trunk is already in 3.0.1+ . But the documentation says - "In 3.0, this will become ... ". Since it is already in 3.0, it might as well be removed.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2320</id>
      <title>Add MergePolicy to IndexWriterConfig</title>
      <description>Now that IndexWriterConfig is in place, I'd like to move MergePolicy to it as well. The change is not straightforward and so I've kept it for a separate issue. MergePolicy requires in its ctor an IndexWriter, however none can be passed to it before an IndexWriter actually exists. And today IW may create an MP just for it to be overridden by the application one line afterwards. I don't want to make iw member of MP non-final, or settable by extending classes, however it needs to remain protected so they can access it directly. So the proposed changes are: Add a SetOnce object (to o.a.l.util), or Immutable, which can only be set once (hence its name). It'll have the signature SetOnce&lt;T&gt; w/ synchronized set&lt;T&gt; and T get(). T will be declared volatile, so that get() won't be synchronized. MP will define a protected final SetOnce&lt;IndexWriter&gt; writer instead of the current writer. NOTE: this is a bw break. any suggestions are welcomed. MP will offer a public default ctor, together with a set(IndexWriter). IndexWriter will set itself on MP using set(this). Note that if set will be called more than once, it will throw an exception (AlreadySetException - or does someone have a better suggestion, preferably an already existing Java exception?). That's the core idea. I'd like to post a patch soon, so I'd appreciate your review and proposals.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2321</id>
      <title>use packed ints for the terms dict index</title>
      <description>Terms dict index needs to store large RAM resident arrays of ints, but, because their size is bound &amp; variable (depending on the segment/docs), we should used packed ints for them.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2323</id>
      <title>reorganize contrib modules</title>
      <description>it would be nice to reorganize contrib modules, so that they are bundled together by functionality. For example: the wikipedia contrib is a tokenizer, i think really belongs in contrib/analyzers there are two highlighters, i think could be one highlighters package. there are many queryparsers and queries in different places in contrib</description>
      <attachments/>
      <comments>26</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>2324</id>
      <title>Per thread DocumentsWriters that write their own private segments</title>
      <description>See LUCENE-2293 for motivation and more details. I'm copying here Mike's summary he posted on 2293: Change the approach for how we buffer in RAM to a more isolated approach, whereby IW has N fully independent RAM segments in-process and when a doc needs to be indexed it's added to one of them. Each segment would also write its own doc stores and "normal" segment merging (not the inefficient merge we now do on flush) would merge them. This should be a good simplification in the chain (eg maybe we can remove the *PerThread classes). The segments can flush independently, letting us make much better concurrent use of IO &amp; CPU.</description>
      <attachments/>
      <comments>241</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>2326</id>
      <title>Remove SVN.exe and revision numbers from build.xml by svn-copy the backwards branch and linking snowball tests by svn:externals</title>
      <description>As we often need to update backwards tests together with trunk and always have to update the branch first, record rev no, and update build xml, I would simply like to do a svn copy/move of the backwards branch. After a release, this is simply also done: svn rm backwards svn cp releasebranch backwards By this we can simply commit in one pass, create patches in one pass. The snowball tests are currently downloaded by svn.exe, too. These need a fixed version for checkout. I would like to change this to use svn:externals. Will provide patch, soon.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2329</id>
      <title>Use parallel arrays instead of PostingList objects</title>
      <description>This is Mike's idea that was discussed in LUCENE-2293 and LUCENE-2324. In order to avoid having very many long-living PostingList objects in TermsHashPerField we want to switch to parallel arrays. The termsHash will simply be a int[] which maps each term to dense termIDs. All data that the PostingList classes currently hold will then we placed in parallel arrays, where the termID is the index into the arrays. This will avoid the need for object pooling, will remove the overhead of object initialization and garbage collection. Especially garbage collection should benefit significantly when the JVM runs out of memory, because in such a situation the gc mark times can get very long if there is a big number of long-living objects in memory. Another benefit could be to build more efficient TermVectors. We could avoid the need of having to store the term string per document in the TermVector. Instead we could just store the segment-wide termIDs. This would reduce the size and also make it easier to implement efficient algorithms that use TermVectors, because no term mapping across documents in a segment would be necessary. Though this improvement we can make with a separate jira issue.</description>
      <attachments/>
      <comments>42</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2330</id>
      <title>Allow easy extension of IndexWriter</title>
      <description>IndexWriter is not so easy to extend. It hides a lot of useful methods from extending classes as well as useful members (like infoStream). Most of this stuff is very straightforward and I believe it's not exposed for no particular reason. Over in LUCENE-1879 I plan extend IndexWriter to provide a ParallelWriter which will support the parallel indexing requirements. For that I'll need access to several methods and members. I plan to contain in this issue some simple hooks, nothing fancy (and hopefully controversial). I'll leave the rest to specific issues. For now: Introduce a protected default constructor and init(Directory, IndexWriterConfig). That's required because ParallelWriter does not itself index anything, but instead delegates to its Slices. So that ctor is for convenience only, and I'll make it clear (through javadocs) that if one uses it, one needs to call init(). PQ has the same pattern. Expose some members and methods that are useful for extensions (such as config, infoStream etc.). Some candidates are package-private methods, but these will be reviewed and converted on a case by case basis. I don't plan to do anything drastic here, just prepare IW for easier extendability. I'll post a patch after LUCENE-2320 is committed.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2331</id>
      <title>Add NoOpMergePolicy</title>
      <description>I'd like to add a simple and useful MP implementation which does .... nothing ! . I've came across many places where either the following is documented or implemented: "if you want to prevent merges, set mergeFactor to a high enough value". I think a NoOpMergePolicy is just as good, and can REALLY allow you disable merges (except for maybe set mergeFactor to Int.MAX_VAL). As such, NoOpMergePolicy will be introduced as a singleton, and can be used for convenience purposes only. Also, for Parallel Index it's important, because I'd like the slices to never do any merges, unless ParallelWriter decides so. So they should be set w/ that MP. I have a patch ready. Waiting for LUCENE-2320 to go in, so that I don't need to change it afterwards. About the name - I like the name, but suggestions are welcome. I thought of a NullMergePolicy, but I don't like 'Null' used for a NoOp.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2332</id>
      <title>Merge CharTermAttribute and deprecations to stable</title>
      <description>This should be merged to trunk until flex lands, so the analyzers can be ported to new api.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2334</id>
      <title>IndexReader.close() should call IndexReader.decRef() unconditionally ??</title>
      <description>IndexReader.close() is defined: /** * Closes files associated with this index. * Also saves any new deletions to disk. * No other methods should be called after this has been called. * @throws IOException if there is a low-level IO error */ public final synchronized void close() throws IOException { if (!closed) { decRef(); closed = true; } } This means that if the refCount is bigger than one, close() does not actually close, but it is also true that calling close() again has no effect. Why does close() not simply call decRef() unconditionally? This way if incRef() is called each time an instance of IndexReader were handed out, if close() is called by each recipient when they are done, the last one to call close will actually close the index. As written it seems the API is very confusing – the first close() does one thing, but the next close() does something different. At a minimum the JavaDoc should clarify the behavior.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2335</id>
      <title>optimization: when sorting by field, if index has one segment and field values are not needed, do not load String[] into field cache</title>
      <description>Spinoff from java-dev thread "Sorting with little memory: A suggestion", started by Toke Eskildsen. When sorting by SortField.STRING we currently ask FieldCache for a StringIndex on that field. This can consumes tons of RAM, when the values are mostly unique (eg a title field), as it populates both int[] ords as well as String[] values. But, if the index is only one segment, and the search sets fillFields=false, we don't need the String[] values, just the int[] ords. If the app needs to show the fields it can pull them (for the 1 page) from stored fields. This can be a potent optimization – alot of RAM saved – for optimized indexes. When fixing this we must take care to share the int[] ords if some queries do fillFields=true and some =false... ie, FieldCache will be called twice and it should share the int[] ords across those invocations.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2339</id>
      <title>Allow Directory.copy() to accept a collection of file names to be copied</title>
      <description>Par example, I want to copy files pertaining to a certain commit, and not everything there is in a Directory.</description>
      <attachments/>
      <comments>32</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2341</id>
      <title>explore morfologik integration</title>
      <description>Dawid Weiss mentioned on LUCENE-2298 that there is another Polish stemmer available: http://sourceforge.net/projects/morfologik/ This works differently than LUCENE-2298, and ideally would be another option for users.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2343</id>
      <title>Add support for benchmarking Collectors</title>
      <description>As the title says.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2346</id>
      <title>Explore other in-memory postinglist formats for realtime search</title>
      <description>The current in-memory posting list format might not be optimal for searching. VInt decoding performance and the lack of skip lists would arguably be the biggest bottlenecks. For LUCENE-2312 we should investigate other formats. Some ideas: PFOR or packed ints for posting slices? Maybe even int[] slices instead of byte slices? This would be great for search performance, but the additional memory overhead might not be acceptable. For realtime search it's usually desirable to evaluate the most recent documents first. So using backward pointers instead of forward pointers and having the postinglist pointer point to the most recent docID in a list is something to consider. Skipping: if we use fixed-length postings ([packed] ints) we can do binary search within a slice. We can also locate a pointer then without scanning and thus skip entire slices quickly. Is that sufficient or would we need more skipping layers, so that it's possible to skip directly to particular slices? It would be awesome to find a format that doesn't slow down "normal" indexing, but is very efficient for in-memory searches. If we can't find such a fits-all format, we should have a separate indexing chain for real-time indexing.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2347</id>
      <title>Dump WordNet to SOLR Synonym format</title>
      <description>This enhancement allows you to dump v2 of WordNet to SOLR synonym format! Get all your syns loaded easily. 1. You can load all synonyms from http://wordnetcode.princeton.edu/2.0/ WordNet V2 to SOLR by first using the Sys2Index program http://lucene.apache.org/java/2_2_0/api/org/apache/lucene/wordnet/Syns2Index.html Get WNprolog from http://wordnetcode.princeton.edu/2.0/ 2. We modified this program to work with SOLR (See attached) on amidev.kaango.com in /vol/src/lucene/contrib/wordnet vi /vol/src/lucene/contrib/wordnet/src/java/org/apache/lucene/wordnet/Syns2Solr.java 3. Run ant 4. java -classpath /vol/src/lucene/build/contrib/wordnet/lucene-wordnet-3.1-dev.jar org.apache.lucene.wordnet.Syns2Solr prolog/wn_s.pl solr &gt; index_synonyms.txt</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2350</id>
      <title>Refactor/Cleanup Lucene Spatial</title>
      <description>Lucene spatial needs a lot of work. We should clean it up and allow for people to use different distances (i.e. don't assume Haversine), etc. We should also merge the Solr and Lucene code into a single lib, where possible (starting w/ Distance Utils). Update the distance filter to allow for pluggable distance measures. Also do things like not assume everything is in degrees (users may already store radians, for instance) and use constants for conversions/multiplications instead of division. End goal: No more experimental status. Clean up the APIs, use the more common nomenclature for "tiers" and be consistent across Lucene and Solr.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2351</id>
      <title>optimize automatonquery</title>
      <description>Mike found a few cases in flex where we have some bad behavior with automatonquery. The problem is similar to a database query planner, where sometimes simply doing a full table scan is faster than using an index. We can optimize automatonquery a little bit, and get better performance for fuzzy,wildcard,regex queries. Here is a list of ideas: create commonSuffixRef for infinite automata, not just really-bad linear scan cases do a null check rather than populating an empty commonSuffixRef localize the 'linear' case to not seek, but instead scan, when ping-ponging against loops in the state machine add a mechanism to enable/disable the terms dict cache, e.g. we can disable it for infinite cases, and maybe fuzzy N&gt;1 also. change the use of BitSet to OpenBitSet or long[] gen for path-tracking optimize the backtracking code where it says /* String is good to go as-is */, this need not be a full run(), I think...</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2354</id>
      <title>Convert NumericUtils and NumericTokenStream to use BytesRef instead of Strings/char[]</title>
      <description>After LUCENE-2302, we should use TermToBytesRefAttribute to index using NumericTokenStream. This also should convert the whole NumericUtils to use BytesRef when converting numerics.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2355</id>
      <title>Refactor Directory/Multi/SegmentReader creation/reopening/cloning/closing</title>
      <description>*Reader lifecycle evolved over time to become some heavily tangled mess. It's hard to understand what's going on there, it's even harder to add some fields/logic while ensuring that all possible code paths preserve these fields/interact with the logic properly. While some of said mess is justified by the task at hand, a big part is just badly done copypaste and can be removed. I am currently refactoring this and intended to open an issue with a working patch, but the task winded up somewhat bigger than I expected, so I'm opening it earlier to track stuff encountered/changed/fixed. The list is by no means exhaustive. an iteration to create SRs is copypasted several times, one of them (IW) with wrong iteration bound it is also overly complex and can be folded for create/reopen cases readers sent to IndexReaderWarmer are termindexless/docstoreless on some occasions it is possible to clone() your way to readwrite NRT reader IndexDeletionPolicy is not always preserved through clones/reopens cloned readers share CoreReaders and, consequently, updated termsIndex/docStores threadlocal versions of fieldsReader/termsVector are bound to SR, not CoreReaders and thus are recreated on clone/reopen double-initialization for some fields (someone got lost and did this to be sure I guess), stupid assert checks ( qwe = new(); assert qwe != null ) SR is not always recreated when compound status of underlying segment changes deleting already deleted doc marks deletions dirty and rewrites them lots of synchronization is done around Reader, while it can be narrowed down to norms/deletions/whatever I did some structural modifications: CompositeReader extracts common code from DirectoryReader and MultiReader (complete) ReadonlyDirectoryReader and ReadonlySegmentReader are dead, MutableD/SReaders are introduced and carry all modification logic/fields (DR complete, SR in progress) WriterBackedReader encapsulates NRT reader logic (complete) CoreReaders split into CoreReaders, DocStores, TermInfos. All of these are immutable and SR is cloned when you need to change its mode (in progress)</description>
      <attachments/>
      <comments>7</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2357</id>
      <title>Reduce transient RAM usage while merging by using packed ints array for docID re-mapping</title>
      <description>We allocate this int[] to remap docIDs due to compaction of deleted ones. This uses alot of RAM for large segment merges, and can fail to allocate due to fragmentation on 32 bit JREs. Now that we have packed ints, a simple fix would be to use a packed int array... and maybe instead of storing abs docID in the mapping, we could store the number of del docs seen so far (so the remap would do a lookup then a subtract). This may add some CPU cost to merging but should bring down transient RAM usage quite a bit.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2360</id>
      <title>speedup recycling of per-doc RAM</title>
      <description>Robert found one source of slowness when indexing tiny docs, where we use List.toArray to recycle the byte[] buffers used by per-doc doc store state (stored field, term vectors). This was added in LUCENE-2283, so not yet released.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2362</id>
      <title>Add support for slow filters with batch processing</title>
      <description>Internal implementation of IndexSearch assumes that Filter and scorer has almost equal perfomance. But in our environment we have Filter implementation that is very expensive (in compare to scorer). if we have, let's say, 2k of termdocs selected by scorer (each ~250 docs) and 2k selected by filter, then 250k docs will be fastly checked (and filtered out) by scorer, and 250k docs will be slowly checked by our filter. Using straigthforward implementation makes search out of 60 seconds per query boundary, because each next() or advance() requires N queries to database PER CHECKED DOC. Using read ahead technique allows us to optimze it to 35 seconds per query. Still too slow. The solution to problem is firstly select all documents by scorer and filter them in batch by our filter. Example of implementation (with BitSet) in attachement. Currently it takes only ~300 millseconds per query.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2364</id>
      <title>Add support for terms in BytesRef format to Term, TermQuery, TermRangeQuery &amp; Co.</title>
      <description>It would be good to directly allow BytesRefs in TermQuery and TermRangeQuery (as both queries convert the strings to BytesRef internally). For NumericRange support in Solr it will be needed to support numerics as ByteRef in single-term queries. When this will be added, don't forget to change TestNumericRangeQueryXX to use the BytesRef ctor of TRQ.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2368</id>
      <title>stopword files should be versioned; acessor for default(s) should take a Version property</title>
      <description>The existing language specific stopword files on the trunk have no version info in their filenames – this will make it awkward/confusing to update them as time goes on. LIkewise, many classes have a "getDefaultStopSet()" which makes these methods (when called by client code) suffer from the same API back-compat issues that the Analyzers themselves did before we added Version.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2369</id>
      <title>Locale-based sort by field with low memory overhead</title>
      <description>The current implementation of locale-based sort in Lucene uses the FieldCache which keeps all sort terms in memory. Beside the huge memory overhead, searching requires comparison of terms with collator.compare every time, making searches with millions of hits fairly expensive. This proposed alternative implementation is to create a packed list of pre-sorted ordinals for the sort terms and a map from document-IDs to entries in the sorted ordinals list. This results in very low memory overhead and faster sorted searches, at the cost of increased startup-time. As the ordinals can be resolved to terms after the sorting has been performed, this approach supports fillFields=true. This issue is related to https://issues.apache.org/jira/browse/LUCENE-2335 which contain previous discussions on the subject.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2372</id>
      <title>Replace deprecated TermAttribute by new CharTermAttribute</title>
      <description>After LUCENE-2302 is merged to trunk with flex, we need to carry over all tokenizers and consumers of the TokenStreams to the new CharTermAttribute. We should also think about adding a AttributeFactory that creates a subclass of CharTermAttributeImpl that returns collation keys in toBytesRef() accessor. CollationKeyFilter is then obsolete, instead you can simply convert every TokenStream to indexing only CollationKeys by changing the attribute implementation.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2373</id>
      <title>Create a Codec to work with streaming and append-only filesystems</title>
      <description>Since early 2.x times Lucene used a skip/seek/write trick to patch the length of the terms dict into a place near the start of the output data file. This however made it impossible to use Lucene with append-only filesystems such as HDFS. In the post-flex trunk the following code in StandardTermsDictWriter initiates this: // Count indexed fields up front CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); out.writeLong(0); // leave space for end index pointer and completes this in close(): out.seek(CodecUtil.headerLength(CODEC_NAME)); out.writeLong(dirStart); I propose to change this layout so that this pointer is stored simply at the end of the file. It's always 8 bytes long, and we known the final length of the file from Directory, so it's a single additional seek(length - 8) to read it, which is not much considering the benefits.</description>
      <attachments/>
      <comments>29</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2374</id>
      <title>Add reflection API to AttributeSource/AttributeImpl</title>
      <description>AttributeSource/TokenStream inspection in Solr needs to have some insight into the contents of AttributeImpls. As LUCENE-2302 has some problems with toString() [which is not structured and conflicts with CharSequence's definition for CharTermAttribute], I propose an simple API that get a default implementation in AttributeImpl (just like toString() current): Iterator&lt;Map.Entry&lt;String,?&gt;&gt; AttributeImpl.contentsIterator() returns an iterator (for most attributes its a singleton) of a key-value pair, e.g. "term"&gt;"foobar","startOffset"&gt;Integer.valueOf(0),... AttributeSource gets the same method, it just concat the iterators of each getAttributeImplsIterator() AttributeImpl No backwards problems occur, as the default toString() method will work like before (it just gets iterator and lists), but we simply remove the documentation for the format. (Char)TermAttribute gets a special impl fo toString() according to CharSequence and a corresponding iterator. I also want to remove the abstract hashCode() and equals() methods from AttributeImpl, as they are not needed and just create work for the implementor.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2377</id>
      <title>Enable the use of NoMergePolicy and NoMergeScheduler by Benchmark</title>
      <description>Benchmark allows one to set the MP and MS to use, by defining the class name and then use reflection to instantiate them. However NoMP and NoMS are singletons and therefore reflection does not work for them. Easy fix in CreateIndexTask. I'll post a patch soon.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2378</id>
      <title>Cutover remaining usage of pre-flex APIs</title>
      <description>A number of places still use the pre-flex APIs. This is actually healthy, since it gives us ongoing testing of the back compat emulation layer. But we should at some point cut them all over to flex. Latest we can do this is 4.0, but I'm not sure we should do them all for 3.1... still marking this as 3.1 to "remind us"</description>
      <attachments/>
      <comments>24</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2379</id>
      <title>TermRangeQuery &amp; FieldCacheRangeFilter should accepts BytesRef</title>
      <description>With flex, a term is a byte[] (BytesRef) not a String... we need to push this "up the search stack". TermRangeQuery / FieldCacheRangeFilter.newStringRange now take a String for the upper/lower bounds, but that should be deprecated in favor of BytesRef (a BytesRef can be created from a CharSequence, encoding the chars as UTF8).</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2380</id>
      <title>Add FieldCache.getTermBytes, to load term data as byte[]</title>
      <description>With flex, a term is now an opaque byte[] (typically, utf8 encoded unicode string, but not necessarily), so we need to push this up the search stack. FieldCache now has getStrings and getStringIndex; we need corresponding methods to load terms as native byte[], since in general they may not be representable as String. This should be quite a bit more RAM efficient too, for US ascii content since each character would then use 1 byte not 2.</description>
      <attachments/>
      <comments>31</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2381</id>
      <title>Use packed ints for sort ords (in FieldCache.getStringIndex/.getTermBytesIndex)</title>
      <description>We wastefully use a whole int today, but for enumerated fields (eg "country", "state", "color", "category") this is very wasteful since you could use only a few bits per doc when there are not that many values.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2385</id>
      <title>Move NoDeletionPolicy from benchmark to core</title>
      <description>As the subject says, but I'll also make it a singleton + add some unit tests, as well as some documentation. I'll post a patch hopefully today.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2389</id>
      <title>Enforce TokenStream impl / Analyzer finalness by an assertion</title>
      <description>As noted in LUCENE-1753 and other issues, TokenStream and Analyzers are based on the decorator pattern. At least all TokenStream and Analyzer implementations in Lucene and Solr should be final. The attached patch adds an assertion to the ctors of both classes that does the corresponding checks: Analyzers must be final or private classes or anonymous inner classes TokenStreams must be final or private classes or anonymous inner classes or have a final incrementToken() I will commit this after robert have fixed solr streams.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2391</id>
      <title>Spellchecker uses default IW mergefactor/ramMB settings of 300/10</title>
      <description>These settings seem odd - I'd like to investigate what makes most sense here.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2392</id>
      <title>Enable flexible scoring</title>
      <description>This is a first step (nowhere near committable!), implementing the design iterated to in the recent "Baby steps towards making Lucene's scoring more flexible" java-dev thread. The idea is (if you turn it on for your Field; it's off by default) to store full stats in the index, into a new _X.sts file, per doc (X field) in the index. And then have FieldSimilarityProvider impls that compute doc's boost bytes (norms) from these stats. The patch is able to index the stats, merge them when segments are merged, and provides an iterator-only API. It also has starting point for per-field Sims that use the stats iterator API to compute boost bytes. But it's not at all tied into actual searching! There's still tons left to do, eg, how does one configure via Field/FieldType which stats one wants indexed. All tests pass, and I added one new TestStats unit test. The stats I record now are: field's boost field's unique term count (a b c a a b --&gt; 3) field's total term count (a b c a a b --&gt; 6) total term count per-term (sum of total term count for all docs that have this term) Still need at least the total term count for each field.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2393</id>
      <title>Utility to output total term frequency and df from a lucene index</title>
      <description>This is a pair of command line utilities that provide information on the total number of occurrences of a term in a Lucene index. The first takes a field name, term, and index directory and outputs the document frequency for the term and the total number of occurrences of the term in the index (i.e. the sum of the tf of the term for each document). The second reads the index to determine the top N most frequent terms (by document frequency) and then outputs a list of those terms along with the document frequency and the total number of occurrences of the term. Both utilities are useful for estimating the size of the term's entry in the *prx files and consequent Disk I/O demands.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2394</id>
      <title>Factories for cache creation</title>
      <description>Hello all, I've seen the LUCENE-831 (Complete overhaul of FieldCache API/Implementation) targeted for version 3.1 and I think that maybe, before this overhaul, it would be good to have a more cirurgical change, that would need smaller effort in new unit tests, without behavior changes and almost no performance impact. One way to achieve that is inserting strategically positioned calls to a factory structure that would allow every already developed code to continue working without changes, at the same time giving the opportunity to put alternative factories to work. Focusing on the cache idea (not specifically the FieldCache, that has it's own specific responsabilities, but in the key/value structure that will ultimately hold the cached objects) i've done the small change contained in the patch I'm attaching to this. It has default implementations that encapsulate what was being originally used in FieldCache, so all current test cases passes, and creates the possibility to create a EHCacheFactory or InfinispanCacheFactory, or even MyOwnCachingStructureFactory. With this, it would be easy to take advantage of the features provided by this kind of project in a uniform way and rapidly allowing new possibilities in scalability and tuning. The code in the patch is small (16kb file is small if compared to the hundreds of kbs in other patchs) and even though it doesn't have javadoc right now (sorry) I hope it can be easly understood. So, if Lucene maintainers see that this contribution could be used (in a 2.9.n+1 and 3.0.n+1 and maybe influencing future versions) we could put some more effort in it, documenting, adding necessary unit tests and maybe contributing other factory implementations. What do you think?</description>
      <attachments/>
      <comments>5</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2395</id>
      <title>Add a scoring DistanceQuery that does not need caches and separate filters</title>
      <description>In a chat with Chris Male and my own ideas when implementing for PANGAEA, I thought about the broken distance query in contrib. It lacks the following features: It needs a query/filter for the enclosing bbox (which is constant score) It needs a separate filter for filtering out hits to far away (inside bbox but outside distance limit) It has no scoring, so if somebody wants to sort by distance, he needs to use the custom sort. For that to work, spatial caches distance calculation (which is broken for multi-segment search) The idea is now to combine all three things into one query, but customizeable: We first thought about extending CustomScoreQuery and calculate the distance from FieldCache in the customScore method and return a score of 1 for distance=0, score=0 on the max distance and score&lt;0 for farer hits, that are in the bounding box but not in the distance circle. To filter out such negative scores, we would need to override the scorer in CustomScoreQuery which is priate. My proposal is now to use a very stripped down CustomScoreQuery (but not extend it) that does call a method getDistance(docId) in its scorer's advance and nextDoc that calculates the distance for the current doc. It stores this distance also in the scorer. If the distance &gt; maxDistance it throws away the hit and calls nextDoc() again. The score() method will reurn per default weight.value*(maxDistance - distance)/maxDistance and uses the precalculated distance. So the distance is only calculated one time in nextDoc()/advance(). To be able to plug in custom scoring, the following methods in the query can be overridden: float getDistanceScore(double distance) - returns per default: (maxDistance - distance)/maxDistance; allows score customization DocIdSet getBoundingBoxDocIdSet(Reader, LatLng sw, LatLng ne) - returns an DocIdSet for the bounding box. Per default it returns e.g. the docIdSet of a NRF or a cartesian tier filter. You can even plug in any other DocIdSet, e.g. wrap a Query with QueryWrapperFilter support a setter for the GeoDistanceCalculator that is used by the scorer to get the distance. a LatLng provider (similar to CustomScoreProvider/ValueSource) that returns for a given doc id the lat/lng. This method is called per IndexReader one time in scorer creation and will retrieve the coordinates. By that we support FieldCache or whatever. This query is almost finished in my head, it just needs coding</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2397</id>
      <title>SnapshotDeletionPolicy.snapshot() throws NPE if no commits happened</title>
      <description>SDP throws NPE if no commits occurred and snapshot() was called. I will replace it w/ throwing IllegalStateException. I'll also move TestSDP from o.a.l to o.a.l,index. I'll post a patch soon</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2399</id>
      <title>Add support for ICU's Normalizer2</title>
      <description>While there are separate Case Folding, Normalization, and Ignorable-removal filters in LUCENE-1488, the new ICU Normalizer2 API does this all at once with nfkc_cf (based on the new NFKC_Casefold property in Unicode). This is great, because it provides a ton of unicode functionality that is really needed. And the new Normalizer2 API takes CharSequence and writes to Appendable...</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2400</id>
      <title>ShingleFilter: don't output all-filler shingles/unigrams; also, convert from TermAttribute to CharTermAttribute</title>
      <description>When the input token stream to ShingleFilter has position increments greater than one, filler tokens are inserted for each position for which there is no token in the input token stream. As a result, unigrams (if configured) and shingles can be filler-only. Filler-only output tokens make no sense - these should be removed. Also, because TermAttribute has been deprecated in favor of CharTermAttribute, the patch will also convert TermAttribute usages to CharTermAttribute in ShingleFilter.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2401</id>
      <title>Improve performance of CharTermAttribute(Impl) and also fully implement Appendable</title>
      <description>The Appendable.append(CharSequence) method in CharTermAttributes is good for general use. But like StringBuilder has for some common use cases specialized methods, this does the same and adds separate append methods for String, StringBuilder and CharTermAttribute itsself. This methods enable the compiler to directly link the specialized methods and don't use the instanceof checks. The unspecialized method only does the instanceof checks for longer CharSequences (&gt;8 chars), else it simply iterates. This patch also fixes the required special "null" handling. append() methods are required by Appendable to append "null", if the argument is null. I dont like this, but its required. Maybe we should document, that we dont dont support it. Otherwise, JDK's formatter fails with formatting null.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2402</id>
      <title>Add an explicit method to invoke IndexDeletionPolicy</title>
      <description>Today, if one uses an IDP which holds onto segments, such as SnapshotDeletionPolicy, or any other IDP in the tests, those segments are left in the index even if the IDP no longer references them, until IW.commit() is called (and actually does something). I'd like to add a specific method to IW which will invoke the IDP's logic and get rid of the unused segments w/o forcing the user to call IW.commit(). There are a couple of reasons for that: Segments take up sometimes valuable HD space, and the application may wish to reclaim that space immediately. In some scenarios, the index is updated once in several hours (or even days), and waiting until then may not be acceptable. I think it's a cleaner solution than waiting for the next commit() to happen. One can still wait for it if one wants, but otherwise it will give you the ability to immediately get rid of those segments. TestSnapshotDeletionPolicy includes this code, which only strengthens (IMO) the need for such method: // Add one more document to force writer to commit a // final segment, so deletion policy has a chance to // delete again: Document doc = new Document(); doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS)); writer.addDocument(doc); If IW had an explicit method, that code would not need to exist there at all ... Here comes the fun part - naming the baby: invokeDeletionPolicy – describes exactly what is going to happen. However, if the user did not set IDP at all (relying on default, which I think many do), users won't understand what is it. deleteUnusedSegments - more user-friendly, assuming users understand what 'segments' are. BTW, IW already has deleteUnusedFiles() which only tries to delete unreferenced files that failed to delete before (such as on Windows, due to e.g. open readers). Perhaps instead of inventing a new name, we can change IW.deleteUnusedFiles to call IndexFileDeleter.checkpoint (instead of deletePendingFiles) which deletes those files + calls IDP.onCommit().</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2405</id>
      <title>Benchmark DocMaker no longer allows "off prescription" usage</title>
      <description>The EnwikiDocMaker was a pretty handy tool for indexing wikipedia for demos. Since LUCENE-1595, it is now much harder to create Lucene documents programmatically using the EnwikiContentSource and DocMaker b/c the DocMaker doesn't allow one to programmatically set the properties and ContentSource. Perhaps, we should refactor the Enwiki stuff a little bit out to the Wikipedia package.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2407</id>
      <title>make CharTokenizer.MAX_WORD_LEN parametrizable</title>
      <description>as discussed here http://n3.nabble.com/are-long-words-split-into-up-to-256-long-tokens-tp739914p739914.html it would be nice to be able to parametrize that value.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2408</id>
      <title>Add Document.set/getSourceID, as an optional hint to IndexWriter to improve indexing performance</title>
      <description>(Spinoff from LUCENE-2324). The internal indexer (currently DocumentsWriter &amp; its full indexing chain) has separate *PerThread objects holding buffered postings in RAM until flush. The RAM efficiency of these buffers is very dependent on the term distributions sent to each. As an optimization, today, we use thread affinity (ie we try to assign the same thread to the same *PerThread classes), on the assumption that sometimes that thread may be indexing from its own source of docs. When the assumption applies it means we can have much better overall RAM efficiency since a single *PerThread set of classes handles the term distribution for that source. In the extreme case (many threads, each doing completely orthogonal terms, eg say different languages) this should be a sizable performance gain. But really this is a hack – eg if you index using a dedicated indexing thread pool, then thread binding has nothing to do with source, and you have no way to get this optimization (even though it's still "there"). To fix this, we should add an optional get/setSourceID to Document. It's completely optional for an app to set this... and if they do, it'd be a hint which IW can make use of (in an impl private manner). If they don't we should just fallback to the "best guess" we use today (each thread is its own source). The javadoc would be something like "as a hint to IW, to possibly improve its indexing performance, if you have docs from difference sources you should set the source ID on your Document". And how/whether IW makes use of this information is "under the hood"...</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2409</id>
      <title>add a tokenfilter for icu transforms</title>
      <description>I pulled the ICUTransformFilter out of LUCENE-1488 and create an issue for it here. This is a tokenfilter that applies an ICU Transliterator, which is a context-sensitive way to transform text. These are typically rule-based and you can use ones included with ICU (such as Traditional-Simplified) or you can make your own from your own set of rules. User's Guide: http://userguide.icu-project.org/transforms/general Rule Tutorial: http://userguide.icu-project.org/transforms/general/rules</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2410</id>
      <title>Optimize PhraseQuery</title>
      <description>Looking the scorers for PhraseQuery, I think there are some speedups we could do: The AND part of the scorer (which advances to the next doc that has all the terms), in PhraseScorer.doNext, should do the same optimizing as BooleanQuery's ConjunctionScorer, ie sort terms from rarest to most frequent. I don't think it should use a linked list/firstToLast() that it does today. We do way too much work now when .score() is not called, because we go and find all occurrences of the phrase in the doc, whereas we should stop only after finding the first and then go and count the rest if .score() is called. For the exact case, I think we can use two int arrays to find the matches. The first array holds the count of how many times a term in the phrase "matched" a phrase starting at that position. When that count == the number of terms in the phrase, it's a match. The 2nd is a "gen" array (holds docID when that count was last touched), to avoid clearing. Ie when incrementing the count, if the docID != gen, we reset count to 0. I think this'd be faster than the PQ we now use. Downside of this is if you have immense docs (position gets very large) we'd need 2 immense arrays. It'd be great to do LUCENE-1252 along with this, ie factor PhraseScorer into two AND'd sub-scorers (LUCENE-1252 is open for this). The first one should be ConjunctionScorer, and the 2nd one checks the positions (ie, either the exact or sloppy scorers). This would mean if the PhraseQuery is AND'd w/ other clauses (or, a filter is applied) we would save CPU by not checking the positions for a doc unless all other AND'd clauses accepted the doc.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2413</id>
      <title>Consolidate all (Solr's &amp; Lucene's) analyzers into modules/analysis</title>
      <description>We've been wanting to do this for quite some time now... I think, now that Solr/Lucene are merged, and we're looking at opening an unstable line of development for Solr/Lucene, now is the right time to do it. A standalone module for all analyzers also empowers apps to separately version the analyzers from which version of Solr/Lucene they use, possibly enabling us to remove Version entirely from the analyzers. We should also do LUCENE-2309 (decouple, as much as possible, indexer from the analysis API), but I don't think that issue needs to block this consolidation. Once we do this, there is one place where our users can find all the analyzers that Solr/Lucene provide.</description>
      <attachments/>
      <comments>58</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2414</id>
      <title>add icu-based tokenizer for unicode text segmentation</title>
      <description>I pulled out the last part of LUCENE-1488, the tokenizer itself and cleaned it up some. The idea is simple: First step is to divide text into writing system boundaries (scripts) You supply an ICUTokenizerConfig (or just use the default) which lets you tailor segmentation on a per-writing system basis. This tailoring can be any BreakIterator, so rule-based or dictionary-based or your own. The default implementation (if you do not customize) is just to do UAX#29, but with tailorings for stuff with no clear word division: Thai (uses dictionary-based word breaking) Khmer, Myanmar, Lao (uses custom rules for syllabification) Additionally as more of an example i have a tailoring for hebrew that treats the punctuation special. (People have asked before for ways to make standardanalyzer treat dashes differently, etc)</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2415</id>
      <title>Remove JakarteRegExCapabilities shim to access package protected field</title>
      <description>To access the prefix in Jakarta RegExes we use a shim class in the same package as jakarta. I will remove this and replace by reflection like Robert does in his ICUTokenizer rule compiler. Shim classes have the problem wth signed artifacts, as you cannot insert a new class into a foreign package if you sign regex classes. This shim-removal also allows users to use later jakarta regex versions, if they are in classpath and cannot be removed (even if they have bugs). Performance is no problem, as the prefix is only get once per TermEnum.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2416</id>
      <title>Some improvements to Benchmark</title>
      <description>I've noticed that WriteLineDocTask declares it does not support multi-threading, but taking a closer look I think this is really for no good reason. Most of the work is done by reading from the ContentSource and constructing the document. If those two are mult-threaded (and I think all ContentSources are), then we can synchronize only around writing the actual document to the line file. While investigating that, I've noticed some 1.5 TODOs and some other minor improvements that can be made. If you've wanted to make some minor improvements to benchmark, let me know . I intend to include only minor and trivial ones.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2419</id>
      <title>Improve parallel tests</title>
      <description>As mentioned on the dev@ mailing list here: http://www.lucidimagination.com/search/document/93432a677917b9bd/lucenejunitresultformatter_sometimes_fails_to_lock It would be useful to not create a lockfactory for each test suite (As they are run sequentially in the same separate JVM). Additionally, we create a lot of JVMs (26) for each batch, because we have to run one for each letter. Instead, we use a technique here to divide up the tests with a custom selector: http://blog.code-cop.org/2009/09/parallel-junit.html (I emailed the blog author and received permission to use this code) This gives a nice boost to the speed of overall tests, especially Solr tests, as many start with an "S", but this is no longer a problem.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2421</id>
      <title>Hardening of NativeFSLock</title>
      <description>NativeFSLock create a test lock file which its name might collide w/ another JVM that is running. Very unlikely, but still it happened a couple of times already, since the tests were parallelized. This may result in a false exception thrown from release(), when the lock file's delete() is called and returns false, because the file does not exist (deleted by another JVM already). In addition, release() should give a second attempt to delete() if it fails, since the file may be held temporarily by another process (like AntiVirus) before it fails. The proposed changes are: 1) Use ManagementFactory.getRuntimeMXBean().getName() as part of the test lock name (should include the process Id) 2) In release(), if delete() fails, check if the file indeed exists. If it is, let's attempt a re-delete() few ms later. 3) If (3) still fails, throw an exception. Alternatively, we can attempt a deleteOnExit. I'll post a patch later today.</description>
      <attachments/>
      <comments>33</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2425</id>
      <title>An Anti-Merging Multi-Directory Indexing Framework</title>
      <description>By design, a Lucene index tends to merge documents that span multiple segments into fewer segments, in order to optimize its directory structure, which in turn leads to better search performance. In particular, it relies on a merge policy to specify the set of merge operations that should be performed when the index is optimized. Often times, there's a need to do the exact opposite, which is to "split" the documents. This calls for a mechanism that facilitates sub-division of documents based on a certain (ideally, user-defined) algorithm. By way of example, one may wish to sub-divide (or partition) documents based on parameters such as time, space, real-timeliness, and so on. Herein, we describe an indexing framework that builds on the Lucene index writer and reader, to address use cases wherein documents need to diverge rather than converge. In brief, it associates zero or more sub-directories with the index's directory, which serve to complement it in some manner. The sub-directories (a.k.a. splits) are managed by a split policy, which is notified of all changes made to the index directory (a.k.a. super-directory), thus allowing it to modify its sub-directories as it sees fit. To make the index reader and writer "observable", we extend Lucene's reader and writer with the goal of providing hooks into every method that could potentially change the index. This allows for propagation of such changes to the split policy, which essentially acts as a listener on the index. We refer to each sub-directory (or split) and the super-directory as a sub-index of the containing index (a.k.a. the split index). Note that the sub-directory may not necessarily be co-located with the super-directory. Furthermore, the split policy in turn relies on one or more split rules to determine when to add or remove sub-directories. This allows for a clear separation of the event that triggers a split from the management of those splits.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2426</id>
      <title>change sort order to binary order</title>
      <description>Since flexible indexing, terms are now represented as byte[], but for backwards compatibility reasons, they are not sorted as byte[], but instead as if they were char[]. I think its time to look at sorting terms as byte[]... this would yield the following improvements: terms are more opaque by default, they are byte[] and sort as byte[]. I think this would make lucene friendlier to customizations. numerics and collation are then free to use their own encoding (full byte) rather than avoiding the use of certain bits to remain compatible with char[] sort order. automaton gets simpler because as in LUCENE-2265, it uses byte[] too, and has special hacks because terms are sorted as char[]</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2437</id>
      <title>Indonesian Analyzer</title>
      <description>This is an implementation of http://www.illc.uva.nl/Publications/ResearchReports/MoL-2003-02.text.pdf The only change is that I added an option to disable derivational stemming, in case you want to just remove inflectional particles and possessive pronouns.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2439</id>
      <title>Composite readers (Multi/DirIndexReader) should not subclass IndexReader</title>
      <description>I'd like to change Multi/DirIndexReader so that they no longer implement the low level methods of IndexReader, and instead act more like an ordered collection of sub readers. I think to do this we'd need a new interface, common to atomic readers (SegmentReader) and the composite readers, which IndexSearcher would accept. We should also require that the core Query scorers always receive an atomic reader. We've taken strong initial steps here with flex, by forcing users to use separate MultiFields static methods to obtain Fields/Terms/etc. from a composite reader. This issue is to finish this cutover.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2440</id>
      <title>Add support for custom ExecutorServices in ParallelMultiSearcher</title>
      <description>Right now, the ParallelMultiSearcher uses a cachedThreadPool, which is limitless and a poor choice for a web application, given the threaded nature of the requests (say a webapp with tomcat-default 200 threads and 100 indexes could be looking at 2000 searching threads pretty easily). Support for adding a custom ExecutorService is pretty trivial. Patch forthcoming.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2441</id>
      <title>Create 3.x -&gt; 4.0 index migration tool</title>
      <description>We need a tool to upgrade an index so that 4.0 can read it. I think the only change right now is the cutover to flex's standard codec format, but with LUCENE-2426 we also need to correct the term sort order to be true unicode code point order.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2442</id>
      <title>Remove flex back compat layers &amp; pre-flex APIs</title>
      <description>Now that flex is a 4.0-only feature, we should remove the costly back-compat layers from trunk, and absorb some of them (eg the read-only pre-flex codec) into the index upgrade tool (LUCENE-2441).</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2446</id>
      <title>Add checksums to Lucene segment files</title>
      <description>It would be useful for the different files in a Lucene index to include checksums. This would make it easy to spot corruption while copying index files around; the various cloud efforts assume many more data-copying operations than older single-index implementations. This feature might be much easier to implement if all index files are created in a sequential fashion. This issue therefore depends on LUCENE-2373.</description>
      <attachments/>
      <comments>31</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>2447</id>
      <title>Add support for subsets of searchables inside a MultiSearcher/ParallelMultiSearcher instance's methods at runtime</title>
      <description>Here's the situation: We have a site with a fair few amount of indexes that we're using MultiSearcher/ParallelMultiSearcher for, but the users can select an arbitrary permutation of indexes to search. For example (contrived, but illustratory): the site has indexes numbered 1 - 10; user A wants to search in all 10; user B wants to search indexes 1, 2 and 3, user C wants to search even-numbered indexes. From Lucene 3.0.1, the only way to do this is to continually instantiate a new MultiSearcher based on every permutation of indexes that a user wants, which is not ideal at all. What I've done is add a new parameter to all methods in MultiSearcher that use the searchables array (docFreq, search, rewrite and createDocFrequencyMap), a Set&lt;Searchable&gt; which is checked for isEmpty() and contains() for every iteration over the searchables[]. The actual logic has been moved into these methods and the old methods have become overloads that pass a Collections.emptySet() into those methods, so I do not expect there to be a very noticeable performance impact as a result of this modification, if it's measurable at all. I didn't modify the test for MultiSearcher very much, just enough to illustrate the that subsetting of the search results works, since no other logic has changed. If I need to do more for the testing, let me know and I'll do it. I've attached the patches for MultiSearcher.java, ParallelMultiSearcher.java and TestMultiSearcher.java.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2450</id>
      <title>Explore write-once attr bindings in the analysis chain</title>
      <description>I'd like to propose a new means of tracking attrs through the analysis chain, whereby a given stage in the pipeline cannot overwrite attrs from stages before it (write once). It can only write to new attrs (possibly w/ the same name) that future stages can see; it can never alter the attrs or bindings from the prior stages. I coded up a prototype chain in python (I'll attach), showing the equivalent of WhitespaceTokenizer -&gt; StopFilter -&gt; SynonymFilter -&gt; Indexer. Each stage "sees" a frozen namespace of attr bindings as its input; these attrs are all read-only from its standpoint. Then, it writes to an "output namespace", which is read/write, eg it can add new attrs, remove attrs from its input, change the values of attrs. If that stage doesn't alter a given attr it "passes through", unchanged. This would be an enormous change to how attrs are managed... so this is very very exploratory at this point. Once we decouple indexer from analysis, creating such an alternate chain should be possible – it'd at least be a good test that we've decoupled enough I think the idea offers some compelling improvements over the "global read/write namespace" (AttrFactory) approach we have today: Injection filters can be more efficient – they need not capture/restoreState at all No more need for the initial tokenizer to "clear all attrs" – each stage becomes responsible for clearing the attrs it "owns" You can truly stack stages (vs having to make a custom AttrFactory) – eg you could make a Bocu1 stage which can stack onto any other stage. It'd look up the CharTermAttr, remove it from its output namespace, and add a BytesRefTermAttr. Indexer should be more efficient, in that it doesn't need to re-get the attrs on each next() – it gets them up front, and re-uses them. Note that in this model, the indexer itself is just another stage in the pipeline, so you could do some wild things like use 2 indexer stages (writing to different indexes, or maybe the same index but somehow with further processing or something). Also, in this approach, the analysis chain is more informed about the what each stage is allowed to change, up front after the chain is created. EG (say) we will know that only 2 stages write to the term attr, and that only 1 writes posIncr/offset attrs, etc. Not sure if/how this helps us... but it's more strongly typed than what we have today. I think we could use a similar chain for processing a document at the field level, ie, different stages could add/remove/change different fields in the doc....</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2451</id>
      <title>remove dead code from oal.util.cache</title>
      <description>We have dead cache impls in oal.util.cache*; we only use DBLRUCache. These are internal APIs; I'd like to remove all but DBLRUcache.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2453</id>
      <title>Make Index Output Buffer Size Configurable</title>
      <description>Currently, the buffered index input class allows sub-classes and users thereof to specify a size for the input buffer, which by default is 1024 bytes. In practice, this option is leveraged by the simple file and compound segment index input sub-classes. By the same token, it would be nice if the buffered index output class could open up it's buffer size for users to configure. In particular, this would allow sub-classes thereof to align the output buffer size, which by default is 16348 bytes, to that of the underlying directory's data unit. For example, a network-based directory might want to buffer data in multiples of it's maximum transmission unit. To use an existing use-case, the file system-based directory could potentially choose to align it's output buffer size to the operating system's file block size. The proposed change to the buffered index output class involves defining a one-arg constructor that takes a user-defined buffer size, and a default constructor that uses the currently defined buffer size.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2454</id>
      <title>Nested Document query support</title>
      <description>A facility for querying nested documents in a Lucene index as outlined in http://www.slideshare.net/MarkHarwood/proposal-for-nested-document-support-in-lucene</description>
      <attachments/>
      <comments>74</comments>
      <commenters>12</commenters>
    </issue>
    <issue>
      <id>2455</id>
      <title>Some house cleaning in addIndexes*</title>
      <description>Today, the use of addIndexes and addIndexesNoOptimize is confusing - especially on when to invoke each. Also, addIndexes calls optimize() in the beginning, but only on the target index. It also includes the following jdoc statement, which from how I understand the code, is wrong: After this completes, the index is optimized. – optimize() is called in the beginning and not in the end. On the other hand, addIndexesNoOptimize does not call optimize(), and relies on the MergeScheduler and MergePolicy to handle the merges. After a short discussion about that on the list (Thanks Mike for the clarifications!) I understand that there are really two core differences between the two: addIndexes supports IndexReader extensions addIndexesNoOptimize performs better This issue proposes the following: Clear up the documentation of each, spelling out the pros/cons of calling them clearly in the javadocs. Rename addIndexesNoOptimize to addIndexes Remove optimize() call from addIndexes(IndexReader...) Document that clearly in both, w/ a recommendation to call optimize() before on any of the Directories/Indexes if it's a concern. That way, we maintain all the flexibility in the API - addIndexes(IndexReader...) allows for using IR extensions, addIndexes(Directory...) is considered more efficient, by allowing the merges to happen concurrently (depending on MS) and also factors in the MP. So unless you have an IR extension, addDirectories is really the one you should be using. And you have the freedom to call optimize() before each if you care about it, or don't if you don't care. Either way, incurring the cost of optimize() is entirely in the user's hands. BTW, addIndexes(IndexReader...) does not use neither the MergeScheduler nor MergePolicy, but rather call SegmentMerger directly. This might be another place for improvement. I'll look into it, and if it's not too complicated, I may cover it by this issue as well. If you have any hints that can give me a good head start on that, please don't be shy .</description>
      <attachments/>
      <comments>62</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2456</id>
      <title>A Column-Oriented Cassandra-Based Lucene Directory</title>
      <description>Herein, we describe a type of Lucene directory that stores its file in a Cassandra server, which makes for a scalable and robust store for Lucene indices. In brief, the CassandraDirectory maps the concept of a Lucene directory to a column family that belongs to a certain keyspace located in a given Cassandra server. Further, it stores each file under this directory as a row in that column family. Specifically, its files are broken down into blocks (whose sizes are capped), where each block (see FileBlock) is stored as the value of a column in the corresponding row. As per http://wiki.apache.org/cassandra/CassandraLimitations, this is the recommended approach for dealing with large objects, which Lucene files tend to be. In addition, a descriptor of the file (see FileDescriptor) that outlines a map of blocks therein is stored as one of the columns in that row as well. Think of this descriptor as an inode for Cassandra-based files. The exhaustive mapping of a Lucene directory (file) to a Cassandra column family (row) is captured in the ColumnOrientedDirectory (ColumnOrientedFile) inner-class. Specifically, it interprets Cassandra's data model in terms of Lucene's, and vice verca. More importantly, these are the only two inner-classes that have a foot in both the Lucene and Cassandra camps. All writes to a file in this directory occur through a CassandraIndexOutput, which puts the data flushed from a write-behind buffer into the fitting set of blocks. By the same token, all reads from a file in this directory occur through a CassandraIndexInput, which gets the data needed by a read-ahead buffer from the right set of blocks. The last (but not the least) inner-class, CassandraClient, acts as a facade over a Thrift-based Cassandra client. In short, it provides operations to get/put rows/columns in the column family and keyspace associated with this directory. Unlike Lucandra, which attempts to bridge the gap between Lucene and Cassandra at the document-level, the CassandraDirectory is self-sufficient in the sense that it does not require a re-write of any other component in the Lucene stack. In other words, one may use the CassandraDirectory in conjunction with the Lucene IndexWriter and IndexReader, as you would any other kind of Lucene Directory. Moreover, given the the data unit that is transferred to and from Cassandra is a large-sized block, one may expect fewer round trips, and hence better throughputs, from the CassandraDirectory. In conclusion, this directory attempts to marry the rich search-based query language of Lucene with the distributed fault-tolerant database that is Cassandra. By delegating the responsibilities of replication, durability and elasticity to the directory, we free the layers above from such non-functional concerns. Our hope is that users will choose to make their large-scale indices instantly scalable by seamlessly migrating them to this type of directory (using Directory#copyTo(Directory)).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2457</id>
      <title>QueryNode implementors should override equals method</title>
      <description>Discussed on thread: http://markmail.org/thread/gjqk35t7e3y4fo5j "QueryNode(s) are data objects, and it makes sense to override their equals method. But before, we need to define what is a QueryNode equality. Should two nodes be considered equal if they represent syntactically or semantically the same query? e.g. an ORQueryNode created from the query &lt;a OR b OR c&gt; will not have the same children ordering as the query &lt;b OR c OR a&gt;, so they are syntactically not equal, but they are semantically equal, because the order of the OR operands (usually) does not matter when the query is executed. I say it usually does not matter, because it's up to the Query object implementation built from that ORQueryNode object, for this reason, I vote for defining that two query nodes should be equals if they are syntactically equal. I also vote for excluding query node tags from the equality check, because they are not meant to represent the query structure, but to attach extra info to the node, which is usually used for communication between processors."</description>
      <attachments/>
      <comments>6</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2460</id>
      <title>Search Results Filtering Based on Bitwise Operations on Integer Fields</title>
      <description>This package makes it possible to filter results returned from a query based on the results of a bitwise operation on an integer field in the documents returned from the pre-constructed query. You can perform three basic types of operations on these integer fields BitwiseOperation.BITWISE_AND (bitwise AND) BitwiseOperation.BITWISE_OR (bitwise inclusive OR) BitwiseOperation.BITWISE_XOR (bitwise exclusive OR) You can also negate the results of these operations. For example, imagine there is an integer field in the index named "flags" with the a value 8 (1000 in binary). The following results will be expected : 1. A source value of 8 will match during a BitwiseOperation.BITWISE_AND operation, with negate set to false. 2. A source value of 4 will match during a BitwiseOperation.BITWISE_AND operation, with negate set to true. The BitwiseFilter constructor accepts the following values The name of the integer field (A string) The BitwiseOperation object. Example BitwiseOperation.BITWISE_XOR The source value (an integer) A boolean value indicating whether or not to negate the results of the operation A pre-constructed org.apache.lucene.search.Query</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2463</id>
      <title>Improve Greek analysis</title>
      <description>Changed tokenstreams to CharTermAttribute Moved stopwords out of private String[] to a txt file (for use by Solr, etc) Removed TODO / fixed unicode conformance of GreekLowerCaseFilter Reformatted touched files to normal indentation Added inflectional stemmer (Ntais algorithm) all the changes are backwards compatible with Version.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2464</id>
      <title>FastVectorHighlighter: add a FragmentBuilder to return entire field contents</title>
      <description>In Highlightrer, there is a Nullfragmenter. There is a requirement its counterpart in FastVectorhighlighter.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2470</id>
      <title>Add conditional braching/merging to Lucene's analysis pipeline</title>
      <description>Captured from a #lucene brainstorming session with Robert Muir: Lucene's analysis pipeline would be more flexible if it were possible to apply filter(s) to only part of an input stream's tokens, under user-specifiable conditions (e.g. when a given token attribute has a particular value) in a way that did not place that responsibility on individual filters. Two use cases: StandardAnalyzer could directly handle ideographic characters in the same way as CJKTokenizer, which generates bigrams, if it could call ShingleFilter only when the TypeAttribute=&lt;CJK&gt;, or if Robert's new ScriptAttribute=&lt;Ideographic&gt;. Stemming might make sense for some stemmer/domain combinations only when token length exceeds some threshold. For example, a user could configure an analyzer to stem only when CharTermAttribute length is greater than 4 characters. One potential way to achieve this conditional branching facility is with a new kind of filter that can be configured with one or more following filters and condition(s) under which the filter should be engaged. This could be called BranchingFilter. I think a MergingFilter, the inverse of BranchingFilter, is necessary in the current pipeline architecture, to have a single pipeline endpoint. A MergingFilter might be useful in its own right, e.g. to collect document data from multiple sources. Perhaps a conditional merging facility would be useful as well.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2471</id>
      <title>Supporting bulk copies in Directory</title>
      <description>A method can be added to IndexOutput that accepts IndexInput, and writes bytes using it as a source. This should be used for bulk-merge cases (offhand - norms, docstores?). Some Directories can then override default impl and skip intermediate buffers (NIO, MMap, RAM?).</description>
      <attachments/>
      <comments>19</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2472</id>
      <title>The terms index divisor in IW should be set via IWC not via getReader</title>
      <description>The getReader call gives a false sense of security... since if deletions have already been applied (and IW is pooling) the readers have already been loaded with a divisor of 1. Better to set the divisor up front in IWC.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2474</id>
      <title>Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey)</title>
      <description>Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey). A spin of: https://issues.apache.org/jira/browse/LUCENE-2468. Basically, its make a lot of sense to cache things based on IndexReader#getFieldCacheKey, even Lucene itself uses it, for example, with the CachingWrapperFilter. FieldCache enjoys being called explicitly to purge its cache when possible (which is tricky to know from the "outside", especially when using NRT - reader attack of the clones). The provided patch allows to plug a CacheEvictionListener which will be called when the cache should be purged for an IndexReader.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2479</id>
      <title>need the ability to also sort SpellCheck results by freq, instead of just by Edit Distance+freq</title>
      <description>This issue was first noticed and reported in this Solr thread; http://lucene.472066.n3.nabble.com/spellcheck-issues-td489776.html#a489788 Basically, there are situations where it would be useful to sort by freq first, instead of the current "sort by edit distance, and then subsort by freq if edit distance is equal" The author of the thread suggested "What I think would work even better than allowing a custom compareTo function would be to incorporate the frequency directly into the distance function. This would allow for greater control over the trade-off between frequency and edit distance" However, custom compareTo functions are not always be possible (ie if a certain version of Lucene must be used, because it was release with Solr) and incorporating freq directly into the distance function may be overkill (ie depending on the implementation) it is suggested that we have a simple modification of the existing compareTo function in Lucene to allow users to specify if they want the existing sort method or if they want to sort by freq.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2480</id>
      <title>Remove support for pre-3.0 indexes</title>
      <description>We should remove support for 2.x (and 1.9) indexes in 4.0. It seems that nothing can be done in 3x because there is no special code which handles 1.9, so we'll leave it there. This issue should cover: Remove the .zip indexes Remove the unnecessary code from SegmentInfo and SegmentInfos. Mike suggests we compare the version headers at the top of SegmentInfos, in 2.9.x vs 3.0.x, to see which ones can go. remove FORMAT_PRE from FieldInfos Remove old format from TermVectorsReader If you know of other places where code can be removed, then please post a comment here. I don't know when I'll have time to handle it, definitely not in the next few days. So if someone wants to take a stab at it, be my guest.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2481</id>
      <title>Enhance SnapshotDeletionPolicy to allow taking multiple snapshots</title>
      <description>A spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/99161?do=post_view_threaded#99161 I will: Replace snapshot() with snapshot(String), so that one can name/identify the snapshot Add some supporting methods, like release(String), getSnapshots() etc. Some unit tests of course. This is mostly written already - I want to contribute it. I've also written a PersistentSDP, which persists the snapshots on stable storage (a Lucene index in this case) to support opening an IW with existing snapshots already, so they don't get deleted. If it's interesting, I can contribute it as well. Porting my patch to the new API. Should post it soon.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2482</id>
      <title>Index sorter</title>
      <description>A tool to sort index according to a float document weight. Documents with high weight are given low document numbers, which means that they will be first evaluated. When using a strategy of "early termination" of queries (see TimeLimitedCollector) such sorting significantly improves the quality of partial results. (Originally this tool was created by Doug Cutting in Nutch, and used norms as document weights - thus the ordering was limited by the limited resolution of norms. This is a pure Lucene version of the tool, and it uses arbitrary floats from a specified stored field).</description>
      <attachments/>
      <comments>17</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>2483</id>
      <title>When loading FieldCache terms index, make terms data optional</title>
      <description>Spinoff of LUCENE-2380. Now, when you load the terms index (FC.getTermsIndex), it loads two arrays, ord (maps docID -&gt; ord) and lookup (maps ord -&gt; term). But sometimes you don't need the lookup map (and, it's often very costly in RAM usage, much moreso than the ord map). EG if your index is a single segment, and your app doesn't need the values (LUCENE-2335). Or, if you use a sort comparator that resolves ord -&gt; term and v/v (eg using terms dict). So we should make it optional... Also, similarly, we could merge getTerms/getTermsIndex. It's dangerous today if you load terms and then termsIndex because you're wasting tons of RAM; it'd be nicer if we could have a single cache entry that'd "upgrade" itself to be an index (have the ords). This single entry could then serve ords, ords+terms, or just terms.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2485</id>
      <title>IndexWriter should also warm flushed segments</title>
      <description>Spinoff of LUCENE-2311. You can now set a mergedSegmentWarmer on IW, which warms only newly merged segments. But for consistency maybe we should change this to warm all new segments (ie, also flushed ones). We should rename it to something "setSegmentWarmer". Really, the reader pool should be pulled out of IndexWriter, be externally provided, and be responsible for doing warming of new segments.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2491</id>
      <title>Extend Codec with a SegmentInfos writer / reader</title>
      <description>I'm trying to implement a Codec that works with append-only filesystems (HDFS). It's almost done, except for the SegmentInfos.write(dir), which uses ChecksumIndexOutput, which in turn uses IndexOutput.seek() - and seek is not supported on append-only output. I propose to extend the Codec interface to encapsulate also the details of SegmentInfos writing / reading. Patch to follow after some feedback</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2492</id>
      <title>Make PulsingCodec (wrapping StandardCodec) the default codec</title>
      <description>PulsingCodec can provides good gains, by inlining the postings into the terms dict for rare terms. This is especially helpful for primary key like fields, since every term is rare and batch lookups are common (see http://chbits.blogspot.com/2010/06/lucenes-pulsingcodec-on-primary-key.html for a simple perf test), but it should also be a gain for ordinary fields, thanks to Zipf's law. I think we should make it the default....</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2494</id>
      <title>Modify ParallelMultiSearcher to use a CompletionService instead of slowly polling for results</title>
      <description>Right now, the parallel multi searcher creates an array/list of Future&lt;V&gt; representing each of the searchables that's being concurrently searched (and its corresponding search task). As it stands, once the tasks are all submitted to the executor, the array is iterated over, FIFO, and Future.get() is called iteratively. This obviously works, but isn't ideal. It's entirely possible (a situation I've run into) where one of the first searchables represents a large index that takes a long time to search, so the results of the other searchables can't be processed until the large index is done searching. In my case, we have two indexes with several million records that get searched in front of some other indexes, the smallest of which has only a few ten thousand entries and I didn't think it was ideal for the results of the other indexes to wait. I've modified ParallelMultiSearcher to use CompletionServices instead, so that results are processed in the order they are completed, rather than the order that they are submitted. All the tests still pass, and to the best of my knowledge this won't break anything. This have several advantages: 1) Speed - the thread owning the executor doesn't have to wait for the first submitted task to finish in order to process the results of the other tasks, which may have finished first 2) Removed several warnings (even if they are annotated away) due to the ugliness of typecasting generic arrays. 3) Decreased the complexity of the code in some cases, usually by removing the necessity of allocating and filling arrays. With a primed "cache" of searchables, I was getting 700-1200 ms per search, and using the same phrases, with this patch, I am now getting 400-500ms per search Patch is attached.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2495</id>
      <title>Add In/Out/putStream wrapper around Lucene IndexIn/Out/put</title>
      <description>Lucene Directory is an abstraction that builds IndexInput and IndexOutput instances. Sometimes it is useful to add in custom files in the index directory for custom searching. It is often useful in that case to have some sort of bridge between this and code that understand the regular java In/Out/putStream class.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2498</id>
      <title>add sentence boundary charfilter</title>
      <description>From the discussion of LUCENE-2167: It would be nice to have a CharFilter? to mark sentence boundaries. Such functionality would be useful for: prevent phrase queries with 0 slop from matching across sentences inhibiting multiword synonyms, or shingles, etc. For sentence boundary detection we could use Jflex's support for the Unicode Sentence_Break property etc, and the UAX#29 definition as a default grammar. One idea is to just mark the boundaries with a user-provided String. As a simple use-case, a user could then add this string to a stopfilter, and it would introduce a position increment. This would inhibit phrase queries, etc. a user could use the sentence-markers to do more advanced processing downstream.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2499</id>
      <title>Many of the links on the LuceneFAQ result in 404 errors</title>
      <description>http://wiki.apache.org/lucene-java/LuceneFAQ exmples: http://lucene.apache.org/java/docs/api/org/apache/lucene/queryParser/QueryParser.html#setLowercaseExpandedTerms(boolean) http://lucene.apache.org/java/docs/api/org/apache/lucene/search/Filter.html http://lucene.apache.org/java/docs/api/org/apache/lucene/search/CachingWrapperFilter.html Uwe's comment: We could add an additional entry to .htaccess that redirects to the latest released version. We have something similar, but that points to Hudson.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2500</id>
      <title>A Linux-specific Directory impl that bypasses the buffer cache</title>
      <description>I've been testing how we could prevent Lucene's merges from evicting pages from the OS's buffer cache. I tried fadvise/madvise (via JNI) but (frustratingly), I could not get them to work (details at http://chbits.blogspot.com/2010/06/lucene-and-fadvisemadvise.html). The only thing that worked was to use Linux's O_DIRECT flag, which forces all IO to bypass the buffer cache entirely... so I created a Linux-specific Directory impl to do this.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2503</id>
      <title>light/minimal stemming for euro languages</title>
      <description>The snowball stemmers are very aggressive and it would be nice if there were lighter alternatives. Some applications may want to perform less aggressive stemming, for example: http://www.lucidimagination.com/search/document/5d16391e21ca6faf/plural_only_stemmer Good, relevance tested algorithms exist and I think we should provide these alternatives.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2506</id>
      <title>A Stateful Filter That Works Across Index Segments</title>
      <description>By design, Lucene's Filter abstraction is applied once for every segment in the index during searching. In particular, the reader provided to its #getDocIdSet method does not represent the whole underlying index. In other words, if the index has more than one segment the given reader only represents a single segment. As a result, that definition of the filter suffers the limitation of not having the ability to permit/prohibit documents in the search results based on the terms that reside in segments that precede the current one. To address this limitation, we introduce here a StatefulFilter which specifically builds on the Filter class so as to make it capable of remembering terms in segments spanning the whole underlying index. To reiterate, the need for making filters stateful stems from the fact that some, although not most, filters care about the terms that they may have come across in prior segments. It does so by keeping track of the past terms from prior segments in a cache that is maintained in a StatefulTermsEnum instance on a per-thread basis. Additionally, to address the case where a filter might want to accept the last matching term, we keep track of the TermsEnum#docFreq of the terms in the segments filtered thus far. By comparing the sum of such TermsEnum#docFreq with that of the top-level reader, we can tell if the current segment is the last segment in which the current term appears. Ideally, for this to work correctly, we require the user to explicitly set the top-level reader on the StatefulFilter. Knowing what the top-level reader is also helps the StatefulFilter to clean up after itself once the search has concluded. Note that we leave it up to each concrete sub-class of the stateful filter to decide what to remember in its state and what not to. In other words, it can choose to remember as much or as little from prior segments as it deems necessary. In keeping with the TermsEnum interface, which the StatefulTermsEnum class extends, the filter must decide which terms to accept or not, based on the holistic state of the search.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2507</id>
      <title>automaton spellchecker</title>
      <description>The current spellchecker makes an n-gram index of your terms, and queries this for spellchecking. The terms that come back from the n-gram query are then re-ranked by an algorithm such as Levenshtein. Alternatively, we could just do a levenshtein query directly against the index, then we wouldn't need a separate index to rebuild.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2508</id>
      <title>Consolidate Highlighter implementations and a major refactor of the non-termvector highlighter</title>
      <description>Originally, I had planned to create a contrib module to allow people to highlight multiple documents in parallel, but after talking to Uwe in IRC about it, I realized that it was pretty useless. However, I was already sitting on an iterative highlighting algorithm that was much faster (my tests show 20% - 40%) and more accurate and, based on that same IRC conversation, I decided to not let all the work that I had done go to waste and try to contribute it back again. Uwe had mentioned that "More like this" detected term vectors when called and use the term vector implementation when possible, if I recall correctly, so I decided to do that. The patch that I've attached is my first stab at this. It's not nearly complete and full disclosure dictates that I say that it's not fully documented and there are not any unit tests written. I wanted to go ahead and open an issue to get some feedback on the approach that I've taken as well as the fact that it exists will be a proverbial kick in my pants to continue working on it. In short, what I've changed: Completely rewritten the non-tv highlighter to be faster and cleaner. There is some small loss in functionality for now, namely the loss of the GradientHighlighter (I just haven't done this yet) and the lack of exposure of TermFragments and their scores (I can expose this if it is deemed necessary, this is one of the things I'd like feedback on). Moved org.apache.lucene.search.vectorhighlight and org.apache.lucene.search.highlight to a single package with a unified interface, search.highlight (with two sub-packages: search.highlight.termvector and search.highlight.iterative, respectively). Unified the highlighted term formatting into a single interface: highlighter/Formatter and both highlighters use this now. What I need to do before I personally would consider this finished: Finish documentation, most specifically on TermVectorHighlighter. I haven't done this now as I expect things to change up quite a bit before they're finalized and I really hate writing documentation that goes to waste, but I do intend to complete this bullet "Flesh out" the API of search.highlight.Highlighter as it's very barebones right now Continue removing and consolidating duplicate functionality, like I've done with the highlighted word tag generation. What I think I need feedback on, before I can proceed: FastTermVectorHighlighter and the iterative highlighters need completely different sets of information in order to work. The approach I've taken is exposing a vectorHighlight method in the unified interface and a iterativeHighlight method, as well as a single highlight method that takes all the information needed for either of them and I'm unsure if this is the best way to do this. The naming of things; I'm not sure if this is a big issue, or even an issue at all, but I'd like to not break any conventions that may exist that I'm unaware of. How big of a deal is exposing the particular score of a segment from the highlighting interface and does this need to be extended into the term vector highlighting as well? There are a lot of methods in the tv implementation that are marked depracted; since this release will almost definitely break backwards compatibility anyway, are these safe to remove? Any other input anyone else may have I'm going to continue to work on things that I can work on, at least unless someone tells me I'm wasting my time and will look forward to hearing you guys' feedback! As a sidenote because it does seem rather random that I would arbitrarily re-write a working algorithm in the non-tv highlighter, I did it originally because I wanted to parallelize the highlighting (which was a failed experiment) and simply to see if I could make the algorithm faster, as I find that sort of thing particularly fun As a second sidenote, if anyone would like an explanation of the algorithm for the highlighting I devised, and why I feel that it's more accurate, I'd be happy to provide them with one (and benchmarks as well). Thanks, Eddie</description>
      <attachments/>
      <comments>5</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2509</id>
      <title>Improve readability of StandardTermsDictWriter</title>
      <description>One variable is named indexWriter, but it is a termsIndexWriter. Also some layout.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2515</id>
      <title>Expose ExecutorService in ParallelMultiSearcher</title>
      <description>I am constructing multiple lucene indexes to be search in parallel and updated multiple times per second. To keep control over the amounts of search threads created, please allow a ExecutorService to be supplied to the ParallelMultiSearcher so they can share resources. Also, please make it optional to have the shutdown or shutdownNow method of the executor called in (for example) the finalize method of ParallelMultiSearcher.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2517</id>
      <title>changes-to-html: fixes and improvements</title>
      <description>The Lucene Hudson Changes.html looks bad because changes2html.pl doesn't properly handle some new usages in CHANGES.txt.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2518</id>
      <title>Make check of BooleanClause.Occur[] in MultiFieldQueryParser.parse less stubborn</title>
      <description>Update the check in: public static Query parse(Version matchVersion, String query, String[] fields, BooleanClause.Occur[] flags, Analyzer analyzer) throws ParseException { if (fields.length != flags.length) throw new IllegalArgumentException("fields.length != flags.length"); To be: if (fields.length &gt; flags.length) So the consumer can use one Occur array and apply fields selectively. The only danger here is with hitting a non-existent cell in flags, and this check will provide this just as well without limiting usability for such cases.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2521</id>
      <title>Incorporate PROJ.4 Projections library in spatial</title>
      <description>Having other projections available can be useful for people wanting to achieve different results. Consider incorporate the Apache licensed PROJ.4 project into the Spatial module: http://www.jhlabs.com/java/maps/proj/</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2522</id>
      <title>add simple japanese tokenizer, based on tinysegmenter</title>
      <description>TinySegmenter (http://www.chasen.org/~taku/software/TinySegmenter/) is a tiny japanese segmenter. It was ported to java/lucene by Kohei TAKETA &lt;k-tak@void.in&gt;, and is under friendly license terms (BSD, some files explicitly disclaim copyright to the source code, giving a blessing instead) Koji knows the author, and already contacted about incorporating into lucene: I've contacted Takeda-san who is the creater of Java version of TinySegmenter. He said he is happy if his program is part of Lucene. He is a co-author of my book about Solr published in Japan, BTW. ;-)</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2528</id>
      <title>CFSFileDirectory: Allow a Compound Index file to be deployed as a complete index without segment files</title>
      <description>This patch presents a compound index file as a Lucene Directory class. This allows you to deploy one file to a query server instead of deploying a directory with the compound file and two segment files.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2531</id>
      <title>FieldComparator.TermOrdValComparator compares by value unnecessarily</title>
      <description>Digging on LUCENE-2504, I noticed that TermOrdValComparator's compareBottom method falls back on compare-by-value when it needn't. Specifically, if we know the current bottom ord "matches" the current segment, we can skip the value comparison when the ords are the same (ie, return 0) because the ords are exactly comparable. This is hurting string sort perf especially for optimized indices (and also unoptimized indices), and especially for highly redundant (not many unique values) fields. This affects all releases &gt;= 2.9.x, but trunk is likely more severely affected since looking up a value is more costly.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2532</id>
      <title>improve test coverage of multi-segment indices</title>
      <description>Simple patch that adds a test-only helper class, RandomIndexWriter, that lets you add docs, but it will randomly do things like use a different merge policy/scheduler, flush by doc count instead of RAM, flush randomly (so we get multi-segment indices) but also randomly optimize in the end (so we also sometimes test single segment indices).</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2535</id>
      <title>update FieldSelectorResult with a BREAK only result option. consolidate some of the state for break/load</title>
      <description>some field storage strategies can be much improved by immediate break vs. the break-and-load currently provided. this adds another break option, which makes it more advantageous to have member variables of the enum that indicate it's a break/load etc. vs. doing a bunch of grouped == checks. The BREAK option should cause an early termination with include == false in the ParallelReader when the BREAK before any other field, then the document should not be loaded from that reader. This is fairly easy to validate with a test, which is included in the patch.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2539</id>
      <title>add initial capacity based Document constructor</title>
      <description>When loading field from the index without a FieldSelector, and often in user code we know exactly how many fields a Document should have. This patch will simply add the ability to allocate Document memory for precisely. I will include as a separate patch where this would be useful in conjunction with patch LUCENE-2276.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2540</id>
      <title>Document. add get(i) and addAll to make interacting with fieldables and documents easier/faster and more readable</title>
      <description>Working with Document Fieldables is often a pain. getting the ith involves chained method calls and is not very readable: // nice doc.getFieldable(i); // not nice doc.getFields().get(i); also, when combining documents, or otherwise aggregating multiple fields into a single document, // nice doc.addAll(fieldables); // note nice: less readable and more error prone List&lt;Fieldable&gt; fields = ...; for (Fieldable field : fields) { result.add(field); }</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2542</id>
      <title>TopDocsCollector should be abstract super class that is the real "TopDocsCollector" contract, a subclass should implement the priority-queue logic. e.g. PQTopDocsCollector</title>
      <description>TopDocsCollector is both an abstract interface for producing TopDocs as well as a PriorityQueue based implementation. Not all Collectors that could produce TopDocs must use a PriorityQueue, and it would be advantageous to allow the TopDocsCollector to be an "interface" type abstract class, with a PQTopDocsCollector sub-class. While doing this, it'd be good to clean up the generics uses in these classes. As it's odd to create a TopFieldCollector and have to case the TopDocs object, when this can be fixed with generics.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2543</id>
      <title>expose position information in SegmentReader, add method getPosition(term)</title>
      <description>add public long getPosition(Term) to SegmentReader. Also, update the impl of getPosition in TermInfosReader (called by new method) to use enumerator.scanTo() to avoid creating many intermediate Term objects.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2544</id>
      <title>Add 'divisor' to NumericField, allows for easy storage of full precision data, but indexing *starting* at lower precision.</title>
      <description>In some cases, we want to index a timestamp or some other high precision numeric at a much lower precision, but we still want to store the full precision data. Rather than have to do this with two Field objects in the Document, it'd be easier to provide NumericField with a divisor as well as prevision step. The divisor would apply before beginning the trie logic. most often, this is a divide by 1, but that will happen only during the constructor or setXXXValue() in NumericTokenStream. I have the patch for this, or i will after i isolate it.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2545</id>
      <title>improve uses of StringHelper.intern in Field,AbstractField, NumericField etc.</title>
      <description>There are many times for certain field types that intern is still called when it is not needed. We can improve this with pretty simple open up of the constructors etc.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2547</id>
      <title>minimize autoboxing in NumericField</title>
      <description>dicIf you already have a Integer/Long/Double etc. numericField.setLongValue(long) causes an unnecessary auto-unbox. actually, since internal to setLongValue there is: fieldsData = Long.valueOf(value); then, there is an explicit box anyway, so this makes setLongValue(Long) with an auto-box of long roughly the same as setLongValue(long), but better if you started with a Long. Long being replaceable with Integer, Float, Double etc.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2548</id>
      <title>Remove all interning of field names from flex API</title>
      <description>In previous versions of Lucene, interning of fields was important to minimize string comparison cost when iterating TermEnums, to detect changes in field name. As we separated field names from terms in flex, no query compares field names anymore, so the whole performance problematic interning can be removed. I will start with doing this, but we need to carefully review some places e.g. in preflex codec. Maybe before this issue we should remove the Term class completely. Robert?</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2551</id>
      <title>change jdk &amp; icu collation to use byte[]</title>
      <description>Now that term is byte[], we should switch collation to use byte[] instead of 'indexablebinarystring'. This is faster and results in much smaller sort keys. I figure we can work it out here, and fix termrangequery to use byte in parallel, but we can already test sorting etc now.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2555</id>
      <title>Remove shared doc stores</title>
      <description>With per-thread DocumentsWriters sharing doc stores across segments doesn't make much sense anymore. See also LUCENE-2324.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2556</id>
      <title>(Char)TermAttribute cloning memory consumption</title>
      <description>The memory consumption problem with cloning a (Char)TermAttributeImpl object was raised on thread http://markmail.org/thread/bybuerugbk5w2u6z</description>
      <attachments/>
      <comments>18</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2558</id>
      <title>Use sequence ids for deleted docs</title>
      <description>Utilizing the sequence ids created via the update document methods, we will enable IndexReader deleted docs over a sequence id array. One of the decisions is what primitive type to use. We can start off with an int[], then possibly move to a short[] (for lower memory consumption) that wraps around.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2559</id>
      <title>reopen support for SegmentReader</title>
      <description>Reopen for SegmentReader can be supported simply as the following: @Override public synchronized IndexReader reopen() throws CorruptIndexException, IOException { return reopenSegment(this.si,false,readOnly); } @Override public synchronized IndexReader reopen(boolean openReadOnly) throws CorruptIndexException, IOException { return reopenSegment(this.si,false,openReadOnly); }</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2567</id>
      <title>RT Terms Dictionary</title>
      <description>Implement an in RAM terms dictionary for realtime search.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2573</id>
      <title>Tiered flushing of DWPTs by RAM with low/high water marks</title>
      <description>Now that we have DocumentsWriterPerThreads we need to track total consumed RAM across all DWPTs. A flushing strategy idea that was discussed in LUCENE-2324 was to use a tiered approach: Flush the first DWPT at a low water mark (e.g. at 90% of allowed RAM) Flush all DWPTs at a high water mark (e.g. at 110%) Use linear steps in between high and low watermark: E.g. when 5 DWPTs are used, flush at 90%, 95%, 100%, 105% and 110%. Should we allow the user to configure the low and high water mark values explicitly using total values (e.g. low water mark at 120MB, high water mark at 140MB)? Or shall we keep for simplicity the single setRAMBufferSizeMB() config method and use something like 90% and 110% for the water marks?</description>
      <attachments/>
      <comments>56</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2574</id>
      <title>Optimize copies between IndexInput and Output</title>
      <description>We've created an optimized copy of files from Directory to Directory. We've also optimized copyBytes recently. However, we're missing the opposite side of the copy - from IndexInput to Output. I'd like to mimic the FileChannel API by having copyTo on IndexInput and copyFrom on IndexOutput. That way, both sides can optimize the copy process, depending on the type of the IndexInput/Output that they need to copy to/from. FSIndexInput/Output can use FileChannel if the two are FS types. RAMInput/OutputStream can copy to/from the buffers directly, w/o going through intermediate ones. Actually, for RAMIn/Out this might be a big win, because it doesn't care about the type of IndexInput/Output given - it just needs to copy to its buffer directly. If we do this, I think we can consolidate all Dir.copy() impls down to one (in Directory), and rely on the In/Out ones to do the optimized copy. Plus, it will enable someone to do optimized copies between In/Out outside the scope of Directory. If this somehow turns out to be impossible, or won't make sense, then I'd like to optimize RAMDirectory.copy(Dir, src, dest) to not use an intermediate buffer.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2575</id>
      <title>Concurrent byte and int block implementations</title>
      <description>The current *BlockPool implementations aren't quite concurrent. We really need something that has a locking flush method, where flush is called at the end of adding a document. Once flushed, the newly written data would be available to all other reading threads (ie, postings etc). I'm not sure I understand the slices concept, it seems like it'd be easier to implement a seekable random access file like API. One'd seek to a given position, then read or write from there. The underlying management of byte arrays could then be hidden?</description>
      <attachments/>
      <comments>51</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2581</id>
      <title>FastVectorHighlighter: Make FragmentsBuilder use Encoder</title>
      <description>Make FragmentsBuilder use Encoder, as Highlighter does.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2583</id>
      <title>Transparent chunk transformation (compression) of index directory</title>
      <description>In some cases user is willing to sacrifice speed for space efficiency or better data security. I've developed driver for Directory that enables transparent compression (any transformation) of directory files, by using decorator pattern. With current experience, compression ratios are between 1:5 to 1:10, which depends on type of data stored in index. Directory files are sliced into fixed chunks, each chunk separately transformed (eg. compressed, encrypted, ...) and written to supporting (nested) directory for storage. I've create project page at http://code.google.com/p/lucenetransform/ and am also prepared to join contrib/store.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2586</id>
      <title>move intblock/sep codecs into test</title>
      <description>The intblock and sep codecs in core exist to make it easy for people to try different low-level algos for encoding ints. Sep breaks docs, freqs, pos, skip data, payloads into 5 separate files (vs 2 files that standard codec uses). Intblock further enables the docs, freqs, pos files to encode fixed-sized blocks of ints at a time. So an app can easily "subclass" these codecs, using their own int encoder. But these codecs are now concrete, and they use dummy low-level block int encoder (eg encoding 128 ints as separate vints). I'd like to change these to be abstract, and move these dummy codecs into test. The tests would still test these dummy codecs, by rotating them in randomly for all tests. I'd also like to rename IntBlock -&gt; FixedIntBlock, because I'm trying to get a VariableIntBlock working well (for int encoders like Simple9, Simple16, whose block size varies depending on the particular values).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2588</id>
      <title>terms index should not store useless suffixes</title>
      <description>This idea came up when discussing w/ Robert how to improve our terms index... The terms dict index today simply grabs whatever term was at a 0 mod 128 index (by default). But this is wasteful because you often don't need the suffix of the term at that point. EG if the 127th term is aa and the 128th (indexed) term is abcd123456789, instead of storing that full term you only need to store ab. The suffix is useless, and uses up RAM since we load the terms index into RAM. The patch is very simple. The optimization is particularly easy because terms are now byte[] and we sort in binary order. I tested on first 10M 1KB Wikipedia docs, and this reduces the terms index (tii) file from 3.9 MB -&gt; 3.3 MB = 16% smaller (using StandardAnalyzer, indexing body field tokenized but title / date fields untokenized). I expect on noisier terms dicts, especially ones w/ bad terms accidentally indexed, that the savings will be even more. In the future we could do crazier things. EG there's no real reason why the indexed terms must be regular (every N terms), so, we could instead pick terms more carefully, say "approximately" every N, but favor terms that have a smaller net prefix. We can also index more sparsely in regions where the net docFreq is lowish, since we can afford somewhat higher seek+scan time to these terms since enuming their docs will be much faster.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2589</id>
      <title>Add a variable-sized int block codec</title>
      <description>We already have support for fixed block size int block codecs, making it very simple to create a codec from a int encoder algorithms like FOR/PFOR. But algorithms like Simple9/16 are not fixed – they encode a variable number of adjacent ints at once, depending on the specific values of those ints.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2590</id>
      <title>Enable access to the freq information in a Query's sub-scorers</title>
      <description>The ability to gather more details than just the score, of how a given doc matches the current query, has come up a number of times on the user's lists. (most recently in the thread "Query Match Count" by Ryan McV on java-user). EG if you have a simple TermQuery "foo", on each hit you'd like to know how many times "foo" occurred in that doc; or a BooleanQuery +foo +bar, being able to separately see the freq of foo and bar for the current hit. Lucene doesn't make this possible today, which is a shame because Lucene in fact does compute exactly this information; it's just not accessible from the Collector.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2592</id>
      <title>Add static readSnapshotsInfo to PersistentSnapshotDeletionPolicy</title>
      <description>PSDP persists the snapshots information in a Directory. When you open PSDP, it obtains a write lock on the snapshots dir (by keeping an open IndexWriter), and updates the directory when snapshots are created/released. This causes problem in the following scenario – you have two processes, one updates the 'content' index and keeps PSDP open (because it also takes snapshots). Another process wants to read the existing snapshots information and open a read-only IndexReader on the 'content' index. The other process cannot read the existing snapshots information, because p1 keeps a write lock on the snapshots directory. There are two possible solutions: Have PSDP open the IndexWriter over the directory for each snapshot/release. A bit expensive, and unnecessary. Introduce a static readSnapshotsInfo on PSDP which accepts a Directory and returns the snapshots information. IMO it's cleaner, and won't have the performance overhead of opening/closing the IW as before. I'll post a patch (implementing the 2nd approach) shortly. I'd appreciate any comments.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2596</id>
      <title>Impl toString() in MergePolicy and its extensions</title>
      <description>These can be important to see for debugging. We lost them in the cutover to IWC. Just opening this issue to remind us to get them back, before releasing...</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2597</id>
      <title>Query scorers should not use MultiFields</title>
      <description>Lucene does all searching/filtering per-segment, today, but there are a number of tests that directly invoke Scorer.scorer or Filter.getDocIdSet on a composite reader.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2599</id>
      <title>Deprecate Spatial Contrib</title>
      <description>The spatial contrib is blighted by bugs. The latest series, found by Grant and discussed here shows that we need to re-think the cartesian tier implementation. Given the need to create a spatial module containing code taken from both lucene and Solr, it makes sense to deprecate the spatial contrib, and start from scratch in the new module.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2601</id>
      <title>Make getAttribute(Class attClass) Generic</title>
      <description>org.apache.lucene.util.AttributeSource current: public Attribute getAttribute(Class attClass) { final Attribute att = (Attribute) this.attributes.get(attClass); if (att == null) { throw new IllegalArgumentException("This AttributeSource does not have the attribute '" + attClass.getName() + "'."); } return att; } sample usage: TermAttribute termAtt = (TermAttribute)ts.getAttribute(TermAttribute.class) my improvment: @SuppressWarnings("unchecked") public &lt;T&gt; T getAttribute2(Class&lt;? extends Attribute&gt; attClass) { final T att = (T) this.attributes.get(attClass); if (att == null) { throw new IllegalArgumentException("This AttributeSource does not have the attribute '" + attClass.getName() + "'."); } return att; } sample usage: TermAttribute termAtt = ts.getAttribute(TermAttribute.class)</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2602</id>
      <title>Default merge policy should take deletions into account</title>
      <description>LUCENE-1634 added a calibrateSizeByDeletes; we had a TODO to default this to true as of 3.0 but we missed it. I'll fix it now for 3.1 and 4.0. While this is technically a change in back-compat (for 3.x), I think it's fine to make an exception here; this should be a big win for indices that have high doc turnover with time.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2603</id>
      <title>FastVectorHighlighter: add a method to set an arbitrary char that is used when concatenating multiValued data</title>
      <description>If the following multiValued names are in authors field: Michael McCandless Erik Hatcher Otis Gospodnetić Since FragmentsBuilder concatenates multiValued data with a space in BaseFragmentsBuilder.getFragmentSource(): while( buffer.length() &lt; endOffset &amp;&amp; index[0] &lt; values.length ){ if( index[0] &gt; 0 &amp;&amp; values[index[0]].isTokenized() &amp;&amp; values[index[0]].stringValue().length() &gt; 0 ) buffer.append( ' ' ); buffer.append( values[index[0]++].stringValue() ); } an entire field snippet (using LUCENE-2464) will be "Michael McCandless Erik Hatcher Otis Gospodnetić". There is a requirement an arbitrary char (e.g. '/') can be set so that client can separate the snippet easily. i.e. "Michael McCandless/Erik Hatcher/Otis Gospodnetić"</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2604</id>
      <title>add regexpquery to queryparser</title>
      <description>patch that adds RegexpQuery if you /enter an expression between slashes like this/ i didnt do the contrib ones but could add it there too if it seems like a good idea.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2606</id>
      <title>optimize contrib/regex for flex</title>
      <description>changes RegexCapabilities match(String) to match(BytesRef) the jakarta and jdk impls uses CharacterIterator/CharSequence matching against the utf16result instead. i also reuse the matcher for jdk, i don't see why we didnt do this before but it makes sense esp since we reuse the CSQ</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2608</id>
      <title>Allow for specification of spell checker accuracy when calling suggestSimilar</title>
      <description>There is really no need for accuracy to be a class variable in the Spellchecker</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2609</id>
      <title>Generate jar containing test classes.</title>
      <description>The test classes are useful for writing unit tests for code external to the Lucene project. It would be helpful to build a jar of these classes and publish them as a maven dependency.</description>
      <attachments/>
      <comments>64</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2610</id>
      <title>addIndexes(Directory...) should not trigger merge on flush()</title>
      <description>IndexWriter.addIndexes(Directory..) calls flush() w/ triggerMerge=true. This beats the purpose of the changes done to addIndexes to not merge any segments and leave it as the application's choice. The change is very simple - pass false instead of true. I don't plan to post a patch, however opened an issue in case some want to comment about it.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2611</id>
      <title>IntelliJ IDEA and Eclipse setup</title>
      <description>Setting up Lucene/Solr in IntelliJ IDEA or Eclipse can be time-consuming. The attached patches add a new top level directory dev-tools/ with sub-dirs idea/ and eclipse/ containing basic setup files for trunk, as well as top-level ant targets named "idea" and "eclipse" that copy these files into the proper locations. This arrangement avoids the messiness attendant to in-place project configuration files directly checked into source control. The IDEA configuration includes modules for Lucene and Solr, each Lucene and Solr contrib, and each analysis module. A JUnit run configuration per module is included. The Eclipse configuration includes a source entry for each source/test/resource location and classpath setup: a library entry for each jar. For IDEA, once ant idea has been run, the only configuration that must be performed manually is configuring the project-level JDK. For Eclipse, once ant eclipse has been run, the user has to refresh the project (right-click on the project and choose Refresh). If these patches is committed, Subversion svn:ignore properties should be added/modified to ignore the destination IDEA and Eclipse configuration locations. Iam Jambour has written up on the Lucene wiki a detailed set of instructions for applying the 3.X branch patch for IDEA: http://wiki.apache.org/lucene-java/HowtoConfigureIntelliJ</description>
      <attachments/>
      <comments>97</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>2612</id>
      <title>Add fetch-javacc task to common-build.xml</title>
      <description>I'm kind of tired setting up javacc each time I build a new environment so I added an ant target to common-build.xml. This target fetches javacc 4.1 and extracts it into the configured $ {javacc.home} directory. Nothing fancy but likely to be helpful.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2621</id>
      <title>Extend Codec to handle also stored fields and term vectors</title>
      <description>Currently Codec API handles only writing/reading of term-related data, while stored fields data and term frequency vector data writing/reading is handled elsewhere. I propose to extend the Codec API to handle this data as well.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2624</id>
      <title>add new snowball languages</title>
      <description>Snowball added new languages. This patch adds support for them. http://snowball.tartarus.org/algorithms/armenian/stemmer.html http://snowball.tartarus.org/algorithms/catalan/stemmer.html http://snowball.tartarus.org/algorithms/basque/stemmer.html</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2632</id>
      <title>FilteringCodec, TeeCodec, TeeDirectory</title>
      <description>This issue adds two new Codec implementations: TeeCodec: there have been attempts in the past to implement parallel writing to multiple indexes so that they are all synchronized. This was however complicated due to the complexity of IndexWriter/SegmentMerger logic. The solution presented here offers a similar functionality but working on a different level - as the name suggests, the TeeCodec duplicates index data into multiple output Directories. TeeDirectory (used also in TeeCodec) is a simple abstraction to perform Directory operations on several directories in parallel (effectively mirroring their data). Optionally it's possible to specify a set of suffixes of files that should be mirrored so that non-matching files are skipped. FilteringCodec is related in a remote way to the ideas of index pruning presented in LUCENE-1812 and the concept of tiered search. Since we can use TeeCodec to write to multiple output Directories in a synchronized way, we could also filter out or modify some of the data that is being written. The FilteringCodec provides this functionality, so that you can use like this: IndexWriter --&gt; TeeCodec | | | +--&gt; StandardCodec --&gt; Directory1 +--&gt; FilteringCodec --&gt; StandardCodec --&gt; Directory2 The end result of this chain is two indexes that are kept in sync - one is the full regular index, and the other one is a filtered index.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2636</id>
      <title>Create ChainingCollector</title>
      <description>ChainingCollector allows chaining a bunch of Collectors, w/o them needing to know or care about each other, and be passed into Lucene's search API, since it is a Collector on its own. It is a convenient, yet useful, class. Will post a patch w/ it shortly.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2638</id>
      <title>Make HighFreqTerms.TermStats class public</title>
      <description>It's not possible to use public methods in contrib/misc/... /HighFreqTerms from outside the package because the return type has package visibility. I propose to move TermStats class to a separate file and make it public.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2646</id>
      <title>Implement the Military Grid Reference System for tiling</title>
      <description>The current tile based system in Lucene is broken. We should standardize on a common way of labeling grids and provide that as an option. Based on previous conversations with Ryan McKinley and Chris Male, it seems the Military Grid Reference System (http://en.wikipedia.org/wiki/Military_grid_reference_system) is a good candidate for the replacement due to its standard use of metric tiles of increasing orders of magnitude (1, 10, 100, 1000, etc.)</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2647</id>
      <title>Move &amp; rename the terms dict, index, abstract postings out of oal.index.codecs.standard</title>
      <description>The terms dict components that current live under Standard codec (oal.index.codecs.standard.*) are in fact very generic, and in no way particular to the Standard codec. Already we have many other codecs (sep, fixed int block, var int block, pulsing, appending) that re-use the terms dict writer/reader components. So I'd like to move these out into oal.index.codecs, and rename them: StandardTermsDictWriter/Reader -&gt; PrefixCodedTermsWriter/Reader StandardTermsIndexWriter/Reader -&gt; AbstractTermsIndexWriter/Reader SimpleStandardTermsIndexWriter/Reader -&gt; SimpleTermsIndexWriter/Reader StandardPostingsWriter/Reader -&gt; AbstractPostingsWriter/Reader StandardPostingsWriterImpl/ReaderImple -&gt; StandardPostingsWriter/Reader With this move we have a nice reusable terms dict impl. The terms index impl is still well-decoupled so eg we could [in theory] explore a variable gap terms index. Many codecs, I expect, don't need/want to implement their own terms dict.... There are no code/index format changes here, besides the renaming &amp; fixing all imports/usages of the renamed class.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2648</id>
      <title>Allow PackedInts.ReaderIterator to advance more than one value</title>
      <description>The iterator-like API in LUCENE-2186 makes effective use of PackedInts.ReaderIterator but frequently skips multiple values. ReaderIterator currently requires to loop over ReaderInterator#next() to advance to a certain value. We should allow ReaderIterator to expose a #advance(ord) method to make use-cases like that more efficient. This issue is somewhat part of my efforts to make LUCENE-2186 smaller while breaking it up in little issues for parts which can be generally useful.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2649</id>
      <title>FieldCache should include a BitSet for matching docs</title>
      <description>The FieldCache returns an array representing the values for each doc. However there is no way to know if the doc actually has a value. This should be changed to return an object representing the values and a BitSet for all valid docs.</description>
      <attachments/>
      <comments>114</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>2650</id>
      <title>improve windows defaults in FSDirectory</title>
      <description>Currently windows defaults to SimpleFSDirectory, but this is a problem due to the synchronization. I have been benchmarking queries sequentially and was pretty surprised at how much faster MMapDirectory is, for example for cases that do many seeks. I think we should change the defaults for windows as such: if (WINDOWS and UNMAP_SUPPORTED and 64-bit) use MMapDirectory else use SimpleFSDirectory I think we should just consider doing this for 4.0 only and see how it goes.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2651</id>
      <title>Add support for MMapDirectory's unmap in Apache Harmony</title>
      <description>The setUseUnmap does not work on Apache Harmony, this patch adds support for it. It also fixes a small problem, that unmapping a clone may cause a sigsegv.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2654</id>
      <title>bulk-code each chunk b/w indexed terms in the terms dict</title>
      <description>This is an idea for exploration that came up w/ Robert... In PrefixCodedTermsDict (used by the default Standard codec), we encode each term entry "standalone", using vInts. We store the changed suffix (start, end, bytes), then metadata for the term like docFreq, frq start, prx start, skip start. Each of these ints is a vInt, which is relatively costly. If instead we store the N terms between indexed terms "column-stride", using bulk codec like FOR/PFOR, so that the 32 docFreqs are stored as one block, 32 frq deltas as another, etc., then seek and next should be faster. Ie, we could make decode of the metadata lazy, so that a seek to a term that does not exist may be able avoid any metadata decode entirely. Sequential scanning (lots of .next in a row) would also be faster, even if it needs the metadata since bulk-decode should be faster than multiple vInt decodes.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2655</id>
      <title>Get deletes working in the realtime branch</title>
      <description>Deletes don't work anymore, a patch here will fix this.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2657</id>
      <title>Replace Maven POM templates with full POMs, and change documentation accordingly</title>
      <description>The current Maven POM templates only contain dependency information, the bare bones necessary for uploading artifacts to the Maven repository. The full Maven POMs in the attached patch include the information necessary to run a multi-module Maven build, in addition to serving the same purpose as the current POM templates. Several dependencies are not available through public maven repositories. A profile in the top-level POM can be activated to install these dependencies from the various lib/ directories into your local repository. From the top-level directory: mvn -N -Pbootstrap install Once these non-Maven dependencies have been installed, to run all Lucene/Solr tests via Maven's surefire plugin, and populate your local repository with all artifacts, from the top level directory, run: mvn install When one Lucene/Solr module depends on another, the dependency is declared on the artifact(s) produced by the other module and deposited in your local repository, rather than on the other module's un-jarred compiler output in the build/ directory, so you must run mvn install on the other module before its changes are visible to the module that depends on it. To create all the artifacts without running tests: mvn -DskipTests install I almost always include the clean phase when I do a build, e.g.: mvn -DskipTests clean install</description>
      <attachments/>
      <comments>82</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>2660</id>
      <title>Add alternative search-provider to Lucene site</title>
      <description>Add additional search provider (to existed Lucid Find) search-lucene.com. I believe it was initiated somewhere in general ML discussions. It makes sense to follow the same rationales as those in TIKA-488 and NUTCH-909, since they are applicable here too.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2661</id>
      <title>fold in test cases from Lucene in Action 2nd edition</title>
      <description>Manning Publications Co., publisher of Lucene in Action, 2nd edition (http://manning.com/lucene) wishes to donate all of the book's source code, to fold into Lucene's tests! It'll take some iterating to get the tests folded in... I'll attach the initial patch.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2662</id>
      <title>BytesHash</title>
      <description>This issue will have the BytesHash separated out from LUCENE-2186</description>
      <attachments/>
      <comments>51</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2664</id>
      <title>Add SimpleText codec</title>
      <description>Inspired by Sahin Buyrukbilen's question here: http://www.lucidimagination.com/search/document/b68846e383824653/how_to_export_lucene_index_to_a_simple_text_file#b68846e383824653 I made a simple read/write codec that stores all postings data into a single text file (_X.pst), looking like this: field contents term file doc 0 pos 5 term is doc 0 pos 1 term second doc 0 pos 3 term test doc 0 pos 4 term the doc 0 pos 2 term this doc 0 pos 0 END The codec is fully funtional – all Lucene &amp; Solr tests pass with -Dtests.codec=SimpleText – but, its performance is obviously poor. However, it should be useful for debugging, transparency, understanding just what Lucene stores in its index, etc. And it's a quick way to gain some understanding on how a codec works...</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2665</id>
      <title>Rework FieldCache to be more flexible/general</title>
      <description>The existing FieldCache implementation is very rigid and does not allow much flexibility. In trying to implement simple features, it points to much larger structural problems. This patch aims to take a fresh approach to how we work with the FieldCache.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2667</id>
      <title>Fix FuzzyQuery's defaults, so its fast.</title>
      <description>We worked a lot on FuzzyQuery, but you need to be a rocket scientist to ensure good results. The main problem is that the default distance is 0.5f, which doesn't take into account the length of the string. To add insult to injury, the default number of expansions is 1024 (traditionally from BooleanQuery maxClauseCount) I propose: The syntax of FuzzyQuery is enhanced, so that you can specify raw edits too: such as foobar~2 (all terms within 2 levenshtein edits of foobar). Previously if you specified any amount &gt;=1, you got IllegalArgumentException, so this won't break anyone. You can still use foobar~0.5, and it works just as before The default for minimumSimilarity then becomes LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE, which is 2. This way if you just do foobar~, its always fast. The size of the priority queue is reduced by default from 1024 to a much more reasonable value: 50. This is what FuzzyLikeThis uses. I think its best to just change the defaults for this query, since it was so aweful before. We can add notes in migrate.txt that if you care about using the old values, then you should provide them explicitly, and you will get the same results!</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2669</id>
      <title>NumericRangeQuery.NumericRangeTermsEnum sometimes seeks backwards</title>
      <description>Subclasses of FilteredTermsEnum are "supposed to" seek forwards only (this gives better performance, typically). However, we don't check for this, so I added an assert to do that (while digging into testing the SimpleText codec) and NumericRangeQuery trips the assert! Other MTQs seem not to trip it. I think I know what's happening – say NRQ has term ranges a-c, e-f to seek to, but then while it's .next()'ing through the first range, the first term after c is f. At this point NRQ sees the range a-c is done, and then tries to seek to term e which is before f. Maybe NRQ's accept method should detect this case (where you've accidentally .next()'d into or possibly beyond the next one or more seek ranges)?</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2670</id>
      <title>allow automatontermsenum to work on full byte range</title>
      <description>AutomatonTermsEnum is really agnostic to whats in your byte[], only that its in binary order. so if you wanted to use this on some non-utf8 terms, thats just fine. the patch just does some code cleanup and removes "utf8" references, etc. additionally i changed the pkg-private, lucene-internal byte-oriented ctor, to public, lucene.experimental.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2671</id>
      <title>Add sort missing first/last ability to SortField and ValueComparator</title>
      <description>When SortField and ValueComparator use EntryCreators (from LUCENE-2649) they use a special sort value when the field is missing. This enables lucene to implement 'sort missing last' or 'sort missing first' for numeric values from the FieldCache.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2672</id>
      <title>speed up automaton seeking in nextString</title>
      <description>While testing, i found there are some queries (e.g. wildcard ?????????) that do quite a lot of backtracking. nextString doesn't handle this particularly well, when it walks the DFA, if it hits a dead-end and needs to backtrack, it increments the bytes, and starts over completely. alternatively it could save the path information in an int[], and backtrack() could return a position to restart from, instead of just a boolean.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2674</id>
      <title>improve how MTQs interact with the terms dict cache</title>
      <description>Some small improvements: Adds a TermsEnum.cacheCurrentTerm "hint" (codec can make this a no-op) Removes the FTE.useTermsCache Changes MTQ's TermCollector API to accept the TermsEnum so collectors can eg call .docFreq directly Adds expert ctor to TermQuery allowing you to pass in the docFreq</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2675</id>
      <title>Add support for 3.0 indexes in 2.9 branch</title>
      <description>There was a lot of user requests to be able to read Lucene 3.0 indexes also with 2.9. This would make the migration easier. There is no problem in doing that, as the new stored fields version in Lucene 3.0 is only used to mark a segment's stored fields file as no longer containing compressed fields. But index format did not really change. This patch simply allows FieldsReader to pass a Lucene 3.0 version number, but still writes segments in 2.9 format (as you could suddenly turn on compression for added documents). I added ZIP files for 3.0 indexes for TestBackwards. Without the patch it does not pass, as FieldsReader complains about incorrect version number (although it could read the file easily). If we would release maybe a 2.9.4 release of Lucene we should include that patch.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2679</id>
      <title>IndexWriter.deleteDocuments should have option to not apply to docs indexed in the current IW session</title>
      <description>In LUCENE-2655 we are struggling with how to handle buffered deletes, with the new per-thread RAM buffers (DWPT). But, the only reason why we must maintain a map of del term -&gt; current docID (or sequence ID) is to correctly handle the interleaved adds &amp; deletes case. However, I suspect that for many apps that interleaving never happens. Ie, most apps delete only docs from before the last commit or NRT reopen. For such apps, we don't need a Map... we just need a Set of all del terms to apply to past segments but not to the currently buffered docs. And, importantly, with LUCENE-2655, this would be a single Set, not one per DWPT. It should be a a healthy RAM reduction on buffered deletes, and should make the deletes call faster (add to one set instead of N maps). We of course must still support the interleaved case, and I think it should be the default, but I think we should provide the option for the common-case apps to take advantage of much less RAM usage.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2680</id>
      <title>Improve how IndexWriter flushes deletes against existing segments</title>
      <description>IndexWriter buffers up all deletes (by Term and Query) and only applies them if 1) commit or NRT getReader() is called, or 2) a merge is about to kickoff. We do this because, for a large index, it's very costly to open a SegmentReader for every segment in the index. So we defer as long as we can. We do it just before merge so that the merge can eliminate the deleted docs. But, most merges are small, yet in a big index we apply deletes to all of the segments, which is really very wasteful. Instead, we should only apply the buffered deletes to the segments that are about to be merged, and keep the buffer around for the remaining segments. I think it's not so hard to do; we'd have to have generations of pending deletions, because the newly merged segment doesn't need the same buffered deletions applied again. So every time a merge kicks off, we pinch off the current set of buffered deletions, open a new set (the next generation), and record which segment was created as of which generation. This should be a very sizable gain for large indices that mix deletes, though, less so in flex since opening the terms index is much faster.</description>
      <attachments/>
      <comments>76</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2682</id>
      <title>create test case to verify we support &gt; 2.1B terms</title>
      <description>I created a test case for this... I'm leaving it as @Ignore because it takes more than four hours on a faaast machine (beast) to run. I think we should run this before each release.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2683</id>
      <title>upgrade icu libraries to 4.4.2</title>
      <description>modules/analysis uses 4.4 solr/contrib/extraction uses 4.2.1 I think we should keep them the same version, for consistency, and go to 4.4.2 since it has bugfixes.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2687</id>
      <title>Remove Priority-Queue size trap in MultiTermQuery.TopTermsBooleanQueryRewrite</title>
      <description>These APIs are new in 3.x, so we can do this with no backwards-compatibility issue: Before 3.1, FuzzyQuery had its own internal rewrite method. We exposed this in 3.x as TopTermsBooleanQueryRewrite, and then as subclasses for Scoring and Boost-only variants. The problem I have is that the PQ has a default (large) size of Integer.MAX_VALUE... of course its later limited by the value of BooleanQuery's maxClauseCount, but I think this is a trap. Instead its better to simply remove these defaults and force the user to provide a default (reasonable) size.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2690</id>
      <title>Do MultiTermQuery boolean rewrites per segment</title>
      <description>MultiTermQuery currently rewrites FuzzyQuery (using TopTermsBooleanQueryRewrite), the auto constant rewrite method and the ScoringBQ rewrite methods using a MultiFields wrapper on the top-level reader. This is inefficient. This patch changes the rewrite modes to do the rewrites per segment and uses some additional datastructures (hashed sets/maps) to exclude duplicate terms. All tests currently pass, but FuzzyQuery's tests should not, because it depends for the minimum score handling, that the terms are collected in order.. Robert will fix FuzzyQuery in this issue, too. This patch is just a start.</description>
      <attachments/>
      <comments>42</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2691</id>
      <title>Consolidate Near Real Time and Reopen API semantics</title>
      <description>We should consolidate the IndexWriter.getReader and the IndexReader.reopen semantics, since most people are already using the IR.reopen() method, we should simply add:: IR.reopen(IndexWriter) Initially, it could just call the IW.getReader(), but it probably should switch to just using package private methods for sharing the internals</description>
      <attachments/>
      <comments>32</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2692</id>
      <title>Position Checking Span Queries</title>
      <description>I've created a bunch of new SpanQuery classes that allow one to do things like check to see if a SpanQuery falls between two positions (which is a more general form of SpanFirstQuery) and I've also added one that only includes a match if the payload located at the span match also matches a given payload. With the latter, one can do queries for items w/ specific payloads.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2694</id>
      <title>MTQ rewrite + weight/scorer init should be single pass</title>
      <description>Spinoff of LUCENE-2690 (see the hacked patch on that issue)... Once we fix MTQ rewrite to be per-segment, we should take it further and make weight/scorer init also run in the same single pass as rewrite.</description>
      <attachments/>
      <comments>44</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2695</id>
      <title>DisjunctionMaxScorer allocates 2 arrays per scored doc</title>
      <description>It has this: @Override public float score() throws IOException { int doc = subScorers[0].docID(); float[] sum = { subScorers[0].score() }, max = { sum[0] }; int size = numScorers; scoreAll(1, size, doc, sum, max); scoreAll(2, size, doc, sum, max); return max[0] + (sum[0] - max[0]) * tieBreakerMultiplier; } They are thread-private arrays so possibly/likely JVM can optimize this case (allocate only on the stack) but still I think instead it should have private instance vars for the score/max.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2697</id>
      <title>SortedVIntList should be Serializable</title>
      <description>For some memory intensive searchers we have, it is important to temporarily store the created bitsets to disk SortedVIntList does not implement Serializable: since it could be easily serialized due to the raw data it contains, it should implement it We are currently working with a custom SerializableSortedVIntList: we've just copied the code and added Serializable but we would definitely like to stick on lucene official code rather than such customizations Hope you will accept the proposal</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2699</id>
      <title>Update StandardTokenizer and UAX29Tokenizer to Unicode 6.0.0</title>
      <description>Newly released Unicode 6.0.0 contains some character property changes from the previous release (5.2.0) that affect word segmentation (UAX#29), and JFlex 1.5.0-SNAPSHOT now supports Unicode 6.0.0, so Lucene's UAX#29-based tokenizers should be updated accordingly. Note that the UAX#29 word break rules themselves did not change between Unicode versions 5.2.0 and 6.0.0.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2700</id>
      <title>Expose DocValues via Fields</title>
      <description>DocValues Reader are currently exposed / accessed directly via IndexReader. To integrate the new feature in a more "native" way we should expose the DocValues via Fields on a perSegment level and on MultiFields in the multi reader case. DocValues should be side by side with Fields.terms enabling access to Source, SortedSource and ValuesEnum something like that: public abstract class Fields { ... public DocValues values(); } public abstract class DocValues { /** on disk enum based API */ public abstract ValuesEnum getEnum() throws IOException; /** in memory Random Access API - with enum support - first call loads values in ram*/ public abstract Source getSource() throws IOException; /** sorted in memory Random Access API - optional operation */ public SortedSource getSortedSource(Comparator&lt;BytesRef&gt; comparator) throws IOException, UnsupportedOperationException; /** unloads previously loaded source only but keeps the doc values open */ public abstract unload(); /** closes the doc values */ public abstract close(); }</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2701</id>
      <title>Factor maxMergeSize into findMergesForOptimize in LogMergePolicy</title>
      <description>LogMergePolicy allows you to specify a maxMergeSize in MB, which is taken into consideration in regular merges, yet ignored by findMergesForOptimze. I think it'd be good if we take that into consideration even when optimizing. This will allow the caller to specify two constraints: maxNumSegments and maxMergeMB. Obviously both may not be satisfied, and therefore we will guarantee that if there is any segment above the threshold, the threshold constraint takes precedence and therefore you may end up w/ &lt;maxNumSegments (if it's not 1) after optimize. Otherwise, maxNumSegments is taken into consideration. As part of this change, I plan to change some methods to protected (from private) and members as well. I realized that if one wishes to implement his own LMP extension, he needs to either put it under o.a.l.index or copy some code over to his impl. I'll attach a patch shortly.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2702</id>
      <title>BytesRefHash#get() should expect a BytesRef instances for consistency</title>
      <description>BytesRefHash#get should use a provided BytesRef instances instead of the internally used scratch. This is how all other APIs currently work and we should be consistent.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2711</id>
      <title>BooleanScorer.nextDoc should also delegate to sub-scorer's bulk scoring method</title>
      <description>BooleanScorer uses the bulk score methods of its sub scorers, asking them to score each chunk of 2048 docs. However, its .nextDoc fails to do this, instead manually walking through the sub's docs (calling .nextDoc()), which is slower (though this'd be tiny in practice). As far as I can tell it should delegate to the bulk scorer just like it does in its bulk scorer method.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2715</id>
      <title>optimize fuzzytermsenum per-segment</title>
      <description>we can make fuzzyquery about 3% faster by not creating DFA(s) for each segment. creating the DFAs is still somewhat heavy: i can address this here too, but this is easy.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2716</id>
      <title>Improve automaton's MinimizeOperations.minimizeHopcroft() to not create so many objects</title>
      <description>MinimizeOperations.minimizeHopcroft() creates a lot of objects because of strange arrays and useless ArrayLists with fixed length. E.g. it created List&lt;List&lt;List&lt;&gt;&gt;&gt;. This patch minimizes this and makes the whole method much more GC friendler by using simple arrays or avoiding empty LinkedLists at all (inside reverse array). minimize() is called very very often, especially in tests (MockAnalyzer). A test for the method is prepared by Robert, we found a bug somewhere else in automaton, so this is pending until his issue and fix arrives.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2718</id>
      <title>separate java code from .jj file</title>
      <description>It would make development easier to move most of the java code out from the .jj file and into a real java file.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2719</id>
      <title>Re-add SorterTemplate and use it to provide fast ArraySorting and replace BytesRefHash sorting</title>
      <description>This patch adds back an optimized and rewritten SorterTemplate back to Lucene (removed after release of 3.0). It is of use for several components: Automaton: Automaton needs to sort States and other things. Using Arrays.sort() is slow, because it clones internally to ensure stable search. This component is much faster. This patch adds Arrays.sort() replacements in ArrayUtil that work with natural order or using a Comparator&lt;?&gt;. You can choose between quickSort and mergeSort. BytesRefHash uses another QuickSort algorithm without insertionSort for very short ord arrays. This class uses SorterTemplate to provide the same with insertionSort fallback in a very elegant way. Ideally this class can be used everywhere, where the sort algorithm needs to be separated from the underlying data and you can implement a swap() and compare() function (that get slot numbers instead of real values). This also applies to Solr (Yonik?). SorterTemplate provides quickSort and mergeSort algorithms. Internally for short arrays, it automatically chooses insertionSort (like JDK's Arrays). The quickSort algorith was copied modified from old BytesRefHash. This new class only shares MergeSort with the original CGLIB SorterTemplate, which is no longer maintained.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2722</id>
      <title>Sep codec should store less in terms dict</title>
      <description>I'm working on improving Lucene's performance with int block codecs (FOR/PFOR), but in early perf testing I found that these codecs cause a big perf hit to those MTQs that need to scan many terms but don't end up accepting many of those terms (eg fuzzy, wildcard, regexp). This is because sep codec stores much more in the terms dict, since each file is separate, ie seek points for each of doc, frq, pos, pyl, skp files. So I'd like to shift these seek points to instead be stored in the doc file, except for the doc seek point itself. Since a given query will always need to seek to the doc file, this does not add an extra seek. But it saves tons of vInt decodes for the next/seke intensive MTQs...</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2723</id>
      <title>Speed up Lucene's low level bulk postings read API</title>
      <description>Spinoff from LUCENE-1410. The flex DocsEnum has a simple bulk-read API that reads the next chunk of docs/freqs. But it's a poor fit for intblock codecs like FOR/PFOR (from LUCENE-1410). This is not unlike sucking coffee through those tiny plastic coffee stirrers they hand out airplanes that, surprisingly, also happen to function as a straw. As a result we see no perf gain from using FOR/PFOR. I had hacked up a fix for this, described at in my blog post at http://chbits.blogspot.com/2010/08/lucene-performance-with-pfordelta-codec.html I'm opening this issue to get that work to a committable point. So... I've worked out a new bulk-read API to address performance bottleneck. It has some big changes over the current bulk-read API: You can now also bulk-read positions (but not payloads), but, I have yet to cutover positional queries. The buffer contains doc deltas, not absolute values, for docIDs and positions (freqs are absolute). Deleted docs are not filtered out. The doc &amp; freq buffers need not be "aligned". For fixed intblock codecs (FOR/PFOR) they will be, but for varint codecs (Simple9/16, Group varint, etc.) they won't be. It's still a work in progress...</description>
      <attachments/>
      <comments>52</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2724</id>
      <title>BooleanFilter and ChainedFilter miss to fully optimize for OpenBitSets</title>
      <description>In line 65 of the BooleanFilter class there is an optimization for OpenBitSets, but i miss an optimization in line 62. I would replace the existing line: res = new OpenBitSetDISI(getDISI(shouldFilters, i, reader), reader.maxDoc()); with following code: DocIdSet docIdSet = shouldFilters.get(i).getDocIdSet(reader); if(docIdSet instanceof OpenBitSet) { res = new OpenBitSetDISI(reader.maxDoc()); res.or((OpenBitSet) docIdSet); } else { res = new OpenBitSetDISI(docIdSet.iterator(), reader.maxDoc()); } Same for line 78 and 95, adjusted for not and must filters. That leads to an up to 5 times slower AND-combination in my test, where i had two filters to be AND-combined returning each a cached OpenBitSet, one with a cardinality of 15000 and the other with a cardinality of 13000. The result had a cardinality of 8300. Thats important if you do that 1000 times with a lot more documents. The same must be also done for ChainedFilter in the method initialResult(..). Also, the getDISI method in the BooleanFilter must be replaced by a getDocIdSet(..) method. This is useful because in line 87 the docIdSet is retrieved and in line 92 again when it is not of type OpenBitSet. This may also lead to a performance issue if the getDocIdSet method of a sub filter is not super fast.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2725</id>
      <title>Bengali Analyzer for Lucene has been Developed</title>
      <description>Hi everyone, I am a CSE student of SUST, SYlhet( http://www.sust.edu/). I have noticed that there is no such Bengali Analyzer in Lucene for Bengali Text search and highlight. I have used Standard Analyzer and others but they do not give good result. So, i have developed a Bengali Analyzer. I have tested it for 50 thousand document. And it is being used in Ekushe Finance Search Engine. (http://efinance.com.bd/). Please give me some instruction so that i can contribute that analyzer in Lucene. Thanx.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2734</id>
      <title>Use IndexWriterConfig in benchmark</title>
      <description>We should use IndexWriterConfig instead of deprecated methods in benchmark.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2735</id>
      <title>First Cut at GroupVarInt with FixedIntBlockIndexInput / Output</title>
      <description>I have hacked together a FixedIntBlockIndex impl with Group VarInt encoding - this does way worse than standard codec in benchmarks but I guess that is mainly due to the FixedIntBlockIndex limitations. Once LUCENE-2723 is in / or builds with trunk again I will update and run some tests. The isolated microbenchmark shows that there could be improvements over vint even in java though and I am sure we can make it faster impl. wise.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2742</id>
      <title>Enable native per-field codec support</title>
      <description>Currently the codec name is stored for every segment and PerFieldCodecWrapper is used to enable codecs per fields which has recently brought up some issues (LUCENE-2740 and LUCENE-2741). When a codec name is stored lucene does not respect the actual codec used to encode a fields postings but rather the "top-level" Codec in such a case the name of the top-level codec is "PerField" instead of "Pulsing" or "Standard" etc. The way this composite pattern works make the indexing part of codecs simpler but also limits its capabilities. By recoding the top-level codec in the segments file we rely on the user to "configure" the PerFieldCodecWrapper correctly to open a SegmentReader. If a fields codec has changed in the meanwhile we won't be able to open the segment. The issues LUCENE-2741 and LUCENE-2740 are actually closely related to the way PFCW is implemented right now. PFCW blindly creates codecs per field on request and at the same time doesn't have any control over the file naming nor if a two codec instances are created for two distinct fields even if the codec instance is the same. If so FieldsConsumer will throw an exception since the files it relies on are already created. Having PerFieldCodecWrapper AND a CodecProvider overcomplicates things IMO. In order to use per field codec a user should on the one hand register its custom codecs AND needs to build a PFCW which needs to be maintained in the "user-land" an must not change incompatible once a new IW of IR is created. What I would expect from Lucene is to enable me to register a codec in a provider and then tell the Field which codec it should use for indexing. For reading lucene should determ the codec automatically once a segment is opened. if the codec is not available in the provider that is a different story. Once we instantiate the composite codec in SegmentsReader we only have the codecs which are really used in this segment for free which in turn solves LUCENE-2740. Yet, instead of relying on the user to configure PFCW I suggest to move composite codec functionality inside the core an record the distinct codecs per segment in the segments info. We only really need the distinct codecs used in that segment since the codec instance should be reused to prevent additional files to be created. Lets say we have the follwing codec mapping : field_a:Pulsing field_b:Standard field_c:Pulsing then we create the following mapping: SegmentInfo: [Pulsing, Standard] PerField: [field_a:0, field_b:1, field_c:0] that way we can easily determ which codec is used for which field an build the composite - codec internally on opening SegmentsReader. This ordering has another advantage, if like in that case pulsing and standard use really the same type of files we need a way to distinguish the used files per codec within a segment. We can in turn pass the codec's ord (implicit in the SegmentInfo) to the FieldConsumer on creation to create files with segmentname_ord.ext (or something similar). This solvel LUCENE-2741).</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2743</id>
      <title>SimpleText is too slow</title>
      <description>If you are unlucky enough to get SimpleText codec on the TestBasics (span query) test then it runs very slowly...</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2745</id>
      <title>ArabicAnalyzer - the ability to recognise email addresses host names and so on</title>
      <description>The ArabicAnalyzer does not recognise email addresses, hostnames and so on. For example, adam@hotmail.com will be tokenised to [adam] [hotmail] [com] It would be great if the ArabicAnalyzer can tokenises this to [adam@hotmail.com]. The same applies to hostnames and so on. Can this be resolved? I hope so Thanks MAA</description>
      <attachments/>
      <comments>20</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2746</id>
      <title>Implement PMC Branding Guidelines</title>
      <description>Per the Trademark committee's Branding Requirements, there are a number of things we need to do across our projects to comply. See http://www.apache.org/foundation/marks/pmcs.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2747</id>
      <title>Deprecate/remove language-specific tokenizers in favor of StandardTokenizer</title>
      <description>As of Lucene 3.1, StandardTokenizer implements UAX#29 word boundary rules to provide language-neutral tokenization. Lucene contains several language-specific tokenizers that should be replaced by UAX#29-based StandardTokenizer (deprecated in 3.1 and removed in 4.0). The language-specific analyzers, by contrast, should remain, because they contain language-specific post-tokenization filters. The language-specific analyzers should switch to StandardTokenizer in 3.1. Some usages of language-specific tokenizers will need additional work beyond just replacing the tokenizer in the language-specific analyzer. For example, PersianAnalyzer currently uses ArabicLetterTokenizer, and depends on the fact that this tokenizer breaks tokens on the ZWNJ character (zero-width non-joiner; U+200C), but in the UAX#29 word boundary rules, ZWNJ is not a word boundary. Robert Muir has suggested using a char filter converting ZWNJ to spaces prior to StandardTokenizer in the converted PersianAnalyzer.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2749</id>
      <title>Co-occurrence filter</title>
      <description>The co-occurrence filter to be developed here will output sets of tokens that co-occur within a given window onto a token stream. These token sets can be ordered either lexically (to allow order-independent matching/counting) or positionally (e.g. sliding windows of positionally ordered co-occurring terms that include all terms in the window are called n-grams or shingles). The parameters to this filter will be: window size: this can be a fixed sequence length, sentence/paragraph context (these will require sentence/paragraph segmentation, which is not in Lucene yet), or over the entire token stream (full field width) minimum number of co-occurring terms: &gt;= 2 maximum number of co-occurring terms: &lt;= window size token set ordering (lexical or positional) One use case for co-occurring token sets is as candidates for collocations.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2753</id>
      <title>IndexReader.listCommits should return a List and not an abstract Collection</title>
      <description>Spinoff from here: http://www.mail-archive.com/dev@lucene.apache.org/msg07509.html</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2754</id>
      <title>add spanquery support for all multitermqueries</title>
      <description>I set fix version: 4.0, but possibly we could do this for 3.x too Currently, we have a special SpanRegexQuery in contrib, and issues like LUCENE-522 open for SpanFuzzyQuery. The SpanRegexQuery in contrib is a little messy additionally. For any arbitrary MultiTermQueries to work as a SpanQuery, there are only 3 requirements: The un-rewritten query must extend SpanQuery so it can be included in Span clauses The rewritten query should be SpanOrQuery instead of BooleanQuery The rewritten term clauses should be SpanTermQueries. Instead of having logic like this for each query, i suggest adding two rewrite methods: ScoringSpanBoolean rewrite TopTermsSpanBoolean rewrite as a start i wrote these up, and added a SpanMultiTermQueryWrapper that can be used to wrap any multitermquery this way. there are a few kinks, but I think the MTQ policeman can probably help get through them.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2755</id>
      <title>Some improvements to CMS</title>
      <description>While running optimize on a large index, I've noticed several things that got me to read CMS code more carefully, and find these issues: CMS may hold onto a merge if maxMergeCount is hit. That results in the MergeThreads taking merges from the IndexWriter until they are exhausted, and only then that blocked merge will run. I think it's unnecessary that that merge will be blocked. CMS sorts merges by segments size, doc-based and not bytes-based. Since the default MP is LogByteSizeMP, and I hardly believe people care about doc-based size segments anymore, I think we should switch the default impl. There are two ways to make it extensible, if we want: Have an overridable member/method in CMS that you can extend and override - easy. Have OneMerge be comparable and let the MP determine the order (e.g. by bytes, docs, calibrate deletes etc.). Better, but will need to tap into several places in the code, so more risky and complicated. On the go, I'd like to add some documentation to CMS - it's not very easy to read and follow. I'll work on a patch.</description>
      <attachments/>
      <comments>33</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2757</id>
      <title>Refactor RewriteMethods out of MultiTermQuery</title>
      <description>Policeman work - as usual</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2760</id>
      <title>optimize spanfirstquery, spanpositionrangequery</title>
      <description>SpanFirstQuery and SpanPositionRangeQuery (SpanFirst is just a special case of this), are currently inefficient. Take this worst case example: SpanFirstQuery("the"). Currently the code reads all the positions for the term "the". But when enumerating spans, once we have passed the allowable range we should move on to the next document (skipTo)</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2761</id>
      <title>specialize payload processing from of DocsAndPositionsEnum</title>
      <description>In LUCENE-2760 i started working to try to improve the speed of a few spanqueries. In general the trick there is to avoid processing positions if you dont have to. But, we can improve queries that read lots of positions further by cleaning up SegmentDocsAndPositionsEnum, in nextPosition() this has no less than 3 payloads-related checks. however, a large majority of users/fields have no payloads at all. I think we should specialize this case into a separate implementation and speed up the common case. edit: dyslexia with the jira issue number.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2763</id>
      <title>Swap URL+Email recognizing StandardTokenizer and UAX29Tokenizer</title>
      <description>Currently, in addition to implementing the UAX#29 word boundary rules, StandardTokenizer recognizes email adresses and URLs, but doesn't provide a way to turn this behavior off and/or provide overlapping tokens with the components (username from email address, hostname from URL, etc.). UAX29Tokenizer should become StandardTokenizer, and current StandardTokenizer should be renamed to something like UAX29TokenizerPlusPlus (or something like that). For rationale, see the discussion at the reopened LUCENE-2167.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2765</id>
      <title>Optimize scanning in DocsEnum</title>
      <description>Similar to LUCENE-2761: when we call advance(), after skipping it scans, but this can be optimized better than calling nextDoc() like today // scan for the rest: do { nextDoc(); } while (target &gt; doc); in particular, the freq can be "skipVinted" and the skipDocs (deletedDocs) don't need to be checked during this scanning.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2766</id>
      <title>ParallelReader should support getSequentialSubReaders if possible</title>
      <description>Applications that need to use ParallelReader can't currently use per-segment optimizations because getSequentialSubReaders returns null. Considering the strict requirements on input indexes that ParallelReader already enforces it's usually the case that the additional indexes are built with the knowledge of the primary index, in order to keep the docId-s synchronized. If that's the case then it's conceivable that these indexes could be created with the same number of segments, which in turn would mean that their docId-s are synchronized on a per-segment level. ParallelReader should detect such cases, and in getSequentialSubReader it should return an array of ParallelReader-s created from corresponding segments of input indexes.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2768</id>
      <title>add infrastructure for longer running nightly test cases</title>
      <description>I'm spinning this out of LUCENE-2762... The patch there adds initial infrastructure for tests to pull documents from a line file, and adds a longish running test case using that line file to test NRT. I'd like to see some tests run on more substantial indices based on real data... so this is just a start.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2770</id>
      <title>Optimize SegmentMerger to work on atomic (Segment)Readers where possible</title>
      <description>This is a spin-off from LUCENE-2769: Currently SegmentMerger has some optimizations when it merges segments that are SegmentReaders (e.g. when doing normal indexing or optimizing). But when you do IndexWriter.addIndexes(IndexReader...) the listed IndexReaders may not really be per-segment. SegmentMerger should track down all passed in reads down to the lowest level (Segment)Reader (or other atomic readers like SlowMultiReaderWrapper) and then merge. We can then remove most MultiFields usage (except term merging itsself) and clean up the code. This especially saves lots of memory for merging norms, as no longer the duplicate norms arrays are created when MultiReaders are used!</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2771</id>
      <title>Remove norms() support from non-atomic IndexReaders</title>
      <description>Spin-off from LUCENE-2769: Currently all IndexReaders support norms(), but the core of Lucene never uses it and its even dangerous because of memory usage. We should do the same like with MultiFields and factor it out and throw UOE on non-atomic readers. The SlowMultiReaderWrapper can then manage the norms. Also ParallelReader needs to be fixed.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2773</id>
      <title>Don't create compound file for large segments by default</title>
      <description>Spinoff from LUCENE-2762. CFS is useful for keeping the open file count down. But, it costs some added time during indexing to build, and also ties up temporary disk space, causing eg a large spike on the final merge of an optimize. Since MergePolicy dictates which segments should be CFS, we can change it to only build CFS for "smallish" merges. I think we should also set a maxMergeMB by default so that very large merges aren't done.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2778</id>
      <title>Allow easy extension of RAMDirectory</title>
      <description>RAMDirectory uses RAMFiles to store the data. RAMFile offers a newBuffer() method for extensions to override and allocate buffers differently, from e.g. a pool or something. However, RAMDirectory always allocates RAMFile and doesn't allow allocating a RAMFile extension, which makes RAMFile.newBuffer() unusable. I think we can simply introduce a newRAMFile() method on RAMDirectory and make the RAMFiles map protected, and it will allow really extending RAMDir. I will post a patch later.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2779</id>
      <title>Use ConcurrentHashMap in RAMDirectory</title>
      <description>RAMDirectory synchronizes on its instance in many places to protect access to map of RAMFiles, in addition to updating the sizeInBytes member. In many places the sync is done for 'read' purposes, while only in few places we need 'write' access. This looks like a perfect use case for ConcurrentHashMap Also, syncing around sizeInBytes is unnecessary IMO, since it's an AtomicLong ... I'll post a patch shortly.</description>
      <attachments/>
      <comments>52</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>2780</id>
      <title>optimize spantermquery</title>
      <description>Looking at http://www.lucidimagination.com/search/document/c2c6f660ddde4f7f/dismaxqparserplugin_and_tokenization , I saw a user building DisjunctionMaxQuery / BooleanQuery with SpanTermQuerys. I wonder if users know that doing this is much slower than just using TermQuery? I agree it makes little sense to use SpanTermQuery if you arent going to use it inside a SpanNear etc, but on the other hand, I think its a little non-intuitive that it wouldnt be just as fast in a case like this. I could see this complicating queryparsing etc for users that want to sometimes use positions etc. SpanTermQuery is the same as TermQuery, except tf is computed as (#of spans * sloppyFreq(spanLength) For this case, #ofspans = tf and spanLength for a single term is always 1. Maybe we should optimize SpanTermQuery to return TermScorer, with just this special tf computation. This would avoid reading positions for anyone that does this.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2784</id>
      <title>Change all FilteredTermsEnum impls into TermsEnum decorators</title>
      <description>Currently, FilteredTermsEnum has two ctors: FilteredTermsEnum(IndexReader reader, String field) FilteredTermsEnum(TermsEnum tenum) But most of our concrete implementations (e.g. TermsRangeEnum) use the IndexReader+field ctor In my opinion we should remove this ctor, and switch over all FilteredTermsEnum implementations to just take a TermsEnum. Advantages: This simplifies FilteredTermsEnum and its subclasses, where they are more decorator-like (perhaps in the future we could compose them) Removes silly checks such as if (tenum == null) in every next() Allows for consumers to pass in enumerators however they want: e.g. its their responsibility if they want to use MultiFields or not, it shouldnt be buried in FilteredTermsEnum. I created a quick patch (all core+contrib+solr tests pass), but i think this opens up more possibilities for refactoring improvements that haven't yet been done in the patch: we should explore these too.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2786</id>
      <title>no need for LowerCaseFilter from ArabicAnalyzer</title>
      <description>No need for this line 171: result = new LowerCaseFilter(result); in ArabicAnalyzer simply because there is no lower case or upper case in Arabic language. it is totally not related to each other.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2787</id>
      <title>disable atime for DirectIOLinuxDirectory</title>
      <description>In Linux's open(): O_NOATIME (Since Linux 2.6.8) Do not update the file last access time (st_atime in the inode) when the file is read(2). This flag is intended for use by indexing or backup programs, where its use can significantly reduce the amount of disk activity. This flag may not be effective on all filesystems. One example is NFS, where the server maintains the access time. So we should do this in our linux-specific DirectIOLinuxDirectory. Separately (offtopic), it would be better if this was a LinuxDirectory that only uses O_DIRECT when it should It would be nice to think about an optional modules/native for common platforms similar to what tomcat provides Its easier to test directories like this now (-Dtests.directory)...</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2788</id>
      <title>Make CharFilter reusable</title>
      <description>The CharFilter API lets you wrap a Reader, altering the contents before the Tokenizer sees them. It also allows you to correct the offsets so this is transparent to highlighting. One problem is that the API isn't reusable, if you have a lot of short documents its going to be efficient. Additionally there is some unnecessary wrapping in Tokenizer (see the CharReader.get in the ctor, but not in reset(Reader)!!!)</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2789</id>
      <title>Let codec decide to use compound file system or not</title>
      <description>While working on LUCENE-2186 and in the context of recent mails about consolidating MergePolicy and LogMergePolicy I wanna propose a rather big change how Compund Files are created / handled in IW. Since Codecs have been introduced we have several somewhat different way of how data is written to the index. Sep codec for instance writes different files for index data and DocValues will write one file per field and segment. Eventually codecs need to have more control over how files are written ie. if CFS should be used or not is IMO really a matter of the codec used for writing. On the other hand when you look at IW internals CFS really pollutes the indexing code and relies on information from inside a codec (see SegmentWriteState.flusedFiles) actuall this differentiation spreads across many classes related to indexing including the LogMergePolicy. IMO how new flushed segments are written has nothing to do with MP in the first place and MP currently choses whether a newly flushed segment is CFS or not (correct me if I am wrong), pushing all this logic down to codecs would make lots of code much easier and cleaner. As mike said this would also reduce the API footprint if we make it private to the codec. I can imagine some situations where you really want control over certain fields to be stored as non-CFS and other to be stored as CFS. Codecs might need more information about other segments during a merge to decide if or not to use CFS based on the segments size but we can easily change that API. From a reading point of view we already have Codec#files that can decide case by case what files belong to this codec. let me know the thoughts</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2790</id>
      <title>IndexWriter should call MP.useCompoundFile and not LogMP.getUseCompoundFile</title>
      <description>Spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/112311. I will attach a patch shortly that addresses the issue on trunk.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2791</id>
      <title>WindowsDirectory</title>
      <description>We can use Windows' overlapped IO to do pread() and avoid the performance problems of SimpleFS/NIOFSDir.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2792</id>
      <title>Add a simple FST impl to Lucene</title>
      <description>I implemented the algo described at http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.3698 for incrementally building a finite state transducer (FST) from sorted inputs. This is not a fully general FST impl – it's only able to build up an FST incrementally from input/output pairs that are pre-sorted. Currently the inputs are BytesRefs, and the outputs are pluggable – NoOutputs gets you a simple FSA, PositiveIntOutputs maps to a long, ByteSequenceOutput maps to a BytesRef. The implementation has a low memory overhead, so that it can handle a fairly large set of terms. For example, it can build the FSA for the 9.8M terms from a 10M document wikipedia index in ~8 seconds (on beast), using ~256 MB peak RAM, resulting in an FSA that's ~60 MB. It packs the FST as-it-builds into a compact byte[], and then exposes the API to read nodes/arcs directly from the byte[]. The FST can be quickly saved/loaded to/from a Directory since it's just a big byte[]. The format is similar to what Morfologik uses (http://sourceforge.net/projects/morfologik/). I think there are a number of possible places we can use this in Lucene. For example, I think many apps could hold the entire terms dict in RAM, either at the multi-reader level or maybe per-segment (mapping to file offset or to something else custom to the app), which may possibly be a good speedup for certain MTQs (though, because the format is packed into a byte[], there is a decode cost when visiting arcs). The builder can also prune as it goes, so you get a prefix trie pruned according to how many terms run through the nodes, which makes it faster and even less memory consuming. This may be useful as a replacement for our current binary search terms index since it can achieve higher term density for the same RAM consumption of our current index. As an initial usage to make sure this is exercised, I cutover the SimpleText codec, which currently fully loads all terms into a TreeMap (and has caused intermittent OOME in some tests), to use an FST instead. SimpleText uses a PairOutputs which is able to "pair up" any two other outputs, since it needs to map each input term to an int docFreq and long filePosition. All tests pass w/ SimpleText forced codec, and I think this is committable except I'd love to get some help w/ the generics (confession to the policeman: I had to add @SuppressWarnings( {"unchecked"} )) all over!! Ideally an FST is parameterized by its output type (Integer, BytesRef, etc.). I even added a new @nightly test that makes a largeish set of random terms and tests the resulting FST on different outputs I think it would also be easy to make a variant that uses char[] instead of byte[] as its inputs, so we could eg use this during analysis (Robert's idea). It's already be easy to have a CharSequence output type since the outputs are pluggable. Dawid Weiss (author of HPPC – http://labs.carrotsearch.com/hppc.html – and Morfologik – http://sourceforge.net/projects/morfologik/) was very helpful iterating with me on this (thank you!).</description>
      <attachments/>
      <comments>29</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2793</id>
      <title>Directory createOutput and openInput should take an IOContext</title>
      <description>Today for merging we pass down a larger readBufferSize than for searching because we get better performance. I think we should generalize this to a class (IOContext), which would hold the buffer size, but then could hold other flags like DIRECT (bypass OS's buffer cache), SEQUENTIAL, etc. Then, we can make the DirectIOLinuxDirectory fully usable because we would only use DIRECT/SEQUENTIAL during merging. This will require fixing how IW pools readers, so that a reader opened for merging is not then used for searching, and vice/versa. Really, it's only all the open file handles that need to be different – we could in theory share del docs, norms, etc, if that were somehow possible.</description>
      <attachments/>
      <comments>96</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>2795</id>
      <title>Genericize DirectIOLinuxDir -&gt; UnixDir</title>
      <description>Today DirectIOLinuxDir is tricky/dangerous to use, because you only want to use it for indexWriter and not IndexReader (searching). It's a trap. But, once we do LUCENE-2793, we can make it fully general purpose because then a single native Dir impl can be used. I'd also like to make it generic to other Unices, if we can, so that it becomes UnixDirectory.</description>
      <attachments/>
      <comments>29</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2799</id>
      <title>MMapDirectory not designed for inheritance</title>
      <description>How to reproduce Try to inherit from MMapDirectory to change the openInput logic (open files from different directories). Expected result: Inherit from MMapDirectory, overwrite the one method, done. Actual result: It's impossible to overwrite the method as the inner classes would be missing. It's impossible to fork the inner classes as they depend on a final method with default visibility (cleanMapping). It turns out to be the easiest option to completely for the code and replace just the method in question. Possible fix: Change the visibility of most members and subtypes to be at least protected and avoid the default visibility.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2807</id>
      <title>Improve test debuggability through ant</title>
      <description>Some small improvements would go a long ways... When trying to debug an intermittent fail, I usually run w/ -Dtests.verbose=true and w/ many iters. But because the formatter buffers this can hit OOME, so maybe we make an unbuffered formatter. Also, it'd be nice if we could have the formatter discard output for a given iter if there was no failure, and I think the iters should stop as soon as a failure is hit. Maybe somehow we make a new tests.mode that would switch on these behaviours? Unbuffered formatter is also vital when debugging a deadlock...</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2810</id>
      <title>Explore Alternate Stored Field approaches for highly redundant data</title>
      <description>In some cases (logs, HTML pages w/ boilerplate, etc.), the stored fields for documents contain a lot of redundant information and end up wasting a lot of space across a large collection of documents. For instance, simply compressing a typical log file often results in &gt; 75% compression rates. We should explore mechanisms for applying compression across all the documents for a field (or fields) while still maintaining relatively fast lookup (that being said, in most logging applications, fast retrieval of a given event is not always critical.) For instance, perhaps it is possible to have a part of storage that contains the set of unique values for all the fields and the document field value simply contains a reference (could be as small as a few bits depending on the number of uniq. items) to that value instead of having a full copy. Extending this, perhaps we can leverage some existing compression capabilities in Java to provide this as well. It may make sense to implement this as a Directory, but it might also make sense as a Codec, if and when we have support for changing storage Codecs.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>2814</id>
      <title>stop writing shared doc stores across segments</title>
      <description>Shared doc stores enables the files for stored fields and term vectors to be shared across multiple segments. We've had this optimization since 2.1 I think. It works best against a new index, where you open an IW, add lots of docs, and then close it. In that case all of the written segments will reference slices a single shared doc store segment. This was a good optimization because it means we never need to merge these files. But, when you open another IW on that index, it writes a new set of doc stores, and then whenever merges take place across doc stores, they must now be merged. However, since we switched to shared doc stores, there have been two optimizations for merging the stores. First, we now bulk-copy the bytes in these files if the field name/number assignment is "congruent". Second, we now force congruent field name/number mapping in IndexWriter. This means this optimization is much less potent than it used to be. Furthermore, the optimization adds a lot of hair to IndexWriter/DocumentsWriter; this has been the source of sneaky bugs over time, and causes odd behavior like a merge possibly forcing a flush when it starts. Finally, with DWPT (LUCENE-2324), which gets us truly concurrent flushing, we can no longer share doc stores. So, I think we should turn off the write-side of shared doc stores to pave the path for DWPT to land on trunk and simplify IW/DW. We still must support reading them (until 5.0), but the read side is far less hairy.</description>
      <attachments/>
      <comments>34</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2816</id>
      <title>MMapDirectory speedups</title>
      <description>MMapDirectory has some performance problems: When the file is larger than Integer.MAX_VALUE, we use MultiMMapIndexInput, which does a lot of unnecessary bounds-checks for its buffer-switching etc. Instead, like MMapIndexInput, it should rely upon the contract of these operations in ByteBuffer (which will do a bounds check always and throw BufferUnderflowException). Our 'buffer' is so large (Integer.MAX_VALUE) that its rare this happens and doing our own bounds checks just slows things down. the readInt()/readLong()/readShort() are slow and should just defer to ByteBuffer.readInt(), etc This isn't very important since we don't much use these, but I think there's no reason users (e.g. codec writers) should have to readBytes() + wrap as bytebuffer + get an IntBuffer view when readInt() can be almost as fast...</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2818</id>
      <title>abort() method for IndexOutput</title>
      <description>I'd like to see abort() method on IndexOutput that silently (no exceptions) closes IO and then does silent papaDir.deleteFile(this.fileName()). This will simplify a bunch of error recovery code for IndexWriter and friends, but constitutes an API backcompat break. What do you think?</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2824</id>
      <title>optimizations for bufferedindexinput</title>
      <description>along the same lines as LUCENE-2816: the readVInt/readVLong/readShort/readInt/readLong are not optimal here since they defer to readByte. for example this means checking the buffer's bounds per-byte in readVint instead of per-vint. its an easy win to speed this up, even for the vint case: its essentially always faster, the only slower case is 1024 single-byte vints in a row, in this case we would do a single extra bounds check (1025 instead of 1024)</description>
      <attachments/>
      <comments>20</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2825</id>
      <title>FSDirectory.open should return MMap on 64-bit Solaris</title>
      <description>MMap is ~ 30% faster than NIOFS on this platform.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2826</id>
      <title>LineDocSource should assign stable IDs; docdate field should be NumericField</title>
      <description>Some small enhancements when indexing docs from a line doc source: Assign docid by line number instead of by number-of-docs-indexed; this makes the resulting ID stable when using multiple threads The docdate field is now indexed as a String (possible created through DateTools). I added two numeric fields: one that indexes .getTime() (= long msec) and another that indexes seconds since the day started. This gives us two numeric fields to play with...</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2829</id>
      <title>improve termquery "pk lookup" performance</title>
      <description>For things that are like primary keys and don't exist in some segments (worst case is primary/unique key that only exists in 1) we do wasted seeks. While LUCENE-2694 tries to solve some of this issue with TermState, I'm concerned we could every backport that to 3.1 for example. This is a simpler solution here just to solve this one problem in termquery... we could just revert it in trunk when we resolve LUCENE-2694, but I don't think we should leave things as they are in 3.x</description>
      <attachments/>
      <comments>21</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2830</id>
      <title>Use StringBuilder instead of StringBuffer in benchmark</title>
      <description>Minor change - use StringBuilder instead of StringBuffer in benchmark's code. We don't need the synchronization of StringBuffer in all the places that I've checked. The only place where it could be a problem is in HtmlParser's API - one method accepts a StringBuffer and it's an interface. But I think it's ok to change benchmark's API, back-compat wise and so I'd like to either change it to accept a String, or remove the method altogether – no code in benchmark uses it, and if anyone needs it, he can pass StringReader to the other method.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2831</id>
      <title>Revise Weight#scorer &amp; Filter#getDocIdSet API to pass Readers context</title>
      <description>Spinoff from LUCENE-2694 - instead of passing a reader into Weight#scorer(IR, boolean, boolean) we should / could revise the API and pass in a struct that has parent reader, sub reader, ord of that sub. The ord mapping plus the context with its parent would make several issues way easier. See LUCENE-2694, LUCENE-2348 and LUCENE-2829 to name some.</description>
      <attachments/>
      <comments>59</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2832</id>
      <title>on Windows 64-bit, maybe we should default to a better maxBBufSize in MMapDirectory</title>
      <description>Currently the default max buffer size for MMapDirectory is 256MB on 32bit and Integer.MAX_VALUE on 64bit: public static final int DEFAULT_MAX_BUFF = Constants.JRE_IS_64BIT ? Integer.MAX_VALUE : (256 * 1024 * 1024); But, in windows on 64-bit, you are practically limited to 8TB. This can cause problems in extreme cases, such as: http://www.lucidimagination.com/search/document/7522ee54c46f9ca4/map_failed_at_getsearcher Perhaps it would be good to change this default such that its 256MB on 32Bit OR windows, but leave it at Integer.MAX_VALUE on other 64-bit and "64-bit" (48-bit) systems.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2833</id>
      <title>upgrade contrib/ant's tidy.jar</title>
      <description>contrib/ant uses a Tidy.jar that also includes classes in org.w3c.dom, org.xml.sax, etc. This is no problem if you are an ant user, but if you are an IDE user you need to carefully configure the order of your classpath or things will not compile, as these will override the ones in the Solr libs, for example. The solution is to upgrade the tidy.jar to the newest one that only includes org.w3c.tidy and doesn't cause these problems.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2835</id>
      <title>FieldCache rewrite method for MultiTermQueries</title>
      <description>For some MultiTermQueries, like RangeQuery we have a FieldCacheRangeFilter etc (in this case its particularly optimized). But in the general case, since LUCENE-2784 we can now have a rewrite method to rewrite any MultiTermQuery using the FieldCache, because MultiTermQuery's getEnum no longer takes IndexReader but Terms, and all the FilteredTermsEnums are now just real TermsEnum decorators. In cases like low frequency queries this is actually slower (I think this has been shown for numeric ranges before too), but for the really high-frequency ones like especially ugly wildcards, regexes, fuzzies this can be many times faster using the FieldCache instead, since all the terms are in RAM and automaton can blast through them quicker.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2836</id>
      <title>FieldCache rewrite method for MultiTermQueries</title>
      <description>For some MultiTermQueries, like RangeQuery we have a FieldCacheRangeFilter etc (in this case its particularly optimized). But in the general case, since LUCENE-2784 we can now have a rewrite method to rewrite any MultiTermQuery using the FieldCache, because MultiTermQuery's getEnum no longer takes IndexReader but Terms, and all the FilteredTermsEnums are now just real TermsEnum decorators. In cases like low frequency queries this is actually slower (I think this has been shown for numeric ranges before too), but for the really high-frequency cases like especially ugly wildcards, regexes, fuzzies, etc, this can be several times faster using the FieldCache instead, since all the terms are in RAM and automaton can blast through them quicker.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2837</id>
      <title>Collapse Searcher/Searchable/IndexSearcher; remove contrib/remote; merge PMS into IndexSearcher</title>
      <description>We've discussed cleaning up our *Searcher stack for some time... I think we should try to do this before releasing 4.0. So I'm attaching an initial patch which: Removes Searcher, Searchable, absorbing all their methods into IndexSearcher Removes contrib/remote Removes MultiSearcher Absorbs ParallelMultiSearcher into IndexSearcher (ie you can now pass useThreads=true, or a custom ES to the ctor) The patch is rough – I just ripped stuff out, did search/replace to IndexSearcher, etc. EG nothing is directly testing using threads with IndexSearcher, but before committing I think we should add a newSearcher to LuceneTestCase, which randomly chooses whether the searcher uses threads, and cutover tests to use this instead of making their own IndexSearcher. I think MultiSearcher has a useful purpose, but as it is today it's too low-level, eg it shouldn't be involved in rewriting queries: the Query.combine method is scary. Maybe in its place we make a higher level class, with limited API, that's able to federate search across multiple IndexSearchers? It'd also be able to optionally use thread per IndexSearcher.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>2838</id>
      <title>ConstantScoreQuery should directly support wrapping Query and simply strip off scores</title>
      <description>Especially in MultiTermQuery rewrite modes we often simply need to strip off scores from Queries and make them constant score. Currently the code to do this looks quite ugly: new ConstantScoreQuery(new QueryWrapperFilter(query)) As the name says, QueryWrapperFilter should make any other Query constant score, so why does it not take a Query as ctor param? This question was aldso asked quite often by my customers and is simply correct, if you think about it. Looking closer into the code, it is clear that this would also speed up MTQs: One additional wrapping and method calls can be removed Maybe we can even deprecate QueryWrapperFilter in 3.1 now (it's now only used in tests and the use-case for this class is not really available) and LUCENE-2831 does not need the stupid hack to make Simon's assertions pass CSQ now supports out-of-order scoring and topLevel scoring, so a CSQ on top-level now directly feeds the Collector. For that a small trick is used: The score(Collector) calls are directly delegated and the scores are stripped by wrapping the setScorer() method in Collector During that I found a visibility bug in Scorer (LUCENE-2839): The method "boolean score(Collector collector, int max, int firstDocID)" should be public not protected, as its not solely intended to be overridden by subclasses and is called from other classes, too! This leads to no compiler bugs as the other classes that calls it is mainly BooleanScorer(2) and thats in same package, but visibility is wrong. I will open an issue for that and fix it at least in trunk where we have no backwards-requirement.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2841</id>
      <title>CommonGramsFilter improvements</title>
      <description>Currently CommonGramsFilter expects users to remove the common words around which output token ngrams are formed, by appending a StopFilter to the analysis pipeline. This is inefficient in two ways: captureState() is called on (trailing) stopwords, and then the whole stream has to be re-examined by the following StopFilter. The current ctor should be deprecated, and another ctor added with a boolean option controlling whether the common words should be output as unigrams. If common words are configured to be output as unigrams, captureState() will still need to be called, as it is now. If the common words are not configured to be output as unigrams, rather than calling captureState() for the trailing token in each output token ngram, the term text, position and offset can be maintained in the same way as they are now for the leading token: using a System.arrayCopy()'d term buffer and a few ints for positionIncrement and offsetd. The user then no longer would need to append a StopFilter to the analysis chain. An example illustrating both possibilities should also be added.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2842</id>
      <title>add Galician analyzer</title>
      <description>Adds analyzer for Galician, based upon "Regras do lematizador para o galego" , and a set of stopwords created in the usual fashion. This is really just an adaptation of the Portuguese RSLP, so I added that too, and modified our existing hand-coded RSLP-S (RSLP's plural-only step) to just be a plural-only flow of RSLP.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2843</id>
      <title>Add variable-gap terms index impl.</title>
      <description>PrefixCodedTermsReader/Writer (used by all "real" core codecs) already supports pluggable terms index impls. The only impl we have now is FixedGapTermsIndexReader/Writer, which picks every Nth (default 32) term and holds it in efficient packed int/byte arrays in RAM. This is already an enormous improvement (RAM reduction, init time) over 3.x. This patch adds another impl, VariableGapTermsIndexReader/Writer, which lets you specify an arbitrary IndexTermSelector to pick which terms are indexed, and then uses an FST to hold the indexed terms. This is typically even more memory efficient than packed int/byte arrays, though, it does not support ord() so it's not quite a fair comparison. I had to relax the terms index plugin api for PrefixCodedTermsReader/Writer to not assume that the terms index impl supports ord. I also did some cleanup of the FST/FSTEnum APIs and impls, and broke out separate seekCeil and seekFloor in FSTEnum. Eg we need seekFloor when the FST is used as a terms index but seekCeil when it's holding all terms in the index (ie which SimpleText uses FSTs for).</description>
      <attachments/>
      <comments>27</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2844</id>
      <title>benchmark geospatial performance based on geonames.org</title>
      <description>See comments for details. In particular, the original patch "benchmark-geo.patch" is fairly different than LUCENE-2844.patch</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2846</id>
      <title>omitTF is viral, but omitNorms is anti-viral.</title>
      <description>omitTF is viral. if you add document 1 with field "foo" as omitTF, then document 2 has field "foo" without omitTF, they are both treated as omitTF. but omitNorms is the opposite. if you have a million documents with field "foo" with omitNorms, then you add just one document without omitting norms, now you suddenly have a million 'real norms'. I think it would be good for omitNorms to be viral too, just for consistency, and also to prevent huge byte[]'s. but another option is to make omitTF anti-viral, which is more "schemaless" i guess.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2854</id>
      <title>Deprecate SimilarityDelegator and Similarity.lengthNorm</title>
      <description>SimilarityDelegator is a back compat trap (see LUCENE-2828). Apps should just [statically] subclass Sim or DefaultSim; if they really need "runtime subclassing" then they can make their own app-level delegator. Also, Sim.computeNorm subsumes lengthNorm, so we should deprecate lengthNorm in favor of computeNorm.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2856</id>
      <title>Create IndexWriter event listener, specifically for merges</title>
      <description>The issue will allow users to monitor merges occurring within IndexWriter using a callback notifier event listener. This can be used by external applications such as Solr to monitor large segment merges.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2862</id>
      <title>Track total term freq per term</title>
      <description>Right now we track docFreq for each term (how many docs have the term), but the totalTermFreq (total number of occurrences of this term, ie sum of freq() for each doc that has the term) is also a useful stat (for flex scoring, PulsingCodec, etc.).</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2864</id>
      <title>add maxtf to fieldinvertstate</title>
      <description>the maximum within-document TF is a very useful scoring value, we should expose it so that people can use it in scoring consider the following sim: @Override public float idf(int docFreq, int numDocs) { return 1.0F; /* not used */ } @Override public float computeNorm(String field, FieldInvertState state) { return state.getBoost() / (float) Math.sqrt(state.getMaxTF()); } which is surprisingly effective, but more interesting for practical reasons.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2865</id>
      <title>Pass a context struct to Weight#scorer instead of naked booleans</title>
      <description>Weight#scorer(AtomicReaderContext, boolean, boolean) is hard to extend if another boolean like "needsScoring" or similar flags / information need to be passed to Scorers. An immutable struct would make such an extension trivial / way easier.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2867</id>
      <title>Change contrib QP API that uses CharSequence as string identifier</title>
      <description>There are some API methods on contrib queryparser that expects CharSequence as identifier. This is wrong, since it may lead to incorrect or mislead behavior, as shown on LUCENE-2855. To avoid this problem, these APIs will be changed and enforce the use of String instead of CharSequence on version 4. This patch already deprecate the old API methods and add new substitute methods that uses only String.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2868</id>
      <title>It should be easy to make use of TermState; rewritten queries should be shared automatically</title>
      <description>When you have the same query in a query hierarchy multiple times, tremendous savings can now be had if the user knows enough to share the rewritten queries in the hierarchy, due to the TermState addition. But this is clumsy and requires a lot of coding by the user to take advantage of. Lucene should be smart enough to share the rewritten queries automatically. This can be most readily (and powerfully) done by introducing a new method to Query.java: Query rewriteUsingCache(IndexReader indexReader) ... and including a caching implementation right in Query.java which would then work for all. Of course, all callers would want to use this new method rather than the current rewrite().</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2870</id>
      <title>if a segment is 100% deletions, we should just drop it</title>
      <description>I think in IndexWriter if the delCount ever == maxDoc() for a segment we should just drop it? We don't, today, and so we force it to be merged, which is silly. I won't have time for this any time soon so if someone wants to take it, please do!! Should be simple.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2871</id>
      <title>Use FileChannel in FSDirectory</title>
      <description>Explore using FileChannel in FSDirectory to see if it improves write operations performance</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2872</id>
      <title>Terms dict should block-encode terms</title>
      <description>With PrefixCodedTermsReader/Writer we now encode each term standalone, ie its bytes, metadata, details for postings (frq/prox file pointers), etc. But, this is costly when something wants to visit many terms but pull metadata for only few (eg respelling, certain MTQs). This is particularly costly for sep codec because it has more metadata to store, per term. So instead I think we should block-encode all terms between indexed term, so that the metadata is stored "column stride" instead. This makes it faster to enum just terms.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2878</id>
      <title>Allow Scorer to expose positions and payloads aka. nuke spans</title>
      <description>Currently we have two somewhat separate types of queries, the one which can make use of positions (mainly spans) and payloads (spans). Yet Span*Query doesn't really do scoring comparable to what other queries do and at the end of the day they are duplicating lot of code all over lucene. Span*Queries are also limited to other Span*Query instances such that you can not use a TermQuery or a BooleanQuery with SpanNear or anthing like that. Beside of the Span*Query limitation other queries lacking a quiet interesting feature since they can not score based on term proximity since scores doesn't expose any positional information. All those problems bugged me for a while now so I stared working on that using the bulkpostings API. I would have done that first cut on trunk but TermScorer is working on BlockReader that do not expose positions while the one in this branch does. I started adding a new Positions class which users can pull from a scorer, to prevent unnecessary positions enums I added ScorerContext#needsPositions and eventually Scorere#needsPayloads to create the corresponding enum on demand. Yet, currently only TermQuery / TermScorer implements this API and other simply return null instead. To show that the API really works and our BulkPostings work fine too with positions I cut over TermSpanQuery to use a TermScorer under the hood and nuked TermSpans entirely. A nice sideeffect of this was that the Position BulkReading implementation got some exercise which now work all with positions while Payloads for bulkreading are kind of experimental in the patch and those only work with Standard codec. So all spans now work on top of TermScorer ( I truly hate spans since today ) including the ones that need Payloads (StandardCodec ONLY)!! I didn't bother to implement the other codecs yet since I want to get feedback on the API and on this first cut before I go one with it. I will upload the corresponding patch in a minute. I also had to cut over SpanQuery.getSpans(IR) to SpanQuery.getSpans(AtomicReaderContext) which I should probably do on trunk first but after that pain today I need a break first . The patch passes all core tests (org.apache.lucene.search.highlight.HighlighterTest still fails but I didn't look into the MemoryIndex BulkPostings API yet)</description>
      <attachments/>
      <comments>206</comments>
      <commenters>14</commenters>
    </issue>
    <issue>
      <id>2881</id>
      <title>Track FieldInfo per segment instead of per-IW-session</title>
      <description>Currently FieldInfo is tracked per IW session to guarantee consistent global field-naming / ordering. IW carries FI instances over from previous segments which also carries over field properties like isIndexed etc. While having consistent field ordering per IW session appears to be important due to bulk merging stored fields etc. carrying over other properties might become problematic with Lucene's Codec support. Codecs that rely on consistent properties in FI will fail if FI properties are carried over. The DocValuesCodec (DocValuesBranch) for instance writes files per segment and field (using the field id within the file name). Yet, if a segment has no DocValues indexed in a particular segment but a previous segment in the same IW session had DocValues, FieldInfo#docValues will be true since those values are reused from previous segments. We already work around this "limitation" in SegmentInfo with properties like hasVectors or hasProx which is really something we should manage per Codec &amp; Segment. Ideally FieldInfo would be managed per Segment and Codec such that its properties are valid per segment. It also seems to be necessary to bind FieldInfoS to SegmentInfo logically since its really just per segment metadata.</description>
      <attachments/>
      <comments>57</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2885</id>
      <title>Add WaitForMergesTask</title>
      <description>When building an index, if you just .close the IW, you may leave merges still needing to be done... so a WaitForMerges task lets algs fix this.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2886</id>
      <title>Adaptive Frame Of Reference</title>
      <description>We could test the implementation of the Adaptive Frame Of Reference [1] on the lucene-4.0 branch. I am providing the source code of its implementation. Some work needs to be done, as this implementation is working on the old lucene-1458 branch. I will attach a tarball containing a running version (with tests) of the AFOR implementation, as well as the implementations of PFOR and of Simple64 (simple family codec working on 64bits word) that has been used in the experiments in [1]. [1] http://www.deri.ie/fileadmin/documents/deri-tr-afor.pdf</description>
      <attachments/>
      <comments>26</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2887</id>
      <title>Remove/deprecate IndexReader.undeleteAll</title>
      <description>This API is rather dangerous in that it's "best effort" since it can only un-delete docs that have not yet been merged away, or, dropped (as of LUCENE-2010). Given that it exposes impl details of how Lucene prunes deleted docs, I think we should remove this API. Are there legitimate use cases....?</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2892</id>
      <title>Add QueryParser.newFieldQuery</title>
      <description>Note: this patch changes no behavior, just makes QP more subclassable. Currently we have Query getFieldQuery(String field, String queryText, boolean quoted) This contains very hairy methods for producing a query from QP's analyzer. I propose we factor this into newFieldQuery(Analyzer analyzer, String field, String queryText, boolean quoted) Then getFieldQuery just calls newFieldQuery(this.analyzer, field, queryText, quoted); The reasoning is: it can be quite useful to consider the double quote as more than phrases, but a "more exact" search. In the case the user quoted the terms, you might want to analyze the text with an alternate analyzer that: doesn't produce synonyms, doesnt decompose compounds, doesn't use WordDelimiterFilter (you would need to be using preserveOriginal=true at index time for the WDF one), etc etc. This is similar to the way google's double quote operator works, its not defined as phrase but "this exact wording or phrase". For example compare results to a query of tests versus "tests". Currently you can do this without heavy code duplication, but really only if you make a separate field (which is wasteful), and make your custom QP lie about its field... in the examples I listed above you can do this with a single field, yet still have a more exact phrase search.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2894</id>
      <title>Use of google-code-prettify for Lucene/Solr Javadoc</title>
      <description>My company, RONDHUIT uses google-code-prettify (Apache License 2.0) in Javadoc for syntax highlighting: http://www.rondhuit-demo.com/RCSS/api/com/rondhuit/solr/analysis/JaReadingSynonymFilterFactory.html I think we can use it for Lucene javadoc (java sample code in overview.html etc) and Solr javadoc (Analyzer Factories etc) to improve or simplify our life.</description>
      <attachments/>
      <comments>29</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2896</id>
      <title>in advance(), don't try to skip if there is evidence it will fail</title>
      <description>There are TODO's about this in the code everywhere, and this was part of Mike speeding up ExactPhraseScorer. I think the codec should do this.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2897</id>
      <title>apply delete-by-Term and docID immediately to newly flushed segments</title>
      <description>Spinoff from LUCENE-2324. When we flush deletes today, we keep them as buffered Term/Query/docIDs that need to be deleted. But, for a newly flushed segment (ie fresh out of the DWPT), this is silly, because during flush we visit all terms and we know their docIDs. So it's more efficient to apply the deletes (for this one segment) at that time. We still must buffer deletes for all prior segments, but these deletes don't need to map to a docIDUpto anymore; ie we just need a Set. This issue should wait until LUCENE-1076 is in since that issue cuts over buffered deletes to a transactional stream.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2899</id>
      <title>Add OpenNLP Analysis capabilities as a module</title>
      <description>Now that OpenNLP is an ASF project and has a nice license, it would be nice to have a submodule (under analysis) that exposed capabilities for it. Drew Farris, Tom Morton and I have code that does: Sentence Detection as a Tokenizer (could also be a TokenFilter, although it would have to change slightly to buffer tokens) NamedEntity recognition as a TokenFilter We are also planning a Tokenizer/TokenFilter that can put parts of speech as either payloads (PartOfSpeechAttribute?) on a token or at the same position. I'd propose it go under: modules/analysis/opennlp</description>
      <attachments/>
      <comments>101</comments>
      <commenters>27</commenters>
    </issue>
    <issue>
      <id>2900</id>
      <title>make applying deletes optional when pulling a new NRT reader</title>
      <description>Usually when you pull an NRT reader, you want all deletes to be applied. But in some expert cases you may not need it (eg you just want to validate that the doc was indexed). Since it's costly to apply deletes, and trivial to add this boolean (we already have a boolean internally), I think we should add it. The deletes are still buffered, and you can always later pull another reader (for "real" searching) with deletes applied.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2903</id>
      <title>Improvement of PForDelta Codec</title>
      <description>There are 3 versions of PForDelta implementations in the Bulk Branch: FrameOfRef, PatchedFrameOfRef, and PatchedFrameOfRef2. The FrameOfRef is a very basic one which is essentially a binary encoding (may result in huge index size). The PatchedFrameOfRef is the implmentation based on the original version of PForDelta in the literatures. The PatchedFrameOfRef2 is my previous implementation which are improved this time. (The Codec name is changed to NewPForDelta.). In particular, the changes are: 1. I fixed the bug of my previous version (in Lucene-1410.patch), where the old PForDelta does not support very large exceptions (since the Simple16 does not support very large numbers). Now this has been fixed in the new LCPForDelta. 2. I changed the PForDeltaFixedIntBlockCodec. Now it is faster than the other two PForDelta implementation in the bulk branch (FrameOfRef and PatchedFrameOfRef). The codec's name is "NewPForDelta", as you can see in the CodecProvider and PForDeltaFixedIntBlockCodec. 3. The performance test results are: 1) My "NewPForDelta" codec is faster then FrameOfRef and PatchedFrameOfRef for almost all kinds of queries, slightly worse then BulkVInt. 2) My "NewPForDelta" codec can result in the smallest index size among all 4 methods, including FrameOfRef, PatchedFrameOfRef, and BulkVInt, and itself) 3) All performance test results are achieved by running with "-server" instead of "-client"</description>
      <attachments/>
      <comments>31</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2906</id>
      <title>Filter to process output of ICUTokenizer and create overlapping bigrams for CJK</title>
      <description>The ICUTokenizer produces unigrams for CJK. We would like to use the ICUTokenizer but have overlapping bigrams created for CJK as in the CJK Analyzer. This filter would take the output of the ICUtokenizer, read the ScriptAttribute and for selected scripts (Han, Kana), would produce overlapping bigrams.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2912</id>
      <title>remove field param from computeNorm, scorePayload ; remove UOE'd lengthNorm, switch SweetSpot to per-field</title>
      <description>In LUCENE-2236 we switched sim to per field (SimilarityProvider returns a per-field similarity). But we didn't completely cleanup there... I think we should now do this: SweetSpotSimilarity loses all its hashmaps. Instead, just configure one per field and return it in your SimilarityProvider. this means for example, all its TF factors can now be configured per-field too, not just the length normalization factors. computeNorm and scorePayload lose their field parameter, as its redundant and confusing. the UOE'd obselete lengthNorm is removed. I also updated javadocs that were pointing to it (this is bad!).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2915</id>
      <title>make CoreCodecProvider convenience class so apps can easily pick per-field codecs</title>
      <description>We already have DefaultCodecProvider, which simply registers all core codecs and uses Standard for all fields, but it's package private. We should make this public, and name it CoreCodecProvider.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2918</id>
      <title>IndexWriter should prune 100% deleted segs even in the NRT case</title>
      <description>We now prune 100% deleted segs on commit from IW or IR (LUCENE-2010), but this isn't quite aggressive enough, because in the NRT case you rarely call commit. Instead, the moment we delete the last doc of a segment, it should be pruned from the in-memory segmentInfos. This way, if you open an NRT reader, or a merge kicks off, or commit is called, the 100% deleted segment is already gone.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2919</id>
      <title>IndexSplitter that divides by primary key term</title>
      <description>Index splitter that divides by primary key term. The contrib MultiPassIndexSplitter we have divides by docid, however to guarantee external constraints it's sometimes necessary to split by a primary key term id. I think this implementation is a fairly trivial change.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2921</id>
      <title>Now that we track the code version at the segment level, we can stop tracking it also in each file level</title>
      <description>Now that we track the code version that created the segment at the segment level, we can stop tracking versions in each file. This has several major benefits: Today the constant names that use to track versions are confusing - they do not state since which version it applies to, and so it's harder to determine which formats we can stop supporting when working on the next major release. Those format numbers are usually negative, but in some cases positive (inconsistency) – we need to remember to increase it "one down" for the negative ones, which I always find confusing. It will remove the format tracking from all the *Writers, and the *Reader will receive the code format (String) and work w/ the appropriate constant (e.g. Constants.LUCENE_30). Centralizing version tracking to SegmentInfo is an advantage IMO. It's not urgent that we do it for 3.1 (though it requires an index format change), because starting from 3.1 all segments track their version number anyway (or migrated to track it), so we can safely release it in follow-on 3x release.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2922</id>
      <title>Optimize BlockTermsReader.seek</title>
      <description>When we seek, we first consult the terms index to find the right block of 32 (default) terms that may hold the target term. Then, we scan that block looking for an exact match. The scanning just uses next() and then compares the full term, but this is actually rather wasteful. First off, since all terms in the block share a common prefix, we should compare the target against that common prefix once, and then only compare the new suffix of each term. Second, since the term suffixes have already been read up front into a byte[], we should do a no-copy comparison (vs today, where we first read a copy into the local BytesRef and then compare). With this opto, I removed the ability for BlockTermsWriter/Reader to support arbitrary term sort order – it's now hardwired to BytesRef.utf8SortedAsUnicode.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2923</id>
      <title>cleanup contrib/demo</title>
      <description>I don't think we should include optimize in the demo; many people start from the demo and may think you must optimize to do searching, and that's clearly not the case. I think we should also use a buffered reader in FileDocument? And... I'm tempted to remove IndexHTML (and the html parser) entirely. It's ancient, and we now have Tika to extract text from many doc formats.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2926</id>
      <title>The "init" target in contrib-build.xml should cause the lucene-core jar to be created</title>
      <description>It would be helpful (e.g. for simplifying the demo contrib build: LUCENE-2923) if the contribs/modules could depend on the up-to-date existence of the lucene-core jar.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2928</id>
      <title>Improve LuceneTestCase javadocs</title>
      <description>Now that the Lucene test-framework javadocs will be published, they should get some attention.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2930</id>
      <title>Store the last term in the terms dictionary</title>
      <description>We can store the last term in the terms dictionary, and allow retrieval of the value. This is useful for applications that for example are storing sequential ids and want to know the max stored in the index.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2931</id>
      <title>Improved javadocs for PriorityQueue#lessThan</title>
      <description>It kills me that I have to inspect the code every time I implement a PriorityQueue.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2933</id>
      <title>Two-stage state expansion for the FST: distance-from-root and child-count criteria.</title>
      <description>In the current implementation FST states are expanded into a binary search on labels (from a vector of transitions) when the child count of a state exceeds a given predefined threshold (NUM_ARCS_FIXED_ARRAY). This threshold affects automaton size and traversal speed (as it turns out when benchmarked). For some degenerate data sets, close-to-the-root nodes could have a small number of children (below the threshold) and yet be traversed on every single seek. A fix of this is to introduce two control thresholds: EXPAND state if (distance-from-root &lt;= MIN_DISTANCE || children-count &gt;= NUM_ARCS_FIXED_ARRAY) My plan is to create a data set that will prove this first and then to implement the workaround above.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2934</id>
      <title>Alternative depth-based DOT layout ordering in FST's Utils</title>
      <description>Utils.toDot() dumps GraphViz's DOT file, but it can be quite difficult to read. This patch provides an alternative layout that is probably a little bit easier on the eyes (well, as far as larger FSTs can be</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2935</id>
      <title>Let Codec consume entire document</title>
      <description>Currently the codec API is limited to consume Terms &amp; Postings upon a segment flush. To enable stored fields &amp; DocValues to make use of the Codec abstraction codecs should allow to pull a consumer ahead of flush time and consume all values from a document's field though a consumer API. An alternative to consuming the entire document would be extending FieldsConsumer to return a StoredValueConsumer / DocValuesConsumer like it is done in DocValues - Branch right now side by side to the TermsConsumer. Yet, extending this has proven to be very tricky and error prone for several reasons: FieldsConsumer requires SegmentWriteState which might be different upon flush compared to when the document is consumed. SegmentWriteState must therefor be created twice 1. when the first docvalues field is indexed 2. when flushed. FieldsConsumer are current pulled for each indexed field no matter if there are terms to be indexed or not. Yet, if we use something like DocValuesCodec which essentially wraps another codec and creates FieldConsumer on demand the wrapped codecs consumer might not be initialized even if the field is indexed. This causes problems once such a field is opened but missing the required files for that codec. I added some harsh logic to work around this which should be prevented. SegmentCodecs are created for each SegmentWriteState which might yield wrong codec IDs depending on how fields numbers are assigned. We currently depend on the fact that all fields for a segment and therefore their codecs are known when SegmentCodecs are build. To enable consuming perDoc values in codecs we need to do that incrementally Codecs should instead provide a DocumentConsumer side by side with the FieldsConsumer created prior to flush. This is also a prerequisite for LUCENE-2621</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2948</id>
      <title>Make var gap terms index a partial prefix trie</title>
      <description>Var gap stores (in an FST) the indexed terms (every 32nd term, by default), minus their non-distinguishing suffixes. However, often times the resulting FST is "close" to a prefix trie in some portion of the terms space. By allowing some nodes of the FST to store all outgoing edges, including ones that do not lead to an indexed term, and by recording that this node is then "authoritative" as to what terms exist in the terms dict from that prefix, we can get some important benefits: It becomes possible to know that a certain term prefix cannot exist in the terms index, which means we can save a disk seek in some cases (like PK lookup, docFreq, etc.) We can query for the next possible prefix in the index, allowing some MTQs (eg FuzzyQuery) to save disk seeks. Basically, the terms index is able to answer questions that previously required seeking/scanning in the terms dict file.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2949</id>
      <title>FastVectorHighlighter FieldTermStack could likely benefit from using TermVectorMapper</title>
      <description>Based on my reading of the FieldTermStack constructor that loads the vector from disk, we could probably save a bunch of time and memory by using the TermVectorMapper callback mechanism instead of materializing the full array of terms into memory and then throwing most of them out.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>2951</id>
      <title>Solr should not wrap TokenStreams in a caching filter when using usePhraseHighlighter=true</title>
      <description>this is a very old limitation, so we should not do this anymore - this makes us pay a penalty for using the caching filter even when we do not need to and there are only basic term queries to highlight.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2952</id>
      <title>Make license checking/maintenance easier/automated</title>
      <description>Instead of waiting until release to check licenses are valid, we should make it a part of our build process to ensure that all dependencies have proper licenses, etc.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2955</id>
      <title>Add utitily class to manage NRT reopening</title>
      <description>I created a simple class, NRTManager, that tries to abstract away some of the reopen logic when using NRT readers. You give it your IW, tell it min and max nanoseconds staleness you can tolerate, and it privately runs a reopen thread to periodically reopen the searcher. It subsumes the SearcherManager from LIA2. Besides running the reopen thread, it also adds the notion of a "generation" containing changes you've made. So eg it has addDocument, returning a long. You can then take that long value and pass it back to the getSearcher method and getSearcher will return a searcher that reflects the changes made in that generation. This gives your app the freedom to force "immediate" consistency (ie wait for the reopen) only for those searches that require it, like a verifier that adds a doc and then immediately searches for it, but also use "eventual consistency" for other searches. I want to also add support for the new "applyDeletions" option when pulling an NRT reader. Also, this is very new and I'm sure buggy – the concurrency is either wrong over overly-locking. But it's a start...</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2957</id>
      <title>generate-maven-artifacts target should include all non-Mavenized Lucene &amp; Solr dependencies</title>
      <description>Currently, in addition to deploying artifacts for all of the Lucene and Solr modules to a repository (by default local), the generate-maven-artifacts target also deploys artifacts for the following non-Mavenized Solr dependencies (lucene_solr_3_1 version given here): solr/lib/commons-csv-1.0-SNAPSHOT-r966014.jar as org.apache.solr:solr-commons-csv:3.1 solr/lib/apache-solr-noggit-r944541.jar as org.apache.solr:solr-noggit:3.1 The following .jar's should be added to the above list (lucene_solr_3_1 version given here): lucene/contrib/icu/lib/icu4j-4_6.jar lucene/contrib/benchmark/lib/xercesImpl-2.9.1-patched-XERCESJ-1257.jar solr/contrib/clustering/lib/carrot2-core-3.4.2.jar** solr/contrib/uima/lib/uima-an-alchemy.jar solr/contrib/uima/lib/uima-an-calais.jar solr/contrib/uima/lib/uima-an-tagger.jar solr/contrib/uima/lib/uima-an-wst.jar solr/contrib/uima/lib/uima-core.jar I think it makes sense to follow the same model as the current non-Mavenized dependencies: groupId = org.apache.solr/.lucene artifactId = solr-/lucene-&lt;original-name&gt;, version = &lt;lucene-solr-release-version&gt;. **The carrot2-core jar doesn't need to be included in trunk's release artifacts, since there already is a Mavenized Java6-compiled jar. branch_3x and lucene_solr_3_1 will need this Solr-specific Java5-compiled maven artifact, though.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2958</id>
      <title>WriteLineDocTask improvements</title>
      <description>Make WriteLineDocTask and LineDocSource more flexible/extendable: allow to emit lines also for empty docs (keep current behavior as default) allow more/less/other fields</description>
      <attachments/>
      <comments>25</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>2959</id>
      <title>[GSoC] Implementing State of the Art Ranking for Lucene</title>
      <description>Lucene employs the Vector Space Model (VSM) to rank documents, which compares unfavorably to state of the art algorithms, such as BM25. Moreover, the architecture is tailored specically to VSM, which makes the addition of new ranking functions a non- trivial task. This project aims to bring state of the art ranking methods to Lucene and to implement a query architecture with pluggable ranking functions. The wiki page for the project can be found at http://wiki.apache.org/lucene-java/SummerOfCode2011ProjectRanking.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2960</id>
      <title>Allow (or bring back) the ability to setRAMBufferSizeMB on an open IndexWriter</title>
      <description>In 3.1 the ability to setRAMBufferSizeMB is deprecated, and removed in trunk. It would be great to be able to control that on a live IndexWriter. Other possible two methods that would be great to bring back are setTermIndexInterval and setReaderTermsIndexDivisor. Most of the other setters can actually be set on the MergePolicy itself, so no need for setters for those (I think).</description>
      <attachments/>
      <comments>36</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>2961</id>
      <title>Remove benchmark/lib/xml-apis.jar - JVM 1.5 already contains the required JAXP 1.3 implementation</title>
      <description>On LUCENE-2957, Uwe wrote: xml-apis.jar is not needed with xerces-2.9 and Java 5, as Java 5 already has these interface classes (JAXP 1.3). Xerces 2.11 needs it, because it ships with Java 6's JAXP release (containing STAX &amp; Co. not available in Java 5). On the #lucene IRC channel, Uwe also wrote: since we are on java 5 since 3.0 we have the javax APIs already available in the JVM xerces until 2.9.x only needs JAXP 1.3 so the only thing you need is xercesImpl.jar and serializer.jar serializer.jar is shared between all apache xml projects, dont know the exact version number ok you dont need it whan you only parse xml as soon as you want to serialize a dom tree or result of an xsl transformation you need it [...] but if we upgrade to latest xerces we need it [the xml-apis jar] again unless we are on java 6 so the one shipped with xerces 2.11 is the 1.4 one because xerces 2.11 supports Stax</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2962</id>
      <title>Skip data should be inlined into the postings lists</title>
      <description>Today, we store all skip data as a separate blob at the end of a given term's postings (if that term occurs in enough docs to warrant skip data). But this adds overhead during decoding – we have to seek to a different place for the initial load, we have to init separate readers, we have to seek again while using the lower levels of the skip data, etc. Also, we have to fully decode all skip information even if we are not going to use it (eg if I only want docIDs, I still must decode position offset and lastPayloadLength). If instead we interleaved skip data into the postings file, we could keep it local, and "private" to each file that needs skipping. This should make it least costly to init and then use the skip data, which'd be a good perf gain for eg PhraseQuery, AndQuery.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2963</id>
      <title>Easier way to run benchmark</title>
      <description>Move Benchmark.main to a more easily accessible method exec() that can be easily invoked by external programs.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2964</id>
      <title>Allow benchmark tasks from alternative packages</title>
      <description>Relax current limitation of all tasks in same package - that of PerfTask. Add a property "alt.tasks.packages" - its value are comma separated full package names. If the task class is not found in the default package, an attempt is made to load it from the alternate packages specified in that property.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2965</id>
      <title>Faster GeoHashUtils</title>
      <description>I found the current implement of org.apache.lucene.spatial.geohash.GeoHashUtils.encode and decode is slow and this is my improvement (400% faster ) /** Encodes the given latitude and longitude into a geohash * @param latitude Latitude to encode @param longitude Longitude to encode @return Geohash encoding of the longitude and latitude */ public static String encode(double latitude, double longitude) { double latL = -90, latH = 90; double lngL = -180, lngH = 180; double mid; // assert PRECISION % 2 == 0; final char[] geohash = new char[PRECISION]; int len = 0; int ch = 0; while (len &lt; PRECISION) { if (longitude &gt; (mid = (lngL + lngH) * 0.5)) { ch |= 16; lngL = mid; } else lngH = mid; if (longitude &gt; (mid = (lngL + lngH) * 0.5)) { ch |= 4; lngL = mid; } else lngH = mid; if (longitude &gt; (mid = (lngL + lngH) * 0.5)) { ch |= 1; lngL = mid; } else { lngH = mid; } if (latitude &gt; (mid = (latL + latH) * 0.5)) { ch |= 8; latL = mid; } else { latH = mid; } if (latitude &gt; (mid = (latL + latH) * 0.5)) { ch |= 2; latL = mid; } else { latH = mid; } geohash[len++] = BASE_32[ch]; ch = 0; if (latitude &gt; (mid = (latL + latH) * 0.5)) { ch |= 16; latL = mid; } else latH = mid; if (longitude &gt; (mid = (lngL + lngH) * 0.5)) { ch |= 8; lngL = mid; } else lngH = mid; if (latitude &gt; (mid = (latL + latH) * 0.5)) { ch |= 4; latL = mid; } else latH = mid; if (longitude &gt; (mid = (lngL + lngH) * 0.5)) { ch |= 2; lngL = mid; } else lngH = mid; if (latitude &gt; (mid = (latL + latH) * 0.5)) { ch |= 1; latL = mid; } else latH = mid; geohash[len++] = BASE_32[ch]; ch = 0; } return new String(geohash); } /** Decodes the given geohash into a latitude and longitude * @param geohash Geohash to deocde @return Array with the latitude at index 0, and longitude at index 1 */ public static double[] decode(String geohash) { double latL = -90.0, latH = 90.0; double lngL = -180.0, lngH = 180.0; double gap; int len = geohash.length(); for (int i = 0; i &lt; len; ) Unknown macro: { switch (geohash.charAt(i++)) { case '0': latH -= (latH - latL) * 0.75; lngH -= (lngH - lngL) * 0.875; break; case '1': latH -= (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.125; lngH -= gap * 0.75; break; case '2': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.5; lngH -= (lngH - lngL) * 0.875; break; case '3': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.5; gap = lngH - lngL; lngL += gap * 0.125; lngH -= gap * 0.75; break; case '4': latH -= (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.625; break; case '5': latH -= (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.375; lngH -= gap * 0.5; break; case '6': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.5; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.625; break; case '7': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.5; gap = lngH - lngL; lngL += gap * 0.375; lngH -= gap * 0.5; break; case '8': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.25; lngH -= (lngH - lngL) * 0.875; break; case '9': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.25; gap = lngH - lngL; lngL += gap * 0.125; lngH -= gap * 0.75; break; case 'b': latL += (latH - latL) * 0.75; lngH -= (lngH - lngL) * 0.875; break; case 'c': latL += (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.125; lngH -= gap * 0.75; break; case 'd': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.25; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.625; break; case 'e': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.25; gap = lngH - lngL; lngL += gap * 0.375; lngH -= gap * 0.5; break; case 'f': latL += (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.625; break; case 'g': latL += (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.375; lngH -= gap * 0.5; break; case 'h': latH -= (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.375; break; case 'j': latH -= (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.625; lngH -= gap * 0.25; break; case 'k': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.5; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.375; break; case 'm': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.5; gap = lngH - lngL; lngL += gap * 0.625; lngH -= gap * 0.25; break; case 'n': latH -= (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.75; lngH -= gap * 0.125; break; case 'p': latH -= (latH - latL) * 0.75; lngL += (lngH - lngL) * 0.875; break; case 'q': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.5; gap = lngH - lngL; lngL += gap * 0.75; lngH -= gap * 0.125; break; case 'r': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.5; lngL += (lngH - lngL) * 0.875; break; case 's': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.25; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.375; break; case 't': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.25; gap = lngH - lngL; lngL += gap * 0.625; lngH -= gap * 0.25; break; case 'u': latL += (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.375; break; case 'v': latL += (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.625; lngH -= gap * 0.25; break; case 'w': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.25; gap = lngH - lngL; lngL += gap * 0.75; lngH -= gap * 0.125; break; case 'x': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.25; lngL += (lngH - lngL) * 0.875; break; case 'y': latL += (latH - latL) * 0.75; gap = lngH - lngL; lngL += gap * 0.75; lngH -= gap * 0.125; break; case 'z': latL += (latH - latL) * 0.75; lngL += (lngH - lngL) * 0.875; break; } switch (geohash.charAt(i++)) { case '0': latH -= (latH - latL) * 0.875; lngH -= (lngH - lngL) * 0.75; break; case '1': gap = latH - latL; latL += gap * 0.125; latH -= gap * 0.75; lngH -= (lngH - lngL) * 0.75; break; case '2': latH -= (latH - latL) * 0.875; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.5; break; case '3': gap = latH - latL; latL += gap * 0.125; latH -= gap * 0.75; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.5; break; case '4': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.625; lngH -= (lngH - lngL) * 0.75; break; case '5': gap = latH - latL; latL += gap * 0.375; latH -= gap * 0.5; lngH -= (lngH - lngL) * 0.75; break; case '6': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.625; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.5; break; case '7': gap = latH - latL; latL += gap * 0.375; latH -= gap * 0.5; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.5; break; case '8': latH -= (latH - latL) * 0.875; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.25; break; case '9': gap = latH - latL; latL += gap * 0.125; latH -= gap * 0.75; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.25; break; case 'b': latH -= (latH - latL) * 0.875; lngL += (lngH - lngL) * 0.75; break; case 'c': gap = latH - latL; latL += gap * 0.125; latH -= gap * 0.75; lngL += (lngH - lngL) * 0.75; break; case 'd': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.625; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.25; break; case 'e': gap = latH - latL; latL += gap * 0.375; latH -= gap * 0.5; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.25; break; case 'f': gap = latH - latL; latL += gap * 0.25; latH -= gap * 0.625; lngL += (lngH - lngL) * 0.75; break; case 'g': gap = latH - latL; latL += gap * 0.375; latH -= gap * 0.5; lngL += (lngH - lngL) * 0.75; break; case 'h': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.375; lngH -= (lngH - lngL) * 0.75; break; case 'j': gap = latH - latL; latL += gap * 0.625; latH -= gap * 0.25; lngH -= (lngH - lngL) * 0.75; break; case 'k': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.375; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.5; break; case 'm': gap = latH - latL; latL += gap * 0.625; latH -= gap * 0.25; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.5; break; case 'n': gap = latH - latL; latL += gap * 0.75; latH -= gap * 0.125; lngH -= (lngH - lngL) * 0.75; break; case 'p': latL += (latH - latL) * 0.875; lngH -= (lngH - lngL) * 0.75; break; case 'q': gap = latH - latL; latL += gap * 0.75; latH -= gap * 0.125; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.5; break; case 'r': latL += (latH - latL) * 0.875; gap = lngH - lngL; lngL += gap * 0.25; lngH -= gap * 0.5; break; case 's': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.375; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.25; break; case 't': gap = latH - latL; latL += gap * 0.625; latH -= gap * 0.25; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.25; break; case 'u': gap = latH - latL; latL += gap * 0.5; latH -= gap * 0.375; lngL += (lngH - lngL) * 0.75; break; case 'v': gap = latH - latL; latL += gap * 0.625; latH -= gap * 0.25; lngL += (lngH - lngL) * 0.75; break; case 'w': gap = latH - latL; latL += gap * 0.75; latH -= gap * 0.125; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.25; break; case 'x': latL += (latH - latL) * 0.875; gap = lngH - lngL; lngL += gap * 0.5; lngH -= gap * 0.25; break; case 'y': gap = latH - latL; latL += gap * 0.75; latH -= gap * 0.125; lngL += (lngH - lngL) * 0.75; break; case 'z': latL += (latH - latL) * 0.875; lngL += (lngH - lngL) * 0.75; break; } } return new double[] { (latL + latH) / 2D, (lngL + lngH) / 2D } ; }</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>2967</id>
      <title>Use linear probing with an additional good bit avalanching function in FST's NodeHash.</title>
      <description>I recently had an interesting discussion with Sebastiano Vigna (fastutil), who suggested that linear probing, given a hash mixing function with good avalanche properties, is a way better method of constructing lookups in associative arrays compared to quadratic probing. Indeed, with linear probing you can implement removals from a hash map without removed slot markers and linear probing has nice properties with respect to modern CPUs (caches). I've reimplemented HPPC's hash maps to use linear probing and we observed a nice speedup (the same applies for fastutils of course). This patch changes NodeHash's implementation to use linear probing. The code is a bit simpler (I think . I also moved the load factor to a constant – 0.5 seems like a generous load factor, especially if we allow large FSTs to be built. I don't see any significant speedup in constructing large automata, but there is no slowdown either (I checked on one machine only for now, but will verify on other machines too).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2968</id>
      <title>SurroundQuery doesn't support SpanNot</title>
      <description>It would be nice if we could do span not in the surround query, as they are quite useful for keeping searches within a boundary (say a sentence)</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2971</id>
      <title>Auto Generate our LICENSE.txt and NOTICE.txt files</title>
      <description>Once LUCENE-2952 is in place, we should be able to automatically generate Lucene and Solr's LICENSE.txt and NOTICE.txt file (without massive duplication)</description>
      <attachments/>
      <comments>6</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2973</id>
      <title>Source distribution packaging targets should make a tarball from "svn export"</title>
      <description>Instead of picking and choosing which stuff to include from a local working copy, Lucene's dist-src/package-tgz-src target and Solr's package-src target should simply perform "svn export" with the same revision and URL as the local working copy.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2977</id>
      <title>WriteLineDocTask should write gzip/bzip2/txt according to the extension of specified output file name</title>
      <description>Since the readers behave this way it would be nice and handy if also this line writer would.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2979</id>
      <title>Simplify configuration API of contrib Query Parser</title>
      <description>The current configuration API is very complicated and inherit the concept used by Attribute API to store token information in token streams. However, the requirements for both (QP config and token stream) are not the same, so they shouldn't be using the same thing. I propose to simplify QP config and make it less scary for people intending to use contrib QP. The task is not difficult, it will just require a lot of code change and figure out the best way to do it. That's why it's a good candidate for a GSoC project. I would like to hear good proposals about how to make the API more friendly and less scaring</description>
      <attachments/>
      <comments>24</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>2981</id>
      <title>Review and potentially remove unused/unsupported Contribs</title>
      <description>Some of our contribs appear to be lacking for development/support or are missing tests. We should review whether they are even pertinent these days and potentially deprecate and remove them. One of the things we did in Mahout when bringing in Colt code was to mark all code that didn't have tests as @deprecated and then we removed the deprecation once tests were added. Those that didn't get tests added over about a 6 mos. period of time were removed. I would suggest taking a hard look at: ant db lucli swing (spatial should be gutted to some extent and moved to modules)</description>
      <attachments/>
      <comments>16</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>2983</id>
      <title>FieldInfos should be read-only if loaded from disk</title>
      <description>Currently FieldInfos create a private FieldNumberBiMap when they are loaded from a directory which is necessary due to some limitation we need to face with IW#addIndexes(Dir). If we add an index via a directory to an existing index field number can conflict with the global field numbers in the IW receiving the directories. Those field number conflicts will remain until those segments are merged and we stabilize again based on the IW global field numbers. Yet, we unnecessarily creating a BiMap here where we actually should enforce read-only semantics since nobody should modify this FieldInfos instance we loaded from the directory. If somebody needs to get a modifiable copy they should simply create a new one and all all FieldInfo instances to it.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2984</id>
      <title>Move hasVectors() &amp; hasProx() responsibility out of SegmentInfo to FieldInfos</title>
      <description>Spin-off from LUCENE-2881 which had this change already but due to some random failures related to this change I remove this part of the patch to make it more isolated and easier to test.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2985</id>
      <title>Build SegmentCodecs incrementally for consistent codecIDs during indexing</title>
      <description>currently we build the SegementCodecs during flush which is fine as long as no codec needs to know which fields it should handle. This will change with DocValues or when we expose StoredFields / TermVectors via Codec (see LUCENE-2621 or LUCENE-2935). The other downside it that we don't have a consistent view of which codec belongs to which field during indexing and all FieldInfo instances are unassigned (set to -1). Instead we should build the SegmentCodecs incrementally as fields come in so no matter when a codec needs to be selected to process a document / field we have the right codec ID assigned.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2990</id>
      <title>Improve ArrayUtil/CollectionUtil.*Sort() methods to early-reaturn on empty or one-element lists/arrays</title>
      <description>It might be a good idea to make CollectionUtil or ArrayUtil return early if the passed-in list or array's length &lt;= 1 because sorting is unneeded then. This improves maybe automaton or other places, as for empty or one-element lists no SorterTermplate is created.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>2993</id>
      <title>Lucene Test Framework javadoc's main page should be specific to the framework, rather than using Lucene Core's overview.html</title>
      <description>An overview.html should be added for Lucene's test framework.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>2997</id>
      <title>PayloadQueryParser addition</title>
      <description>I recently needed to deploy payloads for my search system and ran into a small wall: there was no query parser available for use with Payloads. I through this one together, extending out of the new modular QueryParser structure. Attached is the class file. I didn't know what package this would belong in (whether with the query parser or with the rest of the payload functionality in contrib/analyzers), so it's in the default package for now. I know this is a little, simple thing, but it seemed like something that should probably be included.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>2998</id>
      <title>Forward Port 3.1 Ant release tasks</title>
      <description>In order to get the release candidate built, I put some changes on 3.1 build.xml and haven't forward ported them to 3.2 and 4.0. They revolve around the prepare-release and stage targets in both Lucene and Solr</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3001</id>
      <title>Add TrieFieldHelper lucene so we can write solr compatible Trie* fields w/o solr dependency</title>
      <description>The solr support for numeric fields writes the stored value as binary vs the lucene NumericField We should move this logic to a helper class in lucene core so that libraries that do not depend on solr can write TrieFields that solr can read.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3003</id>
      <title>Move UnInvertedField into Lucene core</title>
      <description>Solr's UnInvertedField lets you quickly lookup all terms ords for a given doc/field. Like, FieldCache, it inverts the index to produce this, and creates a RAM-resident data structure holding the bits; but, unlike FieldCache, it can handle multiple values per doc, and, it does not hold the term bytes in RAM. Rather, it holds only term ords, and then uses TermsEnum to resolve ord -&gt; term. This is great eg for faceting, where you want to use int ords for all of your counting, and then only at the end you need to resolve the "top N" ords to their text. I think this is a useful core functionality, and we should move most of it into Lucene's core. It's a good complement to FieldCache. For this first baby step, I just move it into core and refactor Solr's usage of it. After this, as separate issues, I think there are some things we could explore/improve: The first-pass that allocates lots of tiny byte[] looks like it could be inefficient. Maybe we could use the byte slices from the indexer for this... We can improve the RAM efficiency of the TermIndex: if the codec supports ords, and we are operating on one segment, we should just use it. If not, we can use a more RAM-efficient data structure, eg an FST mapping to the ord. We may be able to improve on the main byte[] representation by using packed ints instead of delta-vInt? Eventually we should fold this ability into docvalues, ie we'd write the byte[] image at indexing time, and then loading would be fast, instead of uninverting</description>
      <attachments/>
      <comments>15</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3006</id>
      <title>Javadocs warnings should fail the build</title>
      <description>We should fail the build when there are javadocs warnings, as this should not be the Release Manager's job to fix all at once right before the release. See http://www.lucidimagination.com/search/document/14bd01e519f39aff/brainstorming_on_improving_the_release_process</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3011</id>
      <title>FST serialization and deserialization from plain DataInput/DataOutput streams.</title>
      <description>Currently the automaton can be saved only to a Directory instance (IndexInput/ IndexOutput).</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3016</id>
      <title>Analyzer for Latvian</title>
      <description>Less aggressive form of Kreslins' phd thesis: A stemming algorithm for Latvian.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3017</id>
      <title>FST should differentiate between final vs non-final stop nodes</title>
      <description>I'm breaking out this one improvement from LUCENE-2948... Currently, if a node has no outgoing edges (a "stop node") the FST forcefully marks this as a final node, but it need not do this. Ie, whether that node is final or not should be orthogonal to whether it has arcs leaving or not.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3019</id>
      <title>FVH: uncontrollable color tags</title>
      <description>The multi-colored tags is a feature of FVH. But it is uncontrollable (or more precisely, unexpected by users) that which color is used for each terms.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3030</id>
      <title>Block tree terms dict &amp; index</title>
      <description>Our default terms index today breaks terms into blocks of fixed size (ie, every 32 terms is a new block), and then we build an index on top of that (holding the start term for each block). But, it should be better to instead break terms according to how they share prefixes. This results in variable sized blocks, but means within each block we maximize the shared prefix and minimize the resulting terms index. It should also be a speedup for terms dict intensive queries because the terms index becomes a "true" prefix trie, and can be used to fast-fail on term lookup (ie returning NOT_FOUND without having to seek/scan a terms block). Having a true prefix trie should also enable much faster intersection with automaton (but this will be a new issue). I've made an initial impl for this (called BlockTreeTermsWriter/Reader). It's still a work in progress... lots of nocommits, and hairy code, but tests pass (at least once!). I made two new codecs, temporarily called StandardTree, PulsingTree, that are just like their counterparts but use this new terms dict. I added a new "exactOnly" boolean to TermsEnum.seek. If that's true and the term is NOT_FOUND, we will (quickly) return NOT_FOUND and the enum is unpositioned (ie you should not call next(), docs(), etc.). In this approach the index and dict are tightly connected, so it does not support a pluggable index impl like BlockTermsWriter/Reader. Blocks are stored on certain nodes of the prefix trie, and can contain both terms and pointers to sub-blocks (ie, if the block is not a leaf block). So there are two trees, tied to one another – the index trie, and the blocks. Only certain nodes in the trie map to a block in the block tree. I think this algorithm is similar to burst tries (http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3499), except it allows terms to be stored on inner blocks (not just leaf blocks). This is important for Lucene because an [accidental] "adversary" could produce a terms dict with way too many blocks (way too much RAM used by the terms index). Still, with my current patch, an adversary can produce too-big blocks... which we may need to fix, by letting the terms index not be a true prefix trie on it's leaf edges. Exactly how the blocks are picked can be factored out as its own policy (but I haven't done that yet). Then, burst trie is one policy, my current approach is another, etc. The policy can be tuned to the terms' expected distribution, eg if it's a primary key field and you only use base 10 for each character then you want block sizes of size 10. This can make a sizable difference on lookup cost. I modified the FST Builder to allow for a "plugin" that freezes the "tail" (changed suffix) of each added term, because I use this to find the blocks.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3034</id>
      <title>If you vary a setting per round and that setting is a long string, the report padding/columns break down.</title>
      <description>This is especially noticeable if you vary a setting where the value is a fully specified class name - in this case, it would be nice if columns in each row still lined up.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3040</id>
      <title>analysis consumers should use reusable tokenstreams</title>
      <description>Some analysis consumers (highlighter, more like this, memory index, contrib queryparser, ...) are using Analyzer.tokenStream but should be using Analyzer.reusableTokenStream instead for better performance.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3041</id>
      <title>Support Query Visting / Walking</title>
      <description>Out of the discussion in LUCENE-2868, it could be useful to add a generic Query Visitor / Walker that could be used for more advanced rewriting, optimizations or anything that requires state to be stored as each Query is visited. We could keep the interface very simple: public interface QueryVisitor { Query visit(Query query); } and then use a reflection based visitor like Earwin suggested, which would allow implementators to provide visit methods for just Querys that they are interested in.</description>
      <attachments/>
      <comments>32</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3048</id>
      <title>Improve BooleanQuery rewrite documentation</title>
      <description>While looking over BooleanQuery#rewrite, I found a couple of things confusing. Why, in the case of a single clause, is the boost set as it is, and whats going on with the lazy initialisation of the cloned BooleanQuery. I'm just adding a few lines of documentation to both situations to clarify this.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3056</id>
      <title>Support Query Rewriting Caching</title>
      <description>Out of LUCENE-3041, its become apparent that using a Visitor / Walker isn't right for caching the rewrites of Querys. Although we still intend to introduce the Query / Walker for advanced query transformations, rewriting still serves a purpose for very specific implementation detail writing. As such, it can be very expensive. So I think we should introduce first class support for rewrite caching. I also feel the key is to make the caching as transparent as possible, to reduce the strain on Query implementors. The TermState idea gave me the idea of maybe making a RewriteState / RewriteCache / RewriteInterceptor, which would be consulted for rewritten Querys. It would then maintain an internal cache that it would check. If a value wasn't found, it'd then call Query#rewrite, and cache the result. By having this external rewrite source, people could 'pre' rewrite Querys if they were particularly expensive but also common.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3058</id>
      <title>FST should allow more than one output for the same input</title>
      <description>For the block tree terms dict, it turns out I need this case.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3060</id>
      <title>Revise ThreadAffinityDocumentsWriterThreadPool queue handling</title>
      <description>Spin-off from LUCENE-3023... In ThreadAffinityDocumentsWriterThreadPool#getAndLock() we had talked about switching from a per-threadstate queue (safeway model) to a single queue (whole foods)</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3061</id>
      <title>Open IndexWriter API to allow custom MergeScheduler implementation</title>
      <description>IndexWriter's getNextMerge() and merge(OneMerge) are package-private, which makes it impossible for someone to implement his own MergeScheduler. We should open up these API, as well as any other that can be useful for custom MS implementations.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3065</id>
      <title>NumericField should be stored in binary format in index (matching Solr's format)</title>
      <description>(Spinoff of LUCENE-3001) Today when writing stored fields we don't record that the field was a NumericField, and so at IndexReader time you get back an "ordinary" Field and your number has turned into a string. See https://issues.apache.org/jira/browse/LUCENE-1701?focusedCommentId=12721972&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12721972 We have spare bits already in stored fields, so, we should use one to record that the field is numeric, and then encode the numeric field in Solr's more-compact binary format. A nice side-effect is we fix the long standing issue that you don't get a NumericField back when loading your document.</description>
      <attachments/>
      <comments>41</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3069</id>
      <title>Lucene should have an entirely memory resident term dictionary</title>
      <description>FST based TermDictionary has been a great improvement yet it still uses a delta codec file for scanning to terms. Some environments have enough memory available to keep the entire FST based term dict in memory. We should add a TermDictionary implementation that encodes all needed information for each term into the FST (custom fst.Output) and builds a FST from the entire term not just the delta.</description>
      <attachments/>
      <comments>94</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>3071</id>
      <title>PathHierarchyTokenizer adaptation for urls: splits reversed</title>
      <description>PathHierarchyTokenizer should be usable to split urls the a "reversed" way (useful for faceted search against urls): www.site.com -&gt; www.site.com, site.com, com Moreover, it should be able to skip a given number of first (or last, if reversed) tokens: /usr/share/doc/somesoftware/INTERESTING/PART Should give with 4 tokens skipped: INTERESTING INTERESTING/PART</description>
      <attachments/>
      <comments>25</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3073</id>
      <title>make compoundfilewriter public</title>
      <description>CompoundFileReader is public, but CompoundFileWriter is not. I propose we make it public + @lucene.internal instead (just in case someone else finds themselves wanting to manipulate cfs files)</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3075</id>
      <title>DocValues should be optionally be stored in a PerCodec CFS file to prevent too many files in the index</title>
      <description>Currently docvalues create one file per field to store the docvalues. Yet this could easily lead to too many open files so me might need to enable CFS per codec to keep the number of files reasonable.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3076</id>
      <title>add -Dtests.codecprovider</title>
      <description>Currently to test a codec (or set of codecs) you have to add them to lucene's core and edit a couple of arrays here and there... It would be nice if when using the test-framework you could instead specify a codecprovider by classname (possibly containing your own set of huper-duper codecs). For example I made the following little codecprovider in contrib: public class AppendingCodecProvider extends CodecProvider { public AppendingCodecProvider() { register(new AppendingCodec()); register(new SimpleTextCodec()); } } Then, I'm able to run tests with 'ant -lib build/contrib/misc/lucene-misc-4.0-SNAPSHOT.jar test-core -Dtests.codecprovider=org.apache.lucene.index.codecs.appending.AppendingCodecProvider', and it always picks from my set of codecs (in this case Appending and SimpleText), and I can set -Dtests.codec=Appending if i want to set just one.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3079</id>
      <title>Faceting module</title>
      <description>Faceting is a hugely important feature, available in Solr today but not [easily] usable by Lucene-only apps. We should fix this, by creating a shared faceting module. Ideally, we factor out Solr's faceting impl, and maybe poach/merge from other impls (eg Bobo browse). Hoss describes some important challenges we'll face in doing this (http://markmail.org/message/5w35c2fr4zkiwsz6), copied here: To look at "faceting" as a concrete example, there are big the reasons faceting works so well in Solr: Solr has total control over the index, knows exactly when the index has changed to rebuild caches, has a strict schema so it can make sense of field types and pick faceting algos accordingly, has multi-phase distributed search approach to get exact counts efficiently across multiple shards, etc... (and there are still a lot of additional enhancements and improvements that can be made to take even more advantage of knowledge solr has because it "owns" the index that we no one has had time to tackle) This is a great list of the things we face in refactoring. It's also important because, if Solr needed to be so deeply intertwined with caching, schema, etc., other apps that want to facet will have the same "needs" and so we really have to address them in creating the shared module. I think we should get a basic faceting module started, but should not cut Solr over at first. We should iterate on the module, fold in improvements, etc., and then, once we can fully verify that cutting over doesn't hurt Solr (ie lose functionality or performance) we can later cutover.</description>
      <attachments/>
      <comments>45</comments>
      <commenters>12</commenters>
    </issue>
    <issue>
      <id>3080</id>
      <title>cutover highlighter to BytesRef</title>
      <description>Highlighter still uses char[] terms (consumes tokens from the analyzer as char[] not as BytesRef), which is causing problems for merging SOLR-2497 to trunk.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3081</id>
      <title>Document Maven nightly builds, artifact generation, and using Maven to build Lucene/Solr</title>
      <description>There should be documentation we can point people to when they ask how to use Maven with Lucene and Solr.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3082</id>
      <title>Add tool to upgrade all segments of an index to last recent supported index format without optimizing</title>
      <description>Currently if you want to upgrade an old index to the format of your current Lucene version, you have to optimize your index or use addIndexes(IndexReader...) [see LUCENE-2893] to copy to a new directory. The optimize() approach fails if your index is already optimized. I propose to add a custom MergePolicy to upgrade all segments to the last format. This MergePolicy could simply also ignore all segments already up-to-date. All segments in prior formats would be merged to a new segment using another MergePolicy's optimize strategy. This issue is different from LUCENE-2893, as it would only support upgrading indexes from previous Lucene versions in-place using the official path. Its a tool for the end user, not a developer tool. This addition should also go to Lucene 3.x, as we need to make users with pre-3.0 indexes go the step through 3.x, else they would not be able to open their index with 4.0. With this tool in 3.x the users could safely upgrade their index without relying on optimize to work on already-optimized indexes.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3084</id>
      <title>MergePolicy.OneMerge.segments should be List&lt;SegmentInfo&gt; not SegmentInfos, Remove Vector&lt;SI&gt; subclassing from SegmentInfos &amp; more refactoring</title>
      <description>SegmentInfos carries a bunch of fields beyond the list of SI, but for merging purposes these fields are unused. We should cutover to List&lt;SI&gt; instead. Also SegmentInfos subclasses Vector&lt;SI&gt;, this should be removed and the collections be hidden inside the class. We can add unmodifiable views on it (asList(), asSet()).</description>
      <attachments/>
      <comments>30</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3086</id>
      <title>add ElisionsFilter to ItalianAnalyzer</title>
      <description>we set this up for french by default, but we don't for italian. we should enable it with the standard italian contractions (e.g. definite articles). the various stemmers for these languages assume this is already being taken care of and don't do anything about it... in general things like snowball assume really dumb tokenization, that you will split on the word-internal ', and they add these to stoplists.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3092</id>
      <title>NRTCachingDirectory, to buffer small segments in a RAMDir</title>
      <description>I created this simply Directory impl, whose goal is reduce IO contention in a frequent reopen NRT use case. The idea is, when reopening quickly, but not indexing that much content, you wind up with many small files created with time, that can possibly stress the IO system eg if merges, searching are also fighting for IO. So, NRTCachingDirectory puts these newly created files into a RAMDir, and only when they are merged into a too-large segment, does it then write-through to the real (delegate) directory. This lets you spend some RAM to reduce I0.</description>
      <attachments/>
      <comments>33</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>3094</id>
      <title>optimize lev automata construction</title>
      <description>in our lev automata algorithm, we compute an upperbound of the maximum possible states (not the true number), and create some "useless" unconnected states "floating around". this isn't harmful, in the original impl we did the Automaton is simply a pointer to the initial state, and all algorithms traverse this list, so effectively the useless states were dropped immediately. But recently we changed automaton to cache its numberedStates, and we set them here, so these useless states are being kept around. it has no impact on performance, but can be really confusing if you are debugging (e.g. toString). Thanks to Dawid Weiss for noticing this. at the same time, forcing an extra traversal is a bit scary, so i did some benchmarking with really long strings and found that actually its helpful to reduce() the number of transitions (typically cuts them in half) for these long strings, as it speeds up some later algorithms. won't see any speedup for short terms, but I think its easier to work with these simpler automata anyway, and it eliminates the confusion of seeing the redundant states without slowing anything down.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3097</id>
      <title>Post grouping faceting</title>
      <description>This issues focuses on implementing post grouping faceting. How to handle multivalued fields. What field value to show with the facet. Where the facet counts should be based on Facet counts can be based on the normal documents. Ungrouped counts. Facet counts can be based on the groups. Grouped counts. Facet counts can be based on the combination of group value and facet value. Matrix counts. And properly more implementation options. The first two methods are implemented in the SOLR-236 patch. For the first option it calculates a DocSet based on the individual documents from the query result. For the second option it calculates a DocSet for all the most relevant documents of a group. Once the DocSet is computed the FacetComponent and StatsComponent use one the DocSet to create facets and statistics. This last one is a bit more complex. I think it is best explained with an example. Lets say we search on travel offers: hotel departure_airport duration Hotel a AMS 5 Hotel a DUS 10 Hotel b AMS 5 Hotel b AMS 10 If we group by hotel and have a facet for airport. Most end users expect (according to my experience off course) the following airport facet: AMS: 2 DUS: 1 The above result can't be achieved by the first two methods. You either get counts AMS:3 and DUS:1 or 1 for both airports.</description>
      <attachments/>
      <comments>37</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3098</id>
      <title>Grouped total count</title>
      <description>When grouping currently you can get two counts: Total hit count. Which counts all documents that matched the query. Total grouped hit count. Which counts all documents that have been grouped in the top N groups. Since the end user gets groups in his search result instead of plain documents with grouping. The total number of groups as total count makes more sense in many situations.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3099</id>
      <title>Grouping module should allow subclasses to set the group key per document</title>
      <description>The new grouping module can only group by a single-valued indexed field. But, if we make the 'getGroupKey' a method that a subclass could override, then I think we could refactor Solr over to the module, because it could do function queries and normal queries via subclass (I think). This also makes the impl more extensible to apps that might have their own interesting group values per document.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3110</id>
      <title>ASCIIFoldingFilter wrongly folds german Umlauts</title>
      <description>the german umlauts are currently mapped as follows. Ä/ä =&gt; A/a Ö/ö =&gt; O/o Ü/ü =&gt; U/u the correct mapping would be Ä/ä =&gt; Ae/ae Ö/ö =&gt; Oe/oe Ü/ü =&gt; Ue/ue so the corresponding rows in the switch statement should be moved down to the ae/oe/ue positions.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3112</id>
      <title>Add IW.add/updateDocuments to support nested documents</title>
      <description>I think nested documents (LUCENE-2454) is a very compelling addition to Lucene. It's also a popular (many votes) issue. Beyond supporting nested document querying, which is already an incredible addition since it preserves the relational model on indexing normalized content (eg, DB tables, XML docs), LUCENE-2454 should also enable speedups in grouping implementation when you group by a nested field. For the same reason, it can also enable very fast post-group facet counting impl (LUCENE-3097) when you what to count(distinct(nestedField)), instead of unique documents, as your "identifier". I expect many apps that use faceting need this ability (to count(distinct(nestedField)) not distinct(docID)). To support these use cases, I believe the only core change needed is the ability to atomically add or update multiple documents, which you cannot do today since in between add/updateDocument calls a flush (eg due to commit or getReader()) could occur. This new API (addDocuments(Iterable&lt;Document&gt;), updateDocuments(Term delTerm, Iterable&lt;Document&gt;) would also further guarantee that the documents are assigned sequential docIDs in the order the iterator provided them, and that the docIDs all reside in one segment. Segment merging never splits segments apart, so this invariant would hold even as merges/optimizes take place.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3118</id>
      <title>Tools for making explanations easier to consume/understand</title>
      <description>Often times, reading Explanations (i.e. the breakdown of scores for a particular query and result, say via Solr's &amp;debugQuery) is a pretty cryptic and hard to do undertaking. I often say people suffer from "explain blindness" from staring at explanation results for too long. We could add a layer of explanation helpers above the core Explain functionality that help people understand better what is going on. The goal is to give a higher level of tools to people who aren't necessarily well versed in all the underpinnings of Lucene's scoring mechanisms but still want information about why something didn't match For instance (brainstorming some things that might be doable): Explain Diff Tool – Given an 1 or more explanations, quickly highlight what the key things are that differentiate the results (i.e. fieldNorm is higher, etc.) Given a query and any document, give a more friendly reason why it ranks lower than others without the need to have to parse through all the pieces of the score, for instance, could you simply say something like, programatically that is, this document scored lower compared to your top 10 b/c it had no values in the foo Field. Could even maybe return codes for these reasons which could then be hooked into actual user messages. I don't have anything concrete patch-wise here, but am putting this up as a way to capture the idea and potentially spur others to think about it.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3121</id>
      <title>FST should offer lookup-by-output API when output strictly increases</title>
      <description>Spinoff from "FST and FieldCache" java-dev thread http://lucene.markmail.org/thread/swoawlv3fq4dntvl FST is able to associate arbitrary outputs with the sorted input keys, but in the special (and, common) case where the function is strictly monotonic (each output only "increases" vs prior outputs), such as mapping to term ords or mapping to file offsets in the terms dict, we should offer a lookup-by-output API that efficiently walks the FST and locates input key (exact or floor or ceil) matching that output.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3122</id>
      <title>Cascaded grouping</title>
      <description>Similar to SOLR-2526, in that you are grouping on 2 separate fields, but instead of treating those fields as a single grouping by a compound key, this change would let you first group on key1 for the primary groups and then secondarily on key2 within the primary groups. Ie, the result you get back would have groups A, B, C (grouped by key1) but then the documents within group A would be grouped by key 2. I think this will be important for apps whose documents are the product of denormalizing, ie where the Lucene document is really a sub-document of a different identifier field. Borrowing an example from LUCENE-3097, you have doctors but each doctor may have multiple offices (addresses) where they practice and so you index doctor X address as your lucene documents. In this case, your "identifier" field (that which "counts" for facets, and should be "grouped" for presentation) is doctorid. When you offer users search over this index, you'd likely want to 1) group by distance (ie, &lt; 0.1 miles, &lt; 0.2 miles, etc., as a function query), but 2) also group by doctorid, ie cascaded grouping. I suspect this would be easier to implement than it sounds: the per-group collector used by the 2nd pass grouping collector for key1's grouping just needs to be another grouping collector. Spookily, though, that collection would also have to be 2-pass, so it could get tricky since grouping is sort of recursing on itself.... once we have LUCENE-3112, though, that should enable efficient single pass grouping by the identifier (doctorid).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3126</id>
      <title>IndexWriter.addIndexes can make any incoming segment into CFS if it isn't already</title>
      <description>Today, IW.addIndexes(Directory) does not modify the CFS-mode of the incoming segments. However, if IndexWriter's MP wants to create CFS (in general), there's no reason why not turn the incoming non-CFS segments into CFS. We anyway copy them, and if MP is not against CFS, we should create a CFS out of them. Will need to use CFW, not sure it's ready for that w/ current API (I'll need to check), but luckily we're allowed to change it (@lucene.internal). This should be done, IMO, even if the incoming segment is large (i.e., passes MP.noCFSRatio) b/c like I wrote above, we anyway copy it. However, if you think otherwise, speak up . I'll take a look at this in the next few days.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3127</id>
      <title>pull CoreReaders out of SegmentReader</title>
      <description>Similar to LUCENE-3117, I think we should pull the CoreReaders class out of SR, to make it easier to navigate the code.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3128</id>
      <title>"ant eclipse" should create an Eclipse project</title>
      <description>The "eclipse" Ant target creates a .classpath file, but not a .project file, so the user has to create an Eclipse project in a separate step. Creating a .project file (if it doesn't exist yet) would make it easier for Eclipse users to build Lucene.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3129</id>
      <title>Single-pass grouping collector based on doc blocks</title>
      <description>LUCENE-3112 enables adding/updating a contiguous block of documents to the index, guaranteed (yet, experimental!) to retain adjacent docID assignment through the full life of the index as long the app doesn't delete individual docs from the block. When an app does this, it can enable neat features like LUCENE-2454 (nested documents), post-group facet counting (LUCENE-3097). It also makes single-pass grouping possible, when you group by the "identifier" field shared by the doc block, since we know we will see a given group only once with all of its docs within one block. This should be faster than the fully general two-pass collectors we already have. I'm working on a patch but not quite there yet...</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3130</id>
      <title>Use BoostAttribute in in TokenFilters to denote Terms that QueryParser should give lower boosts</title>
      <description>A recent thread asked if there was anyway to use QueryTime synonyms such that matches on the original term specified by the user would score higher then matches on the synonym. It occurred to me later that a float Attribute could be set by the SynonymFilter in such situations, and QueryParser could use that float as a boost in the resulting Query. IThis would be fairly straightforward for the simple "synonyms =&gt; BooleamQuery" case, but we'd have to decide how to handle the case of synonyms with multiple terms that produce MTPQ, possibly just punt for now) Likewise, there may be other TokenFilters that "inject" artificial tokens at query time where it also might make sense to have a reduced "boost" factor... SynonymFilter CommonGramsFilter WordDelimiterFilter etc... In all of these cases, the amount of the "boost" could me configured, and for back compact could default to "1.0" (or null to not set a boost at all) Furthermore: if we add a new BoostAttrToPayloadAttrFilter that just copied the boost attribute into the payload attribute, these same filters could give "penalizing" payloads to terms when used at index time) could give "penalizing" payloads to terms.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3131</id>
      <title>XA Resource/Transaction support</title>
      <description>Please add XAResoure/XATransaction support into Lucene core.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3132</id>
      <title>Use FileFilter instead of FileNameFilter in FSDirectory.listAll</title>
      <description>FSDirectory.listAll() uses FileNameFilter, but all it does is check whether the File + name given denotes a directory. For that, it does new File(dir, file).isDirectory(). If we use FileFilter, new File() won't be necessary. This is a trivial thing, I'll post a patch soon.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3133</id>
      <title>Fix QueryParser to handle nested fields</title>
      <description>Once we commit LUCENE-2454, we need to make it easy for apps to enable this with QueryParser. It seems like it's a "schema" like behavior, ie we need to be able to express the join structure of the related fields. And then whenever QP produces a query that spans fields requiring a join, the NestedDocumentQuery is used to wrap the child fields?</description>
      <attachments/>
      <comments>11</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3135</id>
      <title>backport suggest module to branch 3.x</title>
      <description>It would be nice to develop a plan to expose the autosuggest functionality to Lucene users in 3.x There are some complications, such as seeing if we can backport the FST-based functionality, which might require a good bit of work. But I think this would be well-worth it.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3138</id>
      <title>IW.addIndexes should fail fast if an index is too old/new</title>
      <description>Today IW.addIndexes (both Dir and IR versions) do not check the format of the incoming indexes. Therefore it could add a too old/new index and the app will discover that only later, maybe during commit() or segment merges. We should check that up front and fail fast. This issue is relevant only to 4.0 at the moment, which will not support 2.x indexes anymore.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3141</id>
      <title>FastVectorHighlighter - expose FieldFragList.fragInfo for user-customizable FragmentsBuilder</title>
      <description>Needed to build a custom highlightable snippet - snippet should start with the sentence containing the first match, then continue for 250 characters. So created a custom FragmentsBuilder extending SimpleFragmentsBuilder and overriding the createFragments(IndexReader reader, int docId, String fieldName, FieldFragList fieldFragList) method - unit test containing the code is attached to the JIRA. To get this to work, needed to expose (make public) the FieldFragList.fragInfo member variable. This is currently package private, so only FragmentsBuilder implementations within the lucene-highlighter o.a.l.s.vectorhighlight package (such as SimpleFragmentsBuilder) can access it. Since I am just using the lucene-highlighter.jar as an external dependency to my application, the simplest way to access FieldFragList.fragInfo in my class was to make it public.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3145</id>
      <title>FST APIs should support CharsRef too</title>
      <description>The Builder API at heart is IntsRef, but we have sugar to pass in BytesRef, CharSequence, etc. We should add CharsRef too. Likewise we have IntsRefFSTEnum, BytesRefFSTEnum; we should add CharsRef there. Finally the static Util methods should accept CharsRef.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3151</id>
      <title>Make all of Analysis completely independent from Lucene Core</title>
      <description>Lucene's analysis package, including the definitions of Attribute, TokenStream, etc. are quite useful outside of Lucene (for instance, Mahout uses them) for text processing. I'd like to move the definitions, or at least their packaging, to a separate JAR file so that one can consume them w/o needing Lucene core. My draft idea is to have a definition area that Lucene core is dependent on and the rest of the analysis package can then be dependent on the definition area. (I'm open to other ideas as well)</description>
      <attachments/>
      <comments>30</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3166</id>
      <title>src/site should not be built under /docs</title>
      <description>I noticed that the source package contains a "docs" subdir with all the site's docs. Also, the root has index.html which nicely points to all those documents. However, it also points to the Javadocs which are absent. If you "ant javadocs", they are built under build/docs/api, which makes the links in index.html still invalid. Iterating on it shortly, Robert and I think that the following should be done: have src/site docs built under src/site/build. Today they already are, but later "cp -r" under /docs, so we should remove the copy instruction. have "ant docs" or "ant generate-docs" which generates javadocs + copy src/site/build under build/docs. Then all links will still work. remove "docs" from svn and keep them under src/site/build. Marking it a blocker for 3.3 so we revisit before then.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3167</id>
      <title>Make lucene/solr a OSGI bundle through Ant</title>
      <description>We need to make a bundle thriugh Ant, so the binary can be published and no more need the download of the sources. Actually to get a OSGI bundle we need to use maven tools and build the sources. Here the reference for the creation of the OSGI bundle through Maven: https://issues.apache.org/jira/browse/LUCENE-1344 Bndtools could be used inside Ant</description>
      <attachments/>
      <comments>61</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>3168</id>
      <title>Enable Throttling only during nightly builds</title>
      <description>Some of my tests take forever even on a big machine. In order to speed up our tests we should default the IO throttling to NEVER and only run in during nightly.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3169</id>
      <title>Simple release smoke tester (Python script)</title>
      <description>Simple tool... you give it the RC URL, and it loads all artifacts from there and does basic smoke testing of them (verifies sigs/digests match; unpacks; runs "ant test", runs Lucene's demo, etc.).</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3171</id>
      <title>BlockJoinQuery/Collector</title>
      <description>I created a single-pass Query + Collector to implement nested docs. The approach is similar to LUCENE-2454, in that the app must index documents in "join order", as a block (IW.add/updateDocuments), with the parent doc at the end of the block, except that this impl is one pass. Once you join at indexing time, you can take any query that matches child docs and join it up to the parent docID space, using BlockJoinQuery. You then use BlockJoinCollector, which sorts parent docs by provided Sort, to gather results, grouped by parent; this collector finds any BlockJoinQuerys (using Scorer.visitScorers) and retains the child docs corresponding to each collected parent doc. After searching is done, you retrieve the TopGroups from a provided BlockJoinQuery. Like LUCENE-2454, this is less general than the arbitrary joins in Solr (SOLR-2272) or parent/child from ElasticSearch (https://github.com/elasticsearch/elasticsearch/issues/553), since you must do the join at indexing time as a doc block, but it should be able to handle nested joins as well as joins to multiple tables, though I don't yet have test cases for these. I put this in a new Join module (modules/join); I think as we refactor join impls we should put them here.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3177</id>
      <title>Decouple indexer from Document/Field impls</title>
      <description>I think we should define minimal iterator interfaces, IndexableDocument/Field, that indexer requires to index documents. Indexer would consume only these bare minimum interfaces, not the concrete Document/Field/FieldType classes from oal.document package. Then, the Document/Field/FieldType hierarchy is one concrete impl of these interfaces. Apps are free to make their own impls as well. Maybe eventually we make another impl that enforces a global schema, eg factored out of Solr's impl. I think this frees design pressure on our Document/Field/FieldType hierarchy, ie, these classes are free to become concrete fully-featured "user-space" classes with all sorts of friendly sugar APIs for adding/removing fields, getting/setting values, types, etc., but they don't need substantial extensibility/hierarchy. Ie, the extensibility point shifts to IndexableDocument/Field interface. I think this means we can collapse the three classes we now have for a Field (Fieldable/AbstracField/Field) down to a single concrete class (well, except for LUCENE-2308 where we want to break out dedicated classes for different field types...).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3178</id>
      <title>Native MMapDir</title>
      <description>Spinoff from LUCENE-2793. Just like we will create native Dir impl (UnixDirectory) to pass the right OS level IO flags depending on the IOContext, we could in theory do something similar with MMapDir. The problem is MMap is apparently quite hairy... and to pass the flags the native code would need to invoke mmap (I think?), unlike UnixDir where the code "only" has to open the file handle.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3179</id>
      <title>OpenBitSet.prevSetBit()</title>
      <description>Find a previous set bit in an OpenBitSet. Useful for parent testing in nested document query execution LUCENE-2454 .</description>
      <attachments/>
      <comments>42</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3181</id>
      <title>changes2html.pl should collect release dates from JIRA REST API</title>
      <description>Since LUCENE-3163 removed release dates from CHANGES.txt, the Changes.html generated by changes2html.pl no longer contains release dates, except those older release dates that are hard-coded in the script itself. JIRA exposes a REST API through which project info is available. For Lucene - java, lots of info is available in JSON format through https://issues.apache.org/jira/rest/api/2.0.alpha1/project/LUCENE , including a full list of each version's release date.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3186</id>
      <title>DocValues type should be recored in FNX file to early fail if user specifies incompatible type</title>
      <description>Currently segment merger fails if the docvalues type is not compatible across segments. We already catch this problem if somebody changes the values type for a field within one segment but not across segments. in order to do that we should record the type in the fnx fiel alone with the field numbers. I marked this 4.0 since it should not block the landing on trunk</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3187</id>
      <title>Store NumericField precisionStep in fnx file</title>
      <description>This is a similar problem like LUCENE-3186: The following question was sent to user list: http://mail-archives.apache.org/mod_mbox/lucene-java-user/201106.mbox/%3C614C529D389A5944B351F7DFB7594F24012AA214@uksrpblkexb01.detica.com%3E The main problem is that you have to pass the precision step and must knwo the field type of numeric fields before doing a query, else you get wrong results. We can maybe store the type and precision step in fnx file (like we do for stored numeric fields in FieldsWriter). I am not sure whats the best way to do it (without too much code specialization), but it seems a good idea. On the other hand, we don't store references to the Analyzer in the fnx file, so why for numeric field (it's just like an analyzer - if you change it, results are wrong)?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3191</id>
      <title>Add TopDocs.merge to merge multiple TopDocs</title>
      <description>It's not easy today to merge TopDocs, eg produced by multiple shards, supporting arbitrary Sort.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3193</id>
      <title>TwoPhaseCommit interface</title>
      <description>I would like to propose a TwoPhaseCommit interface which declares the methods necessary to implement a 2-phase commit algorithm: prepareCommit() commit() rollback() The prepare/commit ones have variants that take a (Map&lt;String,String&gt; commitData) following the ones we have in IndexWriter. In addition, a TwoPhaseCommitTool which implements a 2-phase commit amongst several TPCs. Having IndexWriter implement that interface will allow running the 2-phase commit algorithm on multiple IWs or IW + any other object that implements the interface. We should mark the interface @lucene.internal so as to not block ourselves in the future. This is pretty advanced stuff anyway. Will post a patch soon</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3196</id>
      <title>Optimize FixedStraightBytes for bytes size == 1</title>
      <description>Currently we read all the bytes in a PagedBytes instance wich is unneeded for single byte values like norms. For fast access this should simply be a straight array.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3198</id>
      <title>Change default Directory impl on 64bit linux to MMap</title>
      <description>Consistently in my NRT testing on Fedora 13 Linux, 64 bit JVM (Oracle 1.6.0_21) I see MMapDir getting better search and merge performance when compared to NIOFSDir. I think we should fix the default.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3199</id>
      <title>Add non-desctructive sort to BytesRefHash</title>
      <description>Currently the BytesRefHash is destructive. We can add a method that returns a non-destructively generated int[].</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3200</id>
      <title>Cleanup MMapDirectory to use only one MMapIndexInput impl with mapping sized of powers of 2</title>
      <description>Robert and me discussed a little bit after Mike's investigations, that using SingleMMapIndexinput together with MultiMMapIndexInput leads to hotspot slowdowns sometimes. We had the following ideas: MultiMMapIndexInput is almost as fast as SingleMMapIndexInput, as the switching between buffer boundaries is done in exception catch blocks. So normal code path is always the same like for Single* Only the seek method uses strange calculations (the modulo is totally bogus, it could be simply: int bufOffset = (int) (pos % maxBufSize); - very strange way of calculating modulo in the original code) Because of speed we suggest to no longer use arbitrary buffer sizes. We should pass only the power of 2 to the indexinput as size. All calculations in seek and anywhere else would be simple bit shifts and AND operations (the and masks for the modulo can be calculated in the ctor like NumericUtils does when calculating precisionSteps). the maximum buffer size will now be 2^30, not 2^31-1. But thats not an issue at all. In my opinion, a buffer size of 2^31-1 is stupid in all cases, as it will no longer fit page boundaries and mmapping gets harder for the O/S. We will provide a patch with those cleanups.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3201</id>
      <title>improved compound file handling</title>
      <description>Currently CompoundFileReader could use some improvements, i see the following problems its CSIndexInput extends bufferedindexinput, which is stupid for directories like mmap. it seeks on every readInternal its not possible for a directory to override or improve the handling of compound files. for example: it seems if you were impl'ing this thing from scratch, you would just wrap the II directly (not extend BufferedIndexInput, and add compound file offset X to seek() calls, and override length(). But of course, then you couldnt throw read past EOF always when you should, as a user could read into the next file and be left unaware. however, some directories could handle this better. for example MMapDirectory could return an indexinput that simply mmaps the 'slice' of the CFS file. its underlying bytebuffer etc naturally does bounds checks already etc, so it wouldnt need to be buffered, not even needing to add any offsets to seek(), as its position would just work. So I think we should try to refactor this so that a Directory can customize how compound files are handled, the simplest case for the least code change would be to add this to Directory.java: public Directory openCompoundInput(String filename) { return new CompoundFileReader(this, filename); } Because most code depends upon the fact compound files are implemented as a Directory and transparent. at least then a subclass could override... but the 'recursion' is a little ugly... we could still label it expert+internal+experimental or whatever.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3203</id>
      <title>Rate-limit IO used by merging</title>
      <description>Large merges can mess up searches and increase NRT reopen time (see http://blog.mikemccandless.com/2011/06/lucenes-near-real-time-search-is-fast.html). A simple rate limiter improves the spikey NRT reopen times during big merges, so I think we should somehow make this possible. Likely this would reduce impact on searches as well. Typically apps that do indexing and searching on same box are in no rush to see the merges complete so this is a good tradeoff.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3204</id>
      <title>Include maven-ant-tasks jar in the source tree and use this jar from generate-maven-artifacts</title>
      <description>Currently, running ant generate-maven-artifacts requires the user to have maven-ant-tasks-*.jar in their Ant classpath, e.g. in ~/.ant/lib/. The build should instead rely on a copy of this jar included in the source tree.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3206</id>
      <title>FST package API refactoring</title>
      <description>The current API is still marked @experimental, so I think there's still time to fiddle with it. I've been using the current API for some time and I do have some ideas for improvement. This is a placeholder for these – I'll post a patch once I have a working proof of concept.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3209</id>
      <title>Memory codec</title>
      <description>This codec stores all terms/postings in RAM. It uses an FST&lt;BytesRef&gt;. This is useful on a primary key field to ensure lookups don't need to hit disk, to keep NRT reopen time fast even under IO contention.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3210</id>
      <title>TieredMergePolicy should expose control over how aggressively segments with deletions are targeted</title>
      <description>TMP today always does a linear pro-rating of a merge's score according to what pctg of the documents are deleted; I'd like to 1) put a power factor in (score is multiplicative), and 2) default it to stronger favoring of merging away deletions.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3211</id>
      <title>Switch Maven build from poorly maintained per-Solr-contrib tests.luceneMatchVersion constant values to Solr-wide LUCENE_CURRENT</title>
      <description>The tests.luceneMatchVersion constants in each Solr contrib's POM haven't been kept up-to-date, and in any case don't need to be maintained per-Solr-contrib - they should all use the same value. Setting tests.luceneMatchVersion to LUCENE_CURRENT for all Solr testing under Maven fixes the problem and makes post-release maintenance a non-issue.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3212</id>
      <title>Supply FilterIndexReader based on any o.a.l.search.Filter</title>
      <description>When coding LUCENE-2919 (PKIndexSplitter), Mike and me had the idea, how to effectively apply filters on the lowest level (before query execution). This is very useful for e.g. security Filters that simply hide some documents. Currently when you apply the filter after searching, lots of useless work was done like scoring filtered documents, iterating term positions (for Phrases),... This patch will provide a FilterIndexReader subclass (4.0 only, 3.x is too complicated to implement), that hides filtered documents by returning them in getDeletedDocs(). In contrast to LUCENE-2919, the filtering will work on per-segment (without SlowMultiReaderWrapper), so per segment search keeps available and reopening can be done very efficient, as the filter is only calculated on openeing new or changed segments. This filter should improve use-cases where the filter can be applied one time before all queries (like security filters) on (re-)opening the IndexReader.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3213</id>
      <title>Use AtomicReaderContext also for CustomScoreProvider</title>
      <description>When moving to AtomicReaderContext, one place was not changed to use it: CustomScoreQuery's CustomScoreProvider. It should also take AtomicReaderContext instead of IndexReader, as this may help users to effectively implement custom scoring there absolute DocIds are needed.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3214</id>
      <title>Ability to mlock certain fields from the terms dict</title>
      <description>This is a hacked up prototype! It works but I'm not sure how to get this to a committable point. The patch invokes mlock() (tested only on Linux), locking pages from the terms dictionary file that hold terms for a specified field. You can only do this with MMapDirectory. I used this to lock pages for the "id" field in the NRT stress test; it's an alternative to MemoryCodec. But, it requires you set up the OS to allow the app/user to lock pages in RAM. It works very well in reducing the NRT reopen latency even when large merges are running...</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3216</id>
      <title>Store DocValues per segment instead of per field</title>
      <description>currently we are storing docvalues per field which results in at least one file per field that uses docvalues (or at most two per field per segment depending on the impl.). Yet, we should try to by default pack docvalues into a single file if possible. To enable this we need to hold all docvalues in memory during indexing and write them to disk once we flush a segment.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3217</id>
      <title>Improve DocValues merging</title>
      <description>Some DocValues impl. still load all values from merged segments into memory during merge. For efficiency we should merge them on the fly without buffering in memory</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3218</id>
      <title>Make CFS appendable</title>
      <description>Currently CFS is created once all files are written during a flush / merge. Once on disk the files are copied into the CFS format which is basically a unnecessary for some of the files. We can at any time write at least one file directly into the CFS which can save a reasonable amount of IO. For instance stored fields could be written directly during indexing and during a Codec Flush one of the written files can be appended directly. This optimization is a nice sideeffect for lucene indexing itself but more important for DocValues and LUCENE-3216 we could transparently pack per field files into a single file only for docvalues without changing any code once LUCENE-3216 is resolved.</description>
      <attachments/>
      <comments>58</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3219</id>
      <title>Change SortField types to an Enum</title>
      <description>When updating my SOLR-2533 patch, one issue was that the int value I had given my new type had been used by another change in the mean time. Since we don't use these fields in a bitset kind of way, we can convert them to an enum.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3225</id>
      <title>Optimize TermsEnum.seek when caller doesn't need next term</title>
      <description>Some codecs are able to save CPU if the caller is only interested in exact matches. EG, Memory codec and SimpleText can do more efficient FSTEnum lookup if they know the caller doesn't need to know the term following the seek term. We have cases like this in Lucene, eg when IW deletes documents by Term, if the term is not found in a given segment then it doesn't need to know the ceiling term. Likewise when TermQuery looks up the term in each segment. I had done this change as part of LUCENE-3030, which is a new terms index that's able to save seeking for exact-only lookups, but now that we have Memory codec that can also save CPU I think we should commit this today. The change adds a "boolean onlyExact" param to seek(BytesRef).</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3226</id>
      <title>rename SegmentInfos.FORMAT_3_1 and improve description in CheckIndex</title>
      <description>A 3.2 user recently asked if something was wrong because CheckIndex was reporting his (newly built) index version as... Segments file=segments_or numSegments=1 version=FORMAT_3_1 [Lucene 3.1] It seems like there are two very confusing pieces of information here... 1) the variable name of SegmentInfos.FORMAT_3_1 seems like poor choice. All other FORMAT_* constants in SegmentInfos are descriptive of the actual change made, and not specific to the version when they were introduced. 2) whatever the name of the FORMAT_* variable, CheckIndex is labeling it "Lucene 3.1", which is missleading since that format is alwasy used in 3.2 (and probably 3.3, etc...). I suggest: a) rename FORMAT_3_1 to something like "FORMAT_SEGMENT_RECORDS_VERSION" b) change CheckIndex so that the label for the "newest" format always ends with " and later" (ie: "Lucene 3.1 and later") so when we release versions w/o a format change we don't have to remember to manual list them in CheckIndex. when we do make format changes and update CheckIndex " and later" can be replaced with " to X.Y" and the new format can be added</description>
      <attachments/>
      <comments>23</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3227</id>
      <title>Add Rewriteable Support to SortField.toString</title>
      <description>I missed adding support for the new Rewriteable SortField type to toString().</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3230</id>
      <title>Make FSDirectory.fsync() public and static</title>
      <description>I find FSDirectory.fsync() (today protected and instance method) very useful as a utility to sync() files. I'd like create a FSDirectory.sync() utility which contains the exact same impl of FSDir.fsync(), and have the latter call it. We can have it part of IOUtils too, as it's a completely standalone utility. I would get rid of FSDir.fsync() if it wasn't protected (as if encouraging people to override it). I doubt anyone really overrides it (our core Directories don't). Also, while reviewing the code, I noticed that if IOE occurs, the code sleeps for 5 msec. If an InterruptedException occurs then, it immediately throws ThreadIE, completely ignoring the fact that it slept due to IOE. Shouldn't we at least pass IOE.getMessage() on ThreadIE? The patch is trivial, so I'd like to get some feedback before I post it.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3231</id>
      <title>Add fixed size DocValues int variants &amp; expose Arrays where possible</title>
      <description>currently we only have variable bit packed ints implementation. for flexible scoring or loading field caches it is desirable to have fixed int implementations for 8, 16, 32 and 64 bit.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3233</id>
      <title>HuperDuperSynonymsFilter™</title>
      <description>The current synonymsfilter uses a lot of ram and cpu, especially at build time. I think yesterday I heard about "huge synonyms files" three times. So, I think we should use an FST-based structure, sharing the inputs and outputs. And we should be more efficient with the tokenStream api, e.g. using save/restoreState instead of cloneAttributes()</description>
      <attachments/>
      <comments>41</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3234</id>
      <title>Provide limit on phrase analysis in FastVectorHighlighter</title>
      <description>With larger documents, FVH can spend a lot of time trying to find the best-scoring snippet as it examines every possible phrase formed from matching terms in the document. If one is willing to accept less-than-perfect scoring by limiting the number of phrases that are examined, substantial speedups are possible. This is analogous to the Highlighter limit on the number of characters to analyze. The patch includes an artifical test case that shows &gt; 1000x speedup. In a more normal test environment, with English documents and random queries, I am seeing speedups of around 3-10x when setting phraseLimit=1, which has the effect of selecting the first possible snippet in the document. Most of our sites operate in this way (just show the first snippet), so this would be a big win for us. With phraseLimit = -1, you get the existing FVH behavior. At larger values of phraseLimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3236</id>
      <title>Make LowerCaseFilter and StopFilter keyword aware, similar to PorterStemFilter</title>
      <description>PorterStemFilter has functionality to detect if a term has been marked as a "keyword" by the KeywordMarkerFilter (KeywordAttribute.isKeyword() == true), and if so, skip stemming. The suggestion is to have the same functionality in other filters where it is applicable. I think it may be particularly applicable to the LowerCaseFilter (ie if it is a keyword, don't mess with the case), and StopFilter (if it is a keyword, then don't filter it out even if it looks like a stop word). Backward compatibility is maintained (in both cases) by adding a new constructor which takes an additional boolean parameter ignoreKeyword. The current constructor will call this new constructor with ignoreKeyword = false. Patches are attached (for LowerCaseFilter and StopFilter). I have verified that the analysis JUnit tests run against the updated code, ie, backward compatibility is maintained.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3242</id>
      <title>Rename Lucene common-build.xml project name</title>
      <description>While adding the new common module, I ran into a name collision with Lucene's common-build.xml project name. I've since renamed the common module's project name to be clearer, but I think we should rename common-build's one as well. Solr's common-build.xml uses common-solr, so lets rename Lucene's to common-lucene.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3243</id>
      <title>FastVectorHighlighter - add position offset to FieldPhraseList.WeightedPhraseInfo.Toffs</title>
      <description>Needed to return position offsets along with highlighted snippets when using FVH for highlighting. Using the (LUCENE-3141) patch I was able to get the fragInfo for a particular Phrase search. Currently the Toffs(Term offsets) class only stores the start and end offset. To get the position offset, I added the position offset information in Toffs and FieldPhraseList class.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3245</id>
      <title>Realtime terms dictionary</title>
      <description>For LUCENE-2312 we need a realtime terms dictionary. While ConcurrentSkipListMap may be used, it has drawbacks in terms of high object overhead which can impact GC collection times and heap memory usage. If we implement a skip list that uses primitive backing arrays, we can hopefully have a data structure that is [as] fast and memory efficient.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3246</id>
      <title>Invert IR.getDelDocs -&gt; IR.getLiveDocs</title>
      <description>Spinoff from LUCENE-1536, where we need to fix the low level filtering we do for deleted docs to "match" Filters (ie, a set bit means the doc is accepted) so that filters can be pushed all the way down to the enums when possible/appropriate. This change also inverts the meaning first arg to TermsEnum.docs/AndPositions (renames from skipDocs to liveDocs).</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3252</id>
      <title>Use single array in fixed straight bytes DocValues if possible</title>
      <description>FixedStraightBytesImpl currently uses a straight array only if the byte size is 1 per document we could further optimize this to use a single array if all the values fit in.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3261</id>
      <title>Faceting module userguide</title>
      <description>In LUCENE-3079 I've uploaded a userguide for the faceting module. I'd like to discuss where is the best place to include the module. We include it with the code (in our SVN), so that it's always attached to some branch (or in other words a release). That way we can have versions of it per releases that reflect API changes. This document is like the file format document, or any other document we put under site-versioned. So we have two places: facet/docs site/userguides Unlike the site, which its PDFs are built automatically by Forrest, we cannot convert ODT to PDF with it, so it's a challenge to put it there. What we do today (in our SVN) is whoever updates the userguide, creates a PDF too, that's easy from OpenOffice. I'll upload the file later when I'm in front of it.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3262</id>
      <title>Facet benchmarking</title>
      <description>A spin off from LUCENE-3079. We should define few benchmarks for faceting scenarios, so we can evaluate the new faceting module as well as any improvement we'd like to consider in the future (such as cutting over to docvalues, implement FST-based caches etc.). Toke attached a preliminary test case to LUCENE-3079, so I'll attach it here as a starting point. We've also done some preliminary job for extending Benchmark for faceting, so I'll attach it here as well. We should perhaps create a Wiki page where we clearly describe the benchmark scenarios, then include results of 'default settings' and 'optimized settings', or something like that.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3266</id>
      <title>Improve FileLocking based on Java 1.6</title>
      <description>Snippet from NativeFSLockFactory: /* * The javadocs for FileChannel state that you should have * a single instance of a FileChannel (per JVM) for all * locking against a given file (locks are tracked per * FileChannel instance in Java 1.4/1.5). Even using the same * FileChannel instance is not completely thread-safe with Java * 1.4/1.5 though. To work around this, we have a single (static) * HashSet that contains the file paths of all currently * locked locks. This protects against possible cases * where different Directory instances in one JVM (each * with their own NativeFSLockFactory instance) have set * the same lock dir and lock prefix. However, this will not * work when LockFactorys are created by different * classloaders (eg multiple webapps). * * TODO: Java 1.6 tracks system wide locks in a thread safe manner * (same FileChannel instance or not), so we may want to * change this when Lucene moves to Java 1.6. */ since we are on 1.6 we should improve this if possible.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3271</id>
      <title>Move 'good' contrib/queries classes to Queries module</title>
      <description>With the Queries module now filled with the FunctionQuery stuff, we should look at closing down contrib/queries. While not a huge contrib, it contains a number of pretty useful classes and some that should go elsewhere. Heres my proposed plan: similar.* -&gt; suggest module regex.* -&gt; queries module BooleanFilter -&gt; queries module under .filters package BoostingQuery -&gt; queries module ChainedFilter -&gt; queries module under .filters package DuplicateFilter -&gt; queries module under .filters package FieldCacheRewriteMethod -&gt; This doesn't belong in this contrib or the queries module. I think we should push it to contrib/misc for the time being. It seems to have quite a few constraints on when its useful. If indeed CONSTANT_SCORE_AUTO rewrite is better, then I dont see a purpose for it. FilterClause -&gt; class inside BooleanFilter FuzzyLikeThisQuery -&gt; suggest module. This class seems a mess with its Similarity hardcoded. With all that said, it does seem to do what it claims and with some cleanup, it could be good. TermsFilter -&gt; queries module under .filters package SlowCollated* -&gt; They can stay in the module till we have a better place to nuke them. One of the implications of the above moves, is that the xml-query-parser, which supports many of the queries, will need to have a dependency on the queries module. But that seems unavoidable at this stage.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3272</id>
      <title>Consolidate Lucene's QueryParsers into a module</title>
      <description>Lucene has a lot of QueryParsers and we should have them all in a single consistent place. The following are QueryParsers I can find that warrant moving to the new module: Lucene Core's QueryParser AnalyzingQueryParser ComplexPhraseQueryParser ExtendableQueryParser Surround's QueryParser PrecedenceQueryParser StandardQueryParser XML-Query-Parser's CoreParser All seem to do a good job at their kind of parsing with extensive tests. One challenge of consolidating these is that many tests use Lucene Core's QueryParser. One option is to just replicate this class in src/test and call it TestingQueryParser. Another option is to convert all tests over to programmatically building their queries (seems like alot of work).</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3274</id>
      <title>Collapse Common module into Lucene core util</title>
      <description>It was suggested by Robert in http://markmail.org/message/wbfuzfamtn2qdvii that we should try to limit the dependency graph between modules and where there is something 'common' it should probably go into lucene core. Given that I haven't added anything to this module except the MutableValue classes, I'm going to collapse them into the util package, remove the module, and correct the dependencies.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3279</id>
      <title>Allow CFS be empty</title>
      <description>since we changed CFS semantics slightly closing a CFS directory on an error can lead to an exception. Yet, an empty CFS is still a valid CFS so for consistency we should allow CFS to be empty. here is an example: 1 tests failed. REGRESSION: org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull Error Message: CFS has no entries Stack Trace: java.lang.IllegalStateException: CFS has no entries at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:139) at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:181) at org.apache.lucene.store.DefaultCompoundFileDirectory.close(DefaultCompoundFileDirectory.java:58) at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:139) at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4252) at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3863) at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37) at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2715) at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2710) at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:2706) at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3513) at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2064) at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2031) at org.apache.lucene.index.TestIndexWriterOnDiskFull.addDoc(TestIndexWriterOnDiskFull.java:539) at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddDocumentOnDiskFull(TestIndexWriterOnDiskFull.java:74) at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1277) at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1195)</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3280</id>
      <title>Add new bit set impl for caching filters</title>
      <description>I think OpenBitSet is trying to satisfy too many audiences, and it's confusing/error-proned as a result. It has int/long variants of many methods. Some methods require in-bound access, others don't; of those others, some methods auto-grow the bits, some don't. OpenBitSet doesn't always know its numBits. I'd like to factor out a more "focused" bit set impl whose primary target usage is a cached Lucene Filter, ie a bit set indexed by docID (int, not long) whose size is known and fixed up front (backed by final long[]) and is always accessed in-bounds.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3282</id>
      <title>BlockJoinQuery: Allow to add a custom child collector, and customize the parent bitset extraction</title>
      <description>It would be nice to allow to add a custom child collector to the BlockJoinQuery to be called on every matching doc (so we can do things with it, like counts and such). Also, allow to extend BlockJoinQuery to have a custom code that converts the filter bitset to an OpenBitSet.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3289</id>
      <title>FST should allow controlling how hard builder tries to share suffixes</title>
      <description>Today we have a boolean option to the FST builder telling it whether it should share suffixes. If you turn this off, building is much faster, uses much less RAM, and the resulting FST is a prefix trie. But, the FST is larger than it needs to be. When it's on, the builder maintains a node hash holding every node seen so far in the FST – this uses up RAM and slows things down. On a dataset that Elmer (see java-user thread "Autocompletion on large index" on Jul 6 2011) provided (thank you!), which is 1.32 M titles avg 67.3 chars per title, building with suffix sharing on took 22.5 seconds, required 1.25 GB heap, and produced 91.6 MB FST. With suffix sharing off, it was 8.2 seconds, 450 MB heap and 129 MB FST. I think we should allow this boolean to be shade-of-gray instead: usually, how well suffixes can share is a function of how far they are from the end of the string, so, by adding a tunable N to only share when suffix length &lt; N, we can let caller make reasonable tradeoffs.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3290</id>
      <title>add FieldInvertState.numUniqueTerms, Terms.sumDocFreq</title>
      <description>For scoring systems like lnu.ltc (http://trec.nist.gov/pubs/trec16/papers/ibm-haifa.mq.final.pdf), we need to supply 3 stats: average tf within d # of unique terms within d average number of unique terms across field If we add FieldInvertState.numUniqueTerms, you can incorporate the first two into your norms/docvalues (once we cut over), the average tf within d being length / numUniqueTerms. to compute the average across the field, we can just write the sum of all terms' docfreqs into the terms dictionary header, and you can then divide this by maxdoc to get the average.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3296</id>
      <title>Enable passing a config into PKIndexSplitter</title>
      <description>I need to be able to pass the IndexWriterConfig into the IW used by PKIndexSplitter.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3297</id>
      <title>FST doesn't fully share common prefix across all outputs</title>
      <description>FST will try to share prefixes of outputs when possible, however in the [I think unusual in practice] case where all outputs share a common prefix, FST really ought to store this just once, on the root arc, but instead it's only able to push back to the N root arcs. It's sort of an off-by-one on how far back the pushing goes... One [synthetic] example where this makes a big difference is the new Test2BPostings test, when it uses MemoryCodec, because this test has 26 terms (letters of alphabet) and each term has exactly the same long (~85 MB) all 1s byte[] as the postings. If we fixed this issue, then the resulting FST would only be ~85 MB but now instead it needs to be ~85 * 26 MB.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3298</id>
      <title>FST has hard limit max size of 2.1 GB</title>
      <description>The FST uses a single contiguous byte[] under the hood, which in java is indexed by int so we cannot grow this over Integer.MAX_VALUE. It also internally encodes references to this array as vInt. We could switch this to a paged byte[] and make the far larger. But I think this is low priority... I'm not going to work on it any time soon.</description>
      <attachments/>
      <comments>37</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>3302</id>
      <title>Leftover legacy enum in IndexReader</title>
      <description>In IndexReader we still have some leftover "handmade" enum from pre-Java5 times. Unfortunately the Generics/Java5 Policeman did not notice it. This patch is just code cleanup, no baclkwards breaks, as code using this enum would not see any difference (because only superclass changes). I will commit this asap.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3305</id>
      <title>Kuromoji code donation - a new Japanese morphological analyzer</title>
      <description>Atilika Inc. (アティリカ株式会社) would like to donate the Kuromoji Japanese morphological analyzer to the Apache Software Foundation in the hope that it will be useful to Lucene and Solr users in Japan and elsewhere. The project was started in 2010 since we couldn't find any high-quality, actively maintained and easy-to-use Java-based Japanese morphological analyzers, and these become many of our design goals for Kuromoji. Kuromoji also has a segmentation mode that is particularly useful for search, which we hope will interest Lucene and Solr users. Compound-nouns, such as 関西国際空港 (Kansai International Airport) and 日本経済新聞 (Nikkei Newspaper), are segmented as one token with most analyzers. As a result, a search for 空港 (airport) or 新聞 (newspaper) will not give you a for in these words. Kuromoji can segment these words into 関西 国際 空港 and 日本 経済 新聞, which is generally what you would want for search and you'll get a hit. We also wanted to make sure the technology has a license that makes it compatible with other Apache Software Foundation software to maximize its usefulness. Kuromoji has an Apache License 2.0 and all code is currently owned by Atilika Inc. The software has been developed by my good friend and ex-colleague Masaru Hasegawa and myself. Kuromoji uses the so-called IPADIC for its dictionary/statistical model and its license terms are described in NOTICE.txt. I'll upload code distributions and their corresponding hashes and I'd very much like to start the code grant process. I'm also happy to provide patches to integrate Kuromoji into the codebase, if you prefer that. Please advise on how you'd like me to proceed with this. Thank you.</description>
      <attachments/>
      <comments>59</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>3306</id>
      <title>disable positions for spellchecker ngram fields</title>
      <description>In LUCENE-2391 we optimized spellchecker (re)build time/ram usage by omitting frequencies/positions/norms for single-valued fields, among other things. Now that we can disable positions but keep freqs, we should disable them for the n-gram fields, because the spellchecker does not use positional queries.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3307</id>
      <title>don't require an analyzer, if all fields are NOT_ANALYZED</title>
      <description>This seems wierd, if you analyze only NOT_ANALYZED fields, you must have an analyzer (null will not work) because documentsinverter wants it for things like offsetGap</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3309</id>
      <title>Add narrow API for loading stored fields, to replace FieldSelector</title>
      <description>I think we should "invert" the FieldSelector API, with a "push" API whereby FieldsReader invokes this API once per field in the document being visited. Implementations of the API can then do arbitrary things like save away the field's size, load the field, clone the IndexInput for later lazy loading, etc. This very thin API would be a mirror image of the very thin index time API we now have (IndexableField) and, importantly, it would have no dependence on our "user space" Document/Field/FieldType impl, so apps are free to do something totally custom. After we have this, we should build the "sugar" API that rebuilds a Document instance (ie IR.document(int docID)) on top of this new thin API. This'll also be a good test that the API is sufficient. Relevant discussions from IRC this morning at http://colabti.org/irclogger/irclogger_log/lucene-dev?date=2011-07-13#l76</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3312</id>
      <title>Break out StorableField from IndexableField</title>
      <description>In the field type branch we have strongly decoupled Document/Field/FieldType impl from the indexer, by having only a narrow API (IndexableField) passed to IndexWriter. This frees apps up use their own "documents" instead of the "user-space" impls we provide in oal.document. Similarly, with LUCENE-3309, we've done the same thing on the doc/field retrieval side (from IndexReader), with the StoredFieldsVisitor. But, maybe we should break out StorableField from IndexableField, such that when you index a doc you provide two Iterables – one for the IndexableFields and one for the StorableFields. Either can be null. One downside is possible perf hit for fields that are both indexed &amp; stored (ie, we visit them twice, lookup their name in a hash twice, etc.). But the upside is a cleaner separation of concerns in API....</description>
      <attachments/>
      <comments>184</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>3315</id>
      <title>Omit termFreq but keep Positions</title>
      <description>It would be useful to have an option to discard term frequency information but still keep the positions. Currently, setOmitTermFreqAndPositions discards both. The recent commit LUCENE-2048 adds the flexibility to discard only the positions. With the new option to discard frequency, position-dependent queries can continue to work, and whoever can afford to ignore the scoring based on tf can save a lot on the index size.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3322</id>
      <title>Disable cross-matches on multi-valued fields</title>
      <description>When searching against a multi-valued field it is often advantageous to disable cross-matches. For instance, if a book contains two authors: "john smith" and "brian jones", a search for "john jones" should not match. Currently the best workaround is to search using a sloppy phrase query. The drawback is that phrases are incompatible with other features such as wildcards and ranges. Additionally, it is left up to the user to know to formulate the query differently if searching a multi-valued field.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3328</id>
      <title>Specialize BooleanQuery if all clauses are TermQueries</title>
      <description>During work on LUCENE-3319 I ran into issues with BooleanQuery compared to PhraseQuery in the exact case. If I disable scoring on PhraseQuery and bypass the position matching, essentially doing a conjunction match, ExactPhraseScorer beats plain boolean scorer by 40% which is a sizeable gain. I converted a ConjunctionScorer to use DocsEnum directly but still didn't get all the 40% from PhraseQuery. Yet, it turned out with further optimizations this gets very close to PhraseQuery. The biggest gain here came from converting the hand crafted loop in ConjunctionScorer#doNext to a for loop which seems to be less confusing to hotspot. In this particular case I think code specialization makes lots of sense since BQ with TQ is by far one of the most common queries. I will upload a patch shortly</description>
      <attachments/>
      <comments>22</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3330</id>
      <title>revise Scorer visitor API</title>
      <description>Currently there is an (expert) API in scorer to allow you to take a scorer, and visit its children etc (e.g. booleanquery). I think we should improve this, so its general to all queries. The current issues are: the current api is hardcoded for booleanquery's Occurs, but we should support other types of children (e.g. disjunctionmax) it can be difficult to use the API, because of the amount of generics and the visitor callback API. the current API enforces a DFS traversal when you might prefer BFS instead.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3331</id>
      <title>consider allowing ScorerContext to specify that you dont need freqs/scores</title>
      <description>This is just an idea for discussion (I have not yet thought everything through: know of a non-scary way to do the patch yet). But I think that it would be useful for ScorerContext to specify something like 'docsOnly', e.g. ConstantScoreQuery could pass this down here: disi = innerWeight.scorer(context, scorerContext); Basically this flag would specify that the caller does not care about freq() and score(), and in this case e.g. TermScorer could use a docs-only bulkpostings for example, and never pull freqs. Additionally, it wouldn't need to create a Similarity.DocScorer, which is just wastefully computing score cache in this case, it could instead pass null, creat a scorer that does not use one, or use a Constant impl that always returns 1 or throws UOE, depending on how we want to specify that score()/freq() should act if the caller does actually call it when this flag is set.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3333</id>
      <title>Specialize DisjunctionScorer if all clauses are TermQueries</title>
      <description>spinnoff from LUCENE-3328 - since we have a specialized conjunction scorer we should also investigate if this pays off in disjunction scoring</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3334</id>
      <title>IOUtils.closeSafely should log suppressed Exceptions in stack trace of original Exception (a new feature of Java 7)</title>
      <description>I was always against Java 6 support, as it brings no really helpful new features into Lucene. But there are several things that make life easier in coming Java 7 (hopefully on July 28th, 2011). One of those is simplier Exception handling and suppression on Closeable, called "Try-With-Resources" (see http://docs.google.com/View?id=ddv8ts74_3fs7483dp, by the way all Lucene classes support these semantics in Java 7 automatically, the cool try-code below would work e.g. for IndexWriter, TokenStreams,...). We already have this functionality in Lucene since adding the IOUtils.closeSafely() utility (which can be removed when Java 7 is the minimum requirement of Lucene - maybe in 10 years): try (Closeable a = new ...; Closeable b = new ...) { ... use Closeables ... } catch (Exception e) { dosomething; throw e; } This code will close a and b in an autogenerated finally block and supress any exception. This is identical to our IOUtils.closeSafely: Exception priorException = null; Closeable a,b; try (Closeable a = new ...; Closeable b = new ...) { a = new ...; b = new ... ... use Closeables ... } catch (Exception e) { priorException = e; dosomething; } finally { IOUtils.closeSafely(priorException, a, b); } So this means we have the same functionality without Java 7, but there is one thing that makes logging/debugging much nicer: The above Java 7 code also adds maybe suppressed Exceptions in those Closeables to the priorException, so when you print the stacktrace, it not only shows the stacktrace of the original Exception, it also prints all Exceptions that were suppressed to throw this Exception (all Closeable.close() failures): org.apache.lucene.util.TestIOUtils$TestException: BASE-EXCEPTION at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:61) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20) at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76) at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1486) at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1404) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) at org.junit.runners.ParentRunner.run(ParentRunner.java:236) at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911) at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768) Suppressed: java.io.IOException: TEST-IO-EXCEPTION-1 at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36) at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58) at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62) ... 26 more Suppressed: java.io.IOException: TEST-IO-EXCEPTION-2 at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36) at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58) at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62) ... 26 more For this in Java 7 a new method was added to Throwable, that allows logging such suppressed Exceptions (it is called automatically by the synthetic bytecode emitted by javac). This patch simply adds this functionality conditionally to IOUtils, so it "registers" all suppressed Exceptions, if running on Java 7. This is done by reflection: once it looks for this method in Throwable.class and if found, it invokes it in closeSafely, so the exceptions thrown on Closeable.close() don't get lost. This makes debugging much easier and logs all problems that may occur. This patch does not change functionality or behaviour, it just adds more nformation to the stack trace in a Java-7-way (similar to the way how Java 1.4 added causes). It works here locally on Java 6 and Java 7, but only Java 7 gets the additional stack traces. For Java 6 nothing changes. Same for Java 5 (if we backport to 3.x). This would be our first Java 7 improvement (a minor one). Next would be NIO2... - but thats not easy to do with reflection only, so we have to wait 10 years</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3336</id>
      <title>Remove a dependency of javadoc tasks on compile</title>
      <description>For building javadocs, the class files must not be compiled, so the dependency is useless and slows down build enormous. A simple ant javadocs ins solr takes several minutes because of that.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3342</id>
      <title>make frozenbuffereddeletes more efficient for terms</title>
      <description>when looking at LUCENE-3340, I thought its also ridiculous how much ram we use for delete by term. so we can save a lot of memory, especially object overhead by being a little more efficient.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3343</id>
      <title>Comparison operators &gt;,&gt;=,&lt;,&lt;= and = support as RangeQuery syntax in QueryParser</title>
      <description>To offer better interoperability with other search engines and to provide an easier and more straight forward syntax, the operators &gt;, &gt;=, &lt;, &lt;= and = should be available to express an open range query. They should at least work for numeric queries. '=' can be made a synonym for ':'.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>3352</id>
      <title>ParametricRangeQueryNodeProcessor support for time zones</title>
      <description>It would be nice if there were a config attribute for setting a time zone for dates in the query. At the moment I am using my own query node processor to implement this, but I stumbled upon ParametricRangeQueryNodeProcessor and it is very close to being usable as-is.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3354</id>
      <title>Extend FieldCache architecture to multiple Values</title>
      <description>I would consider this a bug. It appears lots of people are working around this limitation, why don't we just change the underlying data structures to natively support multiValued fields in the FieldCache architecture? Then functions() will work properly, and we can do things like easily geodist() on a multiValued field. Thoughts?</description>
      <attachments/>
      <comments>26</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>3359</id>
      <title>Option for no Front Encoding of term compression</title>
      <description>Average length of a word in the English language is 5.1 , so Front Encoding of term compression in index is meaningful. But average length of a word in the Chinese language is 2.3. No need Front Encoding for chinese document index?</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3360</id>
      <title>Move FieldCache to IndexReader</title>
      <description>Move the static FieldCache.DEFAULT field instance to atomic IndexReaders, so that FieldCache insanity caused by the WeakHashMap no longer occurs. Add a new method to IndexReader that by default throws an UOE: public FieldCache getFieldCache() The SegmentReader implements this method and returns its own internal FieldCache implementation. This implementation just uses a HashMap&lt;Entry&lt;T&gt;,Object&gt;&gt; to store entries. The SlowMultiReaderWrapper implements this method as well and basically behaves the same as the current FieldCacheImpl. This issue won't solve the insanity that comes from inconsistent usage of a single field (for example retrieve both int[] and DocTermIndex for the same field).</description>
      <attachments/>
      <comments>45</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>3364</id>
      <title>Add score threshold into Scorer.score()</title>
      <description>This is an optimization for scoring. Given a Scorer.score() implementation, where features are gathered to calculate a score. Proposal, add a parameter to score, e.g. score(float threshold) This threshold is the minimum score to "beat" to make it to the current PriorityQueue. This could potential save a great deal of wasted calculation in the cases where recall is large. In our case specifically, some of the features needed to do calculation can be expensive to obtain, it would be nice to have a place to decide whether or not even fetching these features are necessary. Also, if we know the score would be low, simply threshold can be returned. Let me know if this makes sense and I can work on a patch.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3366</id>
      <title>StandardFilter only works with ClassicTokenizer and only when version &lt; 3.1</title>
      <description>The StandardFilter used to remove periods from acronyms and apostrophes-S's where they occurred. And it used to work in conjunction with the StandardTokenizer. Presently, it only does this with ClassicTokenizer and when the lucene match version is before 3.1. Here is a excerpt from the code: public final boolean incrementToken() throws IOException { if (matchVersion.onOrAfter(Version.LUCENE_31)) return input.incrementToken(); // TODO: add some niceties for the new grammar else return incrementTokenClassic(); } It seems to me that in the great refactor of the standard tokenizer, LUCENE-2167, something was forgotten here. I think that if someone uses the ClassicTokenizer then no matter what the version is, this filter should do what it used to do. And the TODO suggests someone forgot to make this filter do something useful for the StandardTokenizer. Or perhaps that idea should be discarded and this class should be named ClassicTokenFilter. In any event, the javadocs for this class appear out of date as there is no mention of ClassicTokenizer, and the wiki is out of date too.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3369</id>
      <title>SpanNearQuery add setMinimumNumberShouldMatch method</title>
      <description>For long string query. Sometimes need BooleanQuery.setMinimumNumberShouldMatch to relax match condition. But it cannot limit match positions. SpanNearQuery can achieve this. Can SpanNearQuery add setMinimumNumberShouldMatch method to relax SpanTermQuery match?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3370</id>
      <title>Support for a "SpanNotNearQuery"</title>
      <description>Sometimes you want to find an instance of a span which does not hit near some other span query. SpanNotQuery only excludes exact hits on the term, but sometimes you want to exclude hits 1 away from the first, and other times you might want the range to be wider. So a SpanNotNearQuery could be useful. SpanNotQuery is actually very close, and adding slop+inOrder support to it is probably sufficient to make a SpanNotNearQuery. There appears to be one project which has done it in this fashion, although this particular code looks like it's out of date: http://www.koders.com/java/fid933A84488EBE1F3492B19DE01B2A4FC1D68DA258.aspx?s=ArrayQuery</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3371</id>
      <title>Support for a "SpanAndQuery" / "SpanAllNearQuery"</title>
      <description>I would like to parse queries like this: a WITHIN 5 WORDS OF (b AND c) This would match cases where both a b span and a c span are within 5 of the same a span. The existing span query classes do not appear to be capable of doing this no matter how they are combined, although replacing the AND with "WITHIN 10 OF" (general rule is to double the first number) at least ensures that no hits are lost (it just returns too many.) I'm not sure how the class would work, but it might be like this: Query q = new SpanAllNearQuery(a, new SpanQuery[] { b, c }, 5, false); The difference from SpanNearQuery is that SpanNearQuery considers the entire collection of terms as a single set to be found near each other, whereas this query would consider each of the terms in the array relative to the first.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3376</id>
      <title>Move ReusableAnalyzerBase into core</title>
      <description>In LUCENE-2309 it was suggested that we should make Analyzer reusability compulsory. ReusableAnalyzerBase is a fantastic way to drive reusability so lets move it into core (so that we can then change all impls over to using it).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3381</id>
      <title>Sandbox remaining contrib queries</title>
      <description>In LUCENE-3271, I moved the 'good' queries from the queries contrib to new destinations (primarily the queries module). The remnants now need to find their home. As suggested in LUCENE-3271, these classes are not bad per se, just odd. So lets create a sandbox contrib that they and other 'odd' contrib classes can go to. We can then decide their fate at another time.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3389</id>
      <title>Add ReuseStrategy to ReusableAnalyzerBase</title>
      <description>ReusableAnalyzerBase currently ignores the fieldName passed into .tokenStream/.reusableTokenStream and reuses the same components for every field. To move all Analyzers over to using ReusableAnalyzerBase, we need to support per-field reuse as well. As Robert pointed out in LUCENE-2309, we can do this by adding a ReuseStrategy which has two impls; one implementing the current global behavior and one providing per-field reuse.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3392</id>
      <title>Combining analyzers output</title>
      <description>It should be easy to combine the output of multiple Analyzers, or TokenStreams. A ComboAnalyzer and a ComboTokenStream class would take multiple instances, and multiplex their output, keeping a rough order of tokens like increasing position then increasing start offset then increasing end offset.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3395</id>
      <title>FreqFilteringScorerWrapper and min/max freq options on TermQuery</title>
      <description>A Solr User was asking about how specify a minimum tf when searching for a term (ie: documents matching "dog" at least 3 times). Based on a conversation with rmuir on IRC, that led me to realize that we now explicitly expose a general "freq()" method on Scorer, and that min/max freq constraints could be implemented as a general Scorer Wrapper. I propose that we add such a wrapper, and add setMinFreq(float)/setMaxFreq(float) methods to TermQuery (similar to the minNumShouldMatches and disableCoord type setters in BooleanQuery) that cause it to be used automatically.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3396</id>
      <title>Make TokenStream Reuse Mandatory for Analyzers</title>
      <description>In LUCENE-2309 it became clear that we'd benefit a lot from Analyzer having to return reusable TokenStreams. This is a big chunk of work, but its time to bite the bullet. I plan to attack this in the following way: Collapse the logic of ReusableAnalyzerBase into Analyzer Add a ReuseStrategy abstraction to Analyzer which controls whether the TokenStreamComponents are reused globally (as they are today) or per-field. Convert all Analyzers over to using TokenStreamComponents. I've already seen that some of the TokenStreams created in tests need some work to be reusable (even if they aren't reused). Remove Analyzer.reusableTokenStream and convert everything over to using .tokenStream (which will now be returning reusable TokenStreams).</description>
      <attachments/>
      <comments>31</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3399</id>
      <title>Enable replace-able field caches</title>
      <description>For LUCENE-2312 we need to be able to synchronously replace field cache values and receive events on when new field cache values are created.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3405</id>
      <title>Rename IOUtils.close methods</title>
      <description>The closeSafely methods that take a boolean suppressExceptions are dangerous... I've renamed to .close (no suppression) and .closeWhileHandlingException (suppresses all exceptions).</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3406</id>
      <title>Add source packaging targets that make a tarball from a local working copy</title>
      <description>I am adding back targets that were removed in &lt;https://issues.apache.org/jira/browse/LUCENE-2973&gt; that are used to create source distribution packaging from a local working copy as new Ant targets. 2 things to note about the patch: 1) For package-local-src-tgz in solr/build.xml, I had to specify additional directories under solr/ that have been added since LUCENE-2973. 2) I couldn't get the package-tgz-local-src in lucene/build.xml to generate the docs folder, which does get added by package-tgz-src. The patch is against the trunk.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3408</id>
      <title>Remove unnecessary memory barriers in DWPT</title>
      <description>Currently DWPT still uses AtomicLong to count the bytesUsed. Each write access issues an implicite memory barrier which is totally unnecessary since we doing everything single threaded on that level. This might be very minor but we shouldn't issue unnecessary memory barriers causing processors to lock their instruction pipeline for no reason.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3410</id>
      <title>Make WordDelimiterFilter's instantiation more readable</title>
      <description>Currently WordDelimiterFilter's constructor is: public WordDelimiterFilter(TokenStream in, byte[] charTypeTable, int generateWordParts, int generateNumberParts, int catenateWords, int catenateNumbers, int catenateAll, int splitOnCaseChange, int preserveOriginal, int splitOnNumerics, int stemEnglishPossessive, CharArraySet protWords) { which means its instantiation is an unreadable combination of 1s and 0s. We should improve this by either using a Builder, 'int flags' or an EnumSet.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3413</id>
      <title>CombiningFilter to recombine tokens into a single token for sorting</title>
      <description>I whipped up this CombiningFilter for the following use case: I've got a bunch of titles of e.g., Books, such as: The Grapes of Wrath Tommy Tommerson saves the World Top of the World The Tales of Beedle the Bard Born Free etc. I want to sort these titles using a String field that includes stopword analysis (e.g., to remove "The"), and synonym filtering (e.g., for grouping), etc. I created an analysis chain in Solr for this that was based off of alphaOnlySort, which looks like this: &lt;fieldType name="alphaOnlySort" class="solr.TextField" sortMissingLast="true" omitNorms="true"&gt; &lt;analyzer&gt; &lt;!-- KeywordTokenizer does no actual tokenizing, so the entire input string is preserved as a single token --&gt; &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt; &lt;!-- The LowerCase TokenFilter does what you expect, which can be when you want your sorting to be case insensitive --&gt; &lt;filter class="solr.LowerCaseFilterFactory" /&gt; &lt;!-- The TrimFilter removes any leading or trailing whitespace --&gt; &lt;filter class="solr.TrimFilterFactory" /&gt; &lt;!-- The PatternReplaceFilter gives you the flexibility to use Java Regular expression to replace any sequence of characters matching a pattern with an arbitrary replacement string, which may include back references to portions of the original string matched by the pattern. See the Java Regular Expression documentation for more information on pattern and replacement string syntax. http://java.sun.com/j2se/1.5.0/docs/api/java/util/regex/package-summary.html --&gt; &lt;filter class="solr.PatternReplaceFilterFactory" pattern="([^a-z])" replacement="" replace="all" /&gt; &lt;/analyzer&gt; &lt;/fieldType&gt; The issue with alphaOnlySort is that it doesn't support stopword remove or synonyms because those are based on the original token level instead of the full strings produced by the KeywordTokenizer (which does not do tokenization). I needed a filter that would allow me to change alphaOnlySort and its analysis chain from using KeywordTokenizer to using WhitespaceTokenizer, and then a way to recombine the tokens at the end. So, take "The Grapes of Wrath". I needed a way for it to get turned into: grapes of wrath And then to combine those tokens into a single token: grapesofwrath The attached CombiningFilter takes care of that. It doesn't do it super efficiently I'm guessing (since I used a StringBuffer), but I'm open to suggestions on how to make it better. One other thing is that apparently this analyzer works fine for analysis (e.g., it produces the desired tokens), however, for sorting in Solr I'm getting null sort tokens. Need to figure out why. Here ya go!</description>
      <attachments/>
      <comments>21</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3414</id>
      <title>Bring Hunspell for Lucene into analysis module</title>
      <description>Some time ago I along with Robert and Uwe, wrote an Stemmer which uses the Hunspell algorithm. It has the benefit of supporting dictionaries for a wide array of languages. It seems to still be being used but has fallen out of date. I think it would benefit from being inside the analysis module where additional features such as decompounding support, could be added.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3415</id>
      <title>Snowball filter to include original word too</title>
      <description>1. Currently, snowball filter deletes the original word and adds the stemmed word to the index. So, if i want to do search with / without stemming, i have to keep 2 fields, one with stemming and one without it. 2. Rather than doing this, if we have configurable item to preserve original, it would solve more business problem. 3. Using single field, i can do search using stemming / without stemming by changing the query filters. The same can also be done for phonetic filters too.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3416</id>
      <title>Allow to pass an instance of RateLimiter to FSDirectory allowing to rate limit merge IO across several directories / instances</title>
      <description>This can come in handy when running several Lucene indices in the same VM, and wishing to rate limit merge across all of them.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3419</id>
      <title>Resolve JUnit assert deprecations</title>
      <description>Many tests use assertEquals methods which have been deprecated. The culprits are assertEquals(float, float), assertEquals(double, double) and assertEquals(Object[], Object[]). Although not a big issue, they annoy me every time I see them so I'm going to fix them.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3424</id>
      <title>Return sequence ids from IW update/delete/add/commit to allow total ordering outside of IW</title>
      <description>Based on the discussion on the mailing list IW should return sequence ids from update/delete/add and commit to allow ordering of events for consistent transaction logs and recovery.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3425</id>
      <title>NRT Caching Dir to allow for exact memory usage, better buffer allocation and "global" cross indices control</title>
      <description>A discussion on IRC raised several improvements that can be made to NRT caching dir. Some of the problems it currently has are: 1. Not explicitly controlling the memory usage, which can result in overusing memory (for example, large new segments being committed because refreshing is too far behind). 2. Heap fragmentation because of constant allocation of (probably promoted to old gen) byte buffers. 3. Not being able to control the memory usage across indices for multi index usage within a single JVM. A suggested solution (which still needs to be ironed out) is to have a BufferAllocator that controls allocation of byte[], and allow to return unused byte[] to it. It will have a cap on the size of memory it allows to be allocated. The NRT caching dir will use the allocator, which can either be provided (for usage across several indices) or created internally. The caching dir will also create a wrapped IndexOutput, that will flush to the main dir if the allocator can no longer provide byte[] (exhausted). When a file is "flushed" from the cache to the main directory, it will return all the currently allocated byte[] to the BufferAllocator to be reused by other "files".</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3426</id>
      <title>optimizer for n-gram PhraseQuery</title>
      <description>If 2-gram is used and the length of query string is 4, for example q="ABCD", QueryParser generates (when autoGeneratePhraseQueries is true) PhraseQuery("AB BC CD") with slop 0. But it can be optimized PhraseQuery("AB CD") with appropriate positions. The idea came from the Japanese paper "N.M-gram: Implementation of Inverted Index Using N-gram with Hash Values" by Mikio Hirabayashi, et al. (The main theme of the paper is different from the idea that I'm using here, though)</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3431</id>
      <title>Make QueryAutoStopWordAnalyzer immutable and reusable</title>
      <description>Currently QueryAutoStopWordAnalyzer allows its list of stop words to be changed after instantiation through its addStopWords() methods. This stops the Analyzer from being reusable since it must instantiate its StopFilters every time. Having these methods means that although the Analyzer can be instantiated once and reused between IndexReaders, the actual analysis stack is not reusable (which is probably the more expensive part). So lets change the Analyzer so that its stop words are set at instantiation time, facilitating reuse.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3433</id>
      <title>Random access non RAM resident IndexDocValues (CSF)</title>
      <description>There should be a way to get specific IndexDocValues by going through the Directory rather than loading all of the values into memory.</description>
      <attachments/>
      <comments>43</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>3434</id>
      <title>Make ShingleAnalyzerWrapper and PerFieldAnalyzerWrapper immutable</title>
      <description>Both ShingleAnalyzerWrapper and PerFieldAnalyzerWrapper have setters which change some state which impacts their analysis stack. If these are going to become reusable, then the state must be immutable as changing it will have no effect. Process will be similar to QueryAutoStopWordAnalyzer, I will remove in trunk and deprecate in 3x.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3436</id>
      <title>Spellchecker "Suggest Mode" Support</title>
      <description>This is a spin-off from SOLR-2585. Currently o.a.l.s.s.SpellChecker and o.a.l.s.s.DirectSpellChecker support two "Suggest Modes": 1. Suggest for terms that are not in the index. 2. Suggest "more popular" terms. This issue is to add a third Suggest Mode: 3. Suggest always. This will assist users in developing context-sensitive spell suggestions and/or did-you-mean suggestions. See SOLR-2585 for a full discussion. Note that o.a.l.s.s.SpellChecker already can support this functionality, if the user passes in a NULL term &amp; IndexReader. This, however, is not intutive. o.a.l.s.s.DirectSpellChecker currently has no support for this third Suggest Mode.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3440</id>
      <title>FastVectorHighlighter: IDF-weighted terms for ordered fragments</title>
      <description>The FastVectorHighlighter uses for every term found in a fragment an equal weight, which causes a higher ranking for fragments with a high number of words or, in the worst case, a high number of very common words than fragments that contains all of the terms used in the original query. This patch provides ordered fragments with IDF-weighted terms: total weight = total weight + IDF for unique term per fragment * boost of query; The ranking-formula should be the same, or at least similar, to that one used in org.apache.lucene.search.highlight.QueryTermScorer. The patch is simple, but it works for us. Some ideas: A better approach would be moving the whole fragments-scoring into a separate class. Switch scoring via parameter Exact phrases should be given a even better score, regardless if a phrase-query was executed or not edismax/dismax-parameters pf, ps and pf^boost should be observed and corresponding fragments should be ranked higher</description>
      <attachments/>
      <comments>59</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3441</id>
      <title>Add NRT support to facets</title>
      <description>Currently LuceneTaxonomyReader does not support NRT - i.e., on changes to LuceneTaxonomyWriter, you cannot have the reader updated, like IndexReader/Writer. In order to do that we need to do the following: Add ctor to LuceneTaxonomyReader to allow you to instantiate it with LuceneTaxonomyWriter. Add API to LuceneTaxonomyWriter to expose its internal IndexReader Change LTR.refresh() to return an LTR, rather than void. This is actually not strictly related to that issue, but since we'll need to modify refresh() impl, I think it'll be good to change its API as well. Since all of facet API is @lucene.experimental, no backwards issues here (and the sooner we do it, the better).</description>
      <attachments/>
      <comments>24</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>3443</id>
      <title>Port 3.x FieldCache.getDocsWithField() to trunk</title>
      <description>[Spinoff from LUCENE-3390] I think the approach in 3.x for handling un-valued docs, and making it possible to specify how such docs are sorted, is better than the solution we have in trunk. I like that FC has a dedicated method to get the Bits for docs with field – easy for apps to directly use. And I like that the bits have their own entry in the FC. One downside is that it's 2 passes to get values and valid bits, but I think we can fix this by passing optional bool to FC.getXXX methods indicating you want the bits, and the populate the FC entry for the missing bits as well. (We can do that for 3.x and trunk). Then it's single pass.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3444</id>
      <title>Distinct field value count per group</title>
      <description>Support a second pass collector that counts unique field values of a field per group. This is just one example of group statistics that one might want.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3445</id>
      <title>Add SearcherManager, to manage IndexSearcher usage across threads and reopens</title>
      <description>This is a simple helper class I wrote for Lucene in Action 2nd ed. I'd like to commit under Lucene (contrib/misc). It simplifies using &amp; reopening an IndexSearcher across multiple threads, by using IndexReader's ref counts to know when it's safe to close the reader. In the process I also factored out a test base class for tests that want to make lots of simultaneous indexing and searching threads, and fixed TestNRTThreads (core), TestNRTManager (contrib/misc) and the new TestSearcherManager (contrib/misc) to use this base class.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3447</id>
      <title>Prettify files are not included with javadoc jars</title>
      <description>The prettify files are not included with the generated javadoc .jars, which means that if someone expands a certain javadocs .jar, and this .jar relies on prettify (such as highlighter and soon facet), the javadocs will not display properly.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3449</id>
      <title>Fix FixedBitSet.nextSetBit/prevSetBit to support the common usage pattern in every programming book</title>
      <description>The usage pattern for nextSetBit/prevSetBit is the following: for(int i=bs.nextSetBit(0); i&gt;=0; i=bs.nextSetBit(i+1)) { // operate on index i here } The problem is that the i+1 at the end can be bs.length(), but the code in nextSetBit does not allow this (same applies to prevSetBit(0)). The above usage pattern is in every programming book, so it should really be supported. The check has to be done in all cases (with the current impl in the calling code). If the check is done inside xxxSetBit() it can also be optimized to be only called seldom and not all the time, like in the ugly looking replacement, thats currently needed: for(int i=bs.nextSetBit(0); i&gt;=0; i=(i&lt;bs.length()-1) ? bs.nextSetBit(i+1) : -1) { // operate on index i here } We should change this and allow out-of bounds indexes for those two methods (they already do some checks in that direction). Enforcing this with an assert is unuseable on the client side. The test code for FixedBitSet also uses this, horrible. Please support the common usage pattern for BitSets.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3451</id>
      <title>Remove special handling of pure negative Filters in BooleanFilter, disallow pure negative queries in BooleanQuery</title>
      <description>We should at least in Lucene 4.0 remove the hack in BooleanFilter that allows pure negative Filter clauses. This is not supported by BooleanQuery and confuses users (I think that's the problem in LUCENE-3450). The hack is buggy, as it does not respect deleted documents and returns them in its DocIdSet. Also we should think about disallowing pure-negative Queries at all and throw UOE.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3454</id>
      <title>rename optimize to a less cool-sounding name</title>
      <description>I think users see the name optimize and feel they must do this, because who wants a suboptimal system? but this probably just results in wasted time and resources. maybe rename to collapseSegments or something?</description>
      <attachments/>
      <comments>57</comments>
      <commenters>15</commenters>
    </issue>
    <issue>
      <id>3467</id>
      <title>Cut over numeric docvalues to fixed straight bytes</title>
      <description>Currently numeric docvalues types are encoded and stored individually which creates massive duplication of writing / indexing code. Yet, almost all of them (except packed ints) are essentially a fixed straight bytes variant.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3468</id>
      <title>FirstPassGroupingCollector should use pollLast()</title>
      <description>Currently FirstPassGroupingCollector uses last and remove method (TreeSet) for replacing a more relevant grouping during grouping. This can be replaced by pollLast since Lucene trunk is now Java 6. TermFirstPassGroupingCollectorJava6 in Solr can be removed as well.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3473</id>
      <title>CheckIndex should verify numUniqueTerms == recomputedNumUniqueTerms</title>
      <description>Just glancing at the code it seems to sorta do this check, but only in the hasOrd==true case maybe (which seems to be testing something else)? It would be nice to verify this also for terms dicts that dont support ord. we should add explicit checks per-field in 4.x, and for-all-fields in 3.x and preflex</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3475</id>
      <title>ShingleFilter should handle positionIncrement of zero, e.g. synonyms</title>
      <description>ShingleFilter is creating shingles for a single term that has been expanded by synonyms when it shouldn't. The position increment is 0. As an example, I have an Analyzer with a SynonymFilter followed by a ShingleFilter. Assuming car and auto are synonyms, the SynonymFilter produces two tokens and position 1: car, auto. The ShingleFilter is then producing 3 tokens, when there should only be two: car, car auto, auto. This behavior seems incorrect.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3477</id>
      <title>Fix JFlex tokenizer compiler warnings</title>
      <description>We get lots of distracting fallthrough warnings running "ant compile" in modules/analysis, from the tokenizers generated from JFlex. Digging a bit, they actually do look spooky. So I managed to edit the JFlex inputs to insert a bunch of break statements in our rules, but I have no idea if this is right/dangerous, and it seems a bit weird having to do such insertions of "naked" breaks. But, this does fix all the warnings, and all tests pass...</description>
      <attachments/>
      <comments>7</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3482</id>
      <title>Refactor grouping module to be more maintainable</title>
      <description>Currently we have 4 types of grouping collectors and 8 concrete subclasses in Lucene / Solr. In current architecture for each type of collector two concrete subclasses need to be created. An implementation optimized for single term based groups and a more general implementation that works with MutableValue to also support grouping by functions. If we want for example group by IndexDocValues each type of grouping collector needs to have three concrete subclasses. This design isn't very maintainable. I think it is best to introduce a concept that knows how deals with dealing groups for all the different sources. Therefore the grouping module should depend on the queries module, so that grouping can reuse the ValueSource concept. A term based concrete impl. of this concept knows for example to use the DocValues.ord() method. Or more generic concrete impl. will use DocValues.ValueFiller.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3483</id>
      <title>Move Function grouping collectors from Solr to grouping module</title>
      <description>Move the Function*Collectors from Solr (inside Grouping source file) to grouping module.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3486</id>
      <title>Add SearcherLifetimeManager, so you can retrieve the same searcher you previously used</title>
      <description>The idea is similar to SOLR-2809 (adding searcher leases to Solr). This utility class sits above whatever your source is for "the current" searcher (eg NRTManager, SearcherManager, etc.), and records (holds a reference to) each searcher in recent history. The idea is to ensure that when a user does a follow-on action (clicks next page, drills down/up), or when two or more searcher invocations within a single user search need to happen against the same searcher (eg in distributed search), you can retrieve the same searcher you used "last time". I think with the new searchAfter API (LUCENE-2215), doing follow-on searches on the same searcher is more important, since the "bottom" (score/docID) held for that API can easily shift when a new searcher is opened. When you do a "new" search, you record the searcher you used with the manager, and it returns to you a long token (currently just the IR.getVersion()), which you can later use to retrieve the same searcher. Separately you must periodically call prune(), to prune the old searchers, ideally from the same thread / at the same time that you open a new searcher.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3488</id>
      <title>Factor out SearcherManager from NRTManager</title>
      <description>Currently we have NRTManager and SearcherManager while NRTManager contains a big piece of the code that is already in SearcherManager. Users are kind of forced to use NRTManager if they want to have SearcherManager goodness with NRT. The integration into NRTManager also forces you to maintain two instances even if you know you always want deletes. To me NRTManager tries to do more than necessary and mixes lots of responsibilities ie. handling searchers and handling indexing generations. NRTManager should use a SearcherManager by aggregation rather than duplicate a lot of logic. SearcherManager should have a NRT and Directory based implementation users can simply choose from.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3490</id>
      <title>Restructure codec hierarchy</title>
      <description>Spinoff of LUCENE-2621. (Hoping we can do some of the renaming etc here in a rote way to make progress). Currently Codec.java only represents a portion of the index, but there are other parts of the index (stored fields, term vectors, fieldinfos, ...) that we want under codec control. There is also some inconsistency about what a Codec is currently, for example Memory and Pulsing are really just PostingsFormats, you might just apply them to a specific field. On the other hand, PreFlex actually is a Codec: it represents the Lucene 3.x index format (just not all parts yet). I imagine we would like SimpleText to be the same way. So, I propose restructuring the classes so that we have something like: CodecProvider &lt;-- dead, replaced by java ServiceProvider mechanism. All indexes are 'readable' if codecs are in classpath. Codec &lt;-- represents the index format (PostingsFormat + FieldsFormat + ...) PostingsFormat: this is what Codec controls today, and Codec will return one of these for a field. FieldsFormat: Stored Fields + Term Vectors + FieldInfos? I think for PreFlex, it doesnt make sense to expose its PostingsFormat as a 'public' class, because preflex can never be per-field so there is no use in allowing you to configure PreFlex for a specific field. Similarly, I think in the future we should do the same thing for SimpleText. Nobody needs SimpleText for production, it should just be a Codec where we try to make as much of the index as plain text and simple as possible for debugging/learning/etc. So we don't need to expose its PostingsFormat. On the other hand, I don't think we need Pulsing or Memory codecs, because its pretty silly to make your entire index use one of their PostingsFormats. To parallel with analysis: PostingsFormat is like Tokenizer and Codec is like Analyzer, and we don't need Analyzers to "show off" every Tokenizer. we can also move the baked in PerFieldCodecWrapper out (it would basically be PerFieldPostingsFormat). Privately it would write the ids to the file like it does today. in the future, all 3.x hairy backwards code would move to PreflexCodec. SimpleTextCodec would get a plain text fieldinfos impl, etc.</description>
      <attachments/>
      <comments>31</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3491</id>
      <title>Portable index format</title>
      <description>This issue presents the idea of a special-purpose Lucene index format that is suitable for cross-version compatibility and resilient to changes in Lucene implementations. Please see the attached document for more details. The code needs branch lucene-2621 to compile and run. It's still incomplete, in a roughly the same way as the Codec api is still incomplete However, a separate tool PortableIndexExporter was implemented to illustrate how serialization of all index parts could look like. Comments and suggestions are welcome!</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3492</id>
      <title>Extract a generic framework for running randomized tests.</title>
      <description>The work on this issue is temporarily at github (lots of experiments and tweaking): https://github.com/carrotsearch/randomizedtesting Or directly: git clone git://github.com/carrotsearch/randomizedtesting.git RandomizedRunner is a JUnit runner, so it is capable of running @Test-annotated test cases. It respects regular lifecycle hooks such as @Before, @After, @BeforeClass or @AfterClass, but it also adds the following: Randomized, but repeatable execution and infrastructure for dealing with randomness: uses pseudo-randomness (so that a given run can be repeated if given the same starting seed) for many things called "random" below, randomly shuffles test methods to ensure they don't depend on each other, randomly shuffles hooks (within a given class) to ensure they don't depend on each other, base class RandomizedTest provides a number of methods for generating random numbers, strings and picking random objects from collections (again, this is fully repeatable given the initial seed if there are no race conditions), the runner provides infrastructure to augment stack traces with information about the initial seeds used for running the test, so that it can be repeated (or it can be determined that the test is not repeatable – this indicates a problem with the test case itself). Thread control: any threads created as part of a test case are assigned the same initial random seed (repeatability), tracks and attempts to terminate any Threads that are created and not terminated inside a test case (not cleaning up causes a test failure), tracks and attempts to terminate test cases that run for too long (default timeout: 60 seconds, adjustable using global property or annotations), Improved validation and lifecycle support: RandomizedRunner uses relaxed contracts of hook methods' accessibility (hook methods can be private). This helps in avoiding problems with method shadowing (static hooks) or overrides that require tedious super.() chaining). Private hooks are always executed and don't affect subclasses in any way, period. @Listeners annotation on a test class allows it to hook into the execution progress and listen to events. @Validators annotation allows a test class to provide custom validation strategies (project-specific). For example a base class can request specific test case naming strategy (or reject JUnit3-like methods, for instance). RandomizedRunner does not "chain" or "suppress" exceptions happening during execution of a test case (including hooks). All exceptions are reported as soon as they happened and multiple failure reports can occur. Most environments we know of then display these failures sequentially allowing a clearer understanding of what actually happened first.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3494</id>
      <title>Remove per-document multiply in FilteredQuery</title>
      <description>Spinoff of LUCENE-1536. In LUCENE-1536, Uwe suggested using FilteredQuery under-the-hood to implement filtered search. But this query is inefficient, it does a per-document multiplication (wrapped.score() * boost()). Instead, it should just pass the boost down in its weight, like BooleanQuery does to avoid this per-document multiply.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3496</id>
      <title>Support grouping by IndexDocValues</title>
      <description>Although IDV is not yet finalized (More particular the SortedSource). I think we already can discuss / investigate implementing grouping by IDV.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3497</id>
      <title>Make DirectoryReader protected methods non-final</title>
      <description>DirectoryReader has protected methods that are overridden and made final. This is silly because it prevents other classes from overriding DirectoryReader. The methods are doOpenIfChanged and a handful of related variables that are private.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3498</id>
      <title>IndexReaderFactory for Lucene</title>
      <description>An IndexReaderFactory can be used by IndexWriter and DirectoryReader to enable subclasses of DR to be instantiated by Lucene, automatically.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3500</id>
      <title>improve getLiveDocs() performance</title>
      <description>On segmentReader, liveDocs is volatile because they used to be loaded on demand. This no longer appears to be the case... liveDocs are always loaded up front. This also means that getLiveDocs() can never fail (even after close), and we can remove the ensureOpen call. Minor optimizations, but volatile reads still do prevent optimizations across that boundary.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3502</id>
      <title>Packed ints: move .getArray into Reader API</title>
      <description>This is a simple code cleanup... it's messy that a consumer of PackedInts.Reader must check whether the impl is Direct8/16/32/64 in order to get an array; it's better to move up the .getArray into the Reader interface and then make the DirectN impls package private.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3507</id>
      <title>Improve Memory Consumption for merging DocValues SortedBytes variants</title>
      <description>Currently SortedBytes are loaded into memory during merge which could be a potential trap. Instead of loading them into Heap memory we can merge those sorted values with much smaller memory and without loading all values into ram.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3509</id>
      <title>Add settings to IWC to optimize IDV indices for CPU or RAM respectivly</title>
      <description>spinnoff from LUCENE-3496 - we are seeing much better performance if required bits for PackedInts are rounded up to a 8/16/32/64. We should add this option to IWC and default to round up ie. more RAM &amp; faster lookups.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3510</id>
      <title>BooleanScorer should not limit number of prohibited clauses</title>
      <description>Today it's limited to 32, because it uses a separate bit in the mask for each clause. But I don't understand why it does this; I think all prohibited clauses can share a single boolean/bit? Any match on a prohibited clause sets this bit and the doc is not collected; we don't need each prohibited clause to have a dedicated bit? We also use the mask for required clauses, but this code is now commented out (we always use BS2 if there are any required clauses); if we re-enable this code (and I think we should, at least in certain cases: I suspect it'd be faster than BS2 in many cases), I think we can cutover to an int count instead of bit masks, and then have no limit on the required clauses sent to BooleanScorer also. Separately I cleaned a few things up about BooleanScorer: all of the embedded scorer methods (nextDoc, docID, advance, score) now throw UOE; pre-allocate the buckets instead of doing it lazily per-sub-collect.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3512</id>
      <title>Expose fields in FieldPhraseList and WeightedPhraseInfo with public getters</title>
      <description>Currently, to write a custom FragListBuilder implementation or custom highlighter, it probably will have to be placed in the org.apache.lucene.search.vectorhighlight package so that it has access to the fields in FieldPhraseList and its inner class WeightedPhraseInfo. It seems that in recent releases, getters were added for fields in many other classes in vectorhighlight, and I think it makes sense to add them for fields in FieldPhraseList as well to support such custom implementations.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3513</id>
      <title>Add SimpleFragListBuilder constructor with margin parameter</title>
      <description>SimpleFragListBuilder would benefit from an additional constructor that takes in margin. Currently, the margin is defined as a constant, so to "implement" a FragListBuilder with a different margin, one has no choice but to copy and paste SimpleFragListBuilder into a new class that must be placed in the org.apache.lucene.search.vectorhighlight package due to accesses of package-protected fields in other classes. If this change were made, the precondition check of the constructor's fragCharSize should probably be altered to ensure that it's less than max(1, margin*3) to allow for a margin of 0.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3514</id>
      <title>deep paging with Sort</title>
      <description>We added IS.searchAfter(Query, Filter) but we don't support Sort yet with this API. I think it might be overkill at least at first to try to implement 12 collector variants for this. I put the following idea on SOLR-1726: One idea would be to start with one or two implementations (maybe in/out of order) for the sorting case, and dont overspecialize it yet. for page 1, the ScoreDoc (FieldDoc really) will be null, so we just return the normal impl anyway. even if our searchAfter isnt huper-duper fast, the user can always make the tradeoff like with page-by-score. they can always just pass null until like page 10 or something if they compute that it only starts to 'help' then.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3517</id>
      <title>Fix pulsingcodec to reuse its enums</title>
      <description>PulsingCodec currently doesnt always reuse its enums, which could lead to behavior like LUCENE-3515. The problem is sometimes it returns the 'wrapped' enum, but other times it returns its 'pulsingenum' depending upon whether terms are pulsed... we can use the fact that these enums allow attributes to keep the reuse information for both so it can reuse when stepping through terms.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3518</id>
      <title>Add sort-by-term with DocValues</title>
      <description>There are two sorted byte[] types with DocValues (BYTES_VAR_SORTED, BYTES_FIXED_SORTED), so you can index this type, but you can't yet sort by it. So I added a FieldComparator just like TermOrdValComparator, except it pulls from the doc values instead. There are some small diffs, eg with doc values there are never null values (see LUCENE-3504).</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3521</id>
      <title>upgrade icu jar to 4.8.1.1 / remove lucenetestcase hack</title>
      <description>This bug fix release fixes problems with icu under java7: http://bugs.icu-project.org/trac/ticket/8734</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3523</id>
      <title>WordBreakSpellChecker</title>
      <description>A spellchecker that generates suggestions by combining two or more terms and/or breaking terms into multiple words. This would typically be used in addition to one of the existing spell checkers to get both traditional and word-break suggestions for the end user.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3524</id>
      <title>Add "direct" PackedInts.Reader impl, that reads directly from disk on each get</title>
      <description>Spinoff from LUCENE-3518. If we had a direct PackedInts.Reader impl we could use that instead of the RandomAccessReaderIterator.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3527</id>
      <title>Implement getDistance() on DirectSpellChecker.INTERNAL_LEVENSHTEIN</title>
      <description>DirectSpellChecker.INTERNAL_LEVENSHTEIN is currently not a full-fledged implementation of StringDistance. But an full implementation is needed for Solr's SpellCheckComponent.finishStage(), and also would be helpful for those trying to take the advice given in LIA 2nd ed section sect8.5.3.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3531</id>
      <title>Improve CachingWrapperFilter to optionally also cache acceptDocs, if identical to liveDocs</title>
      <description>Spinoff from LUCENE-1536: This issue removed the different cache modes completely and always applies the acceptDocs using BitsFilteredDocIdSet.wrap(), the cache only contains raw DocIdSet without any deletions/acceptDocs. For IndexReaders that are seldom reopened, this might not be as performant as it could be. If the acceptDocs==IR.liveDocs, those DocIdSet could also be cached with liveDocs applied.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3534</id>
      <title>Backport FilteredQuery/IndexSearcher changes to 3.x: Remove filter logic from IndexSearcher and delegate to FilteredQuery</title>
      <description>Spinoff from LUCENE-1536: We simplified the code in IndexSearcher to no longer do the filtering there, instead wrap all Query with FilteredQuery, if a non-null filter is given. The conjunction code would then only exist in FilteredQuery which makes it easier to maintain. Currently both implementations differ in 3.x, in trunk we used the more optimized IndexSearcher variant with addition of a simplified in-order conjunction code. This issue will backport those changes (without random access bits).</description>
      <attachments/>
      <comments>12</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3539</id>
      <title>IndexFormatTooOld/NewExc should try to include fileName + directory when possible</title>
      <description>(Spinoff from http://markmail.org/thread/t6s7nn3ve765nojc ) When we throw a too old/new exc we should try to include the full path to the offending file, if possible.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3549</id>
      <title>Remove DocumentBuilder interface from facet module</title>
      <description>The facet module contains an interface called DocumentBuilder, which contains a single method, build(Document) (it's a builder API). We use it in my company to standardize how different modules populate a Document object. I've included it with the facet contribution so that things will compile with as few code changes as possible. Now it's time to do some cleanup and I'd like to start with this interface. If people think that this interface is useful to reside in 'core', then I don't mind moving it there. But otherwise, let's remove it from the code. It has only one impl in the facet module: CategoryDocumentBuilder, and we can certainly do without the interface. More so, it's under o.a.l package which is inappropriate IMO. If it's moved to 'core', it should be under o.a.l.document. If people see any problem with that, please speak up. I will do the changes and post a patch here shortly.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3550</id>
      <title>Create example code for core</title>
      <description>Trunk has gone under lots of API changes. Some of which are not trivial, and the migration path from 3.x to 4.0 seems hard. I'd like to propose some way to tackle this, by means of live example code. The facet module implements this approach. There is live Java code under src/examples that demonstrate some well documented scenarios. The code itself is documented, in addition to javadoc. Also, the code itself is being unit tested regularly. We found it very difficult to keep documentation up-to-date – javadocs always lag behind, Wiki pages get old etc. However, when you have live Java code, you're forced to keep it up-to-date. It doesn't compile if you break the API, it fails to run if you change internal impl behavior. If you keep it simple enough, its documentation stays simple to. And if we are successful at maintaining it (which we must be, otherwise the build should fail), then people should have an easy experience migrating between releases. So say you take the simple scenario "I'd like to index documents which have the fields ID, date and body". Then you create an example class/method that accomplishes that. And between releases, this code gets updated, and people can follow the changes required to implement that scenario. I'm not saying the examples code should always stay optimized. We can aim at that, but I don't try to fool myself thinking that we'll succeed. But at least we can get it compiled and regularly unit tested. I think that it would be good if we introduce the concept of examples such that if a module (core, contrib, modules) have an src/examples, we package it in a .jar and include it with the binary distribution. That's for a first step. We can also have meta examples, under their own module/contrib, that show how to combine several modules together (this might even uncover API problems), but that's definitely a second phase. At first, let's do the "unit examples" (ala unit tests) and better start with core. Whatever we succeed at writing for 4.0 will only help users. So let's use this issue to: List example scenarios that we want to demonstrate for core Building the infrastructure in our build system to package and distribute a module's examples. Please feel free to list here example scenarios that come to mind. We can then track what's been done and what's not. The more we do the better.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3552</id>
      <title>TaxonomyReader/Writer and their Lucene* implementation</title>
      <description>The facet module contains two interfaces TaxonomyWriter and TaxonomyReader, with two implementations Lucene*. We've never actually implemented two TaxonomyWriters/Readers, so I'm not sure if these interfaces are useful anymore. Therefore I'd like to propose that we do either of the following: Remove the interfaces and remove the Lucene part from the implementation classes (to end up with TW/TR impls). Or, Keep the interfaces, but rename the Lucene* impls to Directory*. Whatever we do, I'd like to make the impls/interfaces impl also TwoPhaseCommit. Any preferences?</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3556</id>
      <title>Make DirectoryTaxonomyWriter's indexWriter member private</title>
      <description>DirectoryTaxonomyWriter has a protected indexWriter member. As far as I can tell, for two reasons: protected openIndexWriter method which lets you open your own IW (e.g. with a custom IndexWriterConfig). protected closeIndexWriter which is a hook for letting you close the IW you opened in the previous one. The fixes are trivial IMO: Modify the method to return IW, and have the calling code set DTW's indexWriter member Eliminate closeIW. DTW already has a protected closeResources() which lets you clean other resources you've allocated, so I think that's enough. I'll post a patch shortly.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3557</id>
      <title>Spellchecker should take IndexWriterConfig... deprecate old methods?</title>
      <description>When looking at LUCENE-3490, i realized there was no way to specify the codec for the spellchecker to use. It has the following current methods: indexDictionary(Dictionary dict): this causes optimize! indexDictionary(Dictionary dict, int mergeFactory, int ramMB): this causes optimize! indexDictionary(Dictionary dict, int mergeFactor, int ramMB, boolean optimize) But no way to specify an IndexwriterConfig. Additionally, I don't like that several of these ctors force an optimize in a tricky way, even though it was like this all along. So I think we should add indexDictionary(Dictionary dict, IndexWriterConfig config, boolean optimize). We should either deprecate all the other ctors in 3.x and nuke in trunk, or at least add warnings to the ones that optimize.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3558</id>
      <title>SearcherManager and NRTManager should be in the same package</title>
      <description>I didnt even know NRTManager was still around, because its in the .index package, whereas SearcherManager is in the .search package. Separately, I don't like that this stuff is so 'hard' with core lucene... would it be so bad if this stuff was added to core? I suspect a lot of people have issues with this stuff (see http://www.lucidimagination.com/search/document/37964e5f0e5d733b) for example. Worst case is just that, combine mistakes with trying to manage this stuff with MMap unmapping and total lack of error detection for searching closed readers (LUCENE-3439) and its a mess.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3559</id>
      <title>remove IndexSearcher.docFreq/maxDoc</title>
      <description>As pointed out by Mark on SOLR-1632, these are no longer used by the scoring system. We've added new stats to Lucene, so having these methods on indexsearcher makes no sense. Its confusing to people upgrading if they subclassed IndexSearcher to provide distributed stats, only to find these are not used (LUCENE-3555 has a correct API for them to do this). So I think we should remove these in 4.0.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3560</id>
      <title>add extra safety to concrete codec implementations</title>
      <description>In LUCENE-3490, we reorganized the codec model, and a key part of this is that Codecs are "safer" and don't rely upon client-side configuration: IndexReader doesn't take Codec or anything of that nature, only IndexWriter. Instead for "read" all codecs are initialized from the classpath via a no-arg ctor from Java's Service Provider Mechanism. So, although Codecs can still take parameters in the constructors, be subclassable, etc (for passing to IndexWriter), this enforces that they must write any configuration information they need into the index, so that we don't have a flimsy API. I think we should go even further, for additional safety. Any methods on our concrete codecs that are not intended to be subclassed should be final, and we should add assertions to verify this. For example, SimpleText's files() implementation should be final. If you want to make an extension of simpletext that has additional files, then this is a different index format and should have a different name! Note: This doesn't stop extensibility, only stupid mistakes. For example, this means that Lucene40Codec's postingsFormat() implementation is final, even though it offers a configurable "hook" (getPostingsFormatForField) for you to specify per-field postings formats (which it writes into a .per file into the index, so that it knows how to read each field). private final PostingsFormat postingsFormat = new PerFieldPostingsFormat() { @Override public PostingsFormat getPostingsFormatForField(String field) { return Lucene40Codec.this.getPostingsFormatForField(field); } }; ... @Override public final PostingsFormat postingsFormat() { return postingsFormat; } ... /** Returns the postings format that should be used for writing * new segments of &lt;code&gt;field&lt;/code&gt;. * * The default implementation always returns "Lucene40" */ public PostingsFormat getPostingsFormatForField(String field) { return defaultFormat; }</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3562</id>
      <title>Stop storing TermsEnum in CloseableThreadLocal inside Terms instance</title>
      <description>We have sugar methods in Terms.java (docFreq, totalTermFreq, docs, docsAndPositions) that use a saved thread-private TermsEnum to do the lookups. But on apps that send many threads through Lucene, and/or have many segments, this can add up to a lot of RAM, especially if the codecs impl holds onto stuff. Also, Terms has a close method (closes the CloseableThreadLocal) which must be called, but we fail to do so in some places. These saved enums are the cause of the recent OOME in TestNRTManager (TestNRTManager.testNRTManager -seed 2aa27e1aec20c4a2:-4a5a5ecf46837d0e:-7c4f651f1f0b75d7 -mult 3 -nightly). Really sharing these enums is a holdover from before Lucene queries would share state (ie, save the TermState from the first pass, and use it later to pull enums, get docFreq, etc.). It's not helpful anymore, and it can use gobbs of RAM, so I'd like to remove it.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3564</id>
      <title>rename IndexWriter.rollback to .rollbackAndClose</title>
      <description>Spinoff from LUCENE-3454, where Shai noticed that rollback is trappy since it [unexpected] closes the IW. I think we should rename it to rollbackAndClose.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3566</id>
      <title>Parametrizing H1 and H2</title>
      <description>The DFR normalizations H1 and H2 are parameter-free. This is in line with the original article, but not with the thesis, where H2 accepts a c parameter, nor with information-based models, where H1 also accepts a c parameter.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3569</id>
      <title>Consolidate IndexWriter's optimize, maybeMerge and expungeDeletes under one merge(MP) method</title>
      <description>Today, IndexWriter exposes 3 methods for 'cleaning up' / 'compacting' / 'optimizing' your index: optimize() – merges as much segments as possible (down to 1 segment), and is discouraged in many cases because of its performance implications. maybeMerge() – runs 'subtle' merges. Attempts to balance the index by not leaving too many segments, yet not merging large segments if unneeded. expungeDeletes() – cleans up deleted documents from segments and on the go merges them. a default MP that can be set on IndexWriterConfig, for ongoing merges IW performs (i.e. as a result of flushing a new segment). These methods are confusing in several levels: Their names are misleading, see LUCENE-3454. Why does expungeDeletes need to merge segments? Eventually, they really do what the MergePolicy decides that should be done. I.e., one could write an MP that always merges all segments, and therefore calling maybeMerge would not be so subtle anymore. On the other hand, one could write an MP that never merges large segments (we in fact have several of those), and therefore calling optimize(1) would not end up with one segment. So the proposal is to replace all these methods with a single one merge(MergePolicy) (more on the names later). MergePolicy will have only one method findSegmentsForMerge and the caller will be responsible to configure it in order to perform the needed merges. We will provide ready-to-use MPs: LightMergePolicy – for setting on IWC and doing the ongoing merges IW executes. This one will pick segments respecting various parameters such as mergeFactor, segmentSizes etc. HeavyMergePolicy – for doing the optimize()-style merges. ExpungeDeletesMergePolicy – for expunging deletes (my proposal is to drop segment merging from it, by default). Now about the names: I think that it will be good, API-backcompat wise and in general, if we name that method doMaintenance (as expungeDeletes does not have to merge anything). Instead of MergePolicy we call it MaintenancePolicy and similarly its single method findSegmentsForMaintenance, or getMaintenanceSpecification. I called the MPs Light and Heavy just for the text, I think a better name should be found, but nothing comes up to mind now. It will allow us to use this on 3.x, by deprecating MP and all related methods.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3574</id>
      <title>Add some more constants for newer Java versions to Constants.class, remove outdated ones.</title>
      <description>Preparation for LUCENE-3235: This adds constants to quickly detect Java6 and Java7 to Constants.java. It also deprecated and removes the outdated historical Java versions.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3579</id>
      <title>DirectoryTaxonomyWriter should throw a proper exception if it was closed</title>
      <description>DirTaxoWriter may throw random exceptions (NPE, Already Closed - depend on what API you call) after it has been closed/rollback. We should detect up front that it is already closed, and throw AlreadyClosedException. Also, on LUCENE-3573 Doron pointed out a problem with DTW.rollback – it should call close() rather than refreshReader. I will fix that as well in this issue.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3585</id>
      <title>Integrate the JdbcDirectory-Code from COMPASS-Framework</title>
      <description>The CompassJava Search Engine Framework 2.3.0 beta 1 built on top of Lucene 2.4.1 is not maintained any more. But the JdbcDirectory-Code from COMPASS-Framework is under the Apache License, Version 2.0 I use this code wiht recent Lucene Version 3.4 and I suggest to integrate it in Lucene-Core, as an alternative Index-Store Lucene Jdbc Directory org.apache.lucene.store.jdbc.JdbcDirectory http://static.compassframework.org/docs/latest/jdbcdirectory.html It works only partialy with Lucene 3.4, but can be fiexed</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3586</id>
      <title>Choose a specific Directory implementation running the CheckIndex main</title>
      <description>It should be possible to choose a specific Directory implementation to use during the CheckIndex process when we run it from its main. What about an additional main parameter? In fact, I'm experiencing some problems with MMapDirectory working with a big segment, and after some failed attempts playing with maxChunkSize, I decided to switch to another FSDirectory implementation but I needed to do that on my own main. Should we also consider to use a FileSwitchDirectory? I'm willing to contribute, could you please let me know your thoughts about it?</description>
      <attachments/>
      <comments>23</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3588</id>
      <title>Try harder to prevent SIGSEGV on cloned MMapIndexInputs</title>
      <description>We are unmapping mmapped byte buffers which is disallowed by the JDK, because it has the risk of SIGSEGV when you access the mapped byte buffer after unmapping. We currently prevent this for the main IndexInput by setting its buffer to null, so we NPE if somebody tries to access the underlying buffer. I recently fixed also the stupid curBuf (LUCENE-3200) by setting to null. The big problem are cloned IndexInputs which are generally not closed. Those still contain references to the unmapped ByteBuffer, which lead to SIGSEGV easily. The patch from Mike in LUCENE-3439 prevents most of this in Lucene 3.5, but its still not 100% safe (as it uses non-volatiles). This patch will fix the remaining issues by also setting the buffers of clones to null when the original is closed. The trick is to record weak references of all clones created and close them together with the original. This uses a ConcurrentHashMap&lt;WeakReference&lt;MMapIndexInput&gt;,?&gt; as store with the logic borrowed from WeakHashMap to cleanup the GCed references (using ReferenceQueue). If we respin 3.5, we should maybe also get this in.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3593</id>
      <title>Add a filter returning all document without a value in a field</title>
      <description>In some situations it would be useful to have a Filter that simply returns all document that either have at least one or no value in a certain field. We don't have something like that out of the box and adding it seems straight forward.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3594</id>
      <title>Backport FieldCacheTermsFilter code duplication removal to 3.x</title>
      <description>In trunk I already cleaned up FieldCacheTermsFilter to not duplicate code of FieldCacheRangeFilter. This issue simply backports this.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3596</id>
      <title>DirectoryTaxonomyWriter extensions should be able to set internal index writer config attributes such as info stream</title>
      <description>Current protected openIndexWriter(Directory directory, OpenMode openMode) does not provide access to the IWC it creates. So extensions must reimplement this method completely in order to set e.f. info stream for the internal index writer. This came up in user question: Taxonomy indexer debug</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3598</id>
      <title>Improve InfoStream class in trunk to be more consistent with logging-frameworks like slf4j/log4j/commons-logging</title>
      <description>Followup on a thread by Shai Erea on java-dev@lao: I already discussed with Robert about that, that there is one thing missing. Currently the IW only checks if the infoStream!=null and then passes the message to the method, and that may ignore it. For your requirement it is the case that this is enabled or disabled dynamically. Unfortunately if the construction of the message is heavy, then this wastes resources. I would like to add another method to this class: abstract boolean isEnabled() that can also be implemented. I would then replace all null checks in IW by this method. The default config in IW would be changed to use a NoOutputInfoStream that returns false here and ignores the message. A simple logger wrapper for e.g. log4j / slf4j then could look like (ignoring component, could be enabled): Loger log = YourLoggingFramework.getLogger(IndexWriter.class); public void message(String component, String message) { log.debug(component + ": " + message); } public boolean isEnabled(String component) { return log.isDebugEnabled(); } Using this you could enable/disable logging live by e.g. the log4j management console of your app server by enabling/disabling IndexWriter.class logging. The changes are really simple: PrintStreamInfoStream returns true, always, mabye make it dynamically enable/disable to allow Shai's request infoStream.getDefault() is never null and can never be set to null. Instead the default is a singleton NoOutputInfoStream that returns false of isEnabled(component). All null checks on infoStream should be replaced by infoStream.isEanbled(component), this is possible as always != null. There are no slowdowns by this - it's like Collections.emptyList() instead stupid null checks.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3602</id>
      <title>Add join query to Lucene</title>
      <description>Solr has (psuedo) join query for a while now. I think this should also be available in Lucene.</description>
      <attachments/>
      <comments>43</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3603</id>
      <title>jar-src fails if ${build.dir} does not exist</title>
      <description>Simple fix – make jar-src depend on a target which creates the build.dir. Also, I noticed that build.dir is set in multiple places across our build.xmls, so I'd like to improve that a bit (minor fixes as well).</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3605</id>
      <title>revisit segments.gen sleeping</title>
      <description>in LUCENE-3601, i worked up a change where we intentionally crash() all un-fsynced files in tests to ensure that we are calling sync on files when we should. I think this would be nice to do always (and with some fixes all tests pass). But this is super-slow sometimes because when we corrupt the unsynced segments.gen, it causes SIS.read to take 500ms each time (and in checkindex for some reason we do this twice, which seems wrong). I can workaround this for now for tests (just do a partial crash that avoids corrupting the segments.gen), but I wanted to create this issue for discussion about the sleeping/non-fsyncing of segments.gen, just because i guess its possible someone could hit this slowness.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3610</id>
      <title>Revamp spatial APIs that use primitives (or arrays of primitives) in their args/results so that they use strongly typed objects</title>
      <description>My "spatial awareness" is pretty meek, but LUCENE-3599 seems like a prime example of the types of mistakes that are probably really easy to make with all of the Spatial related APIs that deal with arrays (or sequences) of doubles where specific indexes of those arrays (or sequences) have significant meaning: mainly latitude vs longitude. We should probably reconsider any method that takes in double[] or multiple doubles to express latlon pairs and rewrite them to use the existing LatLng class – or if people think that class is too heavyweight, then add a new lightweight class to handle the strong typing of a basic latlon point instead of just passing around a double[2] or two doubles called "x" and "y" ... public static final class SimpleLatLonPointInRadians { public double latitude; public double longitude; } ...then all those various methods that expect lat+lon pairs in radians (like DistanceUtils.haversine, DistanceUtils.normLat, DistanceUtils.normLng, DistanceUtils.pointOnBearing, DistanceUtils.latLonCorner, etc...) can start having APIs that don't make your eyes bleed when you start trying to understand what order the args go in.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3614</id>
      <title>Add a JUL/SLF4J example InfoStream implementation so IndexWriter can log to JUL/SLF4J</title>
      <description>Followup to LUCENE-3598: Hoss suggested to add a default JUL/SLF4J implementation to contrib/misc (that can also be used by SOLR to log IndexWriter verbose messages to its logging framework).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3615</id>
      <title>Make it easier to run Test2BTerms</title>
      <description>Currently, Test2BTerms has an @Ignore annotation which means that the only way to run it as a test is to edit the file. There are a couple of options to fix this: Add a main() so it can be invoked via the command line outside of the test framework Add some new annotations that mark it as slow or weekly or something like that and have the test target ignore @slow (or whatever) by default, but can also turn it on.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3618</id>
      <title>FST suggester should allow saving to Directory (not just File)</title>
      <description>Currently FSTCompletionLookup has a store method, taking "File storeDir", which it treats as a directory and then saves the FST to file "fst.bin" inside there. I think we should also add a store method taking a Lucene Directory? Eg then I can store my suggest FST in a RAMDir.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3621</id>
      <title>switch appendingcodec to use appending blocktree</title>
      <description>currently it still uses block terms + fixed gap index</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3626</id>
      <title>Make PKIndexSplitter and MultiPassIndexSplitter work per segment</title>
      <description>Spinoff from LUCENE-3624: DocValuesw merger throws exception on IW.addIndexes(SlowMultiReaderWrapper) as string-index like docvalues cannot provide asSortedSource.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3628</id>
      <title>Cut Norms over to DocValues</title>
      <description>since IR is now fully R/O and norms are inside codecs we can cut over to use a IDV impl for writing norms. LUCENE-3606 has some ideas about how this could be implemented</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3629</id>
      <title>Hide storage details for IndexDocValues inside the codec</title>
      <description>Currently ValueType exposes a lot of impl. details about how values are stored. However, since those are really impl details we should move those into the codec and decide during indexing which storage variant we are using. (robert convinced me this is the right thing and we should explore it) We can basically reduce the ValuesType to { Bytes, SortedBytes, FixedInts, Floats, VarInts } . The implementation ie. the codec can decide based on how many unique values and if values have all the same size what storage variant it should use. For merging we would need some statistics exposed on the Source ie. how many unique values and if all value have a fixed size to decide what the target "type" is going to be. This change would make usage of the API a lot easier and less confusing and at the same time it makes merging and type promotion straight forward since we can decide what type we promote to without loading the IDV provider to get the actual size and compare it.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3632</id>
      <title>Fully support doOpenIfChanged(boolean readOnly)/clone(boolean readOnly) in MultiReader and ParallelReader</title>
      <description>Followup from LUCENE-3630: doOpenIfChanged is behaving incorrectly if you pass a boolean to openIfChanged/clone. A partial fix is in LUCENE-3630, but it's not complete. This issue fully supports doOpenIfChanged/clone by conditionally passing the boolean down to the subreaders.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3633</id>
      <title>Remove code duplication in MultiReader/DirectoryReader, make everything inside final</title>
      <description>After making IndexReader readOnly (LUCENE-3606) there is no need to have completely different DirectoryReader and MultiReader, the current code is heavy code duplication and violations against finalness patterns. There are only few differences in reopen and things like isCurrent/getDirectory/... This issue will clean this up by introducing a hidden package-private base class for both and only handling reopen and incRef/decRef different. DirectoryReader is now final and all fields in BaseMultiReader, MultiReader and DirectoryReader are final now. DirectoryReader has now only static factories, no public ctor anymore.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3634</id>
      <title>remove old static main methods in core</title>
      <description>We have a few random static main methods that I think are very rarely used... we should remove them (IndexReader, UTF32ToUTF8, English). The IndexReader main lets you list / extract the sub-files from a CFS... I think we should move this to a new tool in contrib/misc.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3635</id>
      <title>Allow setting arbitrary objects on PerfRunData</title>
      <description>PerfRunData is used as the intermediary objects between PerfRunTasks. Just like we can set IndexReader/Writer on it, it will be good if it allows setting other arbitrary objects that are e.g. created by one task and used by another. A recent example is the enhancement to the benchmark package following the addition of the facet module. We had to add TaxoReader/Writer. The proposal is to add a HashMap&lt;String, Object&gt; that custom PerfTasks can set()/get(). I do not propose to move IR/IW/TR/TW etc. into that map. If however people think that we should, I can do that as well.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3637</id>
      <title>Make IndexReader.decRef() call refCount.decrementAndGet instead of getAndDecrement</title>
      <description>IndexReader.decRef() has this code: final int rc = refCount.getAndDecrement(); if (rc == 1) { I think it will be clearer if it was written like this: final int rc = refCount.decrementAndGet(); if (rc == 0) { It's a very simple change, which makes reading the code (at least IMO) easier. Will post a patch shortly.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3638</id>
      <title>IndexReader.document always return a doc with all the stored fields loaded. And this can be slow for the indexed document contain huge fields</title>
      <description>when generating digest for some documents with huge fields, it should be unnecessary to load the field but just interesting part of the field with the offset information. but indexreader always return the whole field content. afterward, the customized storedfieldsreader will got a repeated loading</description>
      <attachments/>
      <comments>19</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3639</id>
      <title>Add test case support for shard searching</title>
      <description>New test case that helps stress test the APIs to support sharding....</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3640</id>
      <title>remove IndexSearcher.close</title>
      <description>Now that IS is never "heavy" (since you have to pass in your own IR), IS.close is truly a no-op... I think we should remove it.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3643</id>
      <title>Improve FilteredQuery to shortcut on wrapped MatchAllDocsQuery</title>
      <description>Since the rewrite of Lucene trunk to delegate all Filter logic to FilteredQuery, by simply wrapping in IndexSearcher.wrapFilter(), we can do more short circuits and improve query execution. A common use case it to pass MatchAllDocsQuery as query to IndexSearcher and a filter. For the underlying hit collection this is stupid and slow, as MatchAllDocsQuery simply increments the docID and checks acceptDocs. If the filter is sparse, this is a big waste. This patch changes FilteredQuery.rewrite() to short circuit and return ConstantScoreQuery, if the query is MatchAllDocs.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3645</id>
      <title>Remove unnecessary array wrapping when calling varargs methods</title>
      <description>varargs method callers don't have to wrap args in arrays</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3648</id>
      <title>Speed up SegementDocsEnum by making it more friendly for JIT optimizations</title>
      <description>Since we moved the bulk reading into the codec ie. make all bulk reading codec private in LUCENE-3584 we have seen some performance regression on different CPUs. I tried to optimize the implementation to make it more eligible for runtime optimizations, tried to make loops JIT friendly by moving out branches where I can, minimize member access in all loops, use final members where possible and specialize the two common cases With &amp; Without LiveDocs. I will attache a patch and my benchmark results in a minute.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3653</id>
      <title>Lucene Search not scalling</title>
      <description>I've noticed that when doing thousands of searches in a single thread the average time is quite low i.e. a few milliseconds. When adding more concurrent searches doing exactly the same search the average time increases drastically. I've profiled the search classes and found that the whole of lucene blocks on org.apache.lucene.index.SegmentCoreReaders.getTermsReader org.apache.lucene.util.VirtualMethod public synchronized int getImplementationDistance org.apache.lucene.util.AttributeSourcew.getAttributeInterfaces These cause search times to increase from a few milliseconds to up to 2 seconds when doing 500 concurrent searches on the same in memory index. Note: That the index is not being updates at all, so not refresh methods are called at any stage. Some questions: Why do we need synchronization here? There must be a non-lockable solution for these, they basically cause lucene to be ok for single thread applications but disastrous for any concurrent implementation. I'll do some experiments by removing the synchronization from the methods of these classes.</description>
      <attachments/>
      <comments>55</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3654</id>
      <title>Optimize BytesRef comparator to use Unsafe long based comparison (when possible)</title>
      <description>Inspire by Google Guava UnsignedBytes lexi comparator, that uses unsafe to do long based comparisons over the bytes instead of one by one (which yields 2-4x better perf), use similar logic in BytesRef comparator. The code was adapted to support offset/length.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3662</id>
      <title>extend LevenshteinAutomata to support transpositions as primitive edits</title>
      <description>This would be a nice improvement for spell correction: currently a transposition counts as 2 edits, which means users of DirectSpellChecker must use larger values of n (e.g. 2 instead of 1) and larger priority queue sizes, plus some sort of re-ranking with another distance measure for good results. Instead if we can integrate "chapter 7" of http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652 then you can just build an alternative DFA where a transposition is only a single edit (http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance) According to the benchmarks in the original paper, the performance for LevT looks to be very similar to Lev. Support for this is now in moman (https://bitbucket.org/jpbarrette/moman/) thanks to Jean-Philippe Barrette-LaPierre.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3663</id>
      <title>Add a phone number normalization TokenFilter</title>
      <description>Phone numbers can be found in the wild in an infinity variety of formats (e.g. with spaces, parenthesis, dashes, with or without country code, with letters in substitution of numbers). So some Lucene applications can benefit of phone normalization with a TokenFilter that gets a phone number in any format, and outputs it in a standard format, using a default country to guess country code if it's not present.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3664</id>
      <title>Speed up SegementDocsAndPositionsEnum by making it more friendly for JIT optimizations</title>
      <description>LUCENE-3648 applied some refactoring to make SegmentDocsEnum reuse some code and divorce the liveDocs and no-liveDocs case into sep classes to make more friendly for jit optimizations. I did the same thing for SegmentDocsAndPositions[AndPayloads]Enum removing a couple of hundred lines of code abstracting it into a base class. patch follows in a sec...</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3665</id>
      <title>Make WeightedSpanTermExtractor extensible to handle custom query implemenations</title>
      <description>Currently if I have a custom query which subclasses query directly I can't use the QueryScorer for highlighting since it does explicit instanceof checks. In some cases its is possible to rewrite the query before passing it to the highlighter to obtain a primitive query. However I had the usecase where this was not possible ie. the original index was not available on the machine which highlights the results. To still use the highlighter I had to copy a bunch of code due to visibility issues in those classes. I think we can make this extensible with minor effort to allow this usecase without massive code duplication.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3666</id>
      <title>Update org.apache.lucene.analysis package summary</title>
      <description>package.html in lucene/src/java/org/apache/lucene/analysis/ is out of date. It looks like the contents of the branch_3x version haven't changed substantially since the Lucene 2.9 release, e.g. it refers to TermAttribute instead of CharTermAttribute. The trunk version is more modern - it refers to CharTermAttribute - but it also has some issues. E.g., I can see that the LengthFilter discussion doesn't refer to FilteringTokenFilter.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3667</id>
      <title>Consider changing how we set the number of threads to use to run tests.</title>
      <description>The current way we set the number of threads to use is not expressive enough for some systems. My quad core with hyper threading is recognized as 8 CPUs - since I can only override the number of threads to use per core, 8 is as low as I can go. 8 threads can be problematic for me - just the amount of RAM used sometimes can toss me into heavy paging because I only have 8 GB of RAM - the heavy paging can cause my whole system to come to a crawl. Without hacking the build, I don't think I have a lot of workarounds. I'd like to propose that switch from using threadsPerProcessor to threadCount. In some ways, it's not as nice, because it does not try to scale automatically per system. But that auto scaling is often not ideal (hyper threading, wanting to be able to do other work at the same time), so perhaps we just default to 1 or 2 threads and devs can override individually?</description>
      <attachments/>
      <comments>17</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3671</id>
      <title>Add a TypeTokenFilter</title>
      <description>It would be convenient to have a TypeTokenFilter that filters tokens by its type, either with an exclude or include list. This might be a stupid thing to provide for people who use Lucene directly, but it would be very useful to later expose it to Solr and other Lucene-backed search solutions.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3676</id>
      <title>Support SortedSource in MultiDocValues</title>
      <description>MultiDocValues doesn't support Sorted variant ie. SortedSource but throws UnsupportedOperationException. This forces users to work per segment. For consistency we should support sortedsource also if we wrap the DocValues in MDV.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3681</id>
      <title>FST.BYTE2 should save as fixed 2 byte not as vInt</title>
      <description>We currently write BYTE1 as a single byte, but BYTE2/4 as vInt, but I think that's confusing. Also, for the FST for the new Kuromoji analyzer (LUCENE-3305), writing as 2 bytes instead shrank the FST and ran faster, presumably because more values were &gt;= 16384 than were &lt; 128. Separately the whole INPUT_TYPE is very confusing... really all it's doing is "declaring" the allowed range of the characters of the input alphabet, and then the only thing that uses that is the write/readLabel methods (well and some confusing sugar methods in Builder!). Not sure how to fix that yet... It's a simple change but it changes the FST binary format so any users w/ FSTs out there will have to rebuild (FST is marked experimental...).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3684</id>
      <title>Add offsets to postings (D&amp;PEnum)</title>
      <description>I think should explore making start/end offsets a first-class attr in the postings APIs, and fixing the indexer to index them into postings. This will make term vector access cleaner (we now have to jump through hoops w/ non-first-class offset attr). It can also enable efficient highlighting without term vectors / reanalyzing, if the app indexes offsets into the postings.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3685</id>
      <title>Add top-down version of BlockJoinQuery</title>
      <description>Today, BlockJoinQuery can join from child docIDs up to parent docIDs. EG this works well for product (parent) + many SKUs (child) search. But the reverse, which BJQ cannot do, is also useful in some cases. EG say you index songs (child) within albums (parent), but you want to search and present by song not album while involving some fields from the album in the query. In this case you want to wrap a parent query (against album), joining down to the child document space.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3687</id>
      <title>Allow similarity to encode norms other than a single byte</title>
      <description>LUCENE-3628 cut over norms to docvalues. This removes the long standing limitation that norms are a single byte. Yet, we still need to expose this functionality to Similarity to write / encode norms in a different format.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3688</id>
      <title>Bucketing of association value of a certain category</title>
      <description>Add facet requests which collect associations of a certain category into buckets, and returns each bucket as a facet result node. Association type is either int or float, and there are two methods to define buckets. The first by providing buckets which contain pre-defined ranges. The other is by declaring the required number of buckets, where the ranges of the different buckets are dynamicly set to create balanced bucket sizes.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3689</id>
      <title>DocValuesConsumer should implement closeable and release resource in close instead of during finish / flush</title>
      <description>DocValuesConsumer currently doesn't have a close method and releases its resources during finish / flush. This is confusing, makes debugging more complex and mixes concerns. DocValuesConsumer should impl. Closeable and users should release resources safely.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3690</id>
      <title>JFlex-based HTMLStripCharFilter replacement</title>
      <description>A JFlex-based HTMLStripCharFilter replacement would be more performant and easier to understand and maintain.</description>
      <attachments/>
      <comments>40</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3699</id>
      <title>kuromoji dictionary could be more compact</title>
      <description>Reading thru the ipadic documentation, i realized we are storing a lot of redundant information, for example the connection costs for bigram weights are based on POS+inflection data, so its redundant to also separately encode POS and inflection data for each entry. With the patch the dictionary access is also faster and simpler, and TokenInfoDictionary is 1.5MB smaller.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3700</id>
      <title>optionally support naist-jdic for kuromoji</title>
      <description>This is an alternative dictionary, somewhat larger (~25%). we can support it in build.xml so if a user wants to build with it, they can (the resulting jar file will be 500KB larger)</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3701</id>
      <title>FST apis out of sync between trunk/3.x</title>
      <description>Looks like the offender is LUCENE-3030 Not sure if everything is generally useful but it does change the public API (e.g. you can specify FreezeTail to the super-scary Builder ctor among other things). Maybe we should sync up for 3.x?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3704</id>
      <title>No hook provided in IndexWriter.java before merge after setup, for auxiliary indexes</title>
      <description>When trying to manage an auxiliary index to lucene, IndexWriter.java 3.5.0 does not provide a proper hook. Such a hook (see attached proposed patch) permits us to manage an outside index where we get 1 of our segments in parallel to the lucene segments, and we can keep the same docid offsets within the segment. We are attaching a suggested patch that provides an override-able method. We would like this accepted for 3.5+ if possible. Thanks.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3706</id>
      <title>add offsets into lucene40 postings</title>
      <description>LUCENE-3684 added support for IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS, but only SimpleText implements it. I think we should implement it in the other 4.0 codecs (starting with Lucene40PostingsFormat).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3714</id>
      <title>add suggester that uses shortest path/wFST instead of buckets</title>
      <description>Currently the FST suggester (really an FSA) quantizes weights into buckets (e.g. single byte) and puts them in front of the word. This makes it fast, but you lose granularity in your suggestions. Lately the question was raised, if you build lucene's FST with positiveintoutputs, does it behave the same as a tropical semiring wFST? In other words, after completing the word, we instead traverse min(output) at each node to find the 'shortest path' to the best suggestion (with the highest score). This means we wouldnt need to quantize weights at all and it might make some operations (e.g. adding fuzzy matching etc) a lot easier.</description>
      <attachments/>
      <comments>35</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3722</id>
      <title>make similarities/term/collectionstats take long (for &gt; 2B docs)</title>
      <description>As noted by Yonik and Andrzej on SOLR-1632, this would be useful for distributed scoring. we can also add a sugar method add() to both of these to make it easier to sum.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3725</id>
      <title>Add optional packing to FST building</title>
      <description>The FSTs produced by Builder can be further shrunk if you are willing to spend highish transient RAM to do so... our Builder today tries hard not to use much RAM (and has options to tweak down the RAM usage, in exchange for somewhat lager FST), even when building immense FSTs. But for apps that can afford highish transient RAM to get a smaller net FST, I think we should offer packing.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3726</id>
      <title>Default KuromojiAnalyzer to use search mode</title>
      <description>Kuromoji supports an option to segment text in a way more suitable for search, by preventing long compound nouns as indexing terms. In general 'how you segment' can be important depending on the application (see http://nlp.stanford.edu/pubs/acl-wmt08-cws.pdf for some studies on this in chinese) The current algorithm punishes the cost based on some parameters (SEARCH_MODE_PENALTY, SEARCH_MODE_LENGTH, etc) for long runs of kanji. Some questions (these can be separate future issues if any useful ideas come out): should these parameters continue to be static-final, or configurable? should POS also play a role in the algorithm (can/should we refine exactly what we decompound)? is the Tokenizer the best place to do this, or should we do it in a tokenfilter? or both? with a tokenfilter, one idea would be to also preserve the original indexing term, overlapping it: e.g. ABCD -&gt; AB, CD, ABCD(posInc=0) from my understanding this tends to help with noun compounds in other languages, because IDF of the original term boosts 'exact' compound matches. but does a tokenfilter provide the segmenter enough 'context' to do this properly? Either way, I think as a start we should turn on what we have by default: its likely a very easy win.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3728</id>
      <title>better handling of files inside/outside CFS by codec</title>
      <description>Since norms and deletes were moved under Codec (LUCENE-3606, LUCENE-3661), we never really properly addressed the issue of how Codec.files() should work, considering these files are always stored outside of CFS. LUCENE-3606 added a hack, LUCENE-3661 cleaned up the hack a little bit more, but its still a hack. Currently the logic in SegmentInfo.files() is: clearCache() if (compoundFile) { // don't call Codec.files(), hardcoded CFS extensions, etc } else { Codec.files() } // always add files stored outside CFS regardless of CFS setting Codec.separateFiles() if (sharedDocStores) { // hardcoded shared doc store extensions, etc } Also various codec methods take a Directory parameter, but its inconsistent what this Directory is in the case of CFS: for some parts of the index its the CFS directory, for others (deletes, separate norms) its not. I wonder if instead we could restructure this so that SegmentInfo.files() logic is: clearCache() Codec.files() and so that Codec is instead responsible. instead Codec.files logic by default would do the if (compoundFile) thing, and Lucene3x codec itself would only have the if (sharedDocStores) thing, and any part of the codec that wants to put stuff always outside of CFS (e.g. Lucene3x separate norms, deletes) could just use SegmentInfo.dir. Directory parameters in the case of CFS would always consistently be the CFSDirectory. I haven't totally tested if this will work but there is definitely some cleanups we can do either way, and I think it would be a good step to try to clean this up and simplify it.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3730</id>
      <title>Improved Kuromoji search mode segmentation/decompounding</title>
      <description>Kuromoji has a segmentation mode for search that uses a heuristic to promote additional segmentation of long candidate tokens to get a decompounding effect. This heuristic has been improved. Patch is coming up.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3731</id>
      <title>Create a analysis/uima module for UIMA based tokenizers/analyzers</title>
      <description>As discussed in SOLR-3013 the UIMA Tokenizers/Analyzer should be refactored out in a separate module (modules/analysis/uima) as they can be used in plain Lucene. Then the solr/contrib/uima will contain only the related factories.</description>
      <attachments/>
      <comments>35</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3744</id>
      <title>Add support for type whitelist in TypeTokenFilter</title>
      <description>A usual use case for TypeTokenFilter is allowing only a set of token types. That is, listing allowed types, instead of filtered ones. I'm attaching a patch to add a useWhitelist option for that.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3747</id>
      <title>Support Unicode 6.1.0</title>
      <description>Now that Unicode 6.1.0 has been released, Lucene/Solr should support it. JFlex trunk now supports Unicode 6.1.0. Tasks include: Upgrade ICU4J to v49 (after it's released, on 2012-03-21, according to http://icu-project.org). Use icu module tools to regenerate the supplementary character additions to JFlex grammars. Version the JFlex grammars: copy the current implementations to *Impl3&lt;X&gt;; cause the versioning tokenizer wrappers to instantiate this version when the Version c-tor param is in the range 3.1 to the version in which these changes are released (excluding the range endpoints); then change the specified Unicode version in the non-versioned JFlex grammars from 6.0 to 6.1. Regenerate JFlex scanners, including StandardTokenizerImpl, UAX29URLEmailTokenizerImpl, and HTMLStripCharFilter. Using generateJavaUnicodeWordBreakTest.pl, generate and then run WordBreakTestUnicode_6_1_0.java under modules/analysis/common/src/test/org/apache/lucene/analysis/core/</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3748</id>
      <title>EnglishPossessiveFilter should work with Unicode right single quotation mark</title>
      <description>The current EnglishPossessiveFilter (used in EnglishAnalyzer) removes possessives using only the '\'' character (plus 's' or 'S'), but some common systems (German?) insert the Unicode "\u2019" (RIGHT SINGLE QUOTATION MARK) instead and this is not removed when processing UTF-8 text. I propose to change EnglishPossesiveFilter to support '\u2019' as an alternative to '\''.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3750</id>
      <title>Convert Versioned docs to Markdown/New CMS</title>
      <description>Since we are moving our main site to the ASF CMS (LUCENE-2748), we should bring in any new versioned Lucene docs into the same format so that we don't have to deal w/ Forrest anymore.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3751</id>
      <title>Align default Japanese configurations for Lucene and Solr</title>
      <description>The KuromojiAnalyzer in Lucene shoud have the same default configuration as the text_ja field type introduced in schema.xml by SOLR-3056.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3753</id>
      <title>Restructure the Lucene build system</title>
      <description>Split out separate core/, test-framework/, and tools/ modules, each with its own build.xml, under the lucene/ directory, similar to the Solr restructuring done in SOLR-2452.</description>
      <attachments/>
      <comments>27</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3754</id>
      <title>Store generated archive manifests in per-module output directories</title>
      <description>Currently, generated archive manifests are all stored in the same location, so each module's build overwrites the previously built module's manifest. Locating these files in the per-module build dirs will allow them to be rebuilt only when necessary, rather than every time a module's jar target is called.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3756</id>
      <title>Don't allow IndexWriterConfig setters to chain</title>
      <description>Spinoff from LUCENE-3736. I don't like that IndexWriterConfig's setters are chainable; it results in code in our tests like this: IndexWriter writer = new IndexWriter(dir, newIndexWriterConfig( TEST_VERSION_CURRENT, new MockAnalyzer(random)).setMaxBufferedDocs(2).setMergePolicy(newLogMergePolicy())); I think in general we should avoid chaining since it encourages hard to read code (code is already hard enough to read!).</description>
      <attachments/>
      <comments>13</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3757</id>
      <title>Change AtomicReaderContext.leaves() to return itsself as only leave to simplify code and remove an otherwise unneeded ReaderUtil method</title>
      <description>The documentation of IndexReaderContext.leaves() states that it returns (for convenience) all leave nodes, if the context is top-level (directly got from IndexReader), otherwise returns null. This is not correct for AtomicReaderContext, where it returns null always. To make it consistent, the convenience method should simply return itsself as only leave for atomic contexts. This makes the utility method ReaderUtil.leaves() obsolete and simplifies code.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3758</id>
      <title>Allow the ComplexPhraseQueryParser to search order or un-order proximity queries.</title>
      <description>The ComplexPhraseQueryParser use SpanNearQuery, but always set the "inOrder" value hardcoded to "true". This could be configurable.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3759</id>
      <title>Support joining in a distributed environment.</title>
      <description>Add two more methods in JoinUtil to support joining in a distributed manner. Method to retrieve all from values. Method to create a TermsQuery based on a set of from terms. With these two methods distributed joining can be supported following these steps: Retrieve from values from each shard Merge the retrieved from values. Create a TermsQuery based on the merged from terms and send this query to all shards.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>3760</id>
      <title>Cleanup DR.getCurrentVersion/DR.getUserData/DR.getIndexCommit().getUserData()</title>
      <description>Spinoff from Ryan's dev thread "DR.getCommitUserData() vs DR.getIndexCommit().getUserData()"... these methods are confusing/dups right now.</description>
      <attachments/>
      <comments>29</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3761</id>
      <title>Generalize SearcherManager</title>
      <description>I'd like to generalize SearcherManager to a class which can manage instances of a certain type of interfaces. The reason is that today SearcherManager knows how to handle IndexSearcher instances. I have a SearcherManager which manages a pair of IndexSearcher and TaxonomyReader pair. Recently, few concurrency bugs were fixed in SearcherManager, and I realized that I need to apply them to my version as well. Which led me to think why can't we have an SM version which is generic enough so that both my version and Lucene's can benefit from? The way I see SearcherManager, it can be divided into two parts: (1) the part that manages the logic of acquire/release/maybeReopen (i.e., ensureOpen, protect from concurrency stuff etc.), and (2) the part which handles IndexSearcher, or my SearcherTaxoPair. I'm thinking that if we'll have an interface with incRef/decRef/tryIncRef/maybeRefresh, we can make SearcherManager a generic class which handles this interface. I will post a patch with the initial idea, and we can continue from there.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3762</id>
      <title>Upgrade JUnit to 4.10, refactor state-machine of detecting setUp/tearDown call chaining.</title>
      <description>Both Lucene and Solr use JUnit 4.7. I suggest we move forward and upgrade to JUnit 4.10 which provides several infrastructural changes (serializable Description objects, class-level rules, various tweaks). JUnit 4.10 also changes (or fixes, depends how you look at it) the order in which @Before/@After hooks and @Rules are applied. This makes the old state-machine in LuceneTestCase fail (because the order is changed). I rewrote the state machine and used a different, I think simpler, although Uwe may disagree , mechanism in which the hook methods setUp/ tearDown are still there, but they are empty at the top level and serve only to detect whether subclasses chain super.setUp/tearDown properly (if they override anything). In the long term, I would love to just get rid of public setup/teardown methods and make them private (so that they cannot be overriden or even seen by subclasses) but this will require changes to the runner itself.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3766</id>
      <title>Remove/deprecate Tokenizer's default ctor</title>
      <description>I was working on a new Tokenizer... and I accidentally forgot to call super(input) (and super.reset(input) from my reset method)... which then meant my correctOffset() calls were silently a no-op; this is very trappy. Fortunately the awesome BaseTokenStreamTestCase caught this (I hit failures because the offsets were not in fact being corrected). One minimal thing we can do (but it sounds like from Robert there may be reasons why we can't) is add assert input != null in Tokenizer.correctOffset: Index: lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java =================================================================== --- lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java (revision 1242316) +++ lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java (working copy) @@ -82,6 +82,7 @@ * @see CharStream#correctOffset */ protected final int correctOffset(int currentOff) { + assert input != null: "subclass failed to call super(Reader) or super.reset(Reader)"; return (input instanceof CharStream) ? ((CharStream) input).correctOffset(currentOff) : currentOff; } But best would be to remove the default ctor that leaves input null...</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3767</id>
      <title>Explore streaming Viterbi search in Kuromoji</title>
      <description>I've been playing with the idea of changing the Kuromoji viterbi search to be 2 passes (intersect, backtrace) instead of 4 passes (break into sentences, intersect, score, backtrace)... this is very much a work in progress, so I'm just getting my current state up. It's got tons of nocommits, doesn't properly handle the user dict nor extended modes yet, etc. One thing I'm playing with is to add a double backtrace for the long compound tokens, ie, instead of penalizing these tokens so that shorter tokens are picked, leave the scores unchanged but on backtrace take that penalty and use it as a threshold for a 2nd best segmentation...</description>
      <attachments/>
      <comments>30</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3769</id>
      <title>Simplify NRTManager</title>
      <description>NRTManager is hairy now, because the applyDeletes is separately passed to ctor, passed to maybeReopen, passed to getSearcherManager, etc. I think, instead, you should pass it only to the ctor, and if you have some cases needing deletes and others not then you can make two NRTManagers. This should be no less efficient than we have today, just simpler. I think it will also enable NRTManager to subclass ThingyManager (LUCENE-3761).</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3772</id>
      <title>Highlighter needs the whole text in memory to work</title>
      <description>Highlighter methods getBestFragment(s) and getBestTextFragments only accept a String object representing the whole text to highlight. When dealing with very large docs simultaneously, it can lead to heap consumption problems. It would be better if the API could accept a Reader objetct additionally, like Lucene Document Fields do.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3773</id>
      <title>small improvements to DWPTThreadPool</title>
      <description>While working on another issue I cleaned up DWTPThreadPool a little, fixed some naming issues and fixed some todos... patch is coming soon...</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3774</id>
      <title>check-legal isn't doing its job</title>
      <description>In trunk, the check-legal-lucene ant target is not checking any lucene/contrib/**/lib/ directories; the modules/**/lib/ directories are not being checked; and check-legal-solr can't be checking solr/example/lib/**/*.jar, because there are currently .jar files in there that don't have a license. These targets are set up to take in a full list of lib/ directories in which to check, but modules move around, and these lists are not being kept up-to-date. Instead, check-legal-* should run for each module, if the module has a lib/ directory, and it should be specialized for modules that have more than one (solr/core/) or that have a lib/ directory in a non-standard place (lucene/core/).</description>
      <attachments/>
      <comments>64</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3775</id>
      <title>A shell script to generate .gitignore from svn:ignore properties</title>
      <description>Attached is a small shell script that generates .gitignore-new from all svn:ignore properties (on an svn checkout). I was able to successfully regenerate existing .gitignore. There are spurious entries (no recursive rules), but at least it's consistent with svn:ignore?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3776</id>
      <title>NRTManager shouldn't expose its private SearcherManager</title>
      <description>Spinoff from LUCENE-3769. To actually obtain an IndexSearcher from NRTManager, it's a 2-step process now. You must .getSearcherManager(), then .acquire() from the returned SearcherManager. This is very trappy... because if the app incorrectly calls maybeReopen on that private SearcherManager (instead of NRTManager.maybeReopen) then it can unexpectedly cause threads to block forever, waiting for the necessary gen to become visible. This will be hard to debug... I don't like creating trappy APIs. Hopefully once LUCENE-3761 is in, we can fix NRTManager to no longer expose its private SM, instead subclassing ReferenceManaager. Or alternatively, or in addition, maybe we factor out a new interface (SearcherProvider or something...) that only has acquire and release methods, and both NRTManager and ReferenceManager/SM impl that, and we keep NRTManager's SM private.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3778</id>
      <title>Create a grouping convenience class</title>
      <description>Currently the grouping module has many collector classes with a lot of different options per class. I think it would be a good idea to have a GroupUtil (Or another name?) convenience class. I think this could be a builder, because of the many options (sort,sortWithinGroup,groupOffset,groupCount and more) and implementations (term/dv/function) grouping has.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3786</id>
      <title>Create SearcherTaxoManager</title>
      <description>If an application wants to use an IndexSearcher and TaxonomyReader in a SearcherManager-like fashion, it cannot use a separate SearcherManager, and say a TaxonomyReaderManager, because the IndexSearcher and TaxoReader instances need to be in sync. That is, the IS-TR pair must match, or otherwise the category ordinals that are encoded in the search index might not match the ones in the taxonomy index. This can happen if someone reopens the IndexSearcher's IndexReader, but does not refresh the TaxonomyReader, and the category ordinals that exist in the reopened IndexReader are not yet visible to the TaxonomyReader instance. I'd like to create a SearcherTaxoManager (which is a ReferenceManager) which manages an IndexSearcher and TaxonomyReader pair. Then an application will call: SearcherTaxoPair pair = manager.acquire(); try { IndexSearcher searcher = pair.searcher; TaxonomyReader taxoReader = pair.taxoReader; // do something with them } finally { manager.release(pair); pair = null; }</description>
      <attachments/>
      <comments>15</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3788</id>
      <title>Separate getting Directory from IndexReader from its concrete subclasses</title>
      <description>Currently only subclasses of DirectoryReader expose the underlying Directory via public final directory(). IMHO this aspect should be separated from DirectoryReader so that other IndexReader implementations could expose any underlying Directory if they wanted to. Specifically, I have a use case where I'd like to expose a synthetic Directory view of resources used for ParallelCompositeReader.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3789</id>
      <title>Expose FilteredTermsEnum from MTQ</title>
      <description>MTQ#getEnum() is protected and in order to access it you need to be in the o.a.l.search package. here is a relevant snipped from the mailing list discussion getEnum() is protected so it is intended to be called *only* by subclasses (that's the idea behind protected methods). They are also accessible by other classes from the same package, but that's more a Java bug than a feature. The problem with MTQ is that RewriteMethod is a separate *class* and *not a subclass* of MTQ, so the method cannot be called (it can because of the "java bug" called from same package). So theoretically it has to be public otherwise you cannot call getEnum(). Another cleaner fix would be to add a protected final method to RewriteMethod that calls this method from MTQ. So anything subclassing RewriteMethod can get the enum from inside the RewriteMethod class which is the "correct" way to handle it. Delegating to MTQ is then "internal".</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3791</id>
      <title>ant eclipse should setup default project formatting.</title>
      <description>I admit it's selfish. I have a git workflow and I often do "git clean -xfd" which restores a pristine state of the current branch (faster than ant clean . Unfortunately this also results in removal of Eclipse files. "ant eclipse" doesn't reset formatting properly so I need to restore it manually. This patch does two things: it sets project formatting automatically on "ant eclipse", it removes explicit Lucene-formatting.xml to avoid duplication (and potential inconsistencies) between project-specific formatter rules contained in org.eclipse.jdt.core.prefs and the formatter's XML. The XML can be exported manually if needed.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3793</id>
      <title>Use ReferenceManager in DirectoryTaxonomyReader</title>
      <description>DirTaxoReader uses hairy code to protect its indexReader instance from being modified while threads use it. It maintains a ReentrantLock (indexReaderLock) which is obtained on every 'read' access, while refresh() locks it for 'write' operations (refreshing the IndexReader). Instead of all that, now that we have ReferenceManager in place, I think that we can write a ReaderManager&lt;IndexReader&gt; which will be used by DirTR. Every method that requires access to the indexReader will acquire/release (not too different than obtaining/releasing the read lock), and refresh() will call ReaderManager.maybeRefresh(). It will simplify the code and remove some rather long comments, that go into great length explaining why does the code looks like that. This ReaderManager cannot be used for every IndexReader, because DirTR's refresh() logic is special – it reopens the indexReader, and then verifies that the createTime still matches on the reopened reader as well. Otherwise, it closes the reopened reader and fails with an exception. Therefore, this ReaderManager.refreshIfNeeded will need to take the createTime into consideration and fail if they do not match. And while we're at it ... I wonder if we should have a manager for an IndexReader/ParentArray pair? I think that it makes sense because we don't want DirTR to use a ParentArray that does not match the IndexReader. Today this can happen in refresh() if e.g. after the indexReader instance has been replaced, parentArray.refresh(indexReader) fails. DirTR will be left with a newer IndexReader instance, but old (or worse, corrupt?) ParentArray ... I think it'll be good if we introduce clone() on ParentArray, or a new ctor which takes an int[]. I'll work on a patch once I finish with LUCENE-3786.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3795</id>
      <title>Replace spatial contrib module with LSP's spatial-lucene module</title>
      <description>I propose that Lucene's spatial contrib module be replaced with the spatial-lucene module within Lucene Spatial Playground (LSP). LSP has been in development for approximately 1 year by David Smiley, Ryan McKinley, and Chris Male and we feel it is ready. LSP is here: http://code.google.com/p/lucene-spatial-playground/ and the spatial-lucene module is intuitively in svn/trunk/spatial-lucene/. I'll add more comments to prevent the issue description from being too long.</description>
      <attachments/>
      <comments>76</comments>
      <commenters>15</commenters>
    </issue>
    <issue>
      <id>3797</id>
      <title>3xCodec should throw UOE if a DocValuesConsumer is pulled</title>
      <description>currently we just return null if a DVConsumer is pulled from 3.x which is trappy since it causes an NPE in DocFieldProcessor. We should rather throw a UOE.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3801</id>
      <title>Generify FST shortestPaths() to take a comparator</title>
      <description>Not sure we should do this, it costs 5-10% performance for WFSTSuggester. But maybe we can optimize something here, or maybe its just no big deal to us. Because in general, this could be pretty powerful, e.g. if you needed to store some custom stuff in the suggester, you could use pairoutputs, or whatever. And the possibility we might need shortestPaths for other cool things... at the least I just wanted to have the patch up here. I haven't tested this on pairoutputs... but i've tested it with e.g. FloatOutputs and other things and it works fine. I tried to minimize the generics violations, there is only 1 (cannot create generic array).</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3802</id>
      <title>Grouping collector that computes grouped facet counts</title>
      <description>Spinoff from issue SOLR-2898.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3804</id>
      <title>Swap Features and News on the website.</title>
      <description>I think we can do even better, but that is a nice, easy incremental improvement.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3805</id>
      <title>big download button</title>
      <description>Its hard to find where to download.... eventually you see the Download tab, and the actual link to go to get a download is very small. I think we should have a ridiculously large download button: successful projects like firefox,eclipse IDE, openoffice all have this. Hiding our releases can't help... I don't know the CMS, but if this stays open too long I will open up mspaint.exe and figure it out.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3807</id>
      <title>Cleanup suggester API</title>
      <description>Currently the suggester api and especially TermFreqIterator don't play that nice with BytesRef and other paradigms we use in lucene, further the java iterator pattern isn't that useful when it gets to work with TermsEnum, BytesRef etc. We should try to clean up this api step by step moving over to BytesRef including the Lookup class and its interface...</description>
      <attachments/>
      <comments>38</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3810</id>
      <title>Text Alignment in the "status-item" tab of the Website needs to be word wrapped</title>
      <description>If we go to http://lucene.apache.org/core/ or http://lucene.apache.org/solr/ under the Latest Dev section the text is exceeding the css boundary. This is happening on both Firefox and Chrome.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3819</id>
      <title>Clean up what we show in right side bar of website.</title>
      <description>I'd love to remove a couple things - it's pretty crowded on the right side bar. I find the latest JIRA and email displays are hard to read, tend to format badly, and don't offer much value. I'd like to remove them and just leave svn commits and twitter mentions (which are much easier to read and format better). Will help with some info overload on each page.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3823</id>
      <title>Add a field-filtering FilterAtomicReader to 4.0 so ParallelReaders can be better tested (in LTC.maybeWrapReader)</title>
      <description>In addition to the filters in contrib/misc for horizontally filtering (by doc-id) AtomicReader, it would be good to have the same vertically (by field). For now I will add this implementation to test-framework, as it cannot stay in contrib/misc, because LTC will need it for maybeWrapReader. LTC will use this FilterAtomicReader to construct a ParallelAtomicReader out of two (or maybe more) FieldFilterAtomicReaders.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3824</id>
      <title>TermOrdVal/DocValuesComparator does too much work in compareBottom</title>
      <description>We now have logic to fall back to by-value comparison, when the bottom slot is not from the current reader. But this is silly, because if the bottom slot is from a different reader, it means the tie-break case is not possible (since the current reader didn't have the bottom value), so when the incoming ord equals the bottom ord we should always return x &gt; 0. I added a new random string sort test case to TestSort... I also renamed DocValues.SortedSource.getByValue -&gt; getOrdByValue and cleaned up some whitespace.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3825</id>
      <title>Please push maven snapshots to repositories.apache.org again</title>
      <description>Once upon a time, snapshots of the lucene trunk went into the snapshot repo at repositories.apache.org. No longer. Instead, they just sit at: https://builds.apache.org//job/Lucene-Solr-Maven-trunk/lastSuccessfulBuild/artifact/maven_artifacts/ Unfortunately, Jenkins makes a rather mediocre maven repo. the maven-wagon-plugin can't copy it and Nexus can't proxy it.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3826</id>
      <title>Make term offsets available from Spans</title>
      <description>LUCENE-3684 made term offsets available through DocsAndPositionsEnum. It would be nice to have this available through Spans as well.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3827</id>
      <title>Make term offsets work in MemoryIndex</title>
      <description>Fix the logic for retrieving term offsets from DocsAndPositionsEnum on a MemoryIndex, and allow subclasses to access them.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3830</id>
      <title>MappingCharFilter could be improved by switching to an FST.</title>
      <description>MappingCharFilter stores an overly complex tree-like structure for matching input patterns. The input is a union of fixed strings mapped to a set of fixed strings; an fst matcher would be ideal here and provide both memory and speed improvement I bet.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3832</id>
      <title>Port BasicAutomata.stringUnion from Brics to Lucene</title>
      <description>Brics has my code to build Automaton from a set of sorted strings in one step (Daciuk/Mihov's algorithm again). This should be easily portable to Lucene and is quite useful.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3833</id>
      <title>Add an operator to query parser for term quorum (ie: BooleanQuery.setMinimumNumberShouldMatch)</title>
      <description>A project I'm working on requires term quorum searching with stemming turned off. The users are accostomed to Sphinx search, and thus expect a query like [ A AND (B C D)/2 ] to return only documents that contain A or at least two of B, C or D. So this document would match: a b c But this one wouldn't: a b This can be a useful form of fuzzy searching, and I think we support it via the MM parameter, but we lack a user-facing operator for this. It would be great to add it.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3836</id>
      <title>Most Codec.*Format().*Reader() methods should use SegmentReadState</title>
      <description>Codec formats API for opening readers is inconsistent - sometimes it uses SegmentReadState, in other cases it uses individual arguments that are already available via SegmentReadState. This complicates extending the API, e.g. if additional per-segment state would need to be passed to the readers.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3837</id>
      <title>A modest proposal for updateable fields</title>
      <description>I'd like to propose a simple design for implementing updateable fields in Lucene. This design has some limitations, so I'm not claiming it will be appropriate for every use case, and it's obvious it has some performance consequences, but at least it's a start... This proposal uses a concept of "overlays" or "stacked updates", where the original data is not removed but instead it's overlaid with the new data. I propose to reuse as much of the existing APIs as possible, and represent updates as an IndexReader. Updates to documents in a specific segment would be collected in an "overlay" index specific to that segment, i.e. there would be as many overlay indexes as there are segments in the primary index. A field update would be represented as a new document in the overlay index . The document would consist of just the updated fields, plus a field that records the id in the primary segment of the document affected by the update. These updates would be processed as usual via secondary IndexWriter-s, as many as there are primary segments, so the same analysis chains would be used, the same field types, etc. On opening a segment with updates the SegmentReader (see also LUCENE-3836) would check for the presence of the "overlay" index, and if so it would open it first (as an AtomicReader? or it would open individual codec format readers? perhaps it should load the whole thing into memory?), and it would construct an in-memory map between the primary's docId-s and the overlay's docId-s. And finally it would wrap the original format readers with "overlay readers", initialized also with the id map. Now, when consumers of the 4D API would ask for specific data, the "overlay readers" would first re-map the primary's docId to the overlay's docId, and check whether overlay data exists for that docId and this type of data (e.g. postings, stored fields, vectors) and return this data instead of the original. Otherwise they would return the original data. One obvious performance issue with this appraoch is that the sequential access to primary data would translate into random access to the overlay data. This could be solved by sorting the overlay index so that at least the overlay ids increase monotonically as primary ids do. Updates to the primary index would be handled as usual, i.e. segment merges, since the segments with updates would pretend to have no overlays) would just work as usual, only the overlay index would have to be deleted once the primary segment is deleted after merge. Updates to the existing documents that already had some fields updated would be again handled as usual, only underneath they would open an IndexWriter on the overlay index for a specific segment. That's the broad idea. Feel free to pipe in - I started some coding at the codec level but got stuck using the approach in LUCENE-3836. The approach that uses a modified SegmentReader seems more promising.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3842</id>
      <title>Analyzing Suggester</title>
      <description>Since we added shortest-path wFSA search in LUCENE-3714, and generified the comparator in LUCENE-3801, I think we should look at implementing suggesters that have more capabilities than just basic prefix matching. In particular I think the most flexible approach is to integrate with Analyzer at both build and query time, such that we build a wFST with: input: analyzed text such as ghost0christmas0past &lt;-- byte 0 here is an optional token separator output: surface form such as "the ghost of christmas past" weight: the weight of the suggestion we make an FST with PairOutputs&lt;weight,output&gt;, but only do the shortest path operation on the weight side (like the test in LUCENE-3801), at the same time accumulating the output (surface form), which will be the actual suggestion. This allows a lot of flexibility: Using even standardanalyzer means you can offer suggestions that ignore stopwords, e.g. if you type in "ghost of chr...", it will suggest "the ghost of christmas past" we can add support for synonyms/wdf/etc at both index and query time (there are tradeoffs here, and this is not implemented!) this is a basis for more complicated suggesters such as Japanese suggesters, where the analyzed form is in fact the reading, so we would add a TokenFilter that copies ReadingAttribute into term text to support that... other general things like offering suggestions that are more "fuzzy" like using a plural stemmer or ignoring accents or whatever. According to my benchmarks, suggestions are still very fast with the prototype (e.g. ~ 100,000 QPS), and the FST size does not explode (its short of twice that of a regular wFST, but this is still far smaller than TST or JaSpell, etc).</description>
      <attachments/>
      <comments>66</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>3843</id>
      <title>implement PositionLengthAttribute for all tokenstreams where its appropriate</title>
      <description>LUCENE-3767 introduces PositionLengthAttribute, which extends the tokenstream API from a sausage to a real graph. Currently tokenstreams such as WordDelimiterFilter and SynonymsFilter theoretically work at a graph level, but then serialize themselves to a sausage, for example: wi-fi with WDF creates: wi(posinc=1), fi(posinc=1), wifi(posinc=0) So the lossiness is that the 'wifi' is simply stacked ontop of 'fi' PositionLengthAttribute fixes this by allowing a token to declare how far it "spans", so we don't lose any information. While the indexer currently can only support sausages anyway (and for performance reasons, this is probably just fine!), other tokenstream consumers such as queryparsers and suggesters such as LUCENE-3842 can actually make use of this information for better behavior. So I think its ideal if the TokenStream API doesn't reflect the lossiness of the index format, but instead keeps all information, and after LUCENE-3767 is committed we should fix tokenstreams to preserve this information for consumers that can use it.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3846</id>
      <title>Fuzzy suggester</title>
      <description>Would be nice to have a suggester that can handle some fuzziness (like spell correction) so that it's able to suggest completions that are "near" what you typed. As a first go at this, I implemented 1T (ie up to 1 edit, including a transposition), except the first letter must be correct. But there is a penalty, ie, the "corrected" suggestion needs to have a much higher freq than the "exact match" suggestion before it can compete. Still tons of nocommits, and somehow we should merge this / make it work with analyzing suggester too (LUCENE-3842).</description>
      <attachments/>
      <comments>31</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3847</id>
      <title>LuceneTestCase should check for modifications on System properties</title>
      <description>fail the test if changes have been detected. revert the state of system properties before the suite. cleanup after the suite.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3850</id>
      <title>Fix rawtypes warnings for Java 7 compiler</title>
      <description>Java 7 changed the warnings a little bit: Java 6 only knew "unchecked" warning type, applying for all types of generics violations, like missing generics (raw types) Java 7 still knows "unchecked" but only emits warning if the call is really unchecked. Declaration of variables/fields or constructing instances without type param now emits "rawtypes" warning. The changes above causes the Java 7 compile now emit lots of "rawtypes" warnings, where Java 6 is silent. The easy fix is to suppres both warning types: @SuppressWarnings( {"unchecked","rawtypes"} ) for all those places. Changes are easy to do, will provide patch later!</description>
      <attachments/>
      <comments>13</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3852</id>
      <title>Rename BaseMultiReader class to BaseCompositeReader and make public</title>
      <description>Currently the abstract DirectoryReader and MultiReader and ParallelCompositeReader extend a package private class. Users that want to implement a composite reader, should be able to subclass this pkg-private class, as it implements lots of abstract methods, useful for own implementations. In fact MultiReader is a shallow subclass only implementing correct closing&amp;refCounting. By making it public after the rename, the generics problems (type parameter R is not correctly displayed) in the JavaDocs are solved, too.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3856</id>
      <title>Create docvalues based grouped facet collector</title>
      <description>Create docvalues based grouped facet collector. Currently only term based collectors have been implemented (LUCENE-3802).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3861</id>
      <title>DocValues is Closeable/has a close() but says users shouldn't call it</title>
      <description>I think this is confusing: the javadocs say you should not call it, if this is the case, can we make it so that DocValues coming from IR is not actually closeable?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3862</id>
      <title>DocValues getInt() returns long, getFloat() returns double</title>
      <description>I think this is a bit confusing: especially for the case of something like norms where its really an 8 bit byte, a long is confusing. i think we should have the usual getFloat/getDouble/getInt/getShort/getByte instead?</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3863</id>
      <title>DocValues.type() -&gt; DocValues.getType()</title>
      <description>This makes the method easier to find and more clear that it has no side effects... on a few occasions I've looked for this getter and missed it because of the name.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3864</id>
      <title>support offsets in MemoryPostings</title>
      <description>Really we should add this for Sep &amp; Pulsing too... but this is one more</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3866</id>
      <title>Make CompositeReader.getSequentialSubReaders() and the corresponding IndexReaderContext methods return unmodifiable List&lt;R extends IndexReader&gt;</title>
      <description>Since Lucene 2.9 we have the method getSequentialSubReader() returning IndexReader[]. Based on hardly-to-debug errors in user's code, Robert and me realized that returning an array from a public API is an anti-pattern. If the array is intended to be modifiable (like byte[] in terms,...), it is fine to use arrays in public APIs, but not, if the array must be protected from modification. As IndexReaders are 100% unmodifiable in trunk code (no deletions,...), the only possibility to corrumpt the reader is by modifying the array returned by getSequentialSubReaders(). We should prevent this. The same theoretically applies to FieldCache, too - but the party that is afraid of performance problems is too big to fight against that For getSequentialSubReaders there is no performance problem at all. The binary search of reader-ids inside BaseCompositeReader can still be done with the internal protected array, but public APIs should expose only a unmodifiable List. The same applies to leaves() and children() in IndexReaderContext. This change to list would also allow to make CompositeReader and CompositeReaderContext Iterable&lt;R extends IndexReader&gt;, so some loops would look nice.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3875</id>
      <title>ValueSourceFilter</title>
      <description>A ValueSourceFilter is a filter that takes a ValueSource and a threshold value, filtering out documents for which their value returned by the ValueSource is below the threshold. We use the ValueSourceFilter for filtering documents based on their value in an ExternalFileField.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3881</id>
      <title>Create UAX29URLEmailAnalyzer: a standard analyzer that recognizes URLs and emails</title>
      <description>This Analyzer should contain the same components as StandardAnalyzer, except for the tokenizer, which should be UAX29URLEmailTokenizer instead of StandardTokenizer.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3883</id>
      <title>Analysis for Irish</title>
      <description>Adds analysis for Irish. The stemmer is generated from a snowball stemmer. I've sent it to Martin Porter, who says it will be added during the week.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3888</id>
      <title>split off the spell check word and surface form in spell check dictionary</title>
      <description>The "did you mean?" feature by using Lucene's spell checker cannot work well for Japanese environment unfortunately and is the longstanding problem, because the logic needs comparatively long text to check spells, but for some languages (e.g. Japanese), most words are too short to use the spell checker. I think, for at least Japanese, the things can be improved if we split off the spell check word and surface form in the spell check dictionary. Then we can use ReadingAttribute for spell checking but CharTermAttribute for suggesting, for example.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3891</id>
      <title>Documents loaded at search time (IndexReader.document) should be a different class from the index-time Document</title>
      <description>The fact that the Document you can load at search time is the same Document class you had indexed is horribly trappy in Lucene, because, the loaded document necessarily loses information like field boost, whether a field was tokenized, etc. (See LUCENE-3854 for a recent example). We should fix this, statically, so that it's an entirely different class at search time vs index time.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3892</id>
      <title>Add a useful intblock postings format (eg, FOR, PFOR, PFORDelta, Simple9/16/64, etc.)</title>
      <description>On the flex branch we explored a number of possible intblock encodings, but for whatever reason never brought them to completion. There are still a number of issues opened with patches in different states. Initial results (based on prototype) were excellent (see http://blog.mikemccandless.com/2010/08/lucene-performance-with-pfordelta-codec.html ). I think this would make a good GSoC project.</description>
      <attachments/>
      <comments>140</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3893</id>
      <title>TermsQuery should use AutomatonQuery or Terms.intersect</title>
      <description>I think we could see perf gains if TermsFilter sorted the terms, built a minimal automaton, and used TermsEnum.intersect to visit the terms... This idea came up on the dev list recently.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3894</id>
      <title>Make BaseTokenStreamTestCase a bit more evil</title>
      <description>Throw an exception from the Reader while tokenizing, stop after not consuming all tokens, sometimes spoon-feed chars from the reader...</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3901</id>
      <title>Add katakana stem filter to better deal with certain katakana spelling variants</title>
      <description>Many Japanese katakana words end in a long sound that is sometimes optional. For example, パーティー and パーティ are both perfectly valid for "party". Similarly we have センター and センタ that are variants of "center" as well as サーバー and サーバ for "server". I'm proposing that we add a katakana stemmer that removes this long sound if the terms are longer than a configurable length. It's also possible to add the variant as a synonym, but I think stemming is preferred from a ranking point of view.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3902</id>
      <title>public classes with no javadocs</title>
      <description>Here is a list of public classes with no javadocs. I think even some simple javadocs can be valuable for all javadocs classes: in various summaries, we don't see an empty summary for what the class does easier to work with the source in various IDEs that present this stuff on hover, etc better documentation for developers to know what all these classes do. Maybe we don't have time to fix this for 3.x, but it would be great if anybody has good knowledge of these classes and could commit any useful stuff to the javadocs. Here is the list from Mike's tool on LUCENE-3887 rmuir@beast:~/workspace/lucene-branch3x2/dev-tools/scripts$ python checkJavaDocs.py ../../lucene/build/docs/api Check... ../../lucene/build/docs/api/all/org/tartarus/snowball/package-summary.html missing: Among missing: TestApp ../../lucene/build/docs/api/all/org/apache/lucene/spatial/tier/package-summary.html missing: DistanceHandler.Precision ../../lucene/build/docs/api/all/org/apache/lucene/index/package-summary.html missing: MergePolicy.MergeAbortedException ../../lucene/build/docs/api/all/org/apache/lucene/index/pruning/package-summary.html missing: CarmelTopKTermPruningPolicy.ByDocComparator missing: CarmelUniformTermPruningPolicy.ByDocComparator ../../lucene/build/docs/api/all/org/apache/lucene/util/package-summary.html missing: ByteBlockPool.Allocator missing: ByteBlockPool.DirectAllocator missing: ByteBlockPool.DirectTrackingAllocator missing: BytesRefHash.BytesStartArray missing: BytesRefHash.DirectBytesStartArray missing: BytesRefIterator.EmptyBytesRefIterator missing: DoubleBarrelLRUCache.CloneableKey missing: English missing: OpenBitSetDISI missing: PagedBytes.Reader missing: StoreClassNameRule missing: SystemPropertiesInvariantRule missing: UncaughtExceptionsRule.UncaughtExceptionEntry missing: UnicodeUtil.UTF16Result missing: UnicodeUtil.UTF8Result ../../lucene/build/docs/api/all/org/apache/lucene/queryParser/core/nodes/package-summary.html missing: TextableQueryNode missing: PathQueryNode.QueryText missing: PhraseSlopQueryNode missing: ProximityQueryNode.ProximityType missing: ModifierQueryNode.Modifier missing: ParametricQueryNode.CompareOperator missing: ProximityQueryNode.Type ../../lucene/build/docs/api/all/org/apache/lucene/queryParser/core/parser/package-summary.html missing: EscapeQuerySyntax.Type ../../lucene/build/docs/api/all/org/apache/lucene/queryParser/standard/builders/package-summary.html missing: AnyQueryNodeBuilder ../../lucene/build/docs/api/all/org/apache/lucene/queryParser/standard/config/package-summary.html missing: FuzzyConfig missing: StandardQueryConfigHandler.ConfigurationKeys missing: DefaultOperatorAttribute.Operator missing: StandardQueryConfigHandler.Operator ../../lucene/build/docs/api/all/org/apache/lucene/queryParser/standard/parser/package-summary.html missing: EscapeQuerySyntaxImpl missing: StandardSyntaxParser ../../lucene/build/docs/api/all/org/apache/lucene/queryParser/surround/query/package-summary.html missing: DistanceSubQuery missing: SimpleTerm.MatchingTermVisitor missing: AndQuery missing: BasicQueryFactory missing: ComposedQuery missing: DistanceQuery missing: FieldsQuery missing: NotQuery missing: OrQuery missing: SimpleTerm missing: SpanNearClauseFactory missing: SrndPrefixQuery missing: SrndQuery missing: SrndTermQuery missing: SrndTruncQuery missing: TooManyBasicQueries ../../lucene/build/docs/api/all/org/apache/lucene/store/package-summary.html missing: FSDirectory.FSIndexOutput missing: NativePosixUtil missing: NIOFSDirectory.NIOFSIndexInput missing: RAMFile missing: SimpleFSDirectory.SimpleFSIndexInput missing: SimpleFSDirectory.SimpleFSIndexInput.Descriptor missing: WindowsDirectory.WindowsIndexInput missing: MockDirectoryWrapper.Throttling ../../lucene/build/docs/api/all/org/apache/lucene/xmlparser/package-summary.html missing: FilterBuilder missing: CorePlusExtensionsParser missing: DOMUtils missing: FilterBuilderFactory missing: QueryBuilderFactory missing: ParserException ../../lucene/build/docs/api/all/org/apache/lucene/xmlparser/builders/package-summary.html missing: SpanQueryBuilder missing: BooleanFilterBuilder missing: BooleanQueryBuilder missing: BoostingQueryBuilder missing: BoostingTermBuilder missing: ConstantScoreQueryBuilder missing: DuplicateFilterBuilder missing: FilteredQueryBuilder missing: FuzzyLikeThisQueryBuilder missing: LikeThisQueryBuilder missing: MatchAllDocsQueryBuilder missing: RangeFilterBuilder missing: SpanBuilderBase missing: SpanFirstBuilder missing: SpanNearBuilder missing: SpanNotBuilder missing: SpanOrBuilder missing: SpanOrTermsBuilder missing: SpanQueryBuilderFactory missing: SpanTermBuilder missing: TermQueryBuilder missing: TermsFilterBuilder ../../lucene/build/docs/api/all/org/apache/lucene/benchmark/byTask/tasks/package-summary.html missing: BenchmarkHighlighter missing: NewCollationAnalyzerTask.Implementation ../../lucene/build/docs/api/all/org/apache/lucene/benchmark/byTask/feeds/demohtml/package-summary.html missing: Entities missing: HTMLParser missing: Tags ../../lucene/build/docs/api/all/org/apache/lucene/search/package-summary.html missing: DuplicateFilter missing: FieldCache.CreationPlaceholder missing: FieldComparator.NumericComparator&amp;lt;T extends Number&amp;gt; missing: FieldValueHitQueue.Entry missing: QueryTermVector missing: ScoringRewrite&amp;lt;Q extends Query&amp;gt; missing: SpanFilterResult.PositionInfo missing: SpanFilterResult.StartEnd missing: TimeLimitingCollector.TimerThread ../../lucene/build/docs/api/all/org/apache/lucene/search/vectorhighlight/package-summary.html missing: BoundaryScanner missing: BaseFragmentsBuilder missing: FieldFragList.WeightedFragInfo missing: FieldFragList.WeightedFragInfo.SubInfo missing: FieldPhraseList.WeightedPhraseInfo missing: FieldPhraseList.WeightedPhraseInfo.Toffs missing: FieldQuery.QueryPhraseMap missing: FieldTermStack.TermInfo missing: ScoreOrderFragmentsBuilder.ScoreComparator missing: SimpleBoundaryScanner ../../lucene/build/docs/api/all/org/apache/lucene/search/highlight/package-summary.html missing: TokenStreamFromTermPositionVector ../../lucene/build/docs/api/all/org/apache/lucene/search/suggest/package-summary.html missing: Lookup.LookupPriorityQueue ../../lucene/build/docs/api/all/org/apache/lucene/search/suggest/jaspell/package-summary.html missing: JaspellLookup ../../lucene/build/docs/api/all/org/apache/lucene/search/suggest/tst/package-summary.html missing: TSTAutocomplete missing: TSTLookup ../../lucene/build/docs/api/all/org/apache/lucene/facet/index/package-summary.html missing: FacetsPayloadProcessorProvider.FacetsDirPayloadProcessor ../../lucene/build/docs/api/all/org/apache/lucene/facet/taxonomy/writercache/lru/package-summary.html missing: LruTaxonomyWriterCache.LRUType</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3905</id>
      <title>BaseTokenStreamTestCase should test analyzers on real-ish content</title>
      <description>We already have LineFileDocs, that pulls content generated from europarl or wikipedia... I think sometimes BTSTC should test the analyzers on that as well.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3907</id>
      <title>Improve the Edge/NGramTokenizer/Filters</title>
      <description>Our ngram tokenizers/filters could use some love. EG, they output ngrams in multiple passes, instead of "stacked", which messes up offsets/positions and requires too much buffering (can hit OOME for long tokens). They clip at 1024 chars (tokenizers) but don't (token filters). The split up surrogate pairs incorrectly.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3909</id>
      <title>Move Kuromoji to analysis.ja and introduce Japanese* naming</title>
      <description>Lucene/Solr 3.6 and 4.0 will get out-of-the-box Japanese language support through KuromojiAnalyzer, KuromojiTokenizer and various other filters. These filters currently live in org.apache.lucene.analysis.kuromoji. I'm proposing that we move Kuromoji to a new Japanese package org.apache.lucene.analysis.ja in line with how other languages are organized. As part of this, I also think we should rename KuromojiAnalyzer to JapaneseAnalyzer, etc. to further align naming to our conventions by making it very clear that these analyzers are for Japanese. (As much as I like the name "Kuromoji", I think "Japanese" is more fitting.) A potential issue I see with this that I'd like to raise and get feedback on, is that end-users in Japan and elsewhere who use lucene-gosen could have issues after an upgrade since lucene-gosen is in fact releasing its analyzers under the org.apache.lucene.analysis.ja namespace (and we'd have a name clash). I believe users should have the freedom to choose whichever Japanese analyzer, filter, etc. they'd like to use, and I don't want to propose a name change that just creates unnecessary problems for users, but I think the naming proposed above is most fitting for a Lucene/Solr release.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3912</id>
      <title>Improved the checked-in tiny line file docs</title>
      <description>I think it may not have any surrogate pairs.... (it was derived from Europarl).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3915</id>
      <title>Add Japanese filter to replace term attribute with readings</title>
      <description>Koji and Robert are working on LUCENE-3888 that allows spell-checkers to do their similarity matching using a different word than its surface form. This approach is very useful for languages such as Japanese where the surface form and the form we'd like to use for similarity matching is very different. For Japanese, it's useful to use readings for this – probably with some normalization.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3916</id>
      <title>Consider different query and index segmentation for Japanese</title>
      <description>Kuromoji today uses search mode segmentation both at query and index time. The benefit with search mode segmentation is that it segments compounds such as 関西国際空港 (Kansai International Airport) into 関西 (Kansai), 国際 (international), 空港 (airport), and leaves the compound 関西国際空港 as a synonym to 関西. This segmentation allows us to get a match for 空港 (airport), which is good for recall and we'd get good precision when searching for the compound 関西国際空港 because of IDF. However, if we search for the compound 関西国際空港 (Kansai International Airport) our query becomes (by default) an OR-query with terms 関西 (Kansai), 関西国際空港 (Kansai International Airport), 国際 (international) and 空港 (airport). This behaviour is by-design when using OR as the default operator, but this also has the effect of returning generic hits like 空港 (airport) when the user searches for something very specific like 関西国際空港 (Kansai International Airport) – and these hits are also highlighted. This doesn't necessarily mean that ranking is flawed per se, but a user or application might prefer precision over recall. In order to favour precision, we can consider using normal mode segmentation for queries, but retain search mode segmentation on the indexing side. Does anyone have any general opinion on this? What would we do for other language in the case of compound splitting? Perhaps this can be dealt with as a documentation issue with a comment in schema.xml while keeping the current behaviour? Many thanks for any input.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3921</id>
      <title>Add decompose compound Japanese Katakana token capability to Kuromoji</title>
      <description>Japanese morphological analyzer, Kuromoji doesn't have a capability to decompose every Japanese Katakana compound tokens to sub-tokens. It seems that some Katakana tokens can be decomposed, but it cannot be applied every Katakana compound tokens. For instance, "トートバッグ(tote bag)" and "ショルダーバッグ" don't decompose into "トート バッグ" and "ショルダー バッグ" although the IPA dictionary has "バッグ" in its entry. I would like to apply the decompose feature to every Katakana tokens if the sub-tokens are in the dictionary or add the capability to force apply the decompose feature to every Katakana tokens.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3922</id>
      <title>Add Japanese Kanji number normalization to Kuromoji</title>
      <description>Japanese people use Kanji numerals instead of Arabic numerals for writing price, address and so on. i.e 12万4800円(124,800JPY), 二番町三ノ二(3-2 Nibancho) and 十二月(December). So, we would like to normalize those Kanji numerals to Arabic numerals (I don't think we need to have a capability to normalize to Kanji numerals).</description>
      <attachments/>
      <comments>28</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3924</id>
      <title>Optimize buffer size handling in RAMDirectory to make it more GC friendly</title>
      <description>RAMDirectory currently uses a fixed buffer size of 1024 bytes to allocate memory. This is very wasteful for large indexes. Improvements may be: per file buffer sizes based on IOContext and maximum segment size allocate only one buffer for files that are copied from another directory dynamically increae buffer size when files grow (makes seek() complicated)</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3925</id>
      <title>Spatial field types should not store doc frequencies or positions</title>
      <description>It appears the corrections is simply to supply IndexOptions.DOCS_ONLY</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3929</id>
      <title>org.apache.lucene.analysis.fr.FrenchAnalyzer could introduce french accent insensitive search.</title>
      <description>The GermanAnalyzer does the same with the Umlaut for example. Searching for 'gehort' will return 'gehört' and 'gehort' . I expected that the FrenchAnalyzer would also return 'sécuritaires' and 'securitaires' and searching for any of them, but it's not the case</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3931</id>
      <title>Adding "d" character to default ElisionFilter</title>
      <description>As described in Wikipedia (http://fr.wikipedia.org/wiki/%C3%89lision), the d character is used in french as an elision character. E.g.: déclaration d'espèce So, it would be useful to have it as a default elision token. ElisionFilter.java private static final CharArraySet DEFAULT_ARTICLES = CharArraySet.unmodifiableSet( new CharArraySet(Version.LUCENE_CURRENT, Arrays.asList( "l", "m", "t", "qu", "n", "s", "j", "d"), true)); HTH David.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3932</id>
      <title>Improve load time of .tii files</title>
      <description>We have a large 50 gig index which is optimized as one segment, with a 66 MEG .tii file. This index has no norms, and no field cache. It takes about 5 seconds to load this index, profiling reveals that 60% of the time is spent in GrowableWriter.set(index, value), and most of time in set(...) is spent resizing PackedInts.Mutatable current. In the constructor for TermInfosReaderIndex, you initialize the writer with the line, GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false); For our index using four as the bit estimate results in 27 resizes. The last value in indexToTerms is going to be ~ tiiFileLength, and if instead you use, int bitEstimate = (int) Math.ceil(Math.log10(tiiFileLength) / Math.log10(2)); GrowableWriter indexToTerms = new GrowableWriter(bitEstimate, indexSize, false); Load time improves to ~ 2 seconds.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3933</id>
      <title>Cloneable classes can use their class in the clone() function</title>
      <description>Since Java5, we are allowed to use an explicit class when returning clone() It is nicer to use: OpenBitSet copy = original.clone(); then OpenBitSet copy = (OpenBitSet)original.clone();</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3935</id>
      <title>Optimize Kuromoji inner loop - rewrite ConnectionCosts.get() method</title>
      <description>I've been profiling Kuromoji, and not very surprisingly, method ConnectionCosts.get(int forwardId, int backwardId) that looks up costs in the Viterbi is called many many times and contributes to more processing time than I had expected. This method is currently backed by a short[][]. This data stored here structure is a two dimensional array with both dimensions being fixed with 1316 elements in each dimension. (The data is matrix.def in MeCab-IPADIC.) We can rewrite this to use a single one-dimensional array instead, and we will at least save one bounds check, a pointer reference, and we should also get much better cache utilization since this structure is likely to be in very local CPU cache. I think this will be a nice optimization. Working on it...</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3936</id>
      <title>Rename StringIndexDocValues to DocTermsIndexDocValues</title>
      <description>StringIndex doesn't exists any more in the trunk, so the name DocTermsIndex should be used and this is also what it is using.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3938</id>
      <title>Add query time parent child search</title>
      <description>At the moment there is support for index time parent child search with two queries implementations and a collector. The index time parent child search requires that documents are indexed in a block, this isn't ideal for updatability. For example in the case of tv content and subtitles (both being separate documents). Updating already indexed tv content with subtitles would then require to also re-index the subtitles. This issue focuses on the collector part for query time parent child search. I started a while back with implementing this. Basically a two pass search performs a parent child search. In the first pass the top N parent child documents are resolved. In the second pass the parent or top N children are resolved (depending if the hit is a parent or child) and are associated with the top N parent child relation documents. Patch will follow soon.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3942</id>
      <title>SynonymFilter should set pos length att</title>
      <description>Tokenizers/Filters can now produce graphs instead of a single linear chain of tokens, by setting the PositionLengthAttribute, expressing where (how many positions ahead) this token "ends". The default is 1, meaning it ends at the next position, to be backwards compatible. SynonymFilter produces graph output tokens, as long as the output is a single token, but currently never sets the pos length to express this. EG for the rule "wifi network -&gt; hotspot", the hotspot token should have pos length = 2. With LUCENE-3940 this will allow us to verify that the offsets for such tokens are correct...</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3943</id>
      <title>Use ivy cachepath and cachefileset instead of ivy retrieve</title>
      <description>In LUCENE-3930 we moved to resolving all external dependencies using ivy:retrieve. This process places the dependencies into the lib/ folder of the respective modules which was ideal since it replicated the existing build process and limited the number of changes to be made to the build. However it can lead to multiple jars for the same dependency in the lib folder when the dependency is upgraded, and just isn't the most efficient way to use Ivy. Uwe pointed out that when working from svn or in using src releases we can remove the ivy:retrieve calls and make use of ivy:cachepath and ivy:cachefileset to build our classpaths and packages respectively, which will go some way to addressing these limitations – however we still need the build system capable of putting the actual jars into specific lib folders when assembling the binary artifacts</description>
      <attachments/>
      <comments>16</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>3944</id>
      <title>ant clean should remove pom.xml's</title>
      <description>Currently once the pom.xml's are in place, its hard to get them out. Having them can be a little trappy when you're trying to debug the bug. We should facilitate their removal during clean.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3948</id>
      <title>Experiment with placing poms outside of src</title>
      <description>Recent work in LUCENE-3944 has changed how our generated pom.xml files are handled during release preparation, placing them in build/ instead. However get-maven-poms still places the poms inside src/ so you can use them to drive a build. What I think would be ideal is if we could unify the release handling of the poms and the normal building handling, so that the poms can sit outside of src and serve both purposes. Some time ago I investigated how the ANT project handles its own Maven integration and it has its poms sitting in their own directory. They then reference the actual src locations inside the poms. This works for ANT but with a warning since some of their tests don't work due to how the Maven surefire plugin works, so they skip their tests. I have done some quick testing of my own and this process does seem to work for our poms and tests. I now want to take this to a full scale POC and see if it works fully.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3950</id>
      <title>load rat via ivy for rat-sources task</title>
      <description>we now fail the build on rat problems (LUCENE-1866), so we should make it easy to run rat-sources for people to test locally (it takes like 3 seconds total for the whole trunk) Also this is safer than putting rat in your ~/.ant/lib because that adds some classes from commons to your ant classpath (which we currently wrongly use in compile).</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3954</id>
      <title>prepare-release should put changesToHtml and KEYS output in dist/</title>
      <description>Currently changes2html output is in build/docs/changes But really, this just means RM must manually deal with it separately from dist/ I think 'prepare-release' should ensure dist/ houses everything that goes into the RC, Additionally, KEYS files for both lucene/solr which should be fetched from http://people.apache.org/keys/group/lucene.asc automatically. Sorry, i know there are python scripts etc in dev-tools as well as ant logic to do some of this, but it would be much easier for 'prepare-release' to actually make the full release, and some people don't want to give their ssh password to any script like that.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3955</id>
      <title>smokeTestRelease should test solr example</title>
      <description>I think most anyone reviewing the solr artifacts will do this, so really the RM has to do it manually: but we can test 'ant example' from the source dist + java -jar start.jar from solr/example (or/and 'ant run-example'), and also java -jar start.jar from the binary distribution. some basic checks we can do are to run the test_utf8.sh, and to index the example docs (post.jar/post.sh the docs in exampledocs) then do a search.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3957</id>
      <title>Document precision requirements of setBoost calls</title>
      <description>The behaviour of index-time boosts seems pretty erratic (e.g. a boost of 8.0 produces the exact same score as a boost of 9.0) until you become aware that these factors end up encoded in a single byte, with a three-bit mantissa. This consumed a whole day of research for us, and I still believe we were lucky to spot it, given how deeply dug into the code &amp; documentation this information is. I suggest adding a small note to the JavaDoc of setBoost methods in Document, Fieldable, FieldInvertState, and possibly AbstractField, Field, and NumericField. Suggested text: "Note that all index-time boost values end up encoded using Similarity.encodeNormValue, with a 3-bit mantissa – so differences in the boost value of less than 25% may easily be rounded away."</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3959</id>
      <title>Remove @author tags in Lucene/Solr sources</title>
      <description>Lucene/Solr sources should not include @author tags. See the solr-dev@l.a.o and java-dev@l.a.o threads in which this was discussed. The Jenkins builds should fail if they are found, in the same way that nocommit's are currently handled</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3960</id>
      <title>improve build -projecthelp as far as building jars and such</title>
      <description>HossmanSays: "ant compile" in lucene src artifacts builds jars for some contribs not all – kind of confusing. I did a double take before i realized i needed "build-contrib" to get them all and the ones i was seeing were just because of cross-contrb dependencies. Basically the ant -p is confusing in general. The 'default' task is actually 'jar-core', which makes jars, but it has no description (doesnt show up in project help), and neither does 'jar' (which maps to jar-core). solr/build.xml does a much better job about this. The ant -p seems more userfriendly and its obvious what targets do what. For lucene its not. Also solr/build.xml defines 'ant compile' (lucene's doesnt, it maps to compile-core, which then behaves differently than solr for people used to typing it). Basically I think the best solution is to try to make these two build.xml's targets as consistent as possible.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3961</id>
      <title>don't build and rebuild jar files for dependencies in tests</title>
      <description>Hossman's comments about when jars are built had me thinking, its not really great how dependencies are managed currently. say i have contrib/hamburger that depends on contrib/cheese if I do 'ant test' in contrib/hamburger, you end out with a situation where you have no hamburger.jar but you have a cheese.jar. The reason for this: i think is how we implement the contrib-uptodate, via .jar files. I think instead contrib-uptodate shouldnt use actual jar files (cheese.jar) but a simple file we 'touch' like cheese.compiled. This will make the build faster, especially I think the solr tests which uses these dependencies across a lot of lucene modules. we won't constantly jar their stuff.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>3964</id>
      <title>Stage Maven release artifacts</title>
      <description>We need a way to stage Maven artifacts to the Apache release repository.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3966</id>
      <title>smokeTestRelease should accept a local (file://) staging URL</title>
      <description>I'll also fix buildAndPushRelease so it can push to a local URL; this way at any time we can build, push to local staging, and run smoke tester on it, and hopefully nothing fails... But really any tests in smoke tester should ideally be pushed back earlier in our dev process (into jenkins, into "ant test").</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>3968</id>
      <title>Factor MockGraphTokenFilter into LookaheadTokenFilter + random tokens</title>
      <description>MockGraphTokenFilter is rather hairy... I've managed to simplify it (I think!) by breaking apart its two functions... I think LookaheadTokenFilter can be used in the future for other graph aware filters.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>3972</id>
      <title>Improve AllGroupsCollector implementations</title>
      <description>I think that the performance of TermAllGroupsCollectorm, DVAllGroupsCollector.BR and DVAllGroupsCollector.SortedBR can be improved by using BytesRefHash to store the groups instead of an ArrayList.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>3973</id>
      <title>Incorporate PMD / FindBugs</title>
      <description>This has been touched on a few times over the years. Having static analysis as part of our build seems like a big win. For example, we could use PMD to look at System.out.println statements like discussed in LUCENE-3877 and we could possibly incorporate the nocommit / @author checks as well. There are a few things to work out as part of this: Should we use both PMD and FindBugs or just one of them? They look at code from different perspectives (bytecode vs source code) and target different issues. At the moment I'm in favour of trying both but that might be too heavy handed for our needs. What checks should we use? There's no point having the analysis if it's going to raise too many false-positives or problems we don't deem problematic. How should the analysis be integrated in our build? Need to work out when the analysis should run, how it should be incorporated in Ant and/or Maven, what impact errors should have.</description>
      <attachments/>
      <comments>36</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>3976</id>
      <title>Improve error messages for unsupported Hunspell formats</title>
      <description>Our hunspell implementation is never going to be able to support the huge variety of formats that are out there, especially since our impl is based on papers written on the topic rather than being a pure port. Recently we ran into the following suffix rule: SFX CA 0 /CaCp Due to the missing regex conditional, an AOE was being thrown, which made it difficult to diagnose the problem. We should instead try to provide better error messages showing what we were unable to parse.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3978</id>
      <title>redo how our download redirect pages work</title>
      <description>the download "latest" redirect pages are kind of a pain to change when we release a new version... http://lucene.apache.org/core/mirrors-core-latest-redir.html http://lucene.apache.org/solr/mirrors-solr-latest-redir.html</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3985</id>
      <title>Refactor support for thread leaks</title>
      <description>This will be duplicated in the runner and in LuceneTestCase; try to consolidate.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3986</id>
      <title>Support running tests with a simple, non-asserting (and possibly shared) Random</title>
      <description>Robert asked for it. Calling random() (and its methods) can obscure memory visibility issues (because random() is thread local, context-sensitive, etc.). An option (or randomly selected mode) of running with a simple Random (static/ test method level only) would simulate the framework as it was before (reading – possibly non-reproducible thread races but at the same time testing memory visibility issues in the core code).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>3988</id>
      <title>improve test output to be nicer to 80chars long terminals</title>
      <description>these lines tend to always use 82 chars: [junit4] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time: 3.97s Can we remove some of the spaces so it fits? Maybe remove the word 'run' from "Tests run". occasionally (not always) long classnames wrap too 'Running org.apache.lucene.this.that.TestFoo' ... maybe just print the short classname?</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>3995</id>
      <title>In LuceneTestCase.beforeClass, make a new random (also using the class hashcode) to vary defaults</title>
      <description>In LuceneTestCase, we set many static defaults like: default codec default infostream impl default locale default timezone default similarity Currently each test run gets a single seed for the run, which means for example across one test run every single test will have say, SimpleText + infostream=off + Locale=german + timezone=EDT + similarity=BM25 Because of that, we lose lots of basic mixed coverage across tests, and it also means the unfortunate individual who gets SimpleText or other slow options gets a REALLY SLOW test run, rather than amortizing this across all test runs. We should at least make a new random (getRandom() ^ className.hashCode()) to fix this so it works like before, but unfortunately that only fixes it for LuceneTestCase. Won't any subclasses that make random decisions in @BeforeClass (and we have many) still have the same problem? Maybe RandomizedRunner can instead be improved here?</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4004</id>
      <title>Add syntax for DisjunctionMaxQuery to the Query Parser</title>
      <description>I've come up with a use for DisjunctionMaxQuery, but not the dismax parser. I note that the toString() method on that item proposes a syntax with vertical bars. Is there any sympathy for a patch that added this to the standard parser, or some other syntax?</description>
      <attachments/>
      <comments>15</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4008</id>
      <title>Use Java Markdown provided by Ivy to transform BUILD.txt, MIGRATE.txt,.. to simple (better readable) HTML and place under documentation.</title>
      <description>The MIGRATE.txt and other files are very simple formatted and can be converted using Markdown. We can use e.g. pegdown (a Java Markdown converter) to make HTML out of them and place those in the HTML documentation. Theoretically, also CHANGES.txt might be processed by Markdown, we have to try out. Pegdown is extensible, so it could handle LUCENE-xxx JIRA links correctly.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4009</id>
      <title>Implement toString() method in TermsFilter</title>
      <description>LUCENE-1049 introduced a enhanced implementation of the toString() method in the BooleanFilter clause. This was an improvement, however I'm still seeing a lot Lucene filter classes not overriding the toString method resulting in a toString returning the classname and the hashcode of the object. This can be useful sometimes, but it's totally not useful in my case. I want to see the properties set in the filters so I know which Lucene query was created. Now: BooleanFilter(+BooleanFilter(BooleanFilter(+org.apache.lucene.search.TermsFilter@ea81ba60 +org.apache.lucene.search.TermsFilter@26ea3cbc) BooleanFilter(+org.apache.lucene.search.TermsFilter@df621f09 +org.apache.lucene.search.TermsFilter@2f712446))) Wanted behavior: BooleanFilter(+BooleanFilter(BooleanFilter(+inStock:Y +barCode:12345678) BooleanFilter(+isHeavy:N +isDamaged:Y)))</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4010</id>
      <title>AttributeSource api has broken documentation due to java generics bug</title>
      <description>There seems to be a javadocs generation bug, whereby generic type params are not properly escaped. So if you use &lt;A&gt; as a type param (which we do in AttributeSource.java), it produces invalid HTML. The bug seems to be fixed in java 7... You can see the bug here (search for "after adding"): http://lucene.apache.org/core/old_versioned_docs/versions/3_5_0/api/all/org/apache/lucene/util/AttributeSource.html The &lt;A&gt; generic type is gone, and that closing paren is red but should be blue. The 3.6.0 javadocs are OK because we used java7 to generate them... I think we should avoid &lt;A&gt; to workaround it until we are on java 7...</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4012</id>
      <title>Make all query classes serializable with Jackson, and provide a trivial query parser to consume them</title>
      <description>I started off on LUCENE-4004 wanting to use DisjunctionMaxQuery via a parser. However, this wasn't really because I thought that human beans should be improvisationally composing such thing. My real goal was to concoct a query tree over here, and then serialize it to send to Solr over there. It occurs to me that if the Xml parser is pretty good for this, JSON would be better. It further occurs to me that the query classes may already all work with Jackson, and, if they don't, the required tweaks will be quite small. By allowing Jackson to write out class names as needed, you get the ability to serialize any query, so long as the other side has the classes in class path. A trifle verbose, but not as verbose as XML, and furthermore squishable (though not in a URL) via SMILE or BSON. So, the goal of this JIRA is to accumulate tweaks to the query classes to make them more 'bean pattern'. An alternative would be Jackson annotations. However, I suspect that folks would be happier to minimize the level of coupling here; in the extreme, the trivial parser could live in contrib if no one wants a dependency, even optional, on Jackson itself.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4018</id>
      <title>Make accessible subenums in MappingMultiDocsEnum</title>
      <description>The #merge method of the PostingsConsumer receives MappingMultiDocsEnum and MappingMultiDocsAndPositionsEnum as postings enum. In certain case (with specific postings formats), the #merge method needs to be overwritten, and the underlying DocsEnums wrapped by the MappingMultiDocsEnum need to be accessed. The MappingMultiDocsEnum class should provide a method #getSubs similarly to MultiDocsEnum class.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4019</id>
      <title>Parsing Hunspell affix rules without regexp condition</title>
      <description>We found out that some recent Dutch hunspell dictionaries contain suffix or prefix rules like the following: SFX Na N 1 SFX Na 0 ste The rule on the second line doesn't contain the 5th parameter, which should be the condition (a regexp usually). You can usually see a '.' as condition, meaning always (for every character). As explained in LUCENE-3976 the readAffix method throws error. I wonder if we should treat the missing value as a kind of default value, like '.'. On the other hand I haven't found any information about this within the spec. Any thoughts?</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4024</id>
      <title>FuzzyQuery should never do edit distance &gt; 2</title>
      <description>Edit distance 1 and 2 are now very very fast compared to 3.x (100X-200X faster) ... but edit distance 3 will fallback to the super-slow scan all terms in 3.x, which is not graceful degradation. Not sure how to fix it ... mabye we have a SlowFuzzyQuery? And FuzzyQuery throws exc if you try to ask it to be slow? Or, we add boolean (off by default) that you must turn on to allow slow one..?</description>
      <attachments/>
      <comments>13</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4025</id>
      <title>ReferenceManager.maybeRefresh should allow the caller to block</title>
      <description>ReferenceManager.maybeRefresh() returns a boolean today, specifying whether the maybeRefresh logic was executed by the caller or not. If it's false, it means that another thread is currently refreshing and the call returns immediately. I think that that's inconvenient to the caller. I.e., if you wanted to do something like: writer.commit(); searcherManager.maybeRefresh(); searcherManager.acquire(); It'd be better if you could guarantee that when the maybeRefresh() call returned, the follow on acquire() will return a refreshed IndexSearcher. Even if you omit the commit instruction, it'd be good if you can guarantee that. I don't quite see the benefit of having the caller thread not block if there's another thread currently refreshing. In, I believe, most cases, you'd anyway have just one thread calling maybeRefresh(). Even if not, the only benefit of not blocking is if you have commit() followed by maybeRefresh() logic done by some threads, while other threads acquire searchers - maybe then you wouldn't care if another thread is currently doing the refresh? Actually, I tend to think that not blocking is buggy? I mean, what if two threads do commit() + maybeRefresh(). The first thread finishes commit, enters maybeRefresh(), acquires the lock and reopens the Reader. Then the second thread does its commit(), enters maybeRefresh, fails to obtain the lock and exits. Its changes won't be exposed by SM until the next maybeRefresh() is called. So it looks to me like current logic may be buggy in that sense?</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4032</id>
      <title>don't write offsetlength every skip</title>
      <description>We currently write this every skip, but we should try to avoid this (like payloads). This reduces skip data on my test corpus: .frq goes from 52354303 -&gt; 50896066</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4035</id>
      <title>Collation via docvalues</title>
      <description>Currently collated sort is via an Analyzer into an indexedfield, which is uninverted in the fieldcache. Instead we could support this with docvalues, and take advantage of future improvements like LUCENE-3729.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4039</id>
      <title>Add AddIndexesTask to Benchmark</title>
      <description>I was interested in measuring the performance of IndexWriter.addIndexes(Directory) vs. IndexWriter.addIndexes(IndexReader). I wrote an AddIndexesTask and a matching .alg. The task takes a parameter whether to use the IndexReader or Directory variants. I'll upload the patch and describe the perf results.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4040</id>
      <title>Improve QueryParser and supported syntax documentation</title>
      <description>In LUCENE-4024 there were some changes to the fuzzy query syntax. Only the Classic QueryParser really documents its syntax, which makes it hard to know whether the changes effected other QPs. Compounding this issue there are many classes which have no javadocs at all and I found myself quite confused when I consolidated all the QPs into their module. We should do a concerted effort to improve the documentation so that it is clear what syntax is supported by what QPs and so that at least the user facing classes have javadocs. As part of this, I wonder whether we should give the syntax supported by the Classic QueryParser a new name (rather than just Lucene's query syntax) since other QPs can and do support other syntax, and then somehow add some typed control over this, so QPs have to declare programmatically that they support the syntax and so we can verify that by randomly plugging in implementations into tests.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4042</id>
      <title>New snowball stemmers (Irish gaelic and Czech)</title>
      <description>New stemmers have been added to snowball (Irish gaelic and Czech).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4043</id>
      <title>Add scoring support for query time join</title>
      <description>Have similar scoring for query time joining just like the index time block join (with the score mode).</description>
      <attachments/>
      <comments>16</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4045</id>
      <title>Switch Maven test runner from maven-surefire-plugin to com.carrotsearch.randomizedtesting:junit4-maven-plugin</title>
      <description>com.carrotsearch.randomizedtesting:junit4-maven-plugin can be used to run all Lucene/Solr tests under Maven, providing faster execution through load balancing, along with all the other goodies CS.RT brings. (Not to mention it would make testing under Maven much more like testing under Ant.) From a post Dawid Weiss made on the maven-dev mailing list in January: http://labs.carrotsearch.com/randomizedtesting.html [...] Load balancing is just part of what the project is about [...] Maven integration can be seen as an integration test (with scarce documentation yet) here: https://github.com/carrotsearch/randomizedtesting/blob/master/integration-maven/junit4-maven-plugin-tests/src/it/01-basic-test/pom.xml I've used it in another project, so a cleaner example of use from within a POM is here (you should disable surefire or your tests will run twice): https://github.com/carrotsearch/hppc/blob/master/hppc-core/pom.xml#L217 And from a post Dawid made to the lucene-dev mailing list in April: I didn't mention it but there is actually an equivalent of &lt;junit4&gt; task as a maven plugin... it basically redirects to the ant-plugin but has a maven-like facade for passing the basic set of properties. I don't think it makes such a big difference for maven build - we can stick to surefire. Let me know if you'd like to try that other plugin though – an example of a maven pom using it is here: https://github.com/carrotsearch/randomizedtesting/blob/master/examples/maven/pom.xml The CS.RT maven plugin requires Maven v3.0.2+; I asked Dawid whether Maven 2.2.1 could be supported, and in private emails to me, he replied: I looked at it but it seems I need to stick to Maven 3 – there are APIs for filtering artefacts that seem to be available in 3.x only (copied and pasted from surefire and maven core). If you want to dig the code is on github, I won't have the time to look into it in the near future (first short vacation, then a backlog of crap to deal with). I admit I don't have enough Maven powers to actually think of a way to use either surefire or another plugin (depending on a sysproperty or something). This could be a fallback for folks who really need maven 2.x.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4046</id>
      <title>Allows IOException in DocsEnum#freq()</title>
      <description>Currently, DocsEnum#freq() does not allow IOException. This is problematic if somebody wants to implement a codec that allows lazy loading of freq. Frequency will be read and decoded only when #freq() will be called, therefore calling IndexInput's read methods that can throw IOException. The current workaround is to catch the IOException in freq() and ignore it (which is not very nice and not a good solution).</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4048</id>
      <title>Move getLines out of ResourceLoader and require Charset</title>
      <description>ResourceLoader.getLines() is only used by analysis factories. SolrResourceLoader's implementation does the job well and it's unlikely that another ResourceLoader implementation would handle it differently. We should extract the getLines() method out to AbstractAnalysisFactory so it can be used by the factories. Additionally we shouldn't assume the files are encoded in UTF-8, instead we should allow a Charset to be specified. This would take us one step closer to reducing the ResourceLoader interface just to what it says, a loader of resources.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4052</id>
      <title>resurrection of Hits</title>
      <description>I think we should bring back Hits (easy to use), but with less problems than before. We should fix LUCENE-3514 and then we can always use searchAfter on this thing so that it never uses too much RAM. also we can clean up other wierd parts about it.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4055</id>
      <title>Refactor SegmentInfo / FieldInfo to make them extensible</title>
      <description>After LUCENE-4050 is done the resulting SegmentInfo / FieldInfo classes should be made abstract so that they can be extended by Codec-s.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4056</id>
      <title>Japanese Tokenizer (Kuromoji) cannot build UniDic dictionary</title>
      <description>I tried to build a UniDic dictionary for using it along with Kuromoji on Solr 3.6. I think UniDic is a good dictionary than IPA dictionary, so Kuromoji for Lucene/Solr should support UniDic dictionary as standalone Kuromoji does. The following is my procedure: Modified build.xml under lucene/contrib/analyzers/kuromoji directory and run 'ant build-dict', I got the error as the below. build-dict: [java] dictionary builder [java] [java] dictionary format: UNIDIC [java] input directory: /home/kazu/Work/src/solr/brunch_3_6/lucene/build/contrib/analyzers/kuromoji/unidic-mecab1312src [java] output directory: /home/kazu/Work/src/solr/brunch_3_6/lucene/contrib/analyzers/kuromoji/src/resources [java] input encoding: utf-8 [java] normalize entries: false [java] [java] building tokeninfo dict... [java] parse... [java] sort... [java] Exception in thread "main" java.lang.AssertionError [java] encode... [java] at org.apache.lucene.analysis.ja.util.BinaryDictionaryWriter.put(BinaryDictionaryWriter.java:113) [java] at org.apache.lucene.analysis.ja.util.TokenInfoDictionaryBuilder.buildDictionary(TokenInfoDictionaryBuilder.java:141) [java] at org.apache.lucene.analysis.ja.util.TokenInfoDictionaryBuilder.build(TokenInfoDictionaryBuilder.java:76) [java] at org.apache.lucene.analysis.ja.util.DictionaryBuilder.build(DictionaryBuilder.java:37) [java] at org.apache.lucene.analysis.ja.util.DictionaryBuilder.main(DictionaryBuilder.java:82) And the diff of build.xml: =================================================================== — build.xml (revision 1338023) +++ build.xml (working copy) @@ -28,19 +28,31 @@ &lt;property name="maven.dist.dir" location="../../../dist/maven" /&gt; &lt;!-- default configuration: uses mecab-ipadic --&gt; + &lt;!-- &lt;property name="ipadic.version" value="mecab-ipadic-2.7.0-20070801" /&gt; &lt;property name="dict.src.file" value="$ {ipadic.version}.tar.gz" /&gt; &lt;property name="dict.url" value="http://mecab.googlecode.com/files/${dict.src.file}"/&gt; + --&gt; &lt;!-- alternative configuration: uses mecab-naist-jdic &lt;property name="ipadic.version" value="mecab-naist-jdic-0.6.3b-20111013" /&gt; &lt;property name="dict.src.file" value="${ipadic.version} .tar.gz" /&gt; &lt;property name="dict.url" value="http://sourceforge.jp/frs/redir.php?m=iij&amp;f=/naist-jdic/53500/$ {dict.src.file}"/&gt; --&gt; - + + &lt;!-- alternative configuration: uses UniDic --&gt; + &lt;property name="ipadic.version" value="unidic-mecab1312src" /&gt; + &lt;property name="dict.src.file" value="unidic-mecab1312src.tar.gz" /&gt; + &lt;property name="dict.loc.dir" value="/home/kazu/Work/src/nlp/unidic/_archive"/&gt; + &lt;property name="dict.src.dir" value="${build.dir}/${ipadic.version}" /&gt; + &lt;!-- &lt;property name="dict.encoding" value="euc-jp"/&gt; &lt;property name="dict.format" value="ipadic"/&gt; + --&gt; + &lt;property name="dict.encoding" value="utf-8"/&gt; + &lt;property name="dict.format" value="unidic"/&gt; + &lt;property name="dict.normalize" value="false"/&gt; &lt;property name="dict.target.dir" location="./src/resources"/&gt; @@ -58,7 +70,8 @@ &lt;target name="compile-core" depends="jar-analyzers-common, common.compile-core" /&gt; &lt;target name="download-dict" unless="dict.available"&gt; - &lt;get src="${dict.url}" dest="${build.dir}/${dict.src.file} "/&gt; + &lt;!-- get src="$ {dict.url} " dest="$ {build.dir}/${dict.src.file}"/ --&gt; + &lt;copy file="${dict.loc.dir}/${dict.src.file}" tofile="${build.dir} /$ {dict.src.file}"/&gt; &lt;gunzip src="${build.dir}/${dict.src.file} "/&gt; &lt;untar src="$ {build.dir}/${ipadic.version}.tar" dest="${build.dir} "/&gt; &lt;/target&gt;</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4057</id>
      <title>Add Codec.merge()</title>
      <description>Currently individual codec components can override/optimize merging, e.g. the stored fields impl uses bulk copying when possible, and so on. SegmentMerger contains the logic for merging the different codec components, for example it does mergeFieldInfos(), then mergeFields(), mergeTerms(), and so on. Each of these methods interacts with the codec apis to finish the merge. I think it would be cleaner if SegmentMerger called a new method, Codec.merge(), which contained this logic instead. This way someone could customize this process. I think we could probably even push some of the impl-dependent stuff (like matchingSegmentReaders) into the impl and out of SegmentMerger. Setting this for 4.1, I think it would be a nice cleanup but I don't plan on working on this immediately, and I think we can do this in a backwards compatible way in a minor release.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4061</id>
      <title>Improvements to DirectoryTaxonomyWriter (synchronization and others)</title>
      <description>DirTaxoWriter synchronizes in too many places. For instance addCategory() is fully synchronized, while only a small part of it needs to be. Additionally, getCacheMemoryUsage looks bogus - it depends on the type of the TaxoWriterCache. No code uses it, so I'd like to remove it – whoever is interested can query the specific cache impl it has. Currently, only Cl2oTaxoWriterCache supports it. If the changes will be simple, I'll port them to 3.6.1 as well.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4062</id>
      <title>More fine-grained control over the packed integer implementation that is chosen</title>
      <description>In order to save space, Lucene has two main PackedInts.Mutable implentations, one that is very fast and is based on a byte/short/integer/long array (Direct*) and another one which packs bits in a memory-efficient manner (Packed*). The packed implementation tends to be much slower than the direct one, which discourages some Lucene components to use it. On the other hand, if you store 21 bits integers in a Direct32, this is a space loss of (32-21)/32=35%. If you accept to trade some space for speed, you could store 3 of these 21 bits integers in a long, resulting in an overhead of 1/3 bit per value. One advantage of this approach is that you never need to read more than one block to read or write a value, so this can be significantly faster than Packed32 and Packed64 which always need to read/write two blocks in order to avoid costly branches. I ran some tests, and for 10000000 21 bits values, this implementation takes less than 2% more space and has 44% faster writes and 30% faster reads. The 12 bits version (5 values per block) has the same performance improvement and a 6% memory overhead compared to the packed implementation. In order to select the best implementation for a given integer size, I wrote the PackedInts.getMutable(valueCount, bitsPerValue, acceptableOverheadPerValue) method. This method select the fastest implementation that has less than acceptableOverheadPerValue wasted bits per value. For example, if you accept an overhead of 20% (acceptableOverheadPerValue = 0.2f * bitsPerValue), which is pretty reasonable, here is what implementations would be selected: 1: Packed64SingleBlock1 2: Packed64SingleBlock2 3: Packed64SingleBlock3 4: Packed64SingleBlock4 5: Packed64SingleBlock5 6: Packed64SingleBlock6 7: Direct8 8: Direct8 9: Packed64SingleBlock9 10: Packed64SingleBlock10 11: Packed64SingleBlock12 12: Packed64SingleBlock12 13: Packed64 14: Direct16 15: Direct16 16: Direct16 17: Packed64 18: Packed64SingleBlock21 19: Packed64SingleBlock21 20: Packed64SingleBlock21 21: Packed64SingleBlock21 22: Packed64 23: Packed64 24: Packed64 25: Packed64 26: Packed64 27: Direct32 28: Direct32 29: Direct32 30: Direct32 31: Direct32 32: Direct32 33: Packed64 34: Packed64 35: Packed64 36: Packed64 37: Packed64 38: Packed64 39: Packed64 40: Packed64 41: Packed64 42: Packed64 43: Packed64 44: Packed64 45: Packed64 46: Packed64 47: Packed64 48: Packed64 49: Packed64 50: Packed64 51: Packed64 52: Packed64 53: Packed64 54: Direct64 55: Direct64 56: Direct64 57: Direct64 58: Direct64 59: Direct64 60: Direct64 61: Direct64 62: Direct64 Under 32 bits per value, only 13, 17 and 22-26 bits per value would still choose the slower Packed64 implementation. Allowing a 50% overhead would prevent the packed implementation to be selected for bits per value under 32. Allowing an overhead of 32 bits per value would make sure that a Direct* implementation is always selected. Next steps would be to: make lucene components use this getMutable method and let users decide what trade-off better suits them, write a Packed32SingleBlock implementation if necessary (I didn't do it because I have no 32-bits computer to test the performance improvements). I think this would allow more fine-grained control over the speed/space trade-off, what do you think?</description>
      <attachments/>
      <comments>56</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4063</id>
      <title>FrenchLightStemmer performs abusive compression of (arbitrary) repeated characters in long tokens</title>
      <description>FrenchLightStemmer performs aggressive deletions on repeated character sequences, even on numbers. This might be unexpected during full text search.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4064</id>
      <title>Move ContentSource to PerfRunData out of DocMaker</title>
      <description>ContentSource is currently private to DocMaker, however if someone wants to write an AddDocTask that does not use DocMaker, he needs to initialize ContentSource by himself. It would be better if ContentSource is a shared resource on PerfRunData that is used by DocMaker, rather than owned by it. I'll post a patch shortly.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4068</id>
      <title>Improve IW#addDocuments(...) javadoc</title>
      <description>Improve IW#addDocuments(...) javadoc. Describer how blocks can be used regarding to index updates.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4069</id>
      <title>Segment-level Bloom filters</title>
      <description>An addition to each segment which stores a Bloom filter for selected fields in order to give fast-fail to term searches, helping avoid wasted disk access. Best suited for low-frequency fields e.g. primary keys on big indexes with many segments but also speeds up general searching in my tests. Overview slideshow here: http://www.slideshare.net/MarkHarwood/lucene-bloomfilteredsegments Benchmarks based on Wikipedia content here: http://goo.gl/X7QqU Patch based on 3.6 codebase attached. There are no 3.6 API changes currently - to play just add a field with "_blm" on the end of the name to invoke special indexing/querying capability. Clearly a new Field or schema declaration would need adding to APIs to configure the service properly. Also, a patch for Lucene4.0 codebase introducing a new PostingsFormat</description>
      <attachments/>
      <comments>107</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>4072</id>
      <title>CharFilter that Unicode-normalizes input</title>
      <description>I'd like to contribute a CharFilter that Unicode-normalizes input with ICU4J. The benefit of having this process as CharFilter is that tokenizer can work on normalised text while offset-correction ensuring fast vector highlighter and other offset-dependent features do not break. The implementation is available at following repository: https://github.com/ippeiukai/ICUNormalizer2CharFilter Unfortunately this is my unpaid side-project and cannot spend much time to merge my work to Lucene to make appropriate patch. I'd appreciate it if anyone could give it a go. I'm happy to relicense it to whatever that meets your needs.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4079</id>
      <title>The hunspell filter should support compressed Hunspell dictionaries</title>
      <description>OpenOffice dictionaries are often compressed via some aliases on the beginning of the affixe file. The french one for instance. Currently the hunspell filter does not read the aliases.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4082</id>
      <title>Implement explain in ToParentBlockJoinQuery$BlockJoinWeight</title>
      <description>At the moment, ToParentBlockJoinQuery$BlockJoinWeight.explain throws an UnsupportedOperationException. It would be useful if it could instead return the score of parent document, even if the explanation on how that score was calculated is missing.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4087</id>
      <title>Provide consistent IW behavior for illegal meta data changes</title>
      <description>Currently IW fails late and inconsistent if field metadata like an already defined DocValues type or "un"-omitting norms. we can approach this similar to how we handle consistent field number and: throw exception if indexOptions conflict (e.g. omitTF=true versus false) instead of silently dropping positions on merge same with omitNorms same with norms types and docvalues types still keeping field numbers consistent this way we could eliminate all these traps and just give an exception instead.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4091</id>
      <title>FastVectorHighlighter: Getter for FieldFragList.WeightedFragInfo and FieldPhraseList.WeightedPhraseInfo</title>
      <description>This patch introduces getter-methods for FieldFragList.WeightedFragInfo and FieldPhraseList.WeightedPhraseInfo in order to make FieldFragList "plugable" (see LUCENE-3440).</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4098</id>
      <title>Efficient bulk operations for packed integer arrays</title>
      <description>There are some places in Lucene code that {iterate over,set} ranges of values of a packed integer array. Because bit-packing implementations (Packed*) tend be slower than direct implementations, this can take a lot of time. For example, under some scenarii, GrowableWriter can take most of its (averaged) set time in resizing operations. However, some bit-packing schemes, such as the one that is used by Packed64SingleBlock*, allow to implement efficient bulk operations such as get/set/fill. Implementing these bulk operations in {{PackedInts. {Reader,Mutable} }} and using them across other components instead of their single-value counterpart could help improve performance.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4099</id>
      <title>Remove generics from SpatialStrategy and remove SpatialFieldInfo</title>
      <description>Same time ago I added SpatialFieldInfo as a way for SpatialStrategys to declare what information they needed per request. This meant that a Strategy could be used across multiple requests. However it doesn't really need to be that way any more, Strategies are light to instantiate and the generics are just clumsy and annoying. Instead Strategies should just define what they need in their constructor.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4100</id>
      <title>Maxscore - Efficient Scoring</title>
      <description>At Berlin Buzzwords 2012, I will be presenting 'maxscore', an efficient algorithm first published in the IR domain in 1995 by H. Turtle &amp; J. Flood, that I find deserves more attention among Lucene users (and developers). I implemented a proof of concept and did some performance measurements with example queries and lucenebench, the package of Mike McCandless, resulting in very significant speedups. This ticket is to get started the discussion on including the implementation into Lucene's codebase. Because the technique requires awareness about it from the Lucene user/developer, it seems best to become a contrib/module package so that it consciously can be chosen to be used.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>4104</id>
      <title>Clearly document the limit for maximum number of documents in a single index</title>
      <description>Although the "int" in a number of APIs strongly suggests the approximate limit to the number of documents than can exist in a single Lucene index, it would be useful to have the specific number more clearly documented. My reading suggests that the limit is 2^31-2 so that the count of documents, 0 to 2^31-2, will fit in an int as Integer.MAX_VALUE or 2^31-1 or 2,147,483,647. Symbolic definitions of the maximum document number and maximum number of documents, as well as the first document number should also be provided. A subsequent issue will be to detect and throw an exception when that limit is exceeded.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4105</id>
      <title>Detect and report when the limit for maximum number of documents in a single index is exceeded</title>
      <description>LUCENE-4104 documents the limit for maximum number of documents in a single index, but it is highly desirable to have Lucene detect when this limit is reached and throw a sensible exception for the user. I am not sure of the implementation details, but it seems as if IndexWriter.addDocument, addDocuments, addIndexes, and updateDocuments would be the APIs from which a new exception, call is MaxDocumentException would be thrown.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4107</id>
      <title>FastVectorHighlighter: Make FieldFragList "plugable"</title>
      <description>This patch contains: Introduction of abstract class BaseFragListBuilder SimpleFragListBuilder now extends BaseFragListBuilder FieldFragList is now abstract Introduction of SimpleFieldFragList, a simple implementation of FieldFragList Some necessary, minor changes This is the second (see LUCENE-4091) patch aiming at the introduction of another method for weighting fragments (see LUCENE-3440). Tests were okay.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4108</id>
      <title>Add replaceTaxonomy to DirectoryTaxonomyWriter</title>
      <description>I'd like to add the option to replace a taxonomy with another one, "live" on DirectoryTaxoWriter. Similar to how one can do so on IndexWriter, by calling deleteAll() and addIndexes(), it is valuable to be able to do so on DirTW as well. That way, if you need to replace a search index and taxonomy index "live", you will be able to, given this new API.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4113</id>
      <title>FastVectorHighlighter: FieldTermStack.TermInfo should provide term-weight</title>
      <description>This patch contains: FieldTermStack.TermInfo has now a field TermInfo.weight (float), which holds the index-weight (IDF) of the corresponding term FieldPhraseList.WeightedPhraseInfo has now a field WeightedPhraseInfo.termsInfos (ArrayList&lt;TermInfo&gt;), which provides information about the contained terms within the corresponding phrase WeightedPhraseInfo.addIfNoOverlap() no longer dumps the second part of a hyphenated word (for example: social-economics) FieldQueryTest has been updated Regarding test-cases: TermInfo.toString() wasn't modified, thus, test-cases remain pretty much the same except the additional parameter for TermInfo-constructor. This is the third (see LUCENE-4091, LUCENE-4107) patch aiming at the introduction of another method for weighting fragments (see LUCENE-3440).</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4117</id>
      <title>Add FST.BytesReader to all FST arc-reading APIs</title>
      <description>The FST APIs are inconsistent now: some take a FST.BytesReader from the caller and some don't. I think we should make this consistent by having all arc-reading APIs take the BytesReader.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4120</id>
      <title>FST should use packed integer arrays</title>
      <description>There are some places where an int[] could be advantageously replaced with a packed integer array. I am thinking (at least) of: FST.nodeAddress (GrowableWriter) FST.inCounts (GrowableWriter) FST.nodeRefToAddress (read-only Reader) The serialization/deserialization methods should be modified too in order to take advantage of PackedInts.get {Reader,Writer} .</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4122</id>
      <title>Replace Payload with BytesRef</title>
      <description>The Payload class offers a very similar functionality to BytesRef. The code internally uses BytesRef-s to represent payloads, and on indexing and on retrieval this data is repackaged from/to Payload. This seems wasteful. I propose to remove the Payload class and use BytesRef instead, thus avoid this re-wrapping and reducing the API footprint.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4124</id>
      <title>factor ByteBufferIndexInput out of MMapDirectory</title>
      <description>I think we should factor a ByteBufferIndexInput out of MMapDir, leaving only the mmap/unmapping in mmapdir. Its a cleaner separation and would allow it to be used for other purposes (e.g. direct or array-backed buffers)</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4126</id>
      <title>Remove FieldType copy constructor</title>
      <description>Currently FieldTypes can be created using new FieldType(someOtherFieldType) which copies the properties and allows them to then changed. This reduces readability since it hides what properties someOtherFieldType has enabled. We should encourage users (and ourselves) to explicitly state what properties are enabled so to prevent any surprises.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4131</id>
      <title>.cfs/.cfe should have a codecheader</title>
      <description>The new .cfs is more tricky, but I still think we can do it. we should definitely fix this for .cfe</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4132</id>
      <title>IndexWriterConfig live settings</title>
      <description>A while ago there was a discussion about making some IW settings "live" and I remember that RAM buffer size was one of them. Judging from IW code, I see that RAM buffer can be changed "live" as IW never caches it. However, I don't remember which other settings were decided to be "live" and I don't see any documentation in IW nor IWC for that. IW.getConfig mentions: * &lt;b&gt;NOTE:&lt;/b&gt; some settings may be changed on the * returned {@link IndexWriterConfig}, and will take * effect in the current IndexWriter instance. See the * javadocs for the specific setters in {@link * IndexWriterConfig} for details. But there's no text on e.g. IWC.setRAMBuffer mentioning that. I think that it'd be good if we make it easier for users to tell which of the settings are "live" ones. There are few possible ways to do it: Introduce a custom @live.setting tag on the relevant IWC.set methods, and add special text for them in build.xml Or, drop the tag and just document it clearly. Separate IWC to two interfaces, LiveConfig and OneTimeConfig (name proposals are welcome !), have IWC impl both, and introduce another IW.getLiveConfig which will return that interface, thereby clearly letting the user know which of the settings are "live". It'd be good if IWC itself could only expose setXYZ methods for the "live" settings though. So perhaps, off the top of my head, we can do something like this: Introduce a Config object, which is essentially what IWC is today, and pass it to IW. IW will create a different object, IWC from that Config and IW.getConfig will return IWC. IWC itself will only have setXYZ methods for the "live" settings. It adds another object, but user code doesn't change - it still creates a Config object when initializing IW, and need to handle a different type if it ever calls IW.getConfig. Maybe that's not such a bad idea?</description>
      <attachments/>
      <comments>42</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4133</id>
      <title>FastVectorHighlighter: A weighted approach for ordered fragments</title>
      <description>The FastVectorHighlighter currently disregards IDF-weights for matching terms within generated fragments. In the worst case, a fragment, which contains high number of very common words, is scored higher, than a fragment that contains all of the terms which have been used in the original query. This patch provides ordered fragments with IDF-weighted terms: For each distinct matching term per fragment: weight = weight + IDF * boost For each fragment: weight = weight * length * 1 / sqrt( length ) weight total weight of fragment IDF inverse document frequency for each distinct matching term boost query boost as provided, for example term^2 length total number of non-distinct matching terms per fragment Method: public void add( int startOffset, int endOffset, List&lt;WeightedPhraseInfo&gt; phraseInfoList ) { float totalBoost = 0; List&lt;SubInfo&gt; subInfos = new ArrayList&lt;SubInfo&gt;(); HashSet&lt;String&gt; distinctTerms = new HashSet&lt;String&gt;(); int length = 0; for( WeightedPhraseInfo phraseInfo : phraseInfoList ){ subInfos.add( new SubInfo( phraseInfo.getText(), phraseInfo.getTermsOffsets(), phraseInfo.getSeqnum() ) ); for ( TermInfo ti : phraseInfo.getTermsInfos()) { if ( distinctTerms.add( ti.getText() ) ) totalBoost += ti.getWeight() * phraseInfo.getBoost(); length++; } } totalBoost *= length * ( 1 / Math.sqrt( length ) ); getFragInfos().add( new WeightedFragInfo( startOffset, endOffset, subInfos, totalBoost ) ); } The ranking-formula should be the same, or at least similar, to that one used in QueryTermScorer. This patch contains: a changed class-member in FieldPhraseList (termInfos to termsInfos) a changed local variable in SimpleFieldFragList (score to totalBoost) adds a missing @override in SimpleFragListBuilder class WeightedFieldFragList, a implementation of FieldFragList class WeightedFragListBuilder, a implementation of BaseFragListBuilder class WeightedFragListBuilderTest, a simple test-case updated docs for FVH Last part (see also LUCENE-4091, LUCENE-4107, LUCENE-4113) of LUCENE-3440.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4138</id>
      <title>Update morfologik (polish stemming) to 1.5.3</title>
      <description>Just released. Updates to the dictionary but most of all – it comes with a clean BSD license (including dictionary data).</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4144</id>
      <title>OOM when call optimize</title>
      <description>The index file is about 6G, when i update the index, it can work good, but i hit a OOM when call the method optimize Caused by: java.lang.OutOfMemoryError: allocLargeObjectOrArray - Object size: 969048, Num elements: 242258 at org.apache.lucene.index.TermInfosReader.ensureIndexIsRead(TermInfosReader.java:90) at org.apache.lucene.index.TermInfosReader.get(TermInfosReader.java:133) at org.apache.lucene.index.SegmentTermDocs.seek(SegmentTermDocs.java:51) at org.apache.lucene.index.IndexReader.termDocs(IndexReader.java:482) at org.apache.lucene.index.IndexReader.deleteDocuments(IndexReader.java:573) at org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:1776) at org.apache.lucene.index.IndexWriter.maybeApplyDeletes(IndexWriter.java:1670) at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:1521) at org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:1351) at org.apache.lucene.index.IndexWriter.maybeFlushRamSegments(IndexWriter.java:1344) at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:763) at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:743)</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4145</id>
      <title>"Unhandled exception" from test framework (in json parsing of test output files?)</title>
      <description>Working on SOLR-3267 i got a weird exception printed to the junit output... [junit4] Unhandled exception in thread: Thread[pumper-events,5,main] [junit4] com.carrotsearch.ant.tasks.junit4.dependencies.com.google.gson.JsonParseException: No such reference: id#org.apache.solr.search.TestSort[3] ...</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4146</id>
      <title>-Dtests.iters combined with -Dtestmethod never fails?</title>
      <description>a test that is hardcoded to fail will report succes if you run it with -Dtests.iters</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4148</id>
      <title>_TestUtil should be able to generate random longs</title>
      <description>It would be helpful in TestPackedInts at least, in order to generate random values (as a workaround, we currently generate a random int between 0 and min(Integer.MAX_VALUE, PackedInts.maxValue(bitsPerValue)). Moreover, it would help to fix nextInt for large ranges (calling nextInt(random, -10, Integer.MAX_VALUE) or even nextInt(random, 0, Integer.MAX_VALUE) currently fails because the range of values is &gt; Integer.MAX_VALUE.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4153</id>
      <title>Fast vector highlighting should be aware of field boundaries in case of mv fields.</title>
      <description>Improve fast vector highlighter to respect field boundaries in the case for multivalued fields.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4156</id>
      <title>Improve implementation of DirectoryTaxonomyWriter.getSize()</title>
      <description>Current implementation of DirectoryTaxonomyWriter.getSize() is synchrionized and invokes indexWriter.maxDoc(), both harming performance.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4157</id>
      <title>Improve Spatial Testing</title>
      <description>Looking back at the tests for the Lucene Spatial Module, they seem half-baked. (At least Spatial4j is well tested). I've started working on some improvements: Some tests are in an abstract base class which have a subclass that provides a SpatialContext. The idea was that the same tests could test other contexts (such as geo vs not or different distance calculators (haversine vs vincenty) but this can be done using RandomizedTesting's nifty parameterized test feature, once there is a need to do this. Port the complex geohash recursive prefix tree test that was developed on the Solr side to the Lucene side where it belongs. And some things are not tested or aren't well tested: Distance order as the query score Indexing shapes other than points (i.e. shapes with area / regions)</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4161</id>
      <title>Make PackedInts usable by codecs</title>
      <description>Some codecs might be interested in using PackedInts. {Writer,Reader,ReaderIterator} to read and write fixed-size values efficiently. The problem is that the serialization format is self contained, and always writes the name of the codec, its version, its number of bits per value and its format. For example, if you want to use packed ints to store your postings list, this is a lot of overhead (at least ~60 bytes per term, in case you only use one Writer per term, more otherwise). Users should be able to externalize the storage of metadata to save space. For example, to use PackedInts to store a postings list, one should be able to store the codec name, its version and the number of bits per doc in the header of the terms+postings list instead of having to write it once (or more!) per term.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4163</id>
      <title>Improve concurrency in MMapIndexInput.clone()</title>
      <description>Followup issue from SOLR-3566: Whenever you clone the TermIndex, it also creates a clone of the underlying IndexInputs. In high cocurrent environments, the clone method of MMapIndexInput is a bottleneck (it has heavy work to do to manage the weak references in a synchronized block). Everywhere else in Lucene we use my new WeakIdentityMap for managing concurrent weak maps. For this case I did not do this, as the WeakIdentityMap has no iterators (it doe snot implement Map interface). This issue will add a key and values iterator (the key iterator will not return GC'ed keys), so MMapIndexInput can use WeakIdentityMap backed by ConcurrentHashMap and needs no synchronization. ConcurrentHashMap has better concurrency because it distributes the hash keys in different buckets per thread.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4164</id>
      <title>Improve IndexWriter.addIndexes(IndexReader) javadocs</title>
      <description>IndexWriter.addIndexes(IndexReader) needs some improvements: It falsely states that this method blocks any add/delete documents attempts. It merges all input IndexReaders at once, and not e.g. like MergePolicy does at mergeFactor steps. Therefore I think it'd be good to clarify it to the user, and also encourage him to call this method several times if he has many IndexReaders to merge. And while at it, mentioning that the IR can be opened with termIndexInterval=-1 since we don't need it during merge will be good – saves RAM. I'll attach a patch shortly.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4175</id>
      <title>Include BBox Spatial Strategy</title>
      <description>This is an approach to indexing bounding boxes using 4 numeric fields (xmin,ymin,xmax,ymax) and a flag to say if it crosses the dateline. This is a modification from the Apache 2.0 code from the ESRI Geoportal: http://geoportal.svn.sourceforge.net/svnroot/geoportal/Geoportal/trunk/src/com/esri/gpt/catalog/lucene/SpatialClauseAdapter.java</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4179</id>
      <title>bulk-compress blocktree term suffix lengths</title>
      <description>these should be fairly tiny numbers, it seems wasteful to use a byte-per-term when they typically only need a few bits. also if we have them in bulk, its a step towards random access.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4183</id>
      <title>Simplify CompoundFileDirectory opening in 4.x</title>
      <description>The compiler bug in JDK 8EA let me look at the code again. I opened bug report with simple test case at Oracle, but the code on our side is still too complicated to understand. The attached path for 4.x removes the nested try-finaly block and simpliefies success=true handling (which was in fact broken). It uses a more try-with-resources like approach with only one finally block.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4184</id>
      <title>Replace o.a.l.u.packed.Packed64SingleBlock with an inheritance-based impl</title>
      <description>According tests that Toke Eskildsen and I performed in LUCENE-4062, this impl is consistently faster than the current one on various archs/JVMs.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4189</id>
      <title>Test output should include timestamps (start/end for each test/ suite).</title>
      <description>This adds more verboseness to the output – should this be optional (overrideable using local properties but defaulting to 'off')? [junit4] [11:54:50.259] Suite: org.apache.lucene.index.TestDeletionPolicy [junit4] [11:54:53.706] Completed in 3.45s, 6 tests [junit4] [junit4] [11:54:53.709] Suite: org.apache.lucene.util.TestVirtualMethod [junit4] [11:54:53.725] Completed in 0.02s, 2 tests [junit4] [junit4] [11:54:53.728] Suite: org.apache.lucene.index.TestRollingUpdates [junit4] [11:54:55.700] Completed in 1.97s, 2 tests [junit4] [junit4] [11:54:55.721] Suite: org.apache.lucene.index.TestIndexWriterExceptions [junit4] [11:55:02.394] Completed in 6.67s, 24 tests [junit4] [junit4] [11:55:02.398] Suite: org.apache.lucene.index.TestNoDeletionPolicy [junit4] [11:55:02.548] Completed in 0.15s, 4 tests ...</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4192</id>
      <title>SpatialStrategy: Remove isPolyField() and createField(shape)</title>
      <description>On SpatialStrategy, I think the presence of isPolyField() and the single-field createField(shape) is a but much. They were probably copied from Solr's FieldType design without really thinking much if they were really needed.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4195</id>
      <title>Add javadocs to Codec package.html</title>
      <description>The Codec package.html is pretty basic. Add some overview information.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4197</id>
      <title>Small improvements to Lucene Spatial Module for v4</title>
      <description>This issue is to capture small changes to the Lucene spatial module that don't deserve their own issue.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4199</id>
      <title>Add ANT tool to track/disallow "forbidden" method invocations</title>
      <description>In LUCENE-3877 Greg Bowyer has some asm.jar-based code to inspe ct class files for System.out/err class. I wanted to modify this code to run it in a jar-linter task on ant, so all compiled class files are parsed and method/ctor calls to e.g. new String(byte[]) without charset are forbidden. We would add a list of method signatures that we dont want to have (new FileReader(File), commons.IOUtils.loadFileToString()) and this linter will throw BuildException after static inspection, if any class file in Lucene/Solr (including line numbers) uses any method call. Greg's code would be changed to use visitMethodInsn visitor, very easy.</description>
      <attachments/>
      <comments>32</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4201</id>
      <title>Add Japanese character filter to normalize iteration marks</title>
      <description>For some applications it might be useful to normalize kanji and kana iteration marks such as 々, ゞ, ゝ, ヽ and ヾ to make sure they are treated uniformly.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4202</id>
      <title>allow check-forbidden-apis to look for fields too</title>
      <description>Currently this supports classes and methods, but there are some deprecated fields in the java API, it would be nice to check for those, too.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4203</id>
      <title>Add IndexWriter.tryDeleteDocument, to delete by document id when possible</title>
      <description>Spinoff from LUCENE-4069. In that use case, where the app needs to first lookup a document, then call updateDocument, it's wasteful today because the relatively costly lookup (by a primary key field, eg "id") is done twice. But, since you already resolved the PK to docID on the first lookup, it would be nice to then delete by that docID and then you can call addDocument instead. So I worked out a rough start at this, by adding IndexWriter.tryDeleteDocument. It'd be a very expert API: it takes a SegmentInfo (referencing the segment that contains the docID), and as long as that segment hasn't yet been merged away, it will mark the document for deletion and return true (success). If it has been merged away it returns false and the app must then delete-by-term. It only works if the writer is in NRT mode (ie you've opened an NRT reader). In LUCENE-4069 using tryDeleteDocument gave a ~20% net speedup. I think tryDeleteDocument would also be useful when Solr "updates" a document by loading all stored fields, changing them, and calling updateDocument.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4208</id>
      <title>Spatial distance relevancy should use score of 1/distance</title>
      <description>The SpatialStrategy.makeQuery() at the moment uses the distance as the score (although some strategies – TwoDoubles if I recall might not do anything which would be a bug). The distance is a poor value to use as the score because the score should be related to relevancy, and the distance itself is inversely related to that. A score of 1/distance would be nice. Another alternative is earthCircumference/2 - distance, although I like 1/distance better. Maybe use a different constant than 1. Credit: this is Chris Male's idea.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>4212</id>
      <title>Tests should not use new Random() without args</title>
      <description>They should be using random() etc, and if they create one, it should pass in a seed. Otherwise, they probably won't reproduce.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4215</id>
      <title>DataImportHandler not using its own interface</title>
      <description>It is hard to extend DIH (e.g. to provide our own writers). DIH accepts only the SolrWriter instances, despite they implement the DIHWriter interace.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4217</id>
      <title>Load clover.jar from ivy-cachepath and store license key in SVN</title>
      <description>When clover granted use the license for their clover-2.6.3.jar file they allowed us to ship this license file to every developer. Currently clover setup is very hard for users, so this issue will make it simple. If you want to run tests with clover, just pass -Drun.clover=true to ant clean test. ANT will then download clover via IVY and point it to the license file in our tools folder. The license is supplemented by the original mail from Atlassian, that everybody is allowed to use it with code in the org.apache. java package.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4220</id>
      <title>Replace benchmarks crazy HTML parser by a nekohtml 10-liner</title>
      <description>Benchmark contains a javacc-based HTML parser which of course violates all specs, is huge and error prone. I can replace it by a NEKOHTML based one (approx 10 - 20 lines of code). NEKOHTML is an extension for XERCES (that we already use to read wikipedia), that produces SAX-events or DOM tree out of a HTML file usingg standard XML APIS. We could also use TIKA, but I refuse to download the Internet to get TIKA running for just parsing a HTML file.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4223</id>
      <title>Improve spatial package.html to compare Strategys</title>
      <description>Spin off from LUCENE-4173. One of the most important parts of using the spatial module is deciding which Strategy fits your needs. We should improve the package.html documentation so that it compares the Strategys, focusing on: Underlying index format (lon/lat numeric fields vs grid terms vs ...) Multi-value support Indexable Shapes Queryable Shapes Query operations</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4226</id>
      <title>Efficient compression of small to medium stored fields</title>
      <description>I've been doing some experiments with stored fields lately. It is very common for an index with stored fields enabled to have most of its space used by the .fdt index file. To prevent this .fdt file from growing too much, one option is to compress stored fields. Although compression works rather well for large fields, this is not the case for small fields and the compression ratio can be very close to 100%, even with efficient compression algorithms. In order to improve the compression ratio for small fields, I've written a StoredFieldsFormat that compresses several documents in a single chunk of data. To see how it behaves in terms of document deserialization speed and compression ratio, I've run several tests with different index compression strategies on 100,000 docs from Mike's 1K Wikipedia articles (title and text were indexed and stored): no compression, docs compressed with deflate (compression level = 1), docs compressed with deflate (compression level = 9), docs compressed with Snappy, using the compressing StoredFieldsFormat with deflate (level = 1) and chunks of 6 docs, using the compressing StoredFieldsFormat with deflate (level = 9) and chunks of 6 docs, using the compressing StoredFieldsFormat with Snappy and chunks of 6 docs. For those who don't know Snappy, it is compression algorithm from Google which has very high compression ratios, but compresses and decompresses data very quickly. Format Compression ratio IndexReader.document time ———————————————————————————————————————————————————————————————— uncompressed 100% 100% doc/deflate 1 59% 616% doc/deflate 9 58% 595% doc/snappy 80% 129% index/deflate 1 49% 966% index/deflate 9 46% 938% index/snappy 65% 264% (doc = doc-level compression, index = index-level compression) I find it interesting because it allows to trade speed for space (with deflate, the .fdt file shrinks by a factor of 2, much better than with doc-level compression). One other interesting thing is that index/snappy is almost as compact as doc/deflate while it is more than 2x faster at retrieving documents from disk. These tests have been done on a hot OS cache, which is the worst case for compressed fields (one can expect better results for formats that have a high compression ratio since they probably require fewer read/write operations from disk).</description>
      <attachments/>
      <comments>31</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>4227</id>
      <title>DirectPostingsFormat, storing postings as simple int[] in memory, if you have tons of RAM</title>
      <description>This postings format just wraps Lucene40 (on disk) but then at search time it loads (up front) all terms postings into RAM. You'd use this if you have insane amounts of RAM and want the fastest possible search performance. The postings are not compressed: docIds, positions are stored as straight int[]s. The terms are stored as a skip list (array of byte[]), but I packed all terms together into a single long byte[]: I had started as actual separate byte[] per term but the added pointer deref and loss of locality was a lot (~2X) slower for terms-dict intensive queries like FuzzyQuery. Low frequency postings (docFreq &lt;= 32 by default) store all docs, pos and offsets into a single int[]. High frequency postings store docs as int[], freqs as int[], and positions as int[][] parallel arrays. For skipping I just do a growing binary search. I also made specialized DirectTermScorer and DirectExactPhraseScorer for the high freq case that just pull the int[] and iterate themselves. All tests pass.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4229</id>
      <title>latin text analysis</title>
      <description>Hi a workmate and I played a bit with latin text analysis and created two filter for the solr trunk version. One filter is designed for number conversion like 'iv' -&gt; '4', 'v' -&gt; '5', 'vi' -&gt; '6' ... The second filter is a stemmer for the most common suffixe. The following schema configuration could be a usecase for latin stemming. &lt;fieldType name="text_latin" class="solr.TextField" positionIncrementGap="100"&gt; &lt;analyzer&gt; &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt; &lt;filter class="org.apache.solr.analysis.LatinNumberConvertFilterFactory" strictMode="true"/&gt; &lt;filter class="solr.KeywordMarkerFilterFactory" protected="latin_protwords.txt" /&gt; &lt;filter class="org.apache.solr.analysis.LatinStemFilterFactory" /&gt; &lt;/analyzer&gt; &lt;/fieldType&gt; LatinNumberConvertFilterFactory has one property "strictMode" (default is false). This boolean indicates in which way the computation of the value is done, because not all letter combination are "valid" numbers. With strictMode="true" the output of "ic" is "ic"; With strictMode="false" the output of "ic" is "99" The LatinStemFilterFactory generates for each input token two output token. the first stemmed as noun and the second stemmed as verb. Both filter are aware of the KeywordMarkerFilterFactory. I have attached the svn patch for both filter. In addition I attached to zip files that are needed by filter tests (TestLatinNumberConvertFilter, TestLatinStemFilter). I am sorry for that but i did not find the option to include them into the patch, if there is one. The image latin_analysis.png is an example of the analysis done with the configuration above. For this test we used the jar file latin.analysis.jar Have fun with latin text analysis. It would be great to get some feedback.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4230</id>
      <title>When pulling DocsAndPositionsEnum you should state whether you need payloads</title>
      <description>Payloads are extra-costly today because when pulling a DocsAndPositionsEnum, the codec has no way to know if you need access to the payloads. Tracking the payloads, even if the app never retrieves them, is often costly...</description>
      <attachments/>
      <comments>14</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4235</id>
      <title>Remove enforcing of Filter Rewrite for NRQ queries</title>
      <description>Back in Lucene 2.9, I hardcoded NRQ to always use filter rewrite based on the precisionStep. This is bogus and contradicts our autodetection (which may need some overhaul now in Lucene 4.0). If a query rewrites to only one term, using filter rewrite is stupid and slows down a lot, it can do a simple CSQ(TermQuery()) which is incredibly fast. This issue will remove the line from the CTOR.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4236</id>
      <title>clean up booleanquery conjunction optimizations a bit</title>
      <description>After LUCENE-3505, I want to do a slight cleanup: compute the term conjunctions optimization in scorer(), so its applied even if we have optional and prohibited clauses that dont exist in the segment (e.g. return null) use the term conjunctions optimization when optional.size() == minShouldMatch, as that means they are all mandatory, too. don't return booleanscorer1 when optional.size() == minShouldMatch, because it means we have required clauses and in general BS2 should do a much better job (e.g. use advance).</description>
      <attachments/>
      <comments>27</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>4237</id>
      <title>add ant task to generate optionally ALL javadocs</title>
      <description>As of jira LUCENE-3977 the generation of javadocs has been cleaned up and is now set fix to 'noindex' to keep distributions small. An ant task should make this selectable to have the option for really building ALL javadocs.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4239</id>
      <title>Provide access to PackedInts' low-level blocks &lt;-&gt; values conversion methods</title>
      <description>In LUCENE-4161 we started to make the PackedInts API more flexible so that codecs could use it whenever they need to (un)pack integers. There are two posting formats in progress (For and PFor, LUCENE-3892) that perform a lot of integer (un)packing but the current API still has limits : it only works with long[] arrays, whereas these codecs need to manipulate int[] arrays, the packed reader iterators work great for unpacking long sequences of integers, but they would probably cause a lot of overhead to decode lots of short integer sequences such as the ones that can be generated by For and PFor. I've been looking at the For/PFor branch and it has a PackedIntsDecompress class (http://svn.apache.org/repos/asf/lucene/dev/branches/pforcodec_3892/lucene/core/src/java/org/apache/lucene/codecs/pfor/PackedIntsDecompress.java) which is very similar to oal.util.packed.BulkOperation (package-private), so maybe we should find a way to expose this class so that the For/PFor branch can directly use it.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4240</id>
      <title>Analyzer.getOffsetGap Improvements</title>
      <description>From LUCENE-3151 (Robert Muir's comments): there is no need for the Analyzer to take in an IndexableField object. We can simplify this API: Hey Grant: I know it sounds silly but can we split out the getOffsetGap API change into a separate issue? This would be nice to fix ASAP. I dont understand why it takes IndexableField or took Fieldable. All the other methods here like getPositionIncrementGap take "String fieldName". I think this one should too. I dont think it needs a boolean for tokenized either: returning a 0 for NOT_ANALYZED fields. If you choose NOT_ANALYZED, that should mean the Analyzer is not invoked! If you want to do expert stuff control the offset gaps between values for NOT_ANALYZED fields, then just analyze it instead, with keyword tokenizer!</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4249</id>
      <title>The PayloadTermWeight explanation hides the details of the payload score</title>
      <description>I'm using the PayloadTermQuery and scoring documents using a custom algorithm based on the payloads of the matching terms. The algorithm is implemented in the custom PayloadFunction and I have added an Override for the explain. However, the PayloadTermWeight explanation hides the details of the payload score... Explanation payloadExpl = new Explanation(scorer.getPayloadScore(), "scorePayload(...)"); This is different than the way that PayloadNearSpanWeight explains the payload. It actually asks the payload function for the explanation rather than hiding it: Explanation payloadExpl = function.explain(doc, scorer.payloadsSeen, scorer.payloadScore);</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4251</id>
      <title>Port Solr's ValueSourceRangeFilter to Lucene query module</title>
      <description>Lucene spatial has ValueSourceFilter and Solr has ValueSourceRangeFilter (that has some more features). And someone asked for this in LUCENE-3875 too. It should be ported over to the Lucene query module.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4256</id>
      <title>Improve Analysis Factory configuration workflow</title>
      <description>With the Factorys now available for more general use, I'd like to look at ways to improve the configuration workflow. Currently it's a little disjoint and confusing, especially around using inform(ResourceLoader). What I think we should do is: Remove the need for ResourceLoaderAware and pass in the ResourceLoader in init, so it'd become init(Map&lt;String, String&gt; args, ResourceLoader loader) Consider moving away from the generic args Map and using setters. This gives us better typing and could mitigate bugs due to using the wrong configure key. However it does force the consumer to invoke each setter. If we're going to stick with using the args Map, then move the Version parameter into init as well, rather than being a setter as I currently made it.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4257</id>
      <title>factor the getLines in ResourceLoader to WordListLoader</title>
      <description>This is costly to have as a mandatory method on an interface: and its unrelated to resource loading, and only the factories use it.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4258</id>
      <title>Incremental Field Updates through Stacked Segments</title>
      <description>Shai and I would like to start working on the proposal to Incremental Field Updates outlined here (http://markmail.org/message/zhrdxxpfk6qvdaex).</description>
      <attachments/>
      <comments>75</comments>
      <commenters>13</commenters>
    </issue>
    <issue>
      <id>4259</id>
      <title>Allow reloading of codec/postings format list when classpath changes</title>
      <description>While implementing the SPI for analysis factories, Robert and me found out that Solr does not see codecs/postingsformats or analysis factories outside it's webapp/lib folder. The reason is simple: SPI uses by default the context classloader of the thread calling the codec API for the first time (which is Jetty's webapp classpath). We solved the problem for analysis factories, because those are loaded with the help of SolrResourceLoader, which exports the ClassLoader it uses, so it can return all analysis compoenst visible to the ResourceLoader. This is cool, because you can drop the analysis/smartchinese or analysis/icu module JAR into your plugin folder and Solr will automatically use it. For Codecs and PostingsFormats this is a little bit more complicated: The list of codecs is loaded on clinit of Codecs.class or PostingsFormat.class, which can be very early in Solr's startup. When Solr changes his classpath in ResourceLoader, the new Codecs are never seen, so you cannot drop new codecs into the plugin folder. Similar problems may happen with other webapps, because Tomcat &amp; Jetty have crazy classloader delegations (there were although reports that Lucene did not find their codecs at all - Simon Willnauer told me!) This patch will change NamedSPILoader to provide similar support like java.util.ServiceLoader does in JDK: You can tell NamedSPILoader to reload (means again list the classpath's JAR files and look for new META-INF/services files). This API is statically exported to Codec.reloadCodecs() and PostingsFormat.reploadPostingsFormats(), which takes a ClassLoader. This Classloader will then be checked only for new codecs, old ones or those already visible are not removed/replaced. SolrResourceLoader can then call this static method on each change of its classloader on startup. The implementation presented here is threadsafe, the SPI map is updated in a copy-on-write way and assigned to a volatile field.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4264</id>
      <title>fail with error if analyzer has both a class and nested factories</title>
      <description>IndexSchema should complain if someone configures an analyzer that has both an explicit class as well as nested factories. Example of confusion... https://mail-archives.apache.org/mod_mbox/lucene-solr-user/201207.mbox/%3C1343218757.79608.YahooMailClassic@web121705.mail.ne1.yahoo.com%3E</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4265</id>
      <title>Name eclipse project after the branch: lucene_solr_branch_3x, lucene_solr_branch_4x, lucene_solr_trunk instead of all the same.</title>
      <description>Right now when you run "ant eclipse" all the solr/lucene projects are called "lucene_solr", but what if I have both 4x and 3x, and now trunk? I have to manually edit the classpath. why not name them after the branch they are on.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4271</id>
      <title>Solr LocalParams for Lucene Query Parser</title>
      <description>The Lucene QueryParser should implement Solr's LocalParams syntax directly so that instead of _query_:"{!geodist d=10 p=20.5,30.2}" one could directly use {!geodist d=10 p=20.5,30.2} references: http://wiki.apache.org/solr/LocalParams</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4272</id>
      <title>another idea for updatable fields</title>
      <description>I've been reviewing the ideas for updatable fields and have an alternative proposal that I think would address my biggest concern: not slowing down searching When I look at what Solr and Elasticsearch do here, by basically reindexing from stored fields, I think they solve a lot of the problem: users don't have to "rebuild" their document from scratch just to update one tiny piece. But I think we can do this more efficiently: by avoiding reindexing of the unaffected fields. The basic idea is that we would require term vectors for this approach (as the already store a serialized indexed version of the doc), and so we could just take the other pieces from the existing vectors for the doc. I dont think we should discard the idea because vectors are slow/big today, this seems like something we could fix. Personally I like the idea of not slowing down search performance to solve the problem, I think we should really start from that angle and work towards making the indexing side more efficient, not vice-versa.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4279</id>
      <title>Regenerate Snowball code so its not so heavy</title>
      <description>Spinoff from LUCENE-3841 (and several threads on the list) Currently each SnowballStemmer is pretty heavy since each instance also contains a bunch of Among objects (part of the stemming rules). This normally shouldnt be a problem, except it seems challenging for tomcat users to tune their threadpools (basically they are creating lots of tokenstreams, so lots of SnowballStemmers) Newer snowball just makes these static, and its easy enough to just regenerate so these aren't so heavy, it doesnt fix the real problem but it also doesn't hurt.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4281</id>
      <title>Delegate to default thread factory in NamedThreadFactory</title>
      <description>currently we state that we yield the same behavior as Executors#defaultThreadFactory() but this behavior could change over time even if it is compatible. We should just delegate to the default thread factory instead of creating the threads ourself.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4283</id>
      <title>Support more frequent skip with Block Postings Format</title>
      <description>This change works on the new bulk branch. Currently, our BlockPostingsFormat only supports skipInterval==blockSize. Every time the skipper reaches the last level 0 skip point, we'll have to decode a whole block to read doc/freq data. Also, a higher level skip list will be created only for those df&gt;blockSize^k, which means for most terms, skipping will just be a linear scan. If we increase current blockSize for better bulk i/o performance, current skip setting will be a bottleneck. For ForPF, the encoded block can be easily splitted if we set skipInterval=32*k.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4284</id>
      <title>RFE: stopword filter without lowercase side-effect</title>
      <description>It would appear that accept()-time lowercasing of Tokens is not favourable anymore, due to the @Deprecation of the only constructor in StopFilter that allows this. Please support some way to allow stop-word removal without lowercasing the output: http://stackoverflow.com/questions/11777785</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4285</id>
      <title>Improve FST API usability for mere mortals</title>
      <description>FST technology is something that has brought amazing advances to Lucene, yet the API is hard to use for the vast majority of users like me. I know that performance of FSTs is really important, but surely a lot can be done without sacrificing that. (comments will hold specific ideas and problems)</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4286</id>
      <title>Add flag to CJKBigramFilter to allow indexing unigrams as well as bigrams</title>
      <description>Add an optional flag to the CJKBigramFilter to tell it to also output unigrams. This would allow indexing of both bigrams and unigrams and at query time the analyzer could analyze queries as bigrams unless the query contained a single Han unigram. As an example here is a configuration a Solr fieldType with the analyzer for indexing with the "indexUnigrams" flag set and the analyzer for querying without the flag. &lt;fieldType name="CJK" autoGeneratePhraseQueries="false"&gt; − &lt;analyzer type="index"&gt; &lt;tokenizer class="solr.ICUTokenizerFactory"/&gt; &lt;filter class="solr.CJKBigramFilterFactory" indexUnigrams="true" han="true"/&gt; &lt;/analyzer&gt; &lt;analyzer type="query"&gt; &lt;tokenizer class="solr.ICUTokenizerFactory"/&gt; &lt;filter class="solr.CJKBigramFilterFactory" han="true"/&gt; &lt;/analyzer&gt; &lt;/fieldType&gt; Use case: About 10% of our queries that contain Han characters are single character queries. The CJKBigram filter only outputs single characters when there are no adjacent bigrammable characters in the input. This means we have to create a separate field to index Han unigrams in order to address single character queries and then write application code to search that separate field if we detect a single character Han query. This is rather kludgey. With the optional flag, we could configure Solr as above This is somewhat analogous to the flags in LUCENE-1370 for the ShingleFilter used to allow single word queries (although that uses word n-grams rather than character n-grams.)</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4290</id>
      <title>basic highlighter that uses postings offsets</title>
      <description>We added IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS so you can efficiently compress character offsets in the postings list, but nothing yet makes use of this. Here is a simple highlighter that uses them: it doesn't have many tests or fancy features, but I think its ok for the sandbox/ (maybe with a couple more tests) Additionally I didnt do any benchmarking.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4296</id>
      <title>Update/clean up Maven POMs and documentation</title>
      <description>Remove appassembler-maven-plugin configurations from all POMs - these are unmaintained and bitrotting. Update Hudson CI references -&gt; Jenkins Switch scm URLs to refer to property values, to simplify maintenance Update README.maven to remove mention of modules/, and increase minimum Ant version from 1.7.X to 1.8.2+</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4305</id>
      <title>small exception message improvements for RegExp and SpanNearQuery</title>
      <description>Find attached a suggestion on how two exceptions could be made a bit more verbose for easier debugging.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4312</id>
      <title>Index format to store position length per position</title>
      <description>Mike Mccandless said:TokenStreams are actually graphs. Indexer ignores PositionLengthAttribute.Need change the index format (and Codec APIs) to store an additional int position length per position.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4314</id>
      <title>The specification of DocIdSetIterator is needlessly ambiguous.</title>
      <description>Quoth Lucene at org.apache.lucene.search.DocIdSetIterator.advance: "Advances to the first beyond (see NOTE below) the current whose document number is greater than or equal to &lt;i&gt;target&lt;/i&gt;. [...] NOTE:&lt;/b&gt; when &lt;code&gt; target ≤ current&lt;/code&gt; implementations may opt not to advance beyond their current {@link #docID()} ." However, the same specification contradictorily states that advance must behave as if written: int advance(int target) { int doc; while ((doc = nextDoc()) &lt; target) {} return doc; } That is, with at least one call to nextDoc() always made, unconditionally. This ambiguity can lead to unexpected behavior. In fact, arguably every user of this interface that does not test after every call whether the iterator has exhausted AND has advanced is incorrect. For example, I myself had one experimental implementation (coded against a previous Lucene release) that caused an infinite loop in PhraseScorer.java because, following the above specification, it "opted" not to move the iterator when advance(target) was called with target &lt; current.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4323</id>
      <title>Add max cfs segment size to LogMergePolicy and TieredMergePolicy</title>
      <description>Our application is managing thousands of indexes ranging from a few KB to a few GB in size. To keep the number of files under control and at the same time avoid the overhead of compound file format for large segments, we would like to keep only small segments as CFS. The meaning of "small" here is in absolute byte size terms, not as a percentage of the overall index. It is ok and in fact desirable to have the entire index as CFS as long as it is below the threshold. The attached patch adds a new configuration option maxCFSSegmentSize which sets the absolute limit on the compound file segment size, in addition to the existing noCFSRatio, i.e. the lesser of the two will be used. The default is to allow any size (Long.MAX_VALUE) so that the default behavior is exactly as it was before. The patch is for the trunk as of Aug 23, 2012.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4324</id>
      <title>extend checkJavaDocs.py to methods,constants,fields</title>
      <description>We have a large amount of classes in the source code, its nice that we have checkJavaDocs.py to ensure packages and classes have some human-level description. But I think we need it for methods etc too. (it is also part of our contribution/style guidelines: http://wiki.apache.org/lucene-java/HowToContribute#Making_Changes) The reason is that like classes and packages, once we can enforce this in the build, people will quickly add forgotten documentation soon after their commit when its fresh in their mind. Otherwise, its likely to never happen.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4330</id>
      <title>Add NAIST-jdic support to Kuromoji</title>
      <description>We should look into adding NAIST-jdic support to Kuromoji as this dictionary is better than the current IPADIC. The NAIST-jdic license seems fine, but needs a formal check-off before any inclusion in Lucene.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4332</id>
      <title>Integrate PiTest mutation coverage tool into build</title>
      <description>As discussed briefly on the mailing list, this patch is an attempt to integrate the PiTest mutation coverage tool into the lucene build</description>
      <attachments/>
      <comments>47</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4335</id>
      <title>Builds should regenerate all generated sources</title>
      <description>We have more and more sources that are generated programmatically (query parsers, fuzzy levN tables from Moman, packed ints specialized decoders, etc.), and it's dangerous because developers may directly edit the generated sources and forget to edit the meta-source. It's happened to me several times ... most recently just after landing the BlockPostingsFormat branch. I think we should re-gen all of these in our builds and fail the build if this creates a difference. I know some generators (eg JavaCC) embed timestamps and so always create mods ... we can leave them out of this for starters (or maybe post-process the sources to remove the timestamps) ...</description>
      <attachments/>
      <comments>52</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>4345</id>
      <title>Create a Classification module</title>
      <description>Lucene/Solr can host huge sets of documents containing lots of information in fields so that these can be used as training examples (w/ features) in order to very quickly create classifiers algorithms to use on new documents and / or to provide an additional service. So the idea is to create a contrib module (called 'classification') to host a ClassificationComponent that will use already seen data (the indexed documents / fields) to classify new documents / text fragments. The first version will contain a (simplistic) Lucene based Naive Bayes classifier but more implementations should be added in the future.</description>
      <attachments/>
      <comments>41</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>4360</id>
      <title>Support running the same test suite multiple times in parallel</title>
      <description>The current "test execution multiplier" or: -Dtests.iters=N generates multiple tests (method executions) under a test class (suite). All these tests, however, are bound to a single class so they must run sequentially and on a single JVM (because of how JUnit works – nesting of rules, class hooks, etc.). Mark pointed out that if somebody has a multi-core CPU then it'd be nice to be able to run a single suite in parallel, possibly in combination with tests.iters (so that a single test method is executed X times on Y parallel JVMs). This is surprisingly easy with the randomized runner because it currently accepts "duplicate" suite names and will load-balance them in a normal way. So, if one has Y cores (JVMs) then providing a suite name X times will result in X executions, balanced across Y JVMs. The only problem is how to "multiply" suite names. This can be done in a number of ways, starting from a custom resource collection wrapper and ending at a built-in code in the runner itself. I think the custom collection wrapper approach would be interesting, I'll explore this direction.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4365</id>
      <title>The Maven build can't directly handle complex inter-module dependencies involving the test-framework modules</title>
      <description>The Maven dependency model disallows cyclic dependencies, of which there are now several in the Ant build (considering test and compile dependencies together, as Maven does). All of these cycles involve either the Lucene test-framework or the Solr test-framework. The current Maven build works around this problem by incorporating dependencies' sources into dependent modules' test sources, rather than literally declaring the problematic dependencies as such. (See SOLR-3780 for a recent example of putting this workaround in place for the Solrj module.) But with the factoring out of the Lucene Codecs module, upon which Lucene test-framework has a compile-time dependency, the complexity of the workarounds required to make it all hang together is great enough that I want to attempt a (Maven-build-only) module refactoring. It should require fewer contortions and be more maintainable. The Maven build is currently broken, as of the addition of the Codecs module (LUCENE-4340).</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4368</id>
      <title>make sentinel object in HitQueue a singleton?</title>
      <description>Via jvisualvm, I see that one of my Lucene processes is spending a lot of time in HitQueue.getSentinelObject. That's a very simple method that currently looks like: protected ScoreDoc getSentinelObject() { return new ScoreDoc(Integer.MAX_VALUE, Float.NEGATIVE_INFINITY); } Since the same sentinel is always returned, perhaps the sentinel should be declared as a static variable on HitQueue and then getSentinelObject() would just return that static value?</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4370</id>
      <title>Let Collector know when all docs have been collected</title>
      <description>Collectors are a good point for extension/customization of Lucene/Solr, however sometimes it's necessary to know when the last document has been collected (for example, for flushing cached data). It would be nice to have a method that gets called after the last doc has been collected.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4373</id>
      <title>BBoxStrategy should support query shapes of any type</title>
      <description>It's great that BBoxStrategy has sophisticated shape area similarity based on bounding box, but I think that doesn't have to preclude having a non-rectangular query shape. The bbox to bbox query implemented already is probably pretty pretty fast as can work by numeric range queries, but I'd like this to be the first stage of which the 2nd is a FieldCache based comparison to the query shape if it's not a rectangle.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4374</id>
      <title>Spatial- rename vector.TwoDoublesStrategy to vector.PointVectorStrategy</title>
      <description>TwoDoubles isn't necessarily appropriate since it could be two floats, once it is enhanced to make that configurable. I like PointVector because it's clear it indexes points. Eventually I could imagine a CircleVectorStrategy in the same package. This does suggest BBoxStrategy should be RectVectorStrategy in the vector package.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4376</id>
      <title>Add Query subclasses for selecting documents where a field is empty or not</title>
      <description>Users frequently wish to select documents based on whether a specified sparsely-populated field has a value or not. Lucene should provide specific Query subclasses that optimize for these two cases, rather than force users to guess what workaround might be most efficient. It is simplest for users to use a simple pure wildcard term to check for non-empty fields or a negated pure wildcard term to check for empty fields, but it has been suggested that this can be rather inefficient, especially for text fields with many terms. 1. Add NonEmptyFieldQuery - selects all documents that have a value for the specified field. 2. Add EmptyFieldQuery - selects all documents that do not have a value for the specified field. The query parsers could turn a pure wildcard query (asterisk only) into a NonEmptyFieldQuery, and a negated pure wildcard query into an EmptyFieldQuery. Alternatively, maybe PrefixQuery could detect pure wildcard and automatically "rewrite" it into NonEmptyFieldQuery. My assumption is that if the actual values of the field are not needed, Lucene can much more efficiently simply detect whether values are present, rather than, for example, the user having to create a separate boolean "has value" field that they would query for true or false.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4378</id>
      <title>QueryParsers do not support negative boosts</title>
      <description>Negative query boosts have been supported at the "Query" object level for a long time (resulting in negative scores for matching documents), but evidently we never updated the QueryParsers to know about this - attempting to specify a negative boost in the query string results in a parse error. we should probably add this to the parser grammer(s)</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4385</id>
      <title>IntelliJ build output locations should not be the same as Ant's</title>
      <description>The Eclipse and Maven builds don't share their output directories with Ant. The IntelliJ build should follow suit and put its build output in directories dedicated to that purpose and no other.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4386</id>
      <title>Query parser should generate FieldValueFilter for pure wildcard terms to boost query performance</title>
      <description>In theory, a simple pure wildcard query (a single asterisk) is an inefficient way to select all documents that have any value in a field. Rather than users having to work around this issue by adding a separate boolean "has" field, it would be better to have the query parser directly generate the most efficient Lucene query for detecting all documents that have any value for a specified field. According to the discussion over on LUCENE-4376, the FieldValueFilter is the proper solution. Proposed solution: QueryParserBase.getPrefixQuery could detect when the query is a pure wildcard (a single asterisk) and then generate a FieldValueFilter instead of a PrefixQuery. My understanding from LUCENE-4376 is that the following would work: new ConstantScoreQuery(new FieldValueFilter(fieldname, false)) Oh, and the check for whether "leading wildcard" is enabled would need to be bypassed for this case. I still think it would be better to have PrefixQuery perform this optimization internally so that all apps would benefit, but this should be sufficient to address the main concern. This improvement would improve the classic Lucene query parser and other query parsers based on it, including edismax. There might be other query parsers which won't see the impact of this change, but they can be updated separately. How much performance benefit? Unknown, but supposedly significant. The goal is simply to have a simple pure wildcard be the obvious tool to select fields that have a value in a field.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4388</id>
      <title>ShapeMatcher and ShapeValues</title>
      <description>This patch provides two key interfaces: ShapeMatcher and ShapeValues. The ShapeMatcher concept is borrowed from Ryan McKinley's JtsGeoStrategy which has a similar GeometryTester. ShapeValues is basically a ValueSource/FunctionValues for shapes. This isn't working; I didn't modify any existing classes. I haven't completely thought this through but a SpatialStrategy might expose a makeShapeValues(IndexReader) and/or makeCenterShapeValues(IndexReader) (the latter is the center points of indexed data). A generic Distance ValueSource could easily be implemented in terms of makeCenterShapeValues(). And a strategy could support any query shape simply by implementing makeShapeValues(). I've been thinking about how the API handles strategies supporting indexing multiple shapes and I wonder if that could happen simply via a new MultiShape&lt;Shape&gt;.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4389</id>
      <title>Fix TwoDoubles dateline support</title>
      <description>The dateline support can easily be fixed. After this, the TwoDoublesStrategy might not be particularly useful but at least it won't be buggy if you stay with Rectangle query shapes.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4396</id>
      <title>BooleanScorer should sometimes be used for MUST clauses</title>
      <description>Today we only use BooleanScorer if the query consists of SHOULD and MUST_NOT. If there is one or more MUST clauses we always use BooleanScorer2. But I suspect that unless the MUST clauses have very low hit count compared to the other clauses, that BooleanScorer would perform better than BooleanScorer2. BooleanScorer still has some vestiges from when it used to handle MUST so it shouldn't be hard to bring back this capability ... I think the challenging part might be the heuristics on when to use which (likely we would have to use firstDocID as proxy for total hit count). Likely we should also have BooleanScorer sometimes use .advance() on the subs in this case, eg if suddenly the MUST clause skips 1000000 docs then you want to .advance() all the SHOULD clauses. I won't have near term time to work on this so feel free to take it if you are inspired!</description>
      <attachments/>
      <comments>92</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4399</id>
      <title>Rename AppendingCodec to Appending40Codec</title>
      <description>In order AppendingCodec to follow Lucene codecs version, I think its name should include a version number (so that, for example, if we get to releave Lucene 4.3 with a new Lucene43Codec, there will also be a new Appending43Codec).</description>
      <attachments/>
      <comments>32</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4400</id>
      <title>add support for new commons-codec encoder (nysiis)</title>
      <description>From Thomas on LUCENE-3720: btw. the next release will happen quite soon afaik and will also include a new phonetic encoder called Nysiis, which should perform slightly better than Soundex (see https://issues.apache.org/jira/browse/CODEC-63). Any feedback is very welcome! I didn't do this in LUCENE-3720 because I wanted to fix the bug separately, but this should be pretty easy to add.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4404</id>
      <title>Add ListOfOutputs FST Outputs, replacing UpToTwoPositiveIntOutputs</title>
      <description>Spinoff from LUCENE-3842. This just generalizes the UpToTwoPositiveIntOutputs to a list of any arbitrary output, by wrapping any other Outputs impl. I also made separate methods to write/read a node-final output: since list of values can only occur on a final node output, this impl optimizes and avoids writing an extra byte per label for normal arc labels. This also fixes a bug in Builder that was sometimes failing to join multiple outputs together.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4410</id>
      <title>Make FilteredQuery more flexible with regards to how filters are applied</title>
      <description>Currently FilteredQuery uses either the "old" lucene 3 leap frog approach or pushes the filter down together with accepted docs. Yet there might be more strategies required to fit common usecases like geo-filtering where a rather costly function is applied to each document. Using leap frog this might result in a very slow query if the filter is advanced since it might have linear running time to find the next valid document. We should be more flexible with regards to those usecases and make it possible to either tell FQ what to do or plug in a strategy that applied a filter in a different way. The current FQ impl also uses an heuristic to decide if RA or LeapFrog should be used. This is really an implementation detail of the strategy and not of FQ and should be moved out.</description>
      <attachments/>
      <comments>27</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4412</id>
      <title>Reconsider FunctionValues / ValueSource API</title>
      <description>When documenting a lot of these classes today I found myself confused and it isn't the first time with this API. I think we need to step back and reassess what we want from this API, what use cases its designed to meet, and redesign it from the ground up.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4413</id>
      <title>Better use of exceptions in spatial</title>
      <description>Spatial4j's InvalidShapeException is supposed to be for shape strings that won't parse or invalid coordinates. TwoDoublesStrategy is using it incorrectly to indicate the strategy doesn't support the shape. Perhaps UnsupportedOperationException is more fitting. I kind of prefer it just slightly over IllegalArgumentException because I think of it here as the strategy not supporting that shape. The biggest point here is consistency across the strategies. FYI for context there's also UnsupportedSpatialOperation, an Exception that doesn't follow the naming convention but I'm ok with that. Its c'tor mandates its association with an instance of SpatialOperation. So if a strategy doesn't support an operation then it's got a special exception just for that, but not also true for shape? Seems lopsided to me. I think my preference is for this exception to be UnsupportedSpatialArgument that isn't locked to either the shape or operation. The attached patch doesn't go as far as UnsupportedSpatialArgument but it at least bring consistent behavior.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4419</id>
      <title>Test RecursivePrefixTree indexing non-point data</title>
      <description>RecursivePrefixTreeFilter was modified in ~July 2011 to support spatial filtering of non-point indexed shapes. It seems to work when playing with the capability but it isn't tested. It really needs to be as this is a major feature. I imagine an approach in which some randomly generated rectangles are indexed and then a randomly generated rectangle is queried. The right answer can be calculated brute-force and then compared with the filter. In order to deal with shape imprecision, the randomly generated shapes could be generated to fit a course grid (e.g. round everything to a 1 degree interval).</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4421</id>
      <title>TermsFilter should use TermsEnum.seekExact not seekCeil</title>
      <description>TermsFilter line 82 is: if (termsEnum.seekCeil(br) == TermsEnum.SeekStatus.FOUND) { I expected use of seekExact(...) since the Filter shouldn't need to potentially advance to the one after.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4426</id>
      <title>New ValueSource implementations that wrap DocValues</title>
      <description>We should have ValueSource implementations that wrap DocValues in lucene-queries so that DocValues can be used in function queries.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4427</id>
      <title>remove webapp from lucene/demo</title>
      <description>Spinoff of SOLR-3879: I think the webapp in lucene/demo is a poor "demo" ... we should remove it. EG it does not close its IndexReader, it uses the [very expert] XML QueryParser, it passes Version.LUCENE_CURRENT when creating the StandardAnalyzer ...</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4434</id>
      <title>Make changes-to-html not depend on wget or curl</title>
      <description>The changes-to-html script currently uses WGET or CURL to download the JIRA version lists. When setting up build jobs e.g. on Windows, this makes running changes-to-html a pain. ANT already provides the functionality to download files: Why not download the version list in the ANT macro to a local file and pass this file as an argument to the PERL script? This would also remove the Certificate warnings, as the JDK knows the Thawte certificate and can validate it. I would be happy to assist with that, but my PERL knowledge is limited...</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4440</id>
      <title>FilterCodec should take a delegate Codec in its ctor</title>
      <description>FilterCodec has a delegate() method through which an extension can return its delegate Codec. This method is called on every Codec method. Adrien, on LUCENE-4391, failed to pass a Codec in the ctor, since he couldn't called Codec.forName(). Instead, we should just pass e.g. new Lucene40Codec(). I'll post a patch shortly.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4446</id>
      <title>Switch to BlockPostingsFormat for Lucene 4.1</title>
      <description>This has baked for some time: no crazy fails in hudson or anything. The code (in my opinion) is actually a lot simpler than the current postings format, its faster, the indexes are smaller, and so on. We should probably spend some time just going over the code and adding some more tests and such but I think its time to start looking at cutting over.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4448</id>
      <title>speedups for AnalyzingSuggester</title>
      <description>just some optimizations to the TokenStreamToAutomaton/escape workflow. I still don't understand whats going on with the escaping: this might be suboptimal.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4450</id>
      <title>Distance boost added to Suggester</title>
      <description>A common Suggester use case is to boost the results by closest (auto suggest the whole USA but boost the results in the suggester by geodistance). Would love to get faster response with that. At the Lucene Revolution 2012 in Boston a speaker did discuss using WFST to do this, but I have yet to figure out how to do it).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4458</id>
      <title>some improvements about the latent semantic search, more details are in Description part</title>
      <description>Hi,all,Recently I invented a new ranking algorithm inspired by the theory of spread activation and probabilistic model, which can find the latent semantic relationship between docs and terms and is almost linear time, and I took one afternoon to code and implement this algorithm. And the testing result shows that the speed of this algorithm is much faster than the famous Latent Semantic Analysis algorithm, and the affect is almost as good as the LSA. I wanna share my idea to all of you and add this algorithm to the Lucene project.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4462</id>
      <title>Publishing flushed segments is single threaded and too costly</title>
      <description>Spinoff from http://lucene.markmail.org/thread/4li6bbomru35qn7w The new TestBagOfPostings failed the build because it timed out after 2 hours ... but in digging I found that it was a starvation issue: the 4 threads were flushing segments much faster than the 1 thread could publish them. I think this is because publishing segments (DocumentsWriter.publishFlushedSegment) is actually rather costly (creates CFS file if necessary, writes .si, etc.). I committed a workaround for now, to prevent starvation (see svn diff -c 1394704 https://svn.apache.org/repos/asf/lucene/dev/trunk), but we really should address the root cause by moving these costly ops into flush() so that publishing is a low cost operation.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4470</id>
      <title>Expose SpanFirst in eDismax</title>
      <description>Expose Lucene's SpanFirst capability in Solr's extended Dismax query parser. This issue adds the SF-parameter (SpanFirst) and takes a FIELD~DISTANCE^BOOST formatted value. For example, sf=title~5^2 will give a boost of 2 if one of the normal clauses, originally generated for automatic phrase queries, is located within five positions from the field's start. Unit test is included and all tests pass.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4472</id>
      <title>Add setting that prevents merging on updateDocument</title>
      <description>Currently we always call maybeMerge if a segment was flushed after updateDocument. Some apps and in particular ElasticSearch uses some hacky workarounds to disable that ie for merge throttling. It should be easier to enable this kind of behavior.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>4483</id>
      <title>Make Term constructor javadoc refer to BytesRef.deepCopyOf</title>
      <description>The Term constructor from BytesRef javadoc indicates that a clone needs to be made of the BytesRef. But the clone() method of BytesRef is not what is meant, a deep copy needs to be made.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4487</id>
      <title>SpellChecker : SpellChecker.indexDictionary : Javadoc incorrect</title>
      <description>example code in javadoc seems to be incorrect code changes from LUCENE-3557 means the javadoc SpellChecker spellchecker = new SpellChecker(spellIndexDirectory); // To index a field of a user index: spellchecker.indexDictionary(new LuceneDictionary(my_lucene_reader, a_field)); // To index a file containing words: spellchecker.indexDictionary(new PlainTextDictionary(new File("myfile.txt"))); String[] suggestions = spellchecker.suggestSimilar("misspelt", 5); is incorrect i think something like this is better maybe SpellChecker spellchecker = new SpellChecker(spellIndexDirectory); // To index a field of a user index: spellchecker.indexDictionary(new LuceneDictionary(my_lucene_reader, a_field), new IndexWriterConfig(Version.LUCENE_CURRENT, null), false); // To index a file containing words: spellchecker.indexDictionary(new PlainTextDictionary(new File("myfile.txt")), new IndexWriterConfig(Version.LUCENE_CURRENT, null), false); String[] suggestions = spellchecker.suggestSimilar("misspelt", 5); I'd possibly add something about creating a spellIndexDirectory too, not clear Directory spellIndexDirectory = FSDirectory.open("/dev/tmp/SOME_WORKING_DIR"); SpellChecker spellchecker = new SpellChecker(spellIndexDirectory); // To index a field of a user index: spellchecker.indexDictionary(new LuceneDictionary(my_lucene_reader, a_field), new IndexWriterConfig(Version.LUCENE_CURRENT, null), false); // To index a file containing words: spellchecker.indexDictionary(new PlainTextDictionary(new File("myfile.txt")), new IndexWriterConfig(Version.LUCENE_CURRENT, null), false); String[] suggestions = spellchecker.suggestSimilar("misspelt", 5); would of sent a patch but not familiar with how to get access to the apache git repo todo a pull/push cheers Ant</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4489</id>
      <title>improve LimitTokenCountFilter and/or it's tests</title>
      <description>spinning off a discussion about LimitTokenCountFilter and it's tests from SOLR-3961 (which was about a specific bug in the LimitTokenCountFilterFactory)</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4491</id>
      <title>Make analyzing suggester more flexible</title>
      <description>Today we have a analyzing suggester that is bound to a single key. Yet, if you want to have a totally different surface form compared to the key used to find the suggestion you either have to copy the code or play some super ugly analyzer tricks. For example I want to suggest "Barbar Streisand" if somebody types "strei" in that case the surface form is totally different from the analyzed form. Even one step further I want to embed some meta-data in the suggested key like a user id or some type my surface form could look like "Barbar Streisand|15". Ideally I want to encode this as binary and that might not be a valid UTF-8 byte sequence. I'm actually doing this in production and my only option was to copy the analyzing suggester and some of it's related classes.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4496</id>
      <title>Don't decode unnecessary freq blocks in 4.1 codec</title>
      <description>TermsEnum.docs() has an expert flag to specify you don't require frequencies. This is currently set by some things that don't need it: we should call ForUtil.skipBlock instead of ForUtil.readBlock in this case.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4497</id>
      <title>Don't write posVIntCount in 4.1 codec</title>
      <description>Its confusing and unnecessary that we compute this from docFreq for the doc/freq vint count, but write it for the positions case: its totalTermFreq % BLOCK_SIZE.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4498</id>
      <title>pulse docfreq=1 DOCS_ONLY for 4.1 codec</title>
      <description>We have pulsing codec, but currently this has some downsides: its very general, wrapping an arbitrary postingsformat and pulsing everything in the postings for an arbitrary docfreq/totalTermFreq cutoff reuse is hairy: because it specializes its enums based on these cutoffs, when walking thru terms e.g. merging there is a lot of sophisticated stuff to avoid the worst cases where we clone indexinputs for tons of terms. On the other hand the way the 4.1 codec encodes "primary key" fields is pretty silly, we write the docStartFP vlong in the term dictionary metadata, which tells us where to seek in the .doc to read our one lonely vint. I think its worth investigating that in the DOCS_ONLY docfreq=1 case, we just write the lone doc delta where we would write docStartFP. We can avoid the hairy reuse problem too, by just supporting this in refillDocs() in BlockDocsEnum instead of specializing. This would remove the additional seek for "primary key" fields without really any of the downsides of pulsing today.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4499</id>
      <title>Multi-word synonym filter (synonym expansion)</title>
      <description>I apologize for bringing the multi-token synonym expansion up again. There is an old, unresolved issue at LUCENE-1622 [1] While solving the problem for our needs [2], I discovered that the current SolrSynonym parser (and the wonderful FTS) have almost everything to satisfactorily handle both the query and index time synonym expansion. It seems that people often need to use the synonym filter slightly differently at indexing and query time. In our case, we must do different things during indexing and querying. Example sentence: Mirrors of the Hubble space telescope pointed at XA5 This is what we need (comma marks position bump): indexing: mirrors,hubble|hubble space telescope|hst,space,telescope,pointed,xa5|astroobject#5 querying: +mirrors +(hubble space telescope | hst) +pointed +(xa5|astroboject#5) This translated to following needs: indexing time: single-token synonyms =&gt; return only synonyms multi-token synonyms =&gt; return original tokens AND the synonyms query time: single-token: return only synonyms (but preserve case) multi-token: return only synonyms We need the original tokens for the proximity queries, if we indexed 'hubble space telescope' as one token, we cannot search for 'hubble NEAR telescope' You may (not) be surprised, but Lucene already supports ALL of these requirements. The patch is an attempt to state the problem differently. I am not sure if it is the best option, however it works perfectly for our needs and it seems it could work for general public too. Especially if the SynonymFilterFactory had a preconfigured sets of SynonymMapBuilders - and people would just choose what situation they use. Please look at the unittest. links: [1] https://issues.apache.org/jira/browse/LUCENE-1622 [2] http://labs.adsabs.harvard.edu/trac/ads-invenio/ticket/158 [3] seems to have similar request: http://lucene.472066.n3.nabble.com/Proposal-Full-support-for-multi-word-synonyms-at-query-time-td4000522.html</description>
      <attachments/>
      <comments>10</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4500</id>
      <title>Loosen up DirectSpellChecker's minPrefix requirements</title>
      <description>DirectSpellChecker currently mandates a minPrefix of 1 when editDistance=2. This prohibits a query of "nusglasses" from matching the indexed "sunglasses" term. Granted, there can be performance issues with using a minPrefix of 0, but it's a risk that a user should be allowed to take if needed.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4501</id>
      <title>optimize 4.1 codec's encoding of frequencies</title>
      <description>If we wanted, we could encode freq-1 into the FOR blocks (since it cannot be 0) and save some space.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4512</id>
      <title>Additional memory savings in CompressingStoredFieldsIndex.MEMORY_CHUNK</title>
      <description>Robert had a great idea to save memory with CompressingStoredFieldsIndex.MEMORY_CHUNK: instead of storing the absolute start pointers we could compute the mean number of bytes per chunk of documents and only store the delta between the actual value and the expected value (avgChunkBytes * chunkNumber). By applying this idea to every n(=1024?) chunks, we would even: make sure to never hit the worst case (delta ~= maxStartPointer) reduce memory usage at indexing time.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4515</id>
      <title>Make MemoryIndex more memory efficient</title>
      <description>Currently MemoryIndex uses BytesRef objects to represent terms and holds an int[] per term per field to represent postings. For highlighting this creates a ton of objects for each search that 1. need to be GCed and 2. can't be reused.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4516</id>
      <title>Suggesters: allow to associate a user-specified key (int) with a string</title>
      <description>As a user, I'd like to associate a “foreign key” with a string (rather: final node) in the suggester index (in addition to the rank). For example, I’d like to add “Lucene in Action” with key 1933988177 (the ISBN) and some rank to a WFST or AnalyzingSuggester. A completion would return the completed string and the key associated with each entry (i.e. final nodes get a “key” field (int), which is returned in the LookupResult). That foreign key could also be used for fast de-duping (no more string/byte array comparisons). There may be workarounds for the “foreign key” use case –it seems that lots of data structures would be affected by storing a user-provided key with final nodes, which therefore may not be a viable path. It may be possible to encode the foreign key in the transducer’s output instead. Discussion on java-user@lucene: Mike McCandless: This is maybe the same idea as LUCENE-4491 ? Could you simply stuff your ISBN onto the end of the suggestion (ie enroll Lucene in Action|1933988177)? Dawid Weiss: Just remember that if your suffixes are unique then you'll be expanding the automaton quite a bit (unique suffix paths). D. Mike: That's a good point... encoding into the FST's output may be better.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4517</id>
      <title>Suggesters: allow to pass a user-defined predicate/filter to the completion searcher</title>
      <description>As a user, I'd like to be able to specify a filter during completion lookup which further determines whether some completion should be considered or not. Assume, for example, that I have a suggestion engine for book titles. In my current search, I'm only interested in computer science books, but I can't/don't want to maintain separate WFSTs for each subject area. Given some completion candidate, the filter would be called (with a key and/or the completion string as a parameter) to determine whether or not the completion candidate should be added to the result queue. Note: Adding a filter/predicate to the AnalyzingSuggester is simple, as TopNSearcher&lt;&gt; already uses acceptResult() to test whether some completion should be added - that can be overridden in a derived searcher class which simply calls the predicate. Ideally the suggesters would access some kind of factory to instantiate the searcher to be used (instead of hardwiring it in). Discussion on java-user: Mike McCandless: Exactly! One gotchya is you have to be careful about the maxQueueDepth, because if your acceptResult accepts too few results then the queue may have pruned away paths that would have led to a valid topN path ... We may also invert all of these FST based suggests, and expose building blocks for apps to build up custom suggesters. There are many use cases we need to accommodate and we have a ways to converge on a clear API here ...</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4518</id>
      <title>Suggesters: highlighting (explicit markup of user-typed portions vs. generated portions in a suggestion)</title>
      <description>As a user, I would like the lookup result of the suggestion engine to contain information which allows me to distinguish the user-entered portion from the autocompleted portion of a suggestion. That information can then be used for e.g. highlighting. Notes: It's trivial if the suggestion engine only applies simple prefix search, as then the user-typed prefix is always a true prefix of the completion. However, it's non-trivial as soon as you use an AnalyzingSuggester, where the completion may (in extreme cases) be quite different from the user-provided input. As soon as case/diacritics folding, script adaptation (kanji/hiragana) come into play, the completion is no longer guaranteed to be an extension of the query. Since the caller of the suggestion engine (UI) generally does not know the implementation details, the required information needs to be passed in the LookupResult. Discussion on java-user: &gt; I haven't found a simple solution for the highlighting yet, &gt; particularly when using AnalyzingSuggester (where it's non-trivial). Mike McCandless: Ahh I see ... it is challenging in that case. Hmm. Maybe open an issue for this as well, so we can discuss/iterate?</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4520</id>
      <title>ValueSource.getSortField shouldn't throw IOException</title>
      <description>ValueSource.getSortField just returns a new ValueSourceSortField, whose constructor doesn't declare any checked exceptions. So adding the throws clause to the method declaration means adding pointless try-catch warts to client code.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4524</id>
      <title>Merge DocsEnum and DocsAndPositionsEnum into PostingsEnum</title>
      <description>spinnoff from http://www.gossamer-threads.com/lists/lucene/java-dev/172261 hey folks, I have spend a hell lot of time on the positions branch to make positions and offsets working on all queries if needed. The one thing that bugged me the most is the distinction between DocsEnum and DocsAndPositionsEnum. Really when you look at it closer DocsEnum is a DocsAndFreqsEnum and if we omit Freqs we should return a DocIdSetIter. Same is true for DocsAndPostionsAndPayloadsAndOffsets*YourFancyFeatureHere*Enum. I don't really see the benefits from this. We should rather make the interface simple and call it something like PostingsEnum where you have to specify flags on the TermsIterator and if we can't provide the sufficient enum we throw an exception? I just want to bring up the idea here since it might simplify a lot for users as well for us when improving our positions / offset etc. support. thoughts? Ideas? simon</description>
      <attachments/>
      <comments>28</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>4525</id>
      <title>JavaDoc of SearcherManager#close is not crystal clear</title>
      <description>Raised from http://www.mail-archive.com/java-user@lucene.apache.org/msg40064.html The JavaDoc says: /** * Close this ReferenceManager to future {@link #acquire() acquiring}. Any * references that were previously {@link #acquire() acquired} won't be * affected, and they should still be {@link #release released} when they are * not needed anymore. */ The first sentence is not really clear. I would expect something like: Close this ReferenceManager when the application is shutting down or the underlying index will be disposed. Any references that were previously {@link #acquire() acquired} won't be affected, and they should still be {@link #release released} when they are not needed anymore. Further more, the JavaDoc does not declare that an exception will be thrown on any method.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4527</id>
      <title>CompressingStoredFieldsFormat: encode numStoredFields more efficiently</title>
      <description>Another interesting idea from Robert: many applications have a schema and all documents are likely to have the same number of stored fields. We could save space by using packed ints and the same kind of optimization as ForUtil (requiring only one VInt if all values are equal).</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4528</id>
      <title>Slf4JInfoStream - sends InfoStream messages to SLF4J</title>
      <description>InfoStream doesn't play well with logging. With Slf4JInfoStream, users can send InfoStream messages to the logging library of their choice for processing. Hooray!</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4537</id>
      <title>Move RateLimiter up to Directory and make it IOContext aware</title>
      <description>Currently the RateLimiter only applies to FSDirectory which is fine in general but always requires casts and other dir. impls (custom ones could benefit from this too.) We are also only able to rate limit merge operations which limits the functionality here a lot. Since we have the context information what the IndexOutput is used for we can use that for rate limiting.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4538</id>
      <title>Cache DocValues DirecSource</title>
      <description>Currently the user need to make sure that a direct source is not shared between threads and each time someone calls getDirectSource we create a new source which has a reasonable overhead. We can certainly reduce the overhead (maybe different issue) but it should be easier for the user to get a direct source and handle it. More than that is should be consistent with getSource / loadSource.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4541</id>
      <title>Easier way to access ValueSource given docid</title>
      <description>I was working on improving some sample code and I needed to access a value via a ValueSource given a docid in search results. I was disappointed to see how many steps were needed. Incidentally, Solr had to solve the same problem, and it's got ValueSourceAugmenter for this. I propose a small utility class be created to do something similar, such that the client can merely create an instance and then call a method with a doc id to retrieve the value.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4542</id>
      <title>Make RECURSION_CAP in HunspellStemmer configurable</title>
      <description>Currently there is private static final int RECURSION_CAP = 2; in the code of the class HunspellStemmer. It makes using hunspell with several dictionaries almost unusable, due to bad performance (f.ex. it costs 36ms to stem long sentence in latvian for recursion_cap=2 and 5 ms for recursion_cap=1). It would be nice to be able to tune this number as needed. AFAIK this number (2) was chosen arbitrary. (it's a first issue in my life, so please forgive me any mistakes done).</description>
      <attachments/>
      <comments>24</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>4545</id>
      <title>Better error reporting StemmerOverrideFilterFactory</title>
      <description>If the dictionary contains an error such as a space instead of a tab somewhere in the dictionary it is hard to find the error in a long dictionary. This patch includes the file and line number in the exception, helping to debug it quickly.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4549</id>
      <title>Allow variable buffer size on BufferedIndexOutput</title>
      <description>BufferedIndexInput allows to set the buffersize but BufferedIndexOutput doesn't this could be useful for optimizations related to LUCENE-4537. We should make the apis here consistent.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4554</id>
      <title>CompressingStoredFieldsFormat: don't write the original length at the beginning of the chunk</title>
      <description>Instead the total length should be computed by summing up the lengths of all documents in the chunk.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4556</id>
      <title>FuzzyTermsEnum creates tons of objects</title>
      <description>I ran into this problem in production using the DirectSpellchecker. The number of objects created by the spellchecker shoot through the roof very very quickly. We ran about 130 queries and ended up with &gt; 2M transitions / states. We spend 50% of the time in GC just because of transitions. Other parts of the system behave just fine here. I talked quickly to robert and gave a POC a shot providing a LevenshteinAutomaton#toRunAutomaton(prefix, n) method to optimize this case and build a array based strucuture converted into UTF-8 directly instead of going through the object based APIs. This involved quite a bit of changes but they are all package private at this point. I have a patch that still has a fair set of nocommits but its shows that its possible and IMO worth the trouble to make this really useable in production. All tests pass with the patch - its a start....</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4558</id>
      <title>Make CompressingStoredFieldsFormat more flexible</title>
      <description>My plan consists in making CompressionMode an abstract class instead of an enum and having different codec names per CompressionMode. I think this has two main benefits: it makes Lucene41StoredFieldsFormat cleaner (no need to write a CompressionMode id), it allows for custom CompressionModes.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4560</id>
      <title>Support Filtering Segments During Merge</title>
      <description>Spun off from LUCENE-4557 It is desirable to be able to filter segments during merge. Most often, full reindex of content is not possible. Merging segments can sometimes have negative consequences when fields are have different options (most restrictive option is forced during merge) Being able to filter segments during merges will allow gradually migrating indexed data to new index settings, support pruning/enhancing existing data gradually Use Cases: Migrate IndexOptions for fields (See LUCENE-4557) Gradually Remove index fields no longer used Migrate indexed sort fields to DocValues Support converting data types for indexed data and so on patch will be forthcoming</description>
      <attachments/>
      <comments>21</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4562</id>
      <title>Pair-wise FST key comparator by ords</title>
      <description>It would be useful to have an FST utility method to do a compare() operation between a key in one FST and a key in a second FST, by ords instead of the keys. So the input is the ord for FST1 and an ord for FST2 and the output is -1, 0, 1. The result is the same as if you were to do a Util.getByOutput for both ords against their respective FSTs then compare the resulting byte arrays. The point of this is to speedup LUCENE-3729 further, which impact sorting across segments. I would be surprised if it doesn't have applicability to other problems.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4563</id>
      <title>DirTaxoWriter's Codec - rely on default or use custom?</title>
      <description>Today, DirTaxoWriter opens an IndexWriter using the default Codec. While running tests, I noticed that some of them take a veeeeery long time to complete, some times. Debugging, I realized that they use SimpleText codec b/c that's what the test-framework drew at random. That got me to think if we should really depend on the default Codec, or use a special codec that is more suitable for the taxonomy index's unique characteristics. Basically, the taxonomy index has two fields: One in which the category path is saved, as StringField, and therefore each term is associated with exactly one document Another field with one term, such that a category's parent is written in the position of that term for every document. Initially, I thought that we should really be using PulsingCodec. After a brief chat about it w/ Robert, he said that Lucene41 Codec acts like pulsing for fields like that. So I'm thinking that we should either: Hard-code to Lucene41, if it's indeed useful. Write a completely new Codec, that is special for that case. I.e. Lucene41 may handle these cases efficiently, but its code needs to be prepared for other cases too, therefore we may be able to write something more efficient. I open that as a placeholder, I think that we should first come up w/ a decent benchmark test in order to validate the results. The benchmark package now contains some facet related stuff, so I'll take a look if that's enough.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4564</id>
      <title>Add taxonomy index upgrade utility</title>
      <description>Currently there's no way for an app to upgrade a taxonomy index to the newest index format. The problem is, that unlike search indexes which may merge segments often, the taxonomy index is not likely to do many merges. At some point, most taxonomies become fixed (i.e. new categories are not/rarely added), and therefore it could be that some old segments will never get merged. When we'll release Lucene 5.0, support for 3x indexes will be removed, and so taxonomies that were created w/ 3x won't be read anymore. While one can use IndexUpgrader (I think) to upgrade the taxonomy index, it may not be so trivial for users to realize that, as it may not be so evident from DirTaxoWriter/Reader API that there's a regular Lucene index behind the scenes. A tool, like TaxonomyUpgraderTool, even if simple and using IndexUpgrader, may make it more convenient for user to upgrade their taxonomy index. Opening as a placeholder for 5.0. Also marking as blocker, so we don't forget about it before the release.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4565</id>
      <title>Simplify TaxoReader ParentArray/ChildrenArrays</title>
      <description>TaxoReader exposes two structures which provide information about a categories parent/childs/siblings: ParentArray and ChildrenArrays. ChildrenArrays are derived (i.e. created) from ParentArray. I propose to consolidate all that into one API ParentInfo, or CategoryTreeInfo (a better name?) which will provide the same information, only from one object. So instead of making these calls: int[] parents = taxoReader.getParentArray(); int[] youngestChilds = taxoReader.getChildrenArrays().getYoungestChildArray(); int[] olderSiblings = taxoReader.getChildrenArrays().getOlderSiblingArray(); one would make these calls: int[] parents = taxoReader.getParentInfo().parents(); int[] youngestChilds = taxoReader.getParentInfo().youngestChilds(); int[] olderSiblings = taxoReader.getParentInfo().olderSiblings(); Not a big change, just consolidate more code into one logical place. All of these arrays will continue to be lazily allocated.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4569</id>
      <title>Allow customization of column stride field and norms via indexing chain</title>
      <description>We are building an in-memory indexing format and managing our own segments. We are doing this by implementing a custom IndexingChain. We would like to support column-stride-fields and norms without having to wire in a codec (since we are managing our postings differently) Suggested change is consistent with the api support for passing in a custom InvertedDocConsumer.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4570</id>
      <title>Release ForbiddenAPI checker on Google Code</title>
      <description>Currently there is source code in lucene/tools/src (e.g. Forbidden APIs checker ant task). It would be convenient if you could download this thing in your ant build from ivy (especially if maybe it included our definitions .txt files as resources). In general checking for locale/charset violations in this way is a pretty general useful thing for a server-side app. Can we either release lucene-tools.jar as an artifact, or maybe alternatively move this somewhere else as a standalone project and suck it in ourselves?</description>
      <attachments/>
      <comments>45</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4571</id>
      <title>speedup disjunction with minShouldMatch</title>
      <description>even minShouldMatch is supplied to DisjunctionSumScorer it enumerates whole disjunction, and verifies minShouldMatch condition on every doc: public int nextDoc() throws IOException { assert doc != NO_MORE_DOCS; while(true) { while (subScorers[0].docID() == doc) { if (subScorers[0].nextDoc() != NO_MORE_DOCS) { heapAdjust(0); } else { heapRemoveRoot(); if (numScorers &lt; minimumNrMatchers) { return doc = NO_MORE_DOCS; } } } afterNext(); if (nrMatchers &gt;= minimumNrMatchers) { break; } } return doc; } Stefan Pohl proposes (as well as I get it) to pop nrMatchers-1 scorers from the heap first, and then push them back advancing behind that top doc. For me the question no.1 is there a performance test for minShouldMatch constrained disjunction.</description>
      <attachments/>
      <comments>46</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>4573</id>
      <title>Improve SpatialExample.java to show distance value retrieval</title>
      <description>I want to have SpatialExample.java depict how to get the distance. I also observed that SpatialExample.java isn't tested, despite it being a Junit test. The reason is its name doesn't begin or end in "Test". As a small hack, a test method can be added elsewhere to call it. Patch to follow...</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4575</id>
      <title>Allow IndexWriter to commit, even just commitData</title>
      <description>Spinoff from here http://lucene.472066.n3.nabble.com/commit-with-only-commitData-td4022155.html. In some cases, it is valuable to be able to commit changes to the index, even if the changes are just commitData. Such data is sometimes used by applications to register in the index some global application information/state. The proposal is: Add a setCommitData() API and separate it from commit() and prepareCommit() (simplify their API) When that API is called, flip on the dirty/changes bit, so that this gets committed even if no other changes were made to the index. I will work on a patch a post.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4578</id>
      <title>ICUTokenizerFactory - per-script RBBI customization</title>
      <description>Initially this started out as an idea for a configuration knob on ICUTokenizer that would allow me to tell it not to tokenize on punctuation. Through IRC discussion on #lucene, it sorta ballooned. The committers had a long discussion about it that I don't really understand, so I'll be including it in the comments. I am a Solr user, so I would also need the ability to access the configuration from there, likely either in schema.xml or solrconfig.xml.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4580</id>
      <title>Facet DrillDown should return a ConstantScoreQuery</title>
      <description>DrillDown is a helper class which the user can use to convert a facet value that a user selected into a Query for performing drill-down or narrowing the results. The API has several static methods that create e.g. a Term or Query. Rather than creating a Query, it would make more sense to create a Filter I think. In most cases, the clicked facets should not affect the scoring of documents. Anyway, even if it turns out that it must return a Query (which I doubt), we should at least modify the impl to return a ConstantScoreQuery.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4586</id>
      <title>Change default ResultMode of FacetRequest to PER_NODE_IN_TREE</title>
      <description>Today the default ResultMode is GLOBAL_FLAT, but it should be PER_NODE_IN_TREE. ResultMode is being used whenever you set the depth of FacetRequest to greater than 1. The difference between the two is: PER_NODE_IN_TREE would then compute the top-K categories recursively, for every top category at every level (up to depth). The results are returned in a tree structure as well. For instance: Date 2010 March February 2011 April May GLOBAL_FLAT computes the top categories among all the nodes up to depth, and returns a flat list of categories. GLOBAL_FLAT is faster to compute than PER_NODE_IN_TREE (it just computes top-K among N total categories), however I think that it's less intuitive, and therefore should not be used as a default. In fact, I think this is kind of an expert usage.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4590</id>
      <title>WriteEnwikiLineDoc which writes Wikipedia category pages to a separate file</title>
      <description>It may be convenient to split Wikipedia's line file into two separate files: category-pages and non-category ones. It is possible to split the original line file with grep or such. It is more efficient to do it in advance.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4591</id>
      <title>Make StoredFieldsFormat more configurable</title>
      <description>The current StoredFieldsFormat are implemented with the assumption that only one type of StoredfieldsFormat is used by the index. We would like to be able to configure a StoredFieldsFormat per field, similarly to the PostingsFormat. There is a few issues that need to be solved for allowing that: 1) allowing to configure a segment suffix to the StoredFieldsFormat 2) implement SPI interface in StoredFieldsFormat 3) create a PerFieldStoredFieldsFormat We are proposing to start first with 1) by modifying the signature of StoredFieldsFormat#fieldsReader and StoredFieldsFormat#fieldsWriter so that they use SegmentReadState and SegmentWriteState instead of the current set of parameters. Let us know what you think about this idea. If this is of interest, we can contribute with a first path for 1).</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4598</id>
      <title>Change PayloadIterator to not use top-level reader API</title>
      <description>Currently the facet module uses MultiFields.* to pull the D&amp;PEnum in PayloadIterator, to access the payloads that store the facet ords. It then makes heavy use of .advance and .getPayload to visit all docIDs in the result set. I think we should get some speedup if we go segment by segment instead ...</description>
      <attachments/>
      <comments>24</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4600</id>
      <title>Explore facets aggregation during documents collection</title>
      <description>Today the facet module simply gathers all hits (as a bitset, optionally with a float[] to hold scores as well, if you will aggregate them) during collection, and then at the end when you call getFacetsResults(), it makes a 2nd pass over all those hits doing the actual aggregation. We should investigate just aggregating as we collect instead, so we don't have to tie up transient RAM (fairly small for the bit set but possibly big for the float[]).</description>
      <attachments/>
      <comments>47</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4602</id>
      <title>Use DocValues to store per-doc facet ord</title>
      <description>Spinoff from LUCENE-4600 DocValues can be used to hold the byte[] encoding all facet ords for the document, instead of payloads. I made a hacked up approximation of in-RAM DV (see CachedCountingFacetsCollector in the patch) and the gains were somewhat surprisingly large: Task QPS base StdDev QPS comp StdDev Pct diff HighTerm 0.53 (0.9%) 1.00 (2.5%) 87.3% ( 83% - 91%) LowTerm 7.59 (0.6%) 26.75 (12.9%) 252.6% ( 237% - 267%) MedTerm 3.35 (0.7%) 12.71 (9.0%) 279.8% ( 268% - 291%) I didn't think payloads were THAT slow; I think it must be the advance implementation? We need to separately test on-disk DV to make sure it's at least on-par with payloads (but hopefully faster) and if so ... we should cutover facets to using DV.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4603</id>
      <title>The test framework should report forked JVM PIDs upon heartbeats</title>
      <description>This would help in getting a stack trace of a hung JVM before the timeout and/or in killing the offending JVM. RR issue: https://github.com/carrotsearch/randomizedtesting/issues/135</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4604</id>
      <title>Implement OrdinalPolicy.NO_PARENTS</title>
      <description>Over at LUCENE-4602, Mike explored the idea of writing just the leaf nodes in the fulltree posting, rather than the full hierarchy. I wrote this simple OrdinalPolicy which achieves that: DefaultFacetIndexingParams indexingParams = new DefaultFacetIndexingParams() { @Override protected OrdinalPolicy fixedOrdinalPolicy() { return new OrdinalPolicy() { public void init(TaxonomyWriter taxonomyWriter) {} public boolean shouldAdd(int ordinal) { return false; } }; } }; I think that we should add it as a singleton class to OrdinalPolicy.EXACT_CATEGORIES_ONLY, as wel as make DefaultOrdPolicy as singleton too, under the name FULL_HIERARCHY (feel free to suggest a better name).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4605</id>
      <title>Add FLAG_NONE to DocsEnum and DocsAndPositionsEnum</title>
      <description>Add a convenience constants FLAGS_NONE to DocsEnum and DocsAndPositionsEnum. Today, if someone e.g. wants to get the docs only, he needs to pass 0 as the flags, but the value of 0 is not documented anywhere. I had to dig in the code the verify that indeed that's the value. I'll attach a patch later.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4609</id>
      <title>Write a PackedIntsEncoder/Decoder for facets</title>
      <description>Today the facets API lets you write IntEncoder/Decoder to encode/decode the category ordinals. We have several such encoders, including VInt (default), and block encoders. It would be interesting to implement and benchmark a PackedIntsEncoder/Decoder, with potentially two variants: (1) receives bitsPerValue up front, when you e.g. know that you have a small taxonomy and the max value you can see and (2) one that decides for each doc on the optimal bitsPerValue, writes it as a header in the byte[] or something.</description>
      <attachments/>
      <comments>35</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4610</id>
      <title>Implement a NoParentsAccumulator</title>
      <description>Mike experimented with encoding just the exact categories ordinals on LUCENE-4602, and I added OrdinalPolicy.NO_PARENTS, with a comment saying that this requires a special FacetsAccumulator. The idea is to write the exact categories only for each document, and then at search time count up the parents chain to compute requested facets (I say count, but it can be any weight). One limitation of such accumulator is that it cannot be used when e.g. a document is associated with two categories who share the same parent, because that may result in incorrect weights computed (e.g. a document might have several Authors, and so counting the Author facet may yield wrong counts). So it can be used only when the app knows it doesn't add such facets, or that it always asks to aggregate a 'root' that in its path this criteria doesn't hold (no categories share the same parent).</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4614</id>
      <title>Create dev-tools/eclipse/dot.classpath automatically</title>
      <description>It is a pain to keep the file up-to-date. As it is pure XML we can use a template to produce it automatically. The same trikc like for creating index.html in the docs is used. The patch will produce it automatically from filesets/dirsets in ant. It is still a pain with the duplicate JARs, but maybe we can fix that later.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4616</id>
      <title>Clarify what the score means in SpatialStrategy#makeQuery()</title>
      <description>SpatialStrategy#makeQuery() returns a Query, but the docs don't make it clear with the score value should be.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4619</id>
      <title>Create a specialized path for facets counting</title>
      <description>Mike and I have been discussing that on several issues (LUCENE-4600, LUCENE-4602) and on GTalk ... it looks like the current API abstractions may be responsible for some of the performance loss that we see, compared to specialized code. During our discussion, we've decided to target a specific use case - facets counting and work on it, top-to-bottom by reusing as much code as possible. Specifically, we'd like to implement a FacetsCollector/Accumulator which can do only counting (i.e. respects only CountFacetRequest), no sampling, partitions and complements. The API allows us to do so very cleanly, and in the context of that issue, we'd like to do the following: Implement a FacetsField which takes a TaxonomyWriter, FacetIndexingParams and CategoryPath (List, Iterable, whatever) and adds the needed information to both the taxonomy index as well as the search index. That API is similar in nature to CategoryDocumentBuilder, only easier to consume – it's just another field that you add to the Document. We'll have two extensions for it: PayloadFacetsField and DocValuesFacetsField, so that we can benchmark the two approaches. Eventually, one of them we believe, will be eliminated, and we'll remain w/ just one (hopefully the DV one). Implement either a FacetsAccumulator/Collector which takes a bunch of CountFacetRequests and returns the top-counts. Aggregations are done in-collection, rather than post. Note that we have LUCENE-4600 open for exploring that. Either we finish this exploration here, or do it there. Just FYI that the issue exists. Reuses the CategoryListIterator, IntDecoder and Aggregator code. I'll open a separate issue to explore improving that API to be bulk, and then we can decide if this specialized Collector should use those abstractions, or be really optimized for the facet counting case. At the moment, this path will assume that a document holds multiple dimensions, but only one value from each (i.e. no Author/Shai, Author/Mike for a document), and therefore use OrdPolicy.NO_PARENTS. Later, we'd like to explore how to have this specialized path handle the ALL_PARENTS case too, as it shouldn't be so hard to do.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4620</id>
      <title>Explore IntEncoder/Decoder bulk API</title>
      <description>Today, IntEncoder/Decoder offer a streaming API, where you can encode(int) and decode(int). Originally, we believed that this layer can be useful for other scenarios, but in practice it's used only for writing/reading the category ordinals from payload/DV. Therefore, Mike and I would like to explore a bulk API, something like encode(IntsRef, BytesRef) and decode(BytesRef, IntsRef). Perhaps the Encoder can still be streaming (as we don't know in advance how many ints will be written), dunno. Will figure this out as we go. One thing to check is whether the bulk API can work w/ e.g. facet associations, which can write arbitrary byte[], and so may decoding to an IntsRef won't make sense. This too we'll figure out as we go. I don't rule out that associations will use a different bulk API. At the end of the day, the requirement is for someone to be able to configure how ordinals are written (i.e. different encoding schemes: VInt, PackedInts etc.) and later read, with as little overhead as possible.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4621</id>
      <title>FacetIndexing/SearchParams house cleaning</title>
      <description>FacetIndexingParams lets you configure few things such as OrdinalPolicy, PathPolicy and partitionSize. However, in order to set them you must extend DefaultFacetIndexingParams and override fixedXY(), as the respective getters are final. I'd like to do the following: Turn FacetIndexingParams and FacetSearchParams into concrete classes, rather than interfaces. The reason they are interfaces because one app once wants to have one class which implements both. Since FSP holds FIP, I don't think that's needed. Add setters to FacetIndexingParams for the relevant configuration parameters, rather than forcing someone to extend the class. Extensions should really be for special cases, which we haven't identified so far (except overriding these settings), hence why there's only DefaultFIP/FSP. Add some javadocs... I'm sure, w/ my pedantic and perfectionist nature, that more thing will be done once I get to it .</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4623</id>
      <title>facets should index drill-down fields using DOCS_ONLY</title>
      <description>Today we index as DOCS_AND_POSITIONS, which is necessary because we stuff the payload into one of those tokens. If we indexed under two fields instead, then we could make the drill-down field DOCS_ONLY. But ... once/if we cutover to doc values then we could use one field again.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4625</id>
      <title>Make TotalFacetCounts per-segment</title>
      <description>TotalFacetCounts are used during complements computation today. They are not per-segment and therefore are not NRT friendly. Even regardless to NRT, you need to compute them entirely from scratch whenever you reopen IR. It would be good if we can develop them per-segment. If e.g. AtomicReader had a notion of cachable objects, it could be such an object. That has been discussed many times in the past though, without a consensus. So perhaps we can have a FacetsAtomicReader which manages TFC. But that creates other issues too, like who instantiates that AtomicReader (i.e. we'd need a FacetsCompositeReader too, and potentially IW would need to init that type) ... Let's explore these options, but in general it would be good to have TFC per-segment.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4627</id>
      <title>Migration layer for facets</title>
      <description>Spin-off from LUCENE-4602 (and LUCNE-4623). It will be good if we can develop some migration layer so that users don't need to re-index their content when we change how facets are written in the index. Currently the two open issues are cut over to DV and index drill-down terms w/ DOCS_ONLY, but in the future there could be other changes. I don't think that this layer needs to be very heavy. Something in the form of a FacetsAtomicReaderWrapper. For instance, to support the DV migration, we can implement a PayloadFacetsAtomicReader which translates the payload to DV API (i.e. its docValues() API will actually read from the payload). We'd need some API on IW I think to initialize that reader, so that data can be migrated from payload to DV during segment merges.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4628</id>
      <title>Add common terms query to gracefully handle very high frequent terms dynamically</title>
      <description>I had this problem quite a couple of times the last couple of month that searches very often contained super high frequent terms and disjunction queries became way too slow. The main problem was that stopword filtering wasn't really an option since in the domain those high-freq terms where not really stopwords though. So for instance searching for a song title "this is it" or for a band "A" didn't really fly with stopwords. I thought about that for a while and came up with a query based solution that decides based on a threshold if something is considered a stopword or not and if so it moves the term in two boolean queries one for high-frequent and one for low-frequent such that those high frequent terms are only matched if the low-frequent sub-query produces a match. Yet if all terms are high frequent it makes the entire thing a Conjunction which gave me reasonable results as well as performance.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4631</id>
      <title>Add utility class for getting information about suspicious system concerns at runtime</title>
      <description>(Inspired by an idea Uwe mentioned in LUCENE-4630) Over the years, we've discovered various bugs in JVMs, specific JVM options, combinations of JVM options, combinations of JVMs and operating systems, etc... We've done a fairly decent job of alerting users to these known issues in documentation, and we've attempted to make tests not give false failures if people use these setups – but there is probably more we could be doing to alert users who may not run tests, or read all the docs/blogs. I think we should add a new class to Lucene Core that provides a simple API for applications to get a list of "warning" Strings based on what lucene can dedect from the currently running JVM/OS. This should be something trivial for people writing apps using lucene to integrate into their code and/or tests (even if they don't use the LuceneTestCase) to notify them if when there are known compatibility issues between their enviornment and the version of lucene this class ships with. This code should be executed, and all of the resulting warning messages outputed by... the CheckIndex command line mode on startup the Lucene Demo app(s) on startup infostream (if used) the first time it's configured on an IndexWriterConfig Solr startup logs Solr admin UI (as exposed by the Solr SystemInfoRequestHandler HTTP API) in the Lucene test framework after any test runs ...in addition, specific logic/messages in this code could help implement the annotations Dawid is suggesting in LUCENE-4630.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4632</id>
      <title>Restrict test-framework's write permissions more: Only allow write to per-JVM CWD and the clover.db.dir</title>
      <description>Currently we restring wrting to tests.tempDir after SOLR-4195, but it would be better to restrict more and only let child JVMs write to their working dir and not outside (and maybe corrumpt other JVMs). The problem with current setup is that the child JVM's policy file does not know the runner number nor the absolute working directory (it must be absolute and platform-specific with backslash/slash/... -&gt; new File(".").getAbsolutePath(). Dawid will release a new Junit4 package with a new sysprop passed to every child with its full CWD: junit4.childvm.cwd In that case the policy file would use this property (and the clover.db.dir) to allow write access and allow only read/execute access for the rest of the filesystem.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4634</id>
      <title>PackedInts: streaming API that supports variable numbers of bits per value</title>
      <description>It could be convenient to have a streaming API (writers and iterators, no random access) that supports variable numbers of bits per value. Although this would be much slower than the current fixed-size APIs, it could help save bytes in our codec formats. The API could look like: Iterator { long next(int bitsPerValue); } Writer { void write(long value, int bitsPerValue); // assert PackedInts.bitsRequired(value) &lt;= bitsPerValue; }</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4636</id>
      <title>Upgrade ivy for IVY-1388 - build hangs at "resolve:"</title>
      <description>For certain failures during a lucene/solr build, or if you press ctrl-c at the wrong moment during the build, ivy may leave a lockfile behind. The next time you run a build, ivy will hang with "resolve:" on the screen. The ivy project has a fix, currently not yet released. When it does get released, the version installed by the ivy-bootstrap target needs to be updated.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4642</id>
      <title>Add create(AttributeFactory) to TokenizerFactory and subclasses with ctors taking AttributeFactory, and remove Tokenizer's and subclasses' ctors taking AttributeSource</title>
      <description>All tokenizer implementations have a constructor that takes a given AttributeSource as parameter (LUCENE-1826). These should be removed. TokenizerFactory does not provide an API to create tokenizers with a given AttributeFactory, but quite a few tokenizers have constructors that take an AttributeFactory. TokenizerFactory should add a create(AttributeFactory) method, as should subclasses for tokenizers with AttributeFactory accepting ctors.</description>
      <attachments/>
      <comments>46</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4647</id>
      <title>Simplify CategoryDocumentBuilder</title>
      <description>CategoryDocumentBuilder is used to add facet fields to a document. Today the usage is not so straightforward, and I'd like to simplify it. First, to improve usage but also to make cutover to DocValues easier. This clsas does two operations: (1) adds drill-down terms and (2) creates the fulltree payload. Today, since it does it all on terms, there's a hairy TokenStream which does both these operations in one go. For simplicity, I'd like to break this into two steps: Write a TokenStream which adds the drill-down terms For no associations, terms should be indexed w/ DOCS_ONLY (see LUCENE-4623). With associations, drill-down terms have payload too. So EnhancementsDocumentBuilder should be able to extend this stream. Write some API which can be used to build the fulltree payload (i.e. populate a byte[]). Currently that byte[] will be written to a payload and later to DV. Hopefully, I'd like to have FacetsDocumentBuilder (maybe just FacetsBuilder?) which only handles things with no associations, and EnhancementsDocBuilder which extends whatever is needed to add associations.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4648</id>
      <title>PostingsHighlighter should score only with within-document statistics</title>
      <description>PostingsHighlighter (in sandbox) treats documents to be highlighted as collections of small documents (snippets). But (like the other highlighters), it uses collection-wide stats to score the snippets. It actually calls the indexsearcher method which might even result in a distributed call. I don't think its worth it to do all this. It means we have to reseek to all terms across all segments at the minimum, and it means snippets change over time and so on. I think its good enough to only use within-document stats. And it might speed up highlighting.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4651</id>
      <title>Use generics for the type of assigned class in Classifier</title>
      <description>Currently Classifier#assignClass returns a ClassificationResult which holds the class as a String while there are use cases where this would be not optimal as assigned labels types could be known upfront, therefore having a Classifier&lt;T&gt; returning a ClassificationResult&lt;T&gt; would be better. Side node: current implementations could be improved by using BytesRef instead of String .</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4652</id>
      <title>highlight multiple fields with postings highlighter</title>
      <description>Currently this can only highlight a single field at once. But you might want to highlight body+title at the same time.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4658</id>
      <title>Per-segment tracking of external/side-car data</title>
      <description>Spinoff from David's idea on LUCENE-4258 (https://issues.apache.org/jira/browse/LUCENE-4258?focusedCommentId=13534352&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13534352 ) I made a prototype patch that allows custom per-segment "side-car data". It adds an abstract ExternalSegmentData class. The idea is the app implements this, and IndexWriter will pass each Document through to it, and call on it to do flushing/merging. I added a setter to IndexWriterConfig to enable it, but I think this would really belong in Codec ... I haven't tackled the read-side yet, though this is already usable without that (ie, the app can just open its own files, read them, etc.). The random test case passes. I think for example this might make it easier for Solr/ElasticSearch to implement things like ExternalFileField.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4659</id>
      <title>Cleanup CategoryPath</title>
      <description>CategoryPath is supposed to be a simple object which holds a category path's components, and offers some utility methods that can be used during indexing and search. Currently, it exposes lots of methods which aren't used, unless by tests - I want to get rid of them. Also, the internal implementation manages 3 char[] for holding the path components, while I think it would have been simpler if it maintained a String[]. I'd like to explore that option too (the input is anyway String, so why copy char[]?). Ultimately, I'd like CategoryPath to be immutable. I was able to get rid most of the mutable methods. The ones that remain will probably go away when I move from char[] to String[]. Immuntability is important because in various places in the code we convert a CategoryPath back and forth to String, with TODOs to stop doing that if CP was immutable. Will attach a patch that covers the first step - get rid of unneeded methods and beginning to make it immutable. Perhaps this can be done in multiple commits?</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4661</id>
      <title>Reduce default maxMerge/ThreadCount for ConcurrentMergeScheduler</title>
      <description>I think our current defaults (maxThreadCount=#cores/2, maxMergeCount=maxThreadCount+2) are too high ... I've frequently found merges falling behind and then slowing each other down when I index on a spinning-magnets drive. As a test, I indexed all of English Wikipedia with term-vectors (= heavy on merging), using 6 threads ... at the defaults (maxThreadCount=3, maxMergeCount=5, for my machine) it took 5288 sec to index &amp; wait for merges &amp; commit. When I changed to maxThreadCount=1, maxMergeCount=2, indexing time sped up to 2902 seconds (45% faster). This is on a spinning-magnets disk... basically spinning-magnets disk don't handle the concurrent IO well. Then I tested an OCZ Vertex 3 SSD: at the current defaults it took 1494 seconds and at maxThreadCount=1, maxMergeCount=2 it took 1795 sec (20% slower). Net/net the SSD can handle merge concurrency just fine. I think we should change the defaults: spinning magnet drives are hurt by the current defaults more than SSDs are helped ... apps that know their IO system is fast can always increase the merge concurrency.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>4665</id>
      <title>Upgrade ICU4J to newest version</title>
      <description>Upgrade ICU4J to the latest version - 50.1.1 at the moment. DictionaryBasedBreakIterator was eliminated by the upgrade, so some very small code changes were required. All tests (including nightly and weekly) passed with patch. I did not try running tests multiple times. The branch_4x tests were run on Windows, where I run eclipse. The trunk tests were run on Linux, the platform where I actually use Solr. I have no idea whether the ICU tests are comprehensive enough to detect problems with version upgrades. It is possible that I don't have enough knowledge to complete this upgrade properly. I did my best. A couple of javadocs mentioned the removed class. I changed one (hopefully correctly), but didn't know what the other should say.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4666</id>
      <title>Simplify CompressingStoredFieldsFormat merging</title>
      <description>Merging is currently unnecessarily complex: it tries to compute the size of the compressed block by analyzing the compressed stream although it could use the fields index instead.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4668</id>
      <title>Fix classpaths in classification module</title>
      <description>Classpaths in lucene/classification/build.xml are not using / extending correctly the default base classpaths.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4670</id>
      <title>Add TermVectorsWriter.finish{Doc,Field,Term} to make development of new formats easier</title>
      <description>This is especially useful to LUCENE-4599 where actions have to be taken after a doc/field/term has been added.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4677</id>
      <title>Use vInt to encode node addresses inside FST</title>
      <description>Today we use int, but towards enabling &gt; 2.1G sized FSTs, I'd like to make this vInt instead.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4678</id>
      <title>FST should use paged byte[] instead of single contiguous byte[]</title>
      <description>The single byte[] we use today has several limitations, eg it limits us to &lt; 2.1 GB FSTs (and suggesters in the wild are getting close to this limit), and it causes big RAM spikes during building when a the array has to grow. I took basically the same approach as LUCENE-3298, but I want to break out this patch separately from changing all int -&gt; long for &gt; 2.1 GB support.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4682</id>
      <title>Reduce wasted bytes in FST due to array arcs</title>
      <description>When a node is close to the root, or it has many outgoing arcs, the FST writes the arcs as an array (each arc gets N bytes), so we can e.g. bin search on lookup. The problem is N is set to the max(numBytesPerArc), so if you have an outlier arc e.g. with a big output, you can waste many bytes for all the other arcs that didn't need so many bytes. I generated Kuromoji's FST and found it has 271187 wasted bytes vs total size 1535612 = ~18% wasted. It would be nice to reduce this. One thing we could do without packing is: in addNode, if we detect that number of wasted bytes is above some threshold, then don't do the expansion. Another thing, if we are packing: we could record stats in the first pass about which nodes wasted the most, and then in the second pass (paack) we could set the threshold based on the top X% nodes that waste ... Another idea is maybe to deref large outputs, so that the numBytesPerArc is more uniform ...</description>
      <attachments/>
      <comments>33</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4683</id>
      <title>Change Aggregator and CategoryListIterator to be per-segment</title>
      <description>As another improvement, these two (mostly CategoryListIterator) should be per-segment. I've got a patch nearly ready, will post tomorrow.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4686</id>
      <title>Write a specialized DGapVIntEncoder/Decoder for facets</title>
      <description>Today the default encoder/decoder for facets is DGap(VInt). That is a DGapEncoder wrapping a VIntEncoder. Instead of this wrapping, we can write a specialized DGapVIntEncoder which does it all in one call.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4687</id>
      <title>Lazily initialize TermsEnum in BloomFilterPostingsFormat</title>
      <description>BloomFilteringPostingsFormat initializes its delegate TermsEnum directly inside the Terms#iterator() call which can be a pretty heavy operation if executed thousands of times. I suspect that bloom filter postings are mainly used for primary keys etc. which in turn is mostly a seekExact. Given that, most of the time we don't even need the delegate termsenum since most of the segments won't contain the key and the bloomfilter will likely return false from seekExact without consulting the delegate.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4688</id>
      <title>Reuse TermsEnum in BlockTreeTermsReader</title>
      <description>Opening a TermsEnum comes with a significant cost at this point if done frequently like primary key lookups or if many segments are present. Currently we don't reuse it at all and create a lot of objects even if the enum is just used for a single seekExact (ie. TermQuery). Stressing the Terms#iterator(reuse) call shows significant gains with reuse...</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4690</id>
      <title>Optimize NumericUtils.*ToPrefixCoded(), add versions that don't hash</title>
      <description>As far as I can tell nothing actually uses the hash codes generated by these methods (not even any tests). If someone did want to generate a hash, it would be just as fast to do it on the BytesRef after the fact (or even faster from the input number itself). edit: Uwe pointed out they were used in one place. Other places still don't need it.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4695</id>
      <title>Add utility class for getting live values for a given field during NRT indexing</title>
      <description>This is a simple utility/wrapper class, that holds the field values for recently indexed documents until the NRT reader has refreshed, and exposes a "get" API to get the last indexed value per id. For example one could use this to look up the "version" field for a given id, even when that id was just indexed and not yet visible in the NRT reader. The implementation is fairly simple: it just watches the gen coming out of NRTManager and updates/prunes accordingly. The class is abstract: you must subclass it and impl the lookupFromSearcher method...</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4696</id>
      <title>Allow SpanNearQuery to take a BooleanQuery.</title>
      <description>Currently SpanNearQuery can only take other SpanQuery objects, which include spans, span term and span wrapped multi-term queries, but not Boolean queries. By allowing a Boolean query to added to a SpanNearQuery, we can add f.i. queries that come from a QueryParser and which can not be easily transformed in the corresponding span objects. The main use case here is to find the intersection between two sets of results with the additional restriction that the matched terms from the different queries should be near one another.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4697</id>
      <title>FacetResultNode should be a simple bin, not interface</title>
      <description>FacetResultNode is an interface with a bunch of getter methods. The purpose, I believe, was to return an object that does not allow you modify it. But that's overly defensive I think. I.e., we return to users ScoredDoc and they can happily modify 'doc' and 'score'. If users modify the members' values, they can only affect themselves, as this object is returned after the search has completed. Anyway, today it doesn't even defend itself right, because you can call getSubResults and remove/add elements from the list ... I want to make it a simple bin, w/ public members and get rid of MutableFacetResultNode. Will keep the class not final, since it might be useful for someone to extend it and add additional members, for his/her FacetsCollector purposes.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4698</id>
      <title>Overhaul ShapeFieldCache because its a memory pig</title>
      <description>The org.apache.lucene.spatial.util.ShapeFieldCache* classes together implement a spatial field cache for points, similar to FieldCache for other fields. It supports a variable number of points per document, and it's currently only used by the SpatialPrefixTree strategy because that's the only strategy that supports a variable number of points per document. The other spatial strategies use the FieldCache. The ShapeFieldCache has problems: It's a memory pig. Each point is stored as a Point object, instead of an array of x &amp; y coordinates. Furthermore, each Point is in an ArrayList that exists for each Document. It's not done any differently when your spatial data isn't multi-valued. The cache is not per-segment, it's per-IndexReader, thereby making it un-friendly to NRT search. The cache entries don't self-expire optimally to free up memory. The cache is simply stored in a WeakHashMap&lt;IndexReader,ShapeFieldCache&gt;. The big cache entries are only freed when the WeakHashMap is used and the JVM realizes the IndexSearcher instance has been GC'ed.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4700</id>
      <title>OrdinalPolicy and PathPolicy should be per-CategoryListParams</title>
      <description>Today these two are per FacetIndexingParams, which means that once defined, they affect all category lists. This prevents for example to index one category list with OrdinalPolicy.NO_PARENTS and another with OrdinalPolicy.ALL_PARENTS. Especially now that we know NO_PARENTS is faster (see LUCENE-4600), it will be good if users can control this setting per CategoryListParams, and index only the few facets which a document has more than once (e.g. Author) in a separate category list.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4703</id>
      <title>Add basic tool to print some summary stats about your taxonomy index</title>
      <description>I built a Wikipedia index w/ 9 dimensions but I don't know how many ords each child contributes / how many immediate children under each dim / etc.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4706</id>
      <title>Tool to recover data from .fdt files</title>
      <description>Somebody posted this on the ML. Untested: http://pastebin.com/nmF0j4np</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4708</id>
      <title>Make LZ4 hash tables reusable</title>
      <description>Currently LZ4 compressors instantiate their own hash table for every byte sequence they need to compress. These can be large (256KB for LZ4 HC) so we should try to reuse them across calls.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4709</id>
      <title>Nuke FacetResultNode.residue</title>
      <description>The residue is the count of all categories that did not make it to the top K. But, this is a senseless statistic. Take for example the following case: two documents with categories [A/1, A/2, A/3] and [A/1, A/4, A/5]. If you ask for top-1 category of "A", you'll get A (count=2), A/1 (count=2), but A's residue will be 4! As a user, that number doesn't tell you anything, except maybe when you index only one category per document for a given dimension. But in that case, the residue is root.value - sum(topK.value), which the application can compute if it needs to. In short, we're just wasting CPU cycles for that statistic, so I'm going to remove it.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4713</id>
      <title>SPI: Allow fallback to default ClassLoader if Thread#getContextClassLoader fails</title>
      <description>NOTE: This issue has been renamed from: "Replace calls to Thread#getContextClassLoader with the ClassLoader of the current class" because the revised patch provides a clean fallback path. I am not sure whether it is a design decision or if we can indeed consider this a bug: In core and analysis-common some classes provide on-demand class loading using SPI. In NamedSPILoader, SPIClassIterator, ClasspathResourceLoader and AnalysisSPILoader there are constructors that use the Thread's context ClassLoader by default whenever no particular other ClassLoader was specified. Unfortunately this does not work as expected when the Thread's ClassLoader can't see the required classes that are instantiated downstream with the help of Class.forName (e.g., Codecs, Analyzers, etc.). That's what happened to us here. We currently experiment with running Lucene 2.9 and 4.x in one JVM, both being separated by custom ClassLoaders, each seeing only the corresponding Lucene version and the upstream classpath. While NamedSPILoader and company get successfully loaded by our custom ClassLoader, their instantiation fails because our Thread's Context-ClassLoader cannot find the additionally required classes. We could probably work-around this by using Thread#setContextClassLoader at construction time (and quickly reverting back afterwards), but I have the impression this might just hide the actual problem and cause further trouble when lazy-loading classes later on, and potentially from another Thread. Removing the call to Thread#getContextClassLoader would also align with the behavior of AttributeSource.DEFAULT_ATTRIBUTE_FACTORY, which in fact uses Attribute#getClass().getClassLoader() instead. A simple patch is attached. All tests pass.</description>
      <attachments/>
      <comments>31</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4715</id>
      <title>Add OrdinalPolicy.ALL_BUT_DIMENSION</title>
      <description>With the move of OrdinalPolicy to CategoryListParams, NonTopLevelOrdinalPolicy was nuked. It might be good to restore it, as another enum value of OrdinalPolicy. It's the same like ALL_PARENTS, only doesn't add the dimension ordinal, which could save space as well as computation time. It's good for when you don't care about the count of Date/, but only about its children counts.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4716</id>
      <title>Add OR support to DrillDown</title>
      <description>DrillDown provides helper methods to wrap a baseQuery with drill-down categories. All the categories are AND'ed, and it has been asked on the user list for OR support. While users can construct their own BooleanQuery, it would be useful if DrillDown helped them doing that. I think that a simple Occur additional parameter to DrillDown.query will help to some extent.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4722</id>
      <title>Can we move SortField.Type.SCORE/DOC to singleton SortField instances instead...?</title>
      <description>It's ... weird that you can do eg new SortField("myfield", SortField.Type.SCORE). We already have dedicated SortField.FIELD_SCORE and FIELD_DOC ... so I think apps should use those and never make a new SortField for them?</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4723</id>
      <title>Add AnalyzerFactoryTask to benchmark, and enable analyzer creation via the resulting factories using NewAnalyzerTask</title>
      <description>Benchmark algorithms can't currently use analysis factories. Instead, one must rely on pre-existing analyzers, or write specialized tasks to construct them. Now that all analysis components have factories, benchmark algorithms should be able to use them.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4726</id>
      <title>Make PackedInts encoding from/to byte byte-aligned</title>
      <description>I had to do it for a patch for LUCENE-4609, but even if faceting doesn't end up using PackedInts, I think it would be better if encoding and decoding were byte-aligned instead of long-aligned (although encoding from/to long still needs to be long-aligned).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4727</id>
      <title>Make CommonTermsQuery#setMinShouldMatch useful</title>
      <description>Currently common terms query accepts an integer as minimumNumShouldMatch which is kind of odd since the number of low freq terms is determined during rewrite. We should make this extendable and use a float by default that works similar to the cutoff frequency we use where a range between [0..1) means a fraction of the actual count and a num &gt;= 1.0 an absolute value.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4728</id>
      <title>Allow CommonTermsQuery to be highlighted</title>
      <description>Add support for CommonTermsQuery to all highlighter impls. This might add a dependency (query-jar) to the highlighter so we might think about adding it to core?</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4731</id>
      <title>New ReplicatingDirectory mirrors index files to HDFS</title>
      <description>I've been working on a Directory implementation that mirrors the index files to HDFS (or other Hadoop supported FileSystem). A ReplicatingDirectory delegates all calls to an underlying Directory (supplied in the constructor). The only hooks are the deleteFile and sync calls. We submit deletes and replications to a single scheduler thread to keep things serializer. During a sync call, if "segments.gen" is seen in the list of files, we know a commit is finishing. After calling the deletage's sync method, we initialize an asynchronous replication as follows. Read segments.gen (before leaving ReplicatingDirectory#sync), save the values for later Get a list of local files from ReplicatingDirectory#listAll before leaving ReplicatingDirectory#sync Submit replication task (DirectoryReplicator) to scheduler thread Compare local files to remote files, determine which remote files get deleted, and which need to get copied Submit a thread to copy each file (one thead per file) Submit a thread to delete each file (one thead per file) Submit a "finalizer" thread. This thread waits on the previous two batches of threads to finish. Once finished, this thread generates a new "segments.gen" remotely (using the version and generation number previously read in). I have no idea where this would belong in the Lucene project, so i'll just attach the standalone class instead of a patch. It introduces dependencies on Hadoop core (and all the deps that brings with it).</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4733</id>
      <title>Make CompressingTermVectorsFormat the new default term vectors format?</title>
      <description>In LUCENE-4599, I wrote an alternate term vectors format which has a more compact format, and I think it could replace the current Lucene40TermVectorsFormat for the next (4.2) release?</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4742</id>
      <title>Rename SpatialPrefixTree's "Node" back to "Cell"</title>
      <description>SpatialPrefixTree makes "Node"s which are basically a rectangular spatial region that is more colloquially referred to as a "Cell". It was named "Cell" in the first place and for whatever reason, Ryan and/or Chris renamed it as part of extracting it to a top level class from an inner class. Most comments and variable names still use the "cell" terminology. I'm working on an algorithm that keeps track of a tree of "nodes" and it has gotten confusing which kind of node I'm referring to, as each Node has one cell. In maybe a week or so if there isn't discussion to the contrary, I'm going to commit a rename it back to "Cell". And... while we're on this naming subject, perhaps "SpatialPrefixTree" could be named "SpatialGrid" ? FWIW the variables referring to it are always "grid".</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4745</id>
      <title>Allow FuzzySlop customization in classic QueryParser</title>
      <description>It turns out searching arbitrary fields with define FUZZY_SLOP values could be problematic on some types of values. For example a FUZZY_SLOP on dates is ambiguous and needs a definition of a unit like months, days, minutes, etc. An extension on the query grammar that allows some arbitrary text behind the values in combination with a possibility to override the method parsing those values could solve these kinds of problems.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4746</id>
      <title>Create a move method in Directory.</title>
      <description>I'd like to make a move method for directory. We already have a move for Solr in DirectoryFactory, but it seems it belongs at the directory level really. The default impl can do a copy and delete, but most implementations will be able to optimize to a rename. Besides the move we do for Solr (to move a replicated index into place), it would also be useful for another feature I'd like to add - the ability to merge an index with moves rather than copies. In some cases, you don't need/want to copy all the files and could just rename/move them.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4748</id>
      <title>Add DrillSideways helper class to Lucene facets module</title>
      <description>This came out of a discussion on the java-user list with subject "Faceted search in OR": http://markmail.org/thread/jmnq6z2x7ayzci5k The basic idea is to count "near misses" during collection, ie documents that matched the main query and also all except one of the drill down filters. Drill sideways makes for a very nice faceted search UI because you don't "lose" the facet counts after drilling in. Eg maybe you do a search for "cameras", and you see facets for the manufacturer, so you drill into "Nikon". With drill sideways, even after drilling down, you'll still get the counts for all the other brands, where each count tells you how many hits you'd get if you changed to a different manufacturer. This becomes more fun if you add further drill-downs, eg maybe I next drill down into Resolution=10 megapixels", and then I can see how many 10 megapixel cameras all other manufacturers, and what other resolutions Nikon cameras offer.</description>
      <attachments/>
      <comments>31</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4749</id>
      <title>Expose UIMA AEs config parameters in analysis/uima tools</title>
      <description>As per request on http://markmail.org/thread/4hcibusueckk6osi so that a UIMATokenizer can be initialized with proper configuration parameters to e.g. point to custom resources or put non-default settings.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4750</id>
      <title>Convert DrillDown to DrillDownQuery</title>
      <description>DrillDown is a utility class for creating drill-down queries over a base query and a bunch of categories. We've been asked to support AND, OR and AND of ORs. The latter is not so simple as a static utility method though, so instead we have some sample code ... Rather, I think that we can just create a DrillDownQuery (extends Query) which takes a baseQuery in its ctor and exposes add(CategoryPath...), such that every such group of categories is AND'ed with other groups, and internally they are OR'ed. It's very similar to how you would construct a BooleanQuery, only simpler and specific to facets. Internally, it would build a BooleanQuery and delegate rewrite, createWeight etc to it. That will remove the need for the static utility methods .. or we can keep static term() for convenience.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4752</id>
      <title>Merge segments to sort them</title>
      <description>It would be awesome if Lucene could write the documents out in a segment based on a configurable order. This of course applies to merging segments to. The benefit is increased locality on disk of documents that are likely to be accessed together. This often applies to documents near each other in time, but also spatially.</description>
      <attachments/>
      <comments>54</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>4753</id>
      <title>Make forbidden API checks per-module</title>
      <description>After the forbidden API checker was released separately from Lucene as a Google Code project (forked and improved), including Maven support, the checks on Lucene should be changed to work per-module. The reason for this is: The improved checker is more picky about e.g. extending classes that are forbidden or overriding methods and calling super.method() if they are on the forbidden signatures list. For these checks, it is not enough to have the class files and the rt.jar, you need the whole classpath. The forbidden APIs 1.0 now by default complains if classes are missing from the classpath. It is very hard with the module architecture of Lucene/Solr, to make a uber-classpath, instead the checks should be done per module, so the default compile/test classpath of the module can be used and no crazy path statements with */.jar are needed. This needs some refactoring in the exclusion lists, but the Lucene checks could be done by a macro in common-build, that allows custom exclusion lists for specific modules. Currently, the "strict" checking is disabled for Solr, so the checker only complains about missing classes but does not fail the build: -check-forbidden-java-apis: [forbidden-apis] Reading bundled API signatures: jdk-unsafe-1.6 [forbidden-apis] Reading bundled API signatures: jdk-deprecated-1.6 [forbidden-apis] Reading bundled API signatures: commons-io-unsafe-2.1 [forbidden-apis] Reading API signatures: C:\Users\Uwe Schindler\Projects\lucene\trunk-lusolr3\lucene\tools\forbiddenApis\executors.txt [forbidden-apis] Reading API signatures: C:\Users\Uwe Schindler\Projects\lucene\trunk-lusolr3\lucene\tools\forbiddenApis\servlet-api.txt [forbidden-apis] Loading classes to check... [forbidden-apis] Scanning for API signatures and dependencies... [forbidden-apis] WARNING: The referenced class 'org.apache.lucene.analysis.uima.ae.AEProviderFactory' cannot be loaded. Please fix the classpath! [forbidden-apis] WARNING: The referenced class 'org.apache.lucene.analysis.uima.ae.AEProviderFactory' cannot be loaded. Please fix the classpath! [forbidden-apis] WARNING: The referenced class 'org.apache.lucene.analysis.uima.ae.AEProvider' cannot be loaded. Please fix the classpath! [forbidden-apis] WARNING: The referenced class 'org.apache.lucene.collation.ICUCollationKeyAnalyzer' cannot be loaded. Please fix the classpath! [forbidden-apis] Scanned 2177 (and 1222 related) class file(s) for forbidden API invocations (in 1.80s), 0 error(s). I added almost all missing jars, but those do not seem to be in the solr part of the source tree (i think they are only copied when building artifacts). With making the whole thing per module, we can use the default classpath of the module which makes it much easier.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4756</id>
      <title>Release resources on UIMA tokenizers close()</title>
      <description>BaseUIMATokenizer should release AnalysisEngine and CAS on close()</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4757</id>
      <title>Cleanup FacetsAccumulator API path</title>
      <description>FacetsAccumulator and FacetRequest expose too many things to users, even when they are not needed, e.g. complements and partitions. Also, Aggregator is created per-FacetRequest, while in fact applied per category list. This is confusing, because if you want to do two aggregations, e.g. count and sum-score, you need to separate the two dimensions into two different category lists at indexing time. It's not so easy to refactor everything in one go, since there's a lot of code involved. So in this issue I will: Remove complements from FacetRequest. It is only relevant to CountFacetRequest anyway. In the future, it should be a special Accumulator. Make FacetsAccumulator concrete class, and StandardFacetsAccumulator extend it and handles all the stuff that's relevant to sampling, complements and partitions. Gradually, these things will be migrated to the new API, and hopefully StandardFacetsAccumulator will go away. Aggregator is per-document. I could not break its API b/c some features (e.g. complement) depend on it. So rather I created a new FacetsAggregator, with a bulk, per-segment, API. So far migrated Counting and SumScore to that API. In the new API, you need to override FacetsAccumulator to define an Aggregator for use, the default is CountingFacetsAggregator. Started to refactor FacetResultsHandler, which its API was guided by the use of partitions. I added a simple compute(FacetArrays) to it, which by default delegates to the nasty API, but overridden by specific classes. This will get cleaned further along too. FacetRequest has a .getValueOf() which resolves an ordinal to its value (i.e. which of the two arrays to use). I added FacetRequest.FacetArraysSource and specialize when they are INT or FLOAT, creating a special FacetResultsHandler which does not go back to FR.getValueOf for every ordinal. I think that we can migrate other FacetResultsHandlers to behave like that ... at the expense of code duplication. I also added a TODO to get rid of getValueOf entirely .. will be done separately. Got rid of CountingFacetsCollector and StandardFacetsCollector in favor of a single FacetsCollector which collects matching documents, and optionally scores, per-segment. I wrote a migration class from these per-segment MatchingDocs to ScoredDocIDs (which is global), so that the rest of the code works, but the new code works w/ the optimized per-segment API. I hope performance is still roughly the same w/ these changes too. There will be follow-on issues to migrate more features to the new API, and more cleanups ...</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4758</id>
      <title>'ant jar', 'ant compile' and 'ant compile-test' should recurse</title>
      <description>Right now, 'ant compile' at the top level compiles Lucene core, all non-test Solr sources, and all Lucene modules on which Solr depends. lucene/codecs/, e.g., doesn't get compiled, because 'ant compile' is an alias for 'ant compile-core' under lucene/. Similarly for 'ant jar' (except there is no top-level target for this right now), with some problems under solr/. There is no top-level 'ant compile-test'. All these targets should recurse at all levels. Under lucene/ and solr/, 'ant jar-core' and 'ant compile-core' should be aliased to running the operation under core/.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4759</id>
      <title>Remove FacetRequest.SortBy</title>
      <description>FacetRequest.SortBy lets you specify two sort-by: ORDINAL and VALUE. While VALUE is the default (and breaks ties by ordinal), it's not very clear what's the use of SortBy.ORDINAL. In practice, if you choose to do that, you'd get the first K categories that are the descendant of the requested one, from smallest to highest, or vice versa. But that seems quite useless ... someone could just traverse the counts array (for instance) and filter out all counts==0? Or also, someone can write a FacetResultsHandler which does that... My motivation to remove that is to reduce the number of PQ combinations we have: MinValue, MaxValue (SortBy.VALUE, SortOrder.ASCENDING/DESCENDING) and MinOrdinal, MaxOrdinal. Now there are 4 PQs and I'd like to separately split them out to PQs that handle int vs float. Because today these PQs call Double.compare(), which you need to for floating-point values, but is just a waste for integer values. So removing SortBy will both simplify the API and halve the number of PQs we need to write. Plus ... it doesn't seem such a useful option, to let the user even spend 10 seconds to read the differences between VALUE and ORDINAL.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4760</id>
      <title>Weird IndexWriter.close() usage</title>
      <description>I'm using IndexWriter on such a way that it can be interrupted, since the streams i'm using to output to file can be interrupted. So far so good, but what i'm finding strange is the (only) way i've found to prevent the file lock being held afterwards. Normally i'd do a try / catch / finally block where the finally would close() and handle exceptions from the close. However, IndexWriter "close()" is more like a buffered commit, where many exceptions can occur, so i left it on the main part of the code. try {... index.close(); } catch { /*log*/ } finally { if (IndexWriter.isLocked(cacheDir)) { IndexWriter.unlock(cacheDir); } } Didn't work. The lock couldn't be unlocked (always) if the stream was interrupted So in desperation, i tried to be more literal in my interpretation of the IndexWriter.close() javadoc and tried try { ... indexWriter.close(); } catch (IOException ex) { try { indexWriter.close(); } finally { if (IndexWriter.isLocked(cacheDir)) { IndexWriter.unlock(cacheDir); } } throw ex; } finally { ... } This worked (the lock was always released if a additional close() was invoked in a situation where the lock would be held before trying to unlock it), but i find it really counter-intuitive, and would wish for at least additional javadoc attention, or a redesign on a major API revision.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4761</id>
      <title>Facets package reorganization</title>
      <description>Facets packages have a weird structure IMO. I think that we should organize the packages by feature, and not by functionality (index/search). For example: o.a.l.facet.index – core facets indexing o.a.l.facet.search – core facets search o.a.l.facet.params – all facets params (indexing and search) o.a.l.facet.associations – all associations code (we can break to sub-index/search packages if needed) o.a.l.facet.partitions – all partitions related code o.a.l.facet.sampling – all sampling related code o.a.l.facet.util – consolidate all utils under that, even those that are currently under o.a.l.util o.a.l.facet.encoding – move all encoders under it (from o.a.l.util) o.a.l.facet.taxonomy – all taxonomy related stuff. The motivation – if I want to handle all associations related code, it should be very easy to locate it.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4762</id>
      <title>Improve Facet examples</title>
      <description>Facet examples need some simplification and cleanup. For instance, MultiCLExample uses a Random (with fixed seed) to generate documents and categories and in general they seem to try an reuse a lot of methods. Rather, I think the example code should be concise and short, demonstrating what it needs to demonstrate, even at the expense of duplicating code. That way, a user can just look at an example class to understand how to do something...</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4764</id>
      <title>Faster but more RAM/Disk consuming DocValuesFormat for facets</title>
      <description>The new default DV format for binary fields has much more RAM-efficient encoding of the address for each document ... but it's also a bit slower at decode time, which affects facets because we decode for every collected docID.</description>
      <attachments/>
      <comments>29</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4765</id>
      <title>Multi-valued docvalues field</title>
      <description>The general idea is basically the docvalues parallel to FieldCache.getDocTermOrds/UninvertedField Currently this stuff is used in e.g. grouping and join for multivalued fields, and in solr for faceting.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4766</id>
      <title>Pattern token filter which emits a token for every capturing group</title>
      <description>The PatternTokenizer either functions by splitting on matches, or allows you to specify a single capture group. This is insufficient for my needs. Quite often I want to capture multiple overlapping tokens in the same position. I've written a pattern token filter which accepts multiple patterns and emits tokens for every capturing group that is matched in any pattern. Patterns are not anchored to the beginning and end of the string, so each pattern can produce multiple matches. For instance a pattern like : "(([a-z]+)(\d*))" when matched against: "abc123def456" would produce the tokens: abc123, abc, 123, def456, def, 456 Multiple patterns can be applied, eg these patterns could be used for camelCase analysis: "([A-Z]{2,})", "(?&lt;![A-Z])([A-Z][a-z]+)", "(?:^|\\b|(?&lt;=[0-9_])|(?&lt;=[A-Z]{2}))([a-z]+)", "([0-9]+)" When matched against the string "letsPartyLIKEits1999_dude", they would produce the tokens: lets, Party, LIKE, its, 1999, dude If no token is emitted, the original token is preserved. If the preserveOriginal flag is true, it will output the full original token (ie "letsPartyLIKEits1999_dude") in addition to any matching tokens (but in this case, if a matching token is identical to the original, it will only emit one copy of the full token). Multiple patterns are required to allow overlapping captures, but also means that patterns are less dense and easier to understand. This is my first Java code, so apologies if I'm doing something stupid.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>4768</id>
      <title>Child Traversable To Parent Block Join Query</title>
      <description>Hi everyone! Let me describe what i am trying to do: I have hierarchical documents ('car model' as parent, 'trim' as child) and use block join queries to retrieve them. However, i am not happy with current behavior of ToParentBlockJoinQuery which goes through all parent childs during nextDoc call (accumulating scores and freqs). Consider the following example, you have a query with a custom post condition on top of such bjq: and during post condition you traverse scorers tree (doc-at-time) and want to manually push child scorers of bjq one by one until condition passes or current parent have no more childs. I am attaching the patch with query(and some tests) similar to ToParentBlockJoin but with an ability to traverse childs. (i have to do weird instance of check and cast inside my code) This is a draft only and i will be glad to hear if someone need it or to hear how we can improve it. P.s i believe that proposed query is more generic (low level) than ToParentBJQ and ToParentBJQ can be extended from it and call nextChild() internally during nextDoc(). Also, i think that the problem of traversing hierarchical documents is more complex as lucene have only nextDoc API. What do you think about making api more hierarchy aware? One level document is a special case of multi level document but not vice versa. WDYT? Thanks in advance.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4769</id>
      <title>Add a CountingFacetsAggregator which reads ordinals from a cache</title>
      <description>Mike wrote a prototype of a FacetsCollector which reads ordinals from a CachedInts structure on LUCENE-4609. I ported it to the new facets API, as a FacetsAggregator. I think we should offer users the means to use such a cache, even if it consumes more RAM. Mike tests show that this cache consumed x2 more RAM than if the DocValues were loaded into memory in their raw form. Also, a PackedInts version of such cache took almost the same amount of RAM as straight int[], but the gains were minor. I will post the patch shortly.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4771</id>
      <title>Query-time join collectors could maybe be more efficient</title>
      <description>I was looking @ these collectors on LUCENE-4765 and I noticed: SingleValued collector (SV) pulls FieldCache.getTerms and adds the bytes to a bytesrefhash per-collect. MultiValued collector (MV) pulls FieldCache.getDocTermsOrds, but doesnt use the ords, just looks up each value and adds the bytes per-collect. I think instead its worth investigating if SV should use getTermsIndex, and both collectors just collect-up their per-segment ords in something like a BitSet[maxOrd]. When asked for the terms at the end in getCollectorTerms(), they could merge these into one BytesRefHash. Of course, if you are going to turn around and execute the query against the same searcher anyway (is this the typical case?), this could even be more efficient: No need to hash or instantiate all the terms in memory, we could do postpone the lookups to SeekingTermSetTermsEnum.accept()/nextSeekTerm() i think... somehow</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4772</id>
      <title>Move Facet associations to new FacetsAggregator API</title>
      <description>Move facets associations to the new bulk FacetsAggregator API. Also, today when you index categories with associations, the categories are written to two fields redundantly - the associations field and the counting list field. However, when you aggregate them, you only need to read the associations field. The counting list field is redundant here. If an app requires indexing the categories into two lists, it can do so by adding the categories w/ associations using AssociationFacetFields and the plain categories (w/ their hierarchy etc.) using FacetFields. I will post a patch shortly.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4774</id>
      <title>Add FieldComparator that allows sorting parent docs based on field inside the child docs</title>
      <description>A field comparator for sorting block join parent docs based on the a field in the associated child docs.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4780</id>
      <title>MonotonicAppendingLongBuffer</title>
      <description>IndexWriter uses AppendingLongBuffer in several places, and in a few of them the mapping is monotonically increasing so we could save additional space by only encoding the delta from a linear projection.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4792</id>
      <title>Smaller doc maps</title>
      <description>MergeState.DocMap could leverage MonotonicAppendingLongBuffer to save memory.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4794</id>
      <title>Refactor Spatial RecursivePrefixTreeFilter algorithm for extension</title>
      <description>In the process of implementing algorithms on the SpatialPrefixTree indexed field like "Within" or some variations of Intersects that say collect the distance as side-effect, I find that I need near-copies of the code in RecursivePrefixTreeFilter. RPTF is pretty intense with lots of optimizations. So I refactored out the algorithm such that it makes implementing new algorithms much easier yet benefits from the logic in there. Patch to follow...</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4795</id>
      <title>Add FacetsCollector based on SortedSetDocValues</title>
      <description>Recently (LUCENE-4765) we added multi-valued DocValues field (SortedSetDocValuesField), and this can be used for faceting in Solr (SOLR-4490). I think we should also add support in the facet module? It'd be an option with different tradeoffs. Eg, it wouldn't require the taxonomy index, since the main index handles label/ord resolving. There are at least two possible approaches: On every reopen, build the seg -&gt; global ord map, and then on every collect, get the seg ord, map it to the global ord space, and increment counts. This adds cost during reopen in proportion to number of unique terms ... On every collect, increment counts based on the seg ords, and then do a "merge" in the end just like distributed faceting does. The first approach is much easier so I built a quick prototype using that. The prototype does the counting, but it does NOT do the top K facets gathering in the end, and it doesn't "know" parent/child ord relationships, so there's tons more to do before this is real. I also was unsure how to properly integrate it since the existing classes seem to expect that you use a taxonomy index to resolve ords. I ran a quick performance test. base = trunk except I disabled the "compute top-K" in FacetsAccumulator to make the comparison fair; comp = using the prototype collector in the patch: Task QPS base StdDev QPS comp StdDev Pct diff OrHighLow 18.79 (2.5%) 14.36 (3.3%) -23.6% ( -28% - -18%) HighTerm 21.58 (2.4%) 16.53 (3.7%) -23.4% ( -28% - -17%) OrHighMed 18.20 (2.5%) 13.99 (3.3%) -23.2% ( -28% - -17%) Prefix3 14.37 (1.5%) 11.62 (3.5%) -19.1% ( -23% - -14%) LowTerm 130.80 (1.6%) 106.95 (2.4%) -18.2% ( -21% - -14%) OrHighHigh 9.60 (2.6%) 7.88 (3.5%) -17.9% ( -23% - -12%) AndHighHigh 24.61 (0.7%) 20.74 (1.9%) -15.7% ( -18% - -13%) Fuzzy1 49.40 (2.5%) 43.48 (1.9%) -12.0% ( -15% - -7%) MedSloppyPhrase 27.06 (1.6%) 23.95 (2.3%) -11.5% ( -15% - -7%) MedTerm 51.43 (2.0%) 46.21 (2.7%) -10.2% ( -14% - -5%) IntNRQ 4.02 (1.6%) 3.63 (4.0%) -9.7% ( -15% - -4%) Wildcard 29.14 (1.5%) 26.46 (2.5%) -9.2% ( -13% - -5%) HighSloppyPhrase 0.92 (4.5%) 0.87 (5.8%) -5.4% ( -15% - 5%) MedSpanNear 29.51 (2.5%) 27.94 (2.2%) -5.3% ( -9% - 0%) HighSpanNear 3.55 (2.4%) 3.38 (2.0%) -4.9% ( -9% - 0%) AndHighMed 108.34 (0.9%) 104.55 (1.1%) -3.5% ( -5% - -1%) LowSloppyPhrase 20.50 (2.0%) 20.09 (4.2%) -2.0% ( -8% - 4%) LowPhrase 21.60 (6.0%) 21.26 (5.1%) -1.6% ( -11% - 10%) Fuzzy2 53.16 (3.9%) 52.40 (2.7%) -1.4% ( -7% - 5%) LowSpanNear 8.42 (3.2%) 8.45 (3.0%) 0.3% ( -5% - 6%) Respell 45.17 (4.3%) 45.38 (4.4%) 0.5% ( -7% - 9%) MedPhrase 113.93 (5.8%) 115.02 (4.9%) 1.0% ( -9% - 12%) AndHighLow 596.42 (2.5%) 617.12 (2.8%) 3.5% ( -1% - 8%) HighPhrase 17.30 (10.5%) 18.36 (9.1%) 6.2% ( -12% - 28%) I'm impressed that this approach is only ~24% slower in the worst case! I think this means it's a good option to make available? Yes it has downsides (NRT reopen more costly, small added RAM usage, slightly slower faceting), but it's also simpler (no taxo index to manage).</description>
      <attachments/>
      <comments>40</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>4799</id>
      <title>Enable extraction of originating term for ICU collation keys</title>
      <description>By concatenating generated ICU collation keys bytes with the originating term, it is possible to extract the originating term at a later time. This makes it possible to build a collator sorted facet field and similar multi-value/document structures. ICU collation keys are guaranteed to be terminated by a 0 (https://ssl.icu-project.org/apiref/icu4j48rc1/com/ibm/icu/text/CollationKey.html) and since comparison of keys stop when a 0 is encountered, the addition of the originating term does not affect sort order. As 0 are only used for termination in the key bytes, the extraction of the originating term is unambiguous.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4806</id>
      <title>change FacetIndexingParams.DEFAULT_FACET_DELIM_CHAR to U+001F</title>
      <description>The current delim char takes 3 bytes as UTF-8 ... but U+001F (= INFORMATION_SEPARATOR, which seems appropriate) takes only 1 byte.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4811</id>
      <title>*DocValuesField should extend Field not StoredField</title>
      <description>I think it's confusing because it implies the field will be stored as well as added to doc values ... I also noticed StorableFieldType is unused.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4817</id>
      <title>Add KeywordRepeaterFilter to emit tokens twice once as keyword and once not as keyword</title>
      <description>if you want to have a stemmed and an unstemmed version of a token one for recall and one for precision you have to do two fields today in most of the cases. Yet, most of the stemmers respect the keyword attribute so we could add a token filter that emits the same token twice once as keyword and once plain. Folks would most likely need to combine this RemoveDuplicatesTokenFilter but that way we can have stemmed and unstemmed version in the same field.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>4818</id>
      <title>Create a boolean perceptron classifier</title>
      <description>Create a Lucene based classifier using the perceptron algorithm (see http://en.wikipedia.org/wiki/Perceptron)</description>
      <attachments/>
      <comments>5</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4819</id>
      <title>move Sorted[Set]DocValuesTermsEnum to codec</title>
      <description>Currently a user can instantiate a SortedDocValuesTermsEnum(SortedDocValues). This is a generic termsenum, implementing all operations by lookupOrd(). I think instead this should be the default implementation, and we should have e.g. SortedDocValues.termsEnum() that returns it (codec can implement something fancier). For example the default codec implements lookupOrd as an FST binary search, which means next() on this termsenum is much slower than it needs to be for the places where this enum is actually used (segment merging, OrdinalMap used for faceting in SOLR-4490 and LUCENE-4795) So instead, it can override this method and use an FSTEnum, and these operations are significantly faster (3x faster for me with a simple benchmark with 10M terms).</description>
      <attachments/>
      <comments>16</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4820</id>
      <title>Add optional payload to AnalyzingSuggester</title>
      <description>It's useful to be able to store custom data (eg maybe a primary key or a URL or something) with each suggestion, so that the UI can do things like render an image along with each suggestion, or direct to a specific URL if the user clicks that suggestion, etc.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4821</id>
      <title>AnalyzingSuggester should use end() offset to decide if last token ended or not</title>
      <description>For example, today if you index "i love lucy" and "isla de muerta", and then you ask for suggestions for "i" and for "i " (space after the i) you'll get the same results. But if we use the ending offset, we can determine (I think?) that there were non-token characters after the last token, so that "i " would only suggest "i love lucy".</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4822</id>
      <title>Add PatternKeywordTokenFilter to marks keywords based on regular expressions</title>
      <description>today we need to pass in an explicit set of terms that we want to marks as keywords. It might make sense to allow patterns as well to prevent certain suffixes etc. to be keyworded.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4824</id>
      <title>Query time join returns different results based on the field type</title>
      <description>I'm experiencing different query time joining behavior based on the type of the 'toField' and 'fromField'. Basically I got correct results when both 'toField' and 'fromField' are StringField, but incorrect in case of LongField.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4825</id>
      <title>PostingsHighlighter support for positional queries</title>
      <description>I've been playing around with the brand new PostingsHighlighter. I'm really happy with the result in terms of quality of the snippets and performance. On the other hand, I noticed it doesn't support positional queries. If you make a span query, for example, all the single terms will be highlighted, even though they haven't contributed to the match. That reminds me of the difference between the QueryTermScorer and the QueryScorer (using the standard Highlighter). I've been trying to adapt what the QueryScorer does, especially the extraction of the query terms together with their positions (what WeightedSpanTermExtractor does). Next step would be to take that information into account within the formatter and highlight only the terms that actually contributed to the match. I'm not quite ready yet with a patch to contribute this back, but I certainly intend to do so. That's why I opened the issue and in the meantime I would like to hear what you guys think about it and discuss how best we can fix it. I think it would be a big improvement for this new highlighter, which is already great!</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4830</id>
      <title>Sorter API: use an abstract doc map instead of an array</title>
      <description>The sorter API uses arrays to store the old-&gt;new and new-&gt;old doc IDs mappings. It should rather be an abstract class given that in some cases an array is not required at all (reverse mapping for example).</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4832</id>
      <title>Unbounded getTopGroups for ToParentBlockJoinCollector</title>
      <description>ToParentBlockJoinCollector#getTopGroups method takes several arguments: public TopGroups&lt;Integer&gt; getTopGroups(ToParentBlockJoinQuery query, Sort withinGroupSort, int offset, int maxDocsPerGroup, int withinGroupOffset, boolean fillSortFields) and one of them is maxDocsPerGroup which specifies upper bound of child documents number returned within each group. ToParentBlockJoinCollector collects and caches all child documents matched by given ToParentBlockJoinQuery in OneGroup objects during search so it is possible to create GroupDocs with all matched child documents instead of part of them bounded by maxDocsPerGroup. When you specify maxDocsPerGroup new queues(I mean TopScoreDocCollector/TopFieldCollector) will be created for each group with maxDocsPerGroup objects created within each queue which could lead to redundant memory allocation in case of child documents number within group is less than maxDocsPerGroup. I suppose that there are many cases where you need to get all child documents matched by query so it could be nice to have ability to get top groups with all matched child documents without unnecessary memory allocation. Possible solution is to pass negative maxDocsPerGroup in case when you need to get all matched child documents within each group and check maxDocsPerGroup value: if it is negative then we need to create queue with size of matched child documents number; otherwise create queue with size equals to maxDocsPerGroup.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4835</id>
      <title>Raise maxClauseCount in BooleanQuery to Integer.MAX_VALUE</title>
      <description>Discussion on SOLR-4586 raised the idea of raising the limit on boolean clauses from 1024 to Integer.MAX_VALUE. This should be a safe change. It will change the nature of help requests from "Why can't I do 2000 clauses?" to "Why is my 5000-clause query slow?"</description>
      <attachments/>
      <comments>14</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>4837</id>
      <title>Expose buffer size in BufferedIndexOutput</title>
      <description>We expose a getter on BufferedIndexInput so for consistency it makes sense to expose it on BufferedIndexOutput as well. Wrappers like RateLimiter can also take advantage of this information to adjust their buffer size.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4838</id>
      <title>Add BytesRefHash.find()</title>
      <description>There is no API today to query BytesRefHash for the existence of a certain BytesRef. Rather, you should use .add(), which looks up the given bytes, and hashes them if they are not found, or returns their ord if they are found. I would like to add a simple getOrd API which will return the ord of a given BytesRef, or -1 if not found. I would like to use that API in the facet module, and I need to be able to query the hash without necessarily adding elements to it. I have a simple patch, will post shortly.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4839</id>
      <title>Sorter API: Use TimSort to sort doc IDs and postings lists</title>
      <description>TimSort (http://svn.python.org/projects/python/trunk/Objects/listsort.txt, used by python and Java's Arrays.sort(Object[]) in particular) is a sorting algorithm that performs very well on partially-sorted data. Indeed, with TimSort, sorting an array which is in reverse order or a finite concatenation of sorted arrays is a linear operation (instead of O(n ln)). The sorter API could benefit from this algorithm when using Sorter.REVERSE_DOCS or merging several sorted readers for example.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4843</id>
      <title>LimitTokenPositionFilter: don't emit tokens with positions that exceed the configured limit</title>
      <description>Like LimitTokenCountFilter, except it's the token position that's limited rather than the token count.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4844</id>
      <title>Remove TaxonomyReader.getParent(ord)</title>
      <description>This should have been gone when we introduced .getParallelArrays(). The method simply calls getParallelArrays().parents()[ord], and that's what callers should do. Except one test, no other code in facets calls this method.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4845</id>
      <title>Add AnalyzingInfixSuggester</title>
      <description>Our current suggester impls do prefix matching of the incoming text against all compiled suggestions, but in some cases it's useful to allow infix matching. E.g, Netflix does infix suggestions in their search box. I did a straightforward impl, just using a normal Lucene index, and using PostingsHighlighter to highlight matching tokens in the suggestions. I think this likely only works well when your suggestions have a strong prior ranking (weight input to build), eg Netflix knows the popularity of movies.</description>
      <attachments/>
      <comments>33</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4846</id>
      <title>PostingsHighlighter should allow [expert] customization on how the field values are loaded</title>
      <description>Today it always loads from stored fields (searcher.doc), but it's sometimes useful to customize this, eg if your app separately already loads stored fields then it can avoid double-loading them. Or if your app has some other place to pull the values from ...</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4847</id>
      <title>Sorter API: Fully reuse docs enums</title>
      <description>SortingAtomicReader reuses the filtered docs enums but not the wrapper. In the case of SortingAtomicReader this can be a problem because the wrappers are heavyweight (they load the whole postings list into memory), so an index with many terms with high freqs will make the JVM allocate a lot of memory when browsing the postings lists.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4849</id>
      <title>Make ParallelTaxonomyArrays abstract</title>
      <description>ParallelTaxonomyArrays, while appearing on TaxonomyReader, actually support only one implementation, that of DirectoryTaxonomyReader. I'd like to make it abstract (perhaps share the children/siblings arrays computation) to allow for other taxonomy reader implementations.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4851</id>
      <title>Use Java 7's {Integer,Long,Float,Double}.compare instead of branches</title>
      <description>We can use those methods now that trunk is on Java 7.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4853</id>
      <title>PostingsHighlighter should let you pass in docids directly [expert]</title>
      <description>Today it just takes TopDocs, which is very convenient, except if you are doing grouping and you have a TopGroups. I think it should let you pass in int[] docIDs ...</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4857</id>
      <title>StemmerOverrideFilter should not copy the stem override dictionary in it's ctor.</title>
      <description>Currently the dictionary is cloned each time the token filter is created which is a serious bottleneck if you use this filter with large dictionaries and can also lead to OOMs if lots of those filters sit in ThreadLocals and new threads are added etc. I think cloning the map should be done in the analyzer (which all of our analyzers do btw. but this is the only TF that does that) no need to really copy that map.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4858</id>
      <title>Early termination with SortingMergePolicy</title>
      <description>Spin-off of LUCENE-4752, see https://issues.apache.org/jira/browse/LUCENE-4752?focusedCommentId=13606565&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13606565 and https://issues.apache.org/jira/browse/LUCENE-4752?focusedCommentId=13607282&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13607282 When an index is sorted per-segment, queries that sort according to the index sort order could be early terminated.</description>
      <attachments/>
      <comments>42</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4859</id>
      <title>Expose more stats on IndexReader</title>
      <description>IndexReader exposes stats like totalTermFreq(term) and docFreq(term). I'd like to add more stats from Terms, e.g. getDocCount/getSumDocFreq/getSumTotalTermFreq(field) for convenience. The implementation is very easy to do on CompositeReader, so I'll add these impls to BaseCompositeReader. I'll attach a patch shortly.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4860</id>
      <title>PostingsHighlighter should pass field name to PassageFormatter.format?</title>
      <description>If the app needs to render different fields (eg multi-valued vs single-valued) differently it's tricky now. You can do Passage[0].getMatchTerms()[0].field(), but then that doesn't work if that field hit the empty highlight. I think we should pass the fieldName to format directly? And then maybe change getMatchTerms() to return BytesRef[] instead (the field name is redundant: they are all the same for all passages passed to format).</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4861</id>
      <title>can we use a single PostingsHighlighter for both whole and snippet highlighting?</title>
      <description>Right now, because we pass the BreakIterator to the ctor, you have to make two instances if you sometimes want whole and sometimes want snippets. But I think this is a fairly common use case, eg I want entire title field (with matches highlighted) but I want body field (snippets + highlights). It would be nice to have this work with a single instance ...</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4862</id>
      <title>Ability to terminate queries on a per-segment basis</title>
      <description>Spin-off of LUCENE-4752. The idea is to add a marker exception that tells IndexSearcher to terminate the collection of the current segment.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4863</id>
      <title>Use FST to hold term in StemmerOverrideFilter</title>
      <description>follow-up from LUCENE-4857</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4864</id>
      <title>Add AsyncFSDirectory to work around Windows issues with NIOFS (Lucene 5.0 only)</title>
      <description>On LUCENE-4848 a new directory implementation was proposed that uses AsyncFileChannel to make a sync-less directory implementation (only needed for IndexInput). The problem on Windows is that positional reads are impossible without overlapping (async) I/O, so FileChannel in the JDK has to syncronize all reads, because they consist of an atomic seek and atomic read. AsyncFSDirectoty would not have this issue, but has to take care of thread management, because you need a separate thread to get notified when the read is done. This involves overhead, but might still be better than the synchronization.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4867</id>
      <title>SorterTemplate.merge is slow</title>
      <description>SorterTemplate.mergeSort/timSort can be very slow. For example, I ran a quick benchmark that sorts an Integer[] array of 50M elements, and mergeSort was almost 4x slower than quickSort (~12.8s for quickSort, ~46.5s for mergeSort). This is even worse when the cost of a swap is higher (e.g. parallel arrays). This is due to SorterTemplate.merge. I first feared that this method might not be linear, but it is, so the slowness is due to the fact that this method needs to swap lots of values in order not to require extra memory. Could we make it faster? For reference, I hacked a SorterTemplate instance to use the usual merge routine (that requires n/2 elements in memory), and it was much faster: ~17s on average, so there is room for improvement.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4869</id>
      <title>Optimize IsWithin spatial RPT to use a point cache for false-positive removal</title>
      <description>LUCENE-4644 implemented the "IsWithin" predicate for a RecursivePrefixTree based field. It's slow since it looks across the whole world to ensure it doesn't match docs with data anywhere outside the query shape. It can be configured to only look outside the query shape using a very small buffer distance, and that will filter out documents spanning the query shape boundary, but not indexed shapes comprised of multiple disjoint parts. The solution proposed here is to index a point per disjoint part in such a way that it can be easily retrieved (e.g. DocValues) and then a post-process of WithinPrefixTreeFilter would remove false-positives. This isn't particularly hard/advanced but it requires some advances in some APIs that aren't quite there yet. Spatial4j's ShapeCollection (aka WKT GeometryCollection or Multi*) needs to get released, it needs a vertex iterator. There needs to be code to read and write a set of points to a BinaryDocValues field (1/doc). And finally of course WithinPrefixTreeFilter needs to have a mode in which it uses the smallest buffer and then in the end checks the DocValues to remove false-postivies.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4871</id>
      <title>Sorter API: better compress positions, offsets and payloads in SortingDocsAndPositionsEnum</title>
      <description>SortingDocsAndPositionsEnum could easily save memory by using a Lucene40TCF-like compression method for positions, offsets and payloads: delta-encode positions and startOffsets (with the previous end offset), store the length of the tokens instead of their end offset (endOffset == startOffset + length), use a single bit to say whether the token has a payload.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4875</id>
      <title>Make SorterTemplate.mergeSort run in linear time on sorted arrays</title>
      <description>Through minor modifications, SorterTemplate.mergeSort could run in linear time on sorted arrays, so I think we should do it? The idea is to modify merge so that it returns instantly when compare(pivot-1, pivot) &lt;= 0.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4879</id>
      <title>Filter stack traces on console output.</title>
      <description>We could filter stack traces similar to what ANT's JUnit task does. It'd remove some of the noise and make them shorter. I don't think the lack of stack filtering is particularly annoying and it's always to have an explicit view of what and where happened but since Robert requested this I'll add it. We can always make it a (yet another) test.* option</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4881</id>
      <title>Add a set iterator to SentinelIntSet</title>
      <description>I'm working on code that needs a hash based int Set. It will need to iterate over the values, but SentinalIntSet doesn't have this utility feature. It should be pretty easy to add. FYI this is an out-growth of a question I posed to the dev list, examining 3 different int hash sets out there: SentinalIntSet, IntHashSet (in Lucene facet module) and the 3rd party IntOpenHashSet (HPPC) – see http://lucene.472066.n3.nabble.com/IntHashSet-SentinelIntSet-SortedIntDocSet-td4037516.html I decided to go for SentinalIntSet because it's already in Lucene-core, adding the method I need should be easy, and it has a nice lean implementation.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4883</id>
      <title>Hide FieldCache behind an UninvertingFilterReader</title>
      <description>From a discussion on the mailing list: {{ rmuir: I think instead FieldCache should actually be completely package private and hidden behind a UninvertingFilterReader and accessible via the existing AtomicReader docValues methods. }}</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4887</id>
      <title>FSA NoOutputs should implement merge() allowing duplicate keys</title>
      <description>The NoOutput Object throws NotImplemented if you try to add the same input twice. This can easily be implemented</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4895</id>
      <title>Implement Spatial "disjoint" predicate.</title>
      <description>The "IsDisjointTo" SpatialOperation is not implemented for RecursivePrefixTreeStrategy and some/all others. It has been very low priority because it is simply the inverse of "Intersects" which is universally implemented on the SpatialStrategy implementations. Should spatial "disjoint" count documents that have no spatial data? Arguably, there should be one implementation amongst the SpatialStrategies implemented in terms of "Intersects"; this way each strategy need not deal with it.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4896</id>
      <title>PostingsHighlighter should use a interface of PassageFormatter instead of a class</title>
      <description>In my project I need a custom PassageFormatter to use with PostingsHighlighter. I extended PassageFormatter to override format(...) but if I do that, I don't have access to the private variables. So instead of changing the scope to protected, it should be more usefull to use a interface for PassageFormatter. like public DefaultPassageFormatter implements PassageFormatter.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4897</id>
      <title>Add a sugar API for traversing categories.</title>
      <description>Mike McCandless said in lucene-java-user mailing list. "Maybe we could add some simple sugar APIs? Eg something like Collection&lt;CategoryPath&gt; getChildren(int parentOrd)? (Or maybe it returns Iterator&lt;CategoryPath&gt;?)" What about Collection&lt;Integer&gt; getChildren(int parentOrd)? Integer would be more versatile and can easily be converted to CategoryPath with TaxonomyReader.getPath.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4898</id>
      <title>Allow DirectoryReader.openIfChanged to an IndexCommit when reader is NRT</title>
      <description>This throws an IllegalArRgumentException today, but it's easy to fix with a small change to DirectoryReader. This is useful to do, if you have an NRT reader and want to open a reader against a previous commit point while sharing the SegmentReaders they have in common.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4901</id>
      <title>TestIndexWriterOnJRECrash should work on any JRE vendor via Runtime.halt()</title>
      <description>I successfully compiled Lucene 4.2 with IBM. Then ran unit tests with the nightly option set to "true" The test case TestIndexWriterOnJRECrash was skipped returning "IBM Corporation JRE not supported": [junit4:junit4] Suite: org.apache.lucene.index.TestIndexWriterOnJRECrash [junit4:junit4] IGNOR/A 0.28s | TestIndexWriterOnJRECrash.testNRTThreads [junit4:junit4] &gt; Assumption #1: IBM Corporation JRE not supported. [junit4:junit4] Completed in 0.68s, 1 test, 1 skipped</description>
      <attachments/>
      <comments>23</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4902</id>
      <title>Add a FilterDirectoryReader</title>
      <description>A FilterDirectoryReader would allow you to easily wrap all subreaders of a DirectoryReader with FilterAtomicReaders.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4903</id>
      <title>Add AssertingScorer</title>
      <description>I think we would benefit from having an AssertingScorer that would assert that scorers are advanced correctly, return valid scores (eg. not NaN), ...</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4904</id>
      <title>Sorter API: Make NumericDocValuesSorter able to sort in reverse order</title>
      <description>Today it is only able to sort in ascending order.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4905</id>
      <title>make maxPassages per-field in postingshighlighter</title>
      <description>in case you e.g. want smaller summarizes for a bunch of little fields and then a bit bigger one for the body field and so on. We can do this by only changing the 'expert' methods.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4906</id>
      <title>PostingsHighlighter's PassageFormatter should allow for rendering to arbitrary objects</title>
      <description>For example, in a server, I may want to render the highlight result to JsonObject to send back to the front-end. Today since we render to string, I have to render to JSON string and then re-parse to JsonObject, which is inefficient... Or, if (Rob's idea we make a query that's like MoreLikeThis but it pulls terms from snippets instead, so you get proximity-influenced salient/expanded terms, then perhaps that renders to just an array of tokens or fragments or something from each snippet.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4909</id>
      <title>add proximity scoring to postingshighlighter</title>
      <description>I think this can improve the snippet quality without degrading performance, screwing up the summaries, or adding too much complexity.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4911</id>
      <title>Missing word "cela" in conf/lang/stopwords_fr.txt</title>
      <description>NB: Not sure this defect is assigned to the right component. In file example/solr/collection1/conf/lang/stopwords_fr.txt, there is the word "celà". Though incorrect in French (cf http://fr.wiktionary.org/wiki/cel%C3%A0), it's common, but we may also add the correct spelling (e.g. "cela", whitout accent) to that stopwords list. Another thing: I noticed that "celà" is the only word of the list followed by an unbreakable space. Is that wanted?</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4917</id>
      <title>Generalize the ClassifierTestBase to allow non BytesRef classifiers</title>
      <description>Currently ClassifierTestBase only allows to test classifiers that implement Classifier&lt;BytesRef&gt; while it'd be good to ease test of other types of classifiers.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4921</id>
      <title>Create a DocValuesFormat for sparse doc values</title>
      <description>We could have a special DocValuesFormat in lucene/codecs to better handle sparse doc values. See http://search-lucene.com/m/HUeYW1RlEtc</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4922</id>
      <title>A SpatialPrefixTree based on the Hilbert Curve and variable grid sizes</title>
      <description>My wish-list for an ideal SpatialPrefixTree has these properties: Hilbert Curve ordering Variable grid size per level (ex: 256 at the top, 64 at the bottom, 16 for all in-between) Compact binary encoding (so-called "Morton number") Works for geodetic (i.e. lat &amp; lon) and non-geodetic Some bonus wishes for use in geospatial: Use an equal-area projection such that each cell has an equal area to all others at the same level. When advancing a grid level, if a cell's width is less than half its height. then divide it as 4 vertically stacked instead of 2 by 2. The point is to avoid super-skinny cells which occurs towards the poles and degrades performance. All of this requires some basic performance benchmarks to measure the effects of these characteristics.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4923</id>
      <title>remove minShouldMatch/speed up DisjunctionSumScorer</title>
      <description>LUCENE-4571 added a MinShouldMatchScorer: capable of using advance() on subscorers for minShouldMatch &gt; 1. However, we didn't yet cleanup the (now dead) minShouldMatch logic from DisjunctionSumScorer: which is now only used for pure disjunctions. This can also give some reasonable performance improvements for in-order collection.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4924</id>
      <title>Make DocIdSetIterator.docID() return -1 when not positioned</title>
      <description>Today DocIdSetIterator.docID() can either return -1 or NO_MORE_DOCS when the enum is not positioned. I would like to only allow it to return -1 so that we can have better assertions. (This proposal is for trunk only.)</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4926</id>
      <title>speed up disjunctionmaxscorer</title>
      <description>Applying the same approach as LUCENE-4923 gives ~ 30% improvement according to luceneutil. Task QPS trunk StdDev QPS patch StdDev Pct diff OrMaxHighHigh 17.94 (2.7%) 23.23 (3.1%) 29.5% ( 23% - 36%) OrMaxHighLow 28.08 (2.6%) 37.11 (3.9%) 32.1% ( 25% - 39%) OrMaxHighMed 16.80 (2.7%) 22.25 (3.9%) 32.5% ( 25% - 40%) diff -r e0ea40cf243c perf/TaskParser.java --- a/perf/TaskParser.java Mon Apr 01 14:49:08 2013 +0200 +++ b/perf/TaskParser.java Wed Apr 10 22:35:40 2013 -0400 @@ -31,6 +31,7 @@ import org.apache.lucene.search.BooleanClause; import org.apache.lucene.search.BooleanQuery; import org.apache.lucene.search.CachingWrapperFilter; +import org.apache.lucene.search.DisjunctionMaxQuery; import org.apache.lucene.search.Filter; import org.apache.lucene.search.NumericRangeQuery; import org.apache.lucene.search.Query; @@ -169,6 +170,17 @@ true); sort = null; group = null; + } else if (text.startsWith("disjunctionMax//")) { + final int spot3 = text.indexOf(' '); + if (spot3 == -1) { + throw new RuntimeException("failed to parse query=" + text); + } + DisjunctionMaxQuery dismax = new DisjunctionMaxQuery(1f); + dismax.add(new TermQuery(new Term(fieldName, text.substring(16, spot3)))); + dismax.add(new TermQuery(new Term(fieldName, text.substring(spot3+1).trim()))); + query = dismax; + sort = null; + group = null; } else if (text.startsWith("nrq//")) { // field start end final int spot3 = text.indexOf(' '); cat wikimedium.10M.nostopwords.tasks | grep "^Or" | sed -e "s/Or\([a-zA-Z]*\)\:\ /OrMax\1\:\ disjunctionMax\/\//g" &gt; dismax.tasks</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4930</id>
      <title>Lucene's use of WeakHashMap at index time prevents full use of cores on some multi-core machines, due to contention</title>
      <description>Our project is not optimally using full processing power during under indexing load on Lucene 4.2.0. The reason is the AttributeSource.addAttribute() method, which goes through a WeakHashMap synchronizer, which is apparently single-threaded for a significant amount of time. Have a look at the following trace: "pool-1-thread-28" prio=10 tid=0x00007f47fc104800 nid=0x672b waiting for monitor entry [0x00007f47d19ed000] java.lang.Thread.State: BLOCKED (on object monitor) at java.lang.ref.ReferenceQueue.poll(ReferenceQueue.java:98) waiting to lock &lt;0x00000005c5cd9988&gt; (a java.lang.ref.ReferenceQueue$Lock) at org.apache.lucene.util.WeakIdentityMap.reap(WeakIdentityMap.java:189) at org.apache.lucene.util.WeakIdentityMap.get(WeakIdentityMap.java:82) at org.apache.lucene.util.AttributeSource$AttributeFactory$DefaultAttributeFactory.getClassForInterface(AttributeSource.java:74) at org.apache.lucene.util.AttributeSource$AttributeFactory$DefaultAttributeFactory.createAttributeInstance(AttributeSource.java:65) at org.apache.lucene.util.AttributeSource.addAttribute(AttributeSource.java:271) at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:107) at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:254) at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:256) at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:376) at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1473) at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1148) at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1129) … We’ve had to make significant changes to the way we were indexing in order to not hit this issue as much, such as indexing using TokenStreams which we reuse, when it would have been more convenient to index with just tokens. (The reason is that Lucene internally creates TokenStream objects when you pass a token array to IndexableField, and doesn’t reuse them, and the addAttribute() causes massive contention as a result.) However, as you can see from the trace above, we’re still running into contention due to other addAttribute() method calls that are buried deep inside Lucene. I can see two ways forward. Either not use WeakHashMap or use it in a more efficient way, or make darned sure no addAttribute() calls are done in the main code indexing execution path. (I think it would be easy to fix DocInverterPerField in that way, FWIW. I just don’t know what we’ll run into next.)</description>
      <attachments/>
      <comments>41</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>4936</id>
      <title>docvalues date compression</title>
      <description>DocValues fields can be very wasteful if you are storing dates (like solr's TrieDateField does if you enable docvalues) and don't actually need all the precision: e.g. "date-only" fields like date of birth with no time component, time fields without milliseconds precision, and so on. Ideally we'd compute GCD of all the values to save space (numberOfTrailingZeros is not really enough here), but i think we should at least look for values like 86400000, 3600000, and 1000 to be practical.</description>
      <attachments/>
      <comments>34</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4941</id>
      <title>JoinUtil's TermsQuery should sort terms only once.</title>
      <description>The sorting of the 'from' terms occurs as often as the number of segments. This only needs to happen once.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4942</id>
      <title>Indexed non-point shapes index excessive terms</title>
      <description>Indexed non-point shapes are comprised of a set of terms that represent grid cells. Cells completely within the shape or cells on the intersecting edge that are at the maximum detail depth being indexed for the shape are denoted as "leaf" cells. Such cells have a trailing '+' at the end. Such tokens are actually indexed twice, one with the leaf byte and one without. The TermQuery based PrefixTree Strategy doesn't consider the notion of 'leaf' cells and so the tokens with '+' are completely redundant. The Recursive [algorithm] based PrefixTree Strategy better supports correct search of indexed non-point shapes than TermQuery does and the distinction is relevant. However, the foundational search algorithms used by this strategy (Intersects &amp; Contains; the other 2 are based on these) could each be upgraded to deal with this correctly. Not trivial but very doable. In the end, spatial non-point indexes can probably be trimmed my ~40% by doing this.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4945</id>
      <title>CustomScoreQuery's should have getters for its fields</title>
      <description>It would be convenient if CustomScoreQuery provided access to all its fields, not just isStrict(). Today I found that my subclass of it was forced to store redundant fields just to access the state.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4946</id>
      <title>Refactor SorterTemplate</title>
      <description>When working on TimSort (LUCENE-4839), I was a little frustrated of not being able to add galloping support because it would have required to add new primitive operations in addition to compare and swap. I started working on a prototype that uses inheritance to allow some sorting algorithms to rely on additional primitive operations. You can have a look at https://github.com/jpountz/sorts/tree/master/src/java/net/jpountz/sorts (but beware it is a prototype and still misses proper documentation and good tests). I think it would offer several advantages: no more need to implement setPivot and comparePivot when using in-place merge sort or insertion sort, the ability to use faster stable sorting algorithms at the cost of some memory overhead (our in-place merge sort is very slow), the ability to implement properly algorithms that are useful on specific datasets but require different primitive operations (such as TimSort for partially-sorted data). If you are interested in comparing these implementations with Arrays.sort, there is a Benchmark class in src/examples. What do you think?</description>
      <attachments/>
      <comments>16</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4947</id>
      <title>Java implementation (and improvement) of Levenshtein &amp; associated lexicon automata</title>
      <description>I was encouraged by Mike McCandless to open an issue concerning this after I contacted him privately about it. Thanks Mike! I'd like to submit my Java implementation of the Levenshtein Automaton as a homogenous replacement for the current heterogenous, multi-component implementation in Lucene. Benefits of upgrading include Reduced code complexity Better performance from components that were previously implemented in Python Support for on-the-fly dictionary-automaton manipulation (if you wish to use my dictionary-automaton implementation) The code for all the components is well structured, easy to follow, and extensively commented. It has also been fully tested for correct functionality and performance. The levenshtein automaton implementation (along with the required MDAG reference) can be found in my LevenshteinAutomaton Java library here: https://github.com/klawson88/LevenshteinAutomaton. The minimalistic directed acyclic graph (MDAG) which the automaton code uses to store and step through word sets can be found here: https://github.com/klawson88/MDAG *Transpositions aren't currently implemented. I hope the comment filled, editing-friendly code combined with the fact that the section in the Mihov paper detailing transpositions is only 2 pages makes adding the functionality trivial. Update introduces transposition inclusion in edit distance calculations! *As a result of support for on-the-fly manipulation, the MDAG (dictionary-automaton) creation process incurs a slight speed penalty. In order to have the best of both worlds, i'd recommend the addition of a constructor which only takes sorted input. The complete, easy to follow pseudo-code for the simple procedure can be found in the first article I linked under the references section in the MDAG repository)</description>
      <attachments/>
      <comments>28</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4951</id>
      <title>DrillSidewaysScorer should use Scorer.cost instead of its own heuristic</title>
      <description>Today it does the "first docID" trick to guess the cost of the baseQuery, which is silly now that we have cost API.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4956</id>
      <title>the korean analyzer that has a korean morphological analyzer and dictionaries</title>
      <description>Korean language has specific characteristic. When developing search service with lucene &amp; solr in korean, there are some problems in searching and indexing. The korean analyer solved the problems with a korean morphological anlyzer. It consists of a korean morphological analyzer, dictionaries, a korean tokenizer and a korean filter. The korean anlyzer is made for lucene and solr. If you develop a search service with lucene in korean, It is the best idea to choose the korean analyzer.</description>
      <attachments/>
      <comments>194</comments>
      <commenters>15</commenters>
    </issue>
    <issue>
      <id>4961</id>
      <title>Filters should return null if they don't accept documents</title>
      <description>Today we document that Filter#getDocIdSet can return null if it doesn't accept documents. Infact in the code we sometimes return null and sometimes return DocIdSet.EMPTY_DOCIDSET. Conceptually there is nothing wrong with that but apparently we are not applying our optimisations accordingly ie. some parts of the code check for the EMPTY_DOCIDSET and all check for null. this is also a source of potential bugs like in LUCENE-4940 and I think there are still problems in the ToChildBlock query. Anyways, I think we should be consistent here about when to apply the optimisations and for the sake of caching in CachingWrapperFilter we should make the EMPTY_DOCIDSET and impl detail.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4962</id>
      <title>Add LTC.newReader</title>
      <description>Spinoff from LUCENE-4953. It would be nice to break out the reader wrapping that LTC.newSearcher does into a separate newReader.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4963</id>
      <title>Deprecate broken TokenFilter constructors</title>
      <description>We have some TokenFilters which are only broken with specific options. This includes: TrimFilter when updateOffsets=true StopFilter, JapanesePartOfSpeechStopFilter, KeepWordFilter, LengthFilter, TypeTokenFilter when enablePositionIncrements=false I think we should deprecate these behaviors in 4.4 and remove them in trunk.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4964</id>
      <title>Allow custom drill-down sub-queries</title>
      <description>Today the facet module indexes a term for each facet added to a document, and DrillDown/SidewaysQuery assume this by creating a TermQuery, or OR of TermQuery, for each dimension the app drills down on. I think we should relax this and allow an [expert] arbitrary query to drill down on a given dimension ... e.g., this can enable future dynamic faceting methods, or custom app drill-down methods. It's easy for DrillDownQuery to do this, but requires generalization in DrillSideways, basically just reviving the first approach on LUCENE-4748. This approach is somewhat slower, but more general ... it will keep using the current method as an optimization when it applies. This should also fix the possible performance regression from LUCENE-4952 when scoreSubDocsAtOnce is true, by using the MinShouldMatchSumScorer in that case.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4965</id>
      <title>Add dynamic numeric range faceting</title>
      <description>The facet module today requires the app to compute the hierarchy at index time, eg a timestamp field might use a year/month/day hierarchy. While this gives great performance, since it minimizes the search-time computation, sometimes it's unfortunately useful/necessary to do things entirely at search time, like Solr does. E.g. I'm playing with a prototype Lucene search for Jira issues and I'd like to add a drill down+sideways for "Updated in past day, 2 days, week, month" etc. But because time is constantly advancing, doing this at index time is a not easy ...</description>
      <attachments/>
      <comments>26</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>4966</id>
      <title>Add CachingWrapperFilter.sizeInBytes()</title>
      <description>I think it's useful to be able to check how much RAM a given CWF is using ...</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4967</id>
      <title>Absorb NRTManager entirely into a separate reopen thread class</title>
      <description>I think NRTManager can be drastically simplified by moving all of its logic into a new reopen thread class. All logic for waiting for a specific generation and reopening at different rates would live in this class. This would fully decouple the "wait for generation X to be visible" from which particular ReferenceManager impl you're using, which would make it possible to use the controlled consistency approach of NRTManager with any managers (e.g. SearcherTaxonomyManager).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4969</id>
      <title>add sort comparator that uses OrdinalMap</title>
      <description>OrdinalMap (LUCENE-4795, SOLR-4490) lets you trade off reopen time for faster faceting: it maps per-segment ordinals to a "global" space based on deviation from expected deltas. We could add a sort comparator to the sandbox or similar that uses this, especially in the case you are using this for faceting anyway: you can then safely offload all terms bytes to disk (DiskDV) and still have performant sorting and faceting.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>4973</id>
      <title>SnapshotDeletionPolicy should not require an id</title>
      <description>The id is unnecessary and just adds complexity: SDP can just return the IndexCommit, and when you want to release you pass back the IndexCommit. PersistentSDP can expose release(long gen).</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4975</id>
      <title>Add Replication module to Lucene</title>
      <description>I wrote a replication module which I think will be useful to Lucene users who want to replicate their indexes for e.g high-availability, taking hot backups etc. I will upload a patch soon where I'll describe in general how it works.</description>
      <attachments/>
      <comments>39</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>4976</id>
      <title>PersistentSnapshotDeletionPolicy should save to a single file</title>
      <description>Today it creates a single-document Lucene index, and calls commit() after each snapshot/release. I think we can just use a single file instead, and remove Closeable.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4979</id>
      <title>LiveFieldValues should accept any ReferenceManager</title>
      <description>Today it requires ReferenceManager&lt;IndexSearcher&gt; but it doesn't rely on that at all (it just forwards that IndexSearcher to the subclass's lookup method).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>4981</id>
      <title>Deprecate PositionFilter</title>
      <description>According to the documentation (http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.PositionFilterFactory), PositionFilter is mainly useful to make query parsers generate boolean queries instead of phrase queries although this problem can be solved at query parsing level instead of analysis level (eg. using QueryParser.setAutoGeneratePhraseQueries). So given that PositionFilter corrupts token graphs (see TestRandomChains), I propose to deprecate it.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>4982</id>
      <title>Make MockIndexOutputWrapper check disk full on copyBytes</title>
      <description>While working on the consistency test for Replicator (LUCENE-4975), I noticed that I don't trip disk-full exceptions and tracked it down to MockIndexOutputWrapper.copyBytes not doing these checks like writeBytes. I'd like to add this check.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>4985</id>
      <title>Make it easier to mix different kinds of FacetRequests</title>
      <description>Spinoff from LUCENE-4980, where we added a strange class called RangeFacetsAccumulatorWrapper, which takes an incoming FSP, splits out the FacetRequests into range and non-range, delegates to two accumulators for each set, and then zips the results back together in order. Somehow we should generalize this class and make it work with SortedSetDocValuesAccumulator as well.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4990</id>
      <title>Improve MockDirectoryWrapper.sync</title>
      <description>Currently MockDirWrapper always calls delegate.sync() with a comment that we can relax this to not wear out the hardware for tests. The issue, as discussed on this thread http://lucene.markmail.org/thread/eozdsbdahzhjvizj, is related to NRTCachingDirectory and RateLimiter. The improvements I'd like to make under this issue are: Call delgeate.sync() if: rarely() delegate is NRTCachingDir delegate is RateLimitedDirWrapper and its delegate is NRTCachingDir delegate is TrackingDirWrapper and its delegate is NRTCachingDir Also, today the method either fails to sync all files or succeeds. Rather, we can improve this to randomly throw IOE on each file. Any other Directories that can cause issues when sync() isn't called?</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>4998</id>
      <title>be more precise about IOContext for reads</title>
      <description>Set the context as IOContext.READ / IOContext.READONCE where applicable Motivation: Custom PostingsFormat may want to check the context on SegmentReadState and branch differently, but for this to work properly the context has to be specified correctly up the stack. For example, DirectPostingsFormat only loads postings into memory if the context != MERGE. However a better condition would be context == Context.READ &amp;&amp; !context.readOnce.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5000</id>
      <title>Query serialization using ObjectInput/OutputStream</title>
      <description>Reads and writes queries on ObjectInput/OutputStream. No support for ConstantScoreQuery (no serialization for Filter) nor PayloadNearQuery and PayloadTermQuery (no serialization for PayloadFunction). I might have missed to implement support for some core Queries. Currently supported are: TermQuery, BooleanQuery, WildcardQuery, PhraseQuery, MultiPhraseQuery, FuzzyQuery, RegexpQuery, TermRangeQuery, NumericRangeQuery, DisjunctionMaxQuery, MatchAllDocsQuery, SpanTermQuery, SpanMultiTermQueryWrapper, SpanNearQuery, SpanNotQuery, SpanOrQuery, FieldMaskingSpanQuery, SpanFirstQuery, SpanPositionRangeQuery, SpanPayloadCheckQuery and SpanNearPayloadCheckQuery. Users are allowed to implement and register strategies for their own queries. This will not allow you to serialize any object graph with aggregated Query instances e.g. Map&lt;String, Query&gt;, that would require a new implementation of ObjectOutputStream (one could base that on the GPL2 code from OpenJDK) or instrument the existing implementations to handle Query in private writeObjectA and similar methods. There's a bit of reflection glue in this code in order to read private fields in query implementation. Not too happy about that, but not much to do unless one is to expose a bunch of new getters in all those classes. The class is places in o.a.l.search in order to access package visible fields without getters. If moving to another package this would have to be handled using reflection as with above mentioned private fields.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5005</id>
      <title>Length norm value of DefaultSimilarity for a few terms</title>
      <description>lengthNorm method of DefaultSimilarity is following: public float lengthNorm(FieldInvertState state) { final int numTerms; if (discountOverlaps) numTerms = state.getLength() - state.getNumOverlap(); else numTerms = state.getLength(); return state.getBoost() * ((float) (1.0 / Math.sqrt(numTerms))); } The retrun value is decided by (1.0 / Math.sqrt(numTerms)). The type is float, but this value is encoded to byte length by SmallFloat.floatToByte315. term count 1/sqrt(numTerms) 1/sqrt(numTerms) to byte 1 1.000000 1.0000 2 0.707107 0.6250 3 0.577350 0.5000 4 0.500000 0.5000 5 0.447214 0.4375 The length norm of 3 terms is the same as that of 4 terms.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5007</id>
      <title>smokeTestRelease.py should be able to pass cmdline test args to 'ant test', e.g. "-Dtests.jettyConnector=Socket"; also, "ant nightly-smoke" should be able to pass these args to smokeTestRelease.py</title>
      <description>SOLR-4189 added sensitivity to sysprop tests.jettyConnector to allow setting test mode Jetty to use Socket connector instead of the default SelectChannel connector. New module lucene/replicator is running into the same problem, failing 100% of the time when running under 'ant nightly-smoke' on ASF Jenkins on FreeBSD. At present there's no way from smokeTestRelease.py, or from "ant nightly-smoke", to pass through this sysprop (or any other). Robert Muir wrote on dev@l.o.a about one of the replicator module's failures on FreeBSD: This is a jenkins setup/test harness issue. there needs to be a way for the jetty connector sysprop to be passed all the way thru to ant test running from the smoketester.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5012</id>
      <title>Make graph-based TokenFilters easier</title>
      <description>SynonymFilter has two limitations today: It cannot create positions, so eg dns -&gt; domain name service creates blatantly wrong highlights (SOLR-3390, LUCENE-4499 and others). It cannot consume a graph, so e.g. if you try to apply synonyms after Kuromoji tokenizer I'm not sure what will happen. I've thought about how to fix these issues but it's really quite difficult with the current PosInc/PosLen graph representation, so I'd like to explore an alternative approach.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>5013</id>
      <title>ScandinavianFoldingFilterFactory and ScandinavianNormalizationFilterFactory</title>
      <description>This filter is an augmentation of output from ASCIIFoldingFilter, it discriminate against double vowels aa, ae, ao, oe and oo, leaving just the first one. blåbærsyltetøj == blåbärsyltetöj == blaabaarsyltetoej == blabarsyltetoj räksmörgås == ræksmørgås == ræksmörgaos == raeksmoergaas == raksmorgas Caveats: Since this is a filtering on top of ASCIIFoldingFilter äöåøæ already has been folded down to aoaoae when handled by this filter it will cause effects such as: bøen -&gt; boen -&gt; bon åene -&gt; aene -&gt; ane I find this to be a trivial problem compared to not finding anything at all. Background: Swedish åäö is in fact the same letters as Norwegian and Danish åæø and thus interchangeable in when used between these languages. They are however folded differently when people type them on a keyboard lacking these characters and ASCIIFoldingFilter handle ä and æ differently. When a Swedish person is lacking umlauted characters on the keyboard they consistently type a, a, o instead of å, ä, ö. Foreigners also tend to use a, a, o. In Norway people tend to type aa, ae and oe instead of å, æ and ø. Some use a, a, o. I've also seen oo, ao, etc. And permutations. Not sure about Denmark but the pattern is probably the same. This filter solves that problem, but might also cause new.</description>
      <attachments/>
      <comments>37</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>5020</id>
      <title>Make DrillSidewaysResult ctor public</title>
      <description>DrillSidewaysResult has a package-private ctor which prevents initializing it by an app. I found that it's sometimes useful for e.g. doing some post-processing on the returned TopDocs or List&lt;FacetResult&gt;. Since you cannot return two values from a method, it will be convenient if method could return a new 'processed' DSR. I would also like to make the hits member final.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5021</id>
      <title>NextDoc NPE safety when bulk collecting</title>
      <description>Hello, I would like to apply ACL once as a PostFilter and I therefore need to bulk this call since round trips would severely decrease performances. I tried to just stack them on the DelegatingCollector using this collect : @Override public void collect(int doc) throws IOException { while ((doc = scorer.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) { docs.put(getDocumentId(doc), doc); } batchCollect(); } Depending on the Scorer it may or it may not work. Indeed when the Scorer is "Safe" that is when it handles the case in which the scorer is exhausted and is called once again after exhaustion. This is the case of the (e.g. DisjunctionMaxScorer, ConstantScorer): if (numScorers == 0) return doc = NO_MORE_DOCS; On the other hand, when using the DisjunctionSumScorer, it either asserts on "NO_MORE_DOCS", or it throws a NPE. Shouldn't we copy the DisjunctionMaxScorer mechanism to protect nextDoc of an exausted iterator using either current doc or checking numbers of subScorers ?</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5022</id>
      <title>Add FacetResult.mergeHierarchies</title>
      <description>When you DrillSideways on a hierarchical dimension, and especially when you OR multiple drill-downs together, you get several FacetResults back, one for each category you drill down on. So for example, if you want to drill-down on Date/2010 OR Date/2011/May, the FacetRequests that you need to create (to get the sideways effect) are: Date/, Date/2010, Date/2011 and Date/2011/May. Date/ is because you want to get sideways counts as an alternative to Date/2010, and Date/2011 in order to get months count as an alternative to Date/2011/May. That results in 4 FacetResult objects. Having a utility which merges all FacetResults of the same dimension into a single hierarchical one will be very useful for e.g. apps that want to display the hierarchy. I'm thinking of FacetResult.mergeHierarchies which takes a List&lt;FacetResult&gt; and returns the merged ones, one FacetResult per dimension.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5025</id>
      <title>Allow more than 2.1B "tail nodes" when building FST</title>
      <description>We recently relaxed some of the limits for big FSTs, but there is one more limit I think we should fix. E.g. Aaron hit it in building the world's biggest FST: http://aaron.blog.archive.org/2013/05/29/worlds-biggest-fst/ The issue is NodeHash, which currently uses a GrowableWriter (packed ints impl that can grow both number of bits and number of values): it's indexed by int not long. This is a hash table that's used to share suffixes, so we need random get/put on a long index of long values, i.e. this is logically a long[]. I think one simple way to do this is to make a "paged" GrowableWriter... Along with this we'd need to fix the hash codes to be long not int.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5026</id>
      <title>PagedGrowableWriter</title>
      <description>We already have packed data structures that support more than 2B values such as AppendingLongBuffer and MonotonicAppendingLongBuffer but none of them supports random write-access. We could write a PagedGrowableWriter for this, which would essentially wrap an array of GrowableWriters.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5027</id>
      <title>remove DocConsumer.doAfterFlush</title>
      <description>I think this stuff is dead code and confusing: talking about clearing state across segments when the DWPT is not reused?</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5028</id>
      <title>doShare is pointless in PositiveIntOutputs</title>
      <description>We recently use this in oal.core.fst.PositiveIntOutputs to indicate whether to share outputs. The comment mentioned 'with doShare=false, in some case this may result in a smaller FST'. However, this is not intuitive, as for long type, we always have the smallest output reduced to NO_OUTPUT, thus the smallest one is 'moved' towards root, and no extra output is created. However, if there are many many small outputs around root arcs, when we share outputs, a large output might be pushed into the root arcs. When root arcs are packed as fixed-array, yes the size of FST is increased. But, I suppose this should invoke other intuitive heuristics, instead of the confusing 'doShare'? Besides, this only exist in PositiveIntOutputs.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5034</id>
      <title>Make AppendingLongBuffer's page size configurable</title>
      <description>Depending on the data, it might be interesting to use smaller or larger page sizes.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5035</id>
      <title>FieldCacheImpl.SortedDocValuesImpl should compress addresses to term bytes more efficiently</title>
      <description>Each ordinal in SortedDocValuesImpl has a corresponding address to find its location in the big byte[] to support lookupOrd() Today this uses GrowableWriter with absolute addresses. But it would be much better to use MonotonicAppendingLongBuffer.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5036</id>
      <title>Cleanup StoredFieldsProcessor &amp; TermVectorsConsumer</title>
      <description>While I was looking into the latest failure here I cleaned up StoredFieldsProcessor &amp; TermVectorsConsumer a bit since they still seem to have some leftovers from ancient times where we reused the indexing chains across flushes</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5038</id>
      <title>Don't call MergePolicy / IndexWriter during DWPT Flush</title>
      <description>We currently consult the indexwriter -&gt; merge policy to decide if we need to write CFS or not which is bad in many ways. we should call mergepolicy only during merges we should never sync on IW during DWPT flush we should be able to make the decision if we need to write CFS or not before flush, ie. we could write parts of the flush directly to CFS or even start writing stored fields directly. in the NRT case it might make sense to write all flushes to CFS to minimize filedescriptors independent of the index size. I wonder if we can use a simple boolean for this in the IWC and get away with not consulting merge policy. This would simplify concurrency a lot here already.</description>
      <attachments/>
      <comments>38</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5039</id>
      <title>Refactor DWPT to pass testPoint to infostream</title>
      <description>I try to remove the dependencies to IW from DWPT and testPoint seems to be an evil one. I think we should just specialcase this as a component in the info stream</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5042</id>
      <title>Improve NGramTokenizer</title>
      <description>Now that we fixed NGramTokenizer and NGramTokenFilter to not produce corrupt token streams, the only way to have "true" offsets for n-grams is to use the tokenizer (the filter emits the offsets of the original token). Yet, our NGramTokenizer has a few flaws, in particular: it doesn't have the ability to pre-tokenize the input stream, for example on whitespaces, it doesn't play nice with surrogate pairs. Since we already broke backward compatibility for it in 4.4, I'd like to also fix these issues before we release.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5043</id>
      <title>Eclipse project name change - autogenerated</title>
      <description>The eclipse project name (created by 'ant eclipse') for most of the 4.x versions comes up as "lucene_solr_branch_4x" ... which causes a few problems. Recently I needed to take a look at a particular class in Solr 4.2.1, 4.3.0, and branch_4x. I couldn't load all three projects into Eclipse at the same time, because they have the same project name. Even if I could have, it would have been very confusing. I would like to improve this situation for the future. I have a couple of ideas right up front, both of which seem like reasonable ways to go: 1) Use the directory name, similar to what IntelliJ Idea does. 2) Use the "fakeReleaseVersion" property in the central build.xml.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5046</id>
      <title>Explore preset dictionaries for CompressingStoredFieldsFormat</title>
      <description>I discussed this possible improvement with Stefan Pohl and Andrzej Białecki at Berlin Buzzwords: By having preset dictionaries (which could be user-provided and/or computed on a per-block basis), decompression could be faster since we would never have to decompress several documents from a block in order to access a single document. One drawback is that it would require putting some boundaries in the compressed stream, so it would maybe decrease a little the compression ratio. But then if decompression is faster, we could also afford larger blocks, so I think this is worth exploring.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5049</id>
      <title>Native (C++) implementation of "pure OR" BooleanQuery</title>
      <description>I've been playing with a C++ implementation of BooleanQuery containing only OR'd (SHOULD) TermQuery clauses, collecting top N hits by score. The results are impressive: ~3X speedup for BQ OR over two terms, and also good speedups (~38-78%) for Fuzzy1/2 as well since they rewrite to BQ OR over N terms: Task QPS base StdDev QPS comp StdDev Pct diff MedTerm 69.47 (15.8%) 68.61 (13.4%) -1.2% ( -26% - 33%) HighTerm 55.25 (16.2%) 54.63 (13.9%) -1.1% ( -26% - 34%) LowTerm 333.10 (9.6%) 329.43 (8.0%) -1.1% ( -17% - 18%) IntNRQ 3.37 (2.6%) 3.36 (4.6%) -0.2% ( -7% - 7%) Prefix3 18.91 (2.0%) 19.04 (3.5%) 0.7% ( -4% - 6%) Wildcard 29.40 (1.7%) 29.70 (2.8%) 1.0% ( -3% - 5%) MedPhrase 132.69 (6.2%) 134.66 (7.0%) 1.5% ( -11% - 15%) HighSloppyPhrase 0.82 (3.6%) 0.83 (3.5%) 1.9% ( -5% - 9%) AndHighHigh 19.65 (0.6%) 20.02 (0.8%) 1.9% ( 0% - 3%) HighPhrase 11.74 (6.6%) 11.96 (7.1%) 1.9% ( -11% - 16%) MedSloppyPhrase 29.09 (1.2%) 29.76 (1.9%) 2.3% ( 0% - 5%) LowSloppyPhrase 25.71 (1.4%) 26.98 (1.7%) 4.9% ( 1% - 8%) Respell 173.78 (3.0%) 182.41 (3.7%) 5.0% ( -1% - 12%) MedSpanNear 27.67 (2.5%) 29.07 (2.4%) 5.1% ( 0% - 10%) HighSpanNear 2.95 (2.4%) 3.10 (2.8%) 5.4% ( 0% - 10%) LowSpanNear 8.29 (3.4%) 8.82 (3.3%) 6.4% ( 0% - 13%) AndHighMed 79.32 (1.6%) 84.44 (1.0%) 6.5% ( 3% - 9%) LowPhrase 23.20 (2.0%) 25.14 (1.6%) 8.4% ( 4% - 12%) AndHighLow 594.17 (3.4%) 660.32 (1.9%) 11.1% ( 5% - 16%) Fuzzy2 88.32 (6.4%) 121.44 (1.7%) 37.5% ( 27% - 48%) Fuzzy1 86.34 (6.0%) 153.49 (1.7%) 77.8% ( 66% - 90%) OrHighHigh 16.29 (2.5%) 48.29 (1.3%) 196.5% ( 188% - 205%) OrHighMed 28.98 (2.7%) 87.81 (0.9%) 203.0% ( 194% - 212%) OrHighLow 27.38 (2.6%) 84.94 (1.1%) 210.3% ( 201% - 219%) This is essentially a scaled back attempt at LUCENE-1594 in that it's "hardwired" to "just" the "OR of TermQuery" case.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5050</id>
      <title>CompressingStoredFieldsReader should close the index file as soon as it has been read</title>
      <description>Although CompressingStoredFieldsReader loads the stored fields index into memory, it only closes the index file in close(). Closing at the end of the constructor should help save file descriptors. The same idea applies to CompressingTermVectorsReader.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5052</id>
      <title>bitset codec for off heap filters</title>
      <description>Colleagues, When we filter we don’t care any of scoring factors i.e. norms, positions, tf, but it should be fast. The obvious way to handle this is to decode postings list and cache it in heap (CachingWrappingFilter, Solr’s DocSet). Both of consuming a heap and decoding as well are expensive. Let’s write a posting list as a bitset, if df is greater than segment's maxdocs/8 (what about skiplists? and overall performance?). Beside of the codec implementation, the trickiest part to me is to design API for this. How we can let the app know that a term query don’t need to be cached in heap, but can be held as an mmaped bitset? WDYT?</description>
      <attachments/>
      <comments>21</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>5053</id>
      <title>Expose PagedGrowableWriter memory usage</title>
      <description>The idea is to add PagedGrowableWriter.ramBytesUsed, similarly to PackedInts.Mutable and AppendingLongBuffer.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5057</id>
      <title>Hunspell stemmer generates multiple tokens</title>
      <description>The hunspell stemmer seems to be generating multiple tokens: the original token plus the available stems. It might be a good thing in some cases but it seems to be a different behaviour compared to the other stemmers and causes problems as well. I would rather have an option to decide whether it should output only the available stems, or the stems plus the original token. I'm not sure though if it's possible to have only a single stem indexed, which would be even better in my opinion. When I look at how snowball works only one token is indexed, the stem, and that works great. Probably there's something I'm missing in how hunspell works. Here is my issue: I have a query composed of multiple terms, which is analyzed using stemming and a boolean query is generated out of it. All fine when adding all clauses as should (OR operator), but if I add all clauses as must (AND operator), then I can get back only the documents that contain the stem originated by the exactly same original word. Example for the dutch language I'm working with: fiets (means bicycle in dutch), its plural is fietsen. If I index "fietsen" I get both "fietsen" and "fiets" indexed, but if I index "fiets" I get the only "fiets" indexed. When I query for "fietsen whatever" I get the following boolean query: field:fiets field:fietsen field:whatever. If I apply the AND operator and use must clauses for each subquery, then I can only find the documents that originally contained "fietsen", not the ones that originally contained "fiets", which is not really what stemming is about. Any thoughts on this? I also wonder if it can be a dictionary issue since I see that different words that have the word "fiets" as root don't get the same stems, and using the AND operator at query time is a big issue. I would love to contribute on this and looking forward to your feedback.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5063</id>
      <title>Allow GrowableWriter to store negative values</title>
      <description>For some use-cases, it would be convenient to be able to store negative values in a GrowableWriter, for example to use it in FieldCache: The first term is the minimum value and one could use a GrowableWriter to store deltas between this minimum value and the current value. (The need for negative values comes from the fact that maxValue - minValue might be larger than Long.MAX_VALUE.)</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5064</id>
      <title>Add PagedMutable</title>
      <description>In the same way that we now have a PagedGrowableWriter, we could have a PagedMutable which would behave just like PackedInts.Mutable but would support more than 2B values.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5069</id>
      <title>Can/should we store NumericField's precisionStep in the index?</title>
      <description>I was just helping a user (buzzkills) on IRC on why NumericRangeQuery was failing to hit the expected docs ... and it was because s/he had indexed with precStep=4 but searched with precStep=1. Then we wondered if it'd be possible to somehow catch this, e.g. we could maybe store precStep in FieldInfo, and then fail at search time if you use a "non-matching" precStep? I think you can index fine and then search on a multiple of that? E.g., I can index with precStep=2 but search with precStep=8? But indexing with precStep=4 and searching precStep=1 won't work ...</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5077</id>
      <title>make it easier to use compressed norms</title>
      <description>Lucene42DVConsumer's ctor takes acceptableOverheadRatio, so that you can tradeoff time/space, and we pass PackedInts.FASTEST so we always use 8 bits per value. But the class is package private, so if I want to make my own NormsFormat and pass e.g. PackedInts.COMPACT, I can't ... I think we should make this class public / @experimental?</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5078</id>
      <title>Allow TfIdfSimilarity implementations to encode norm values into more than a single byte</title>
      <description>Continuation from here: http://lucene.markmail.org/message/jtwit3pwu5oiqr2h.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5079</id>
      <title>allow IndexWriter user to tell if there are uncommitted changes.</title>
      <description>IndexWriter already currently tracks if there are uncommitted changes. We should expose this somehow... perhaps a boolean hasUncommittedChanges()?</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5081</id>
      <title>WAH8DocIdSet</title>
      <description>Our filters use bit sets a lot to store document IDs. However, it is likely that most of them are sparse hence easily compressible. Having efficient compressed sets would allow for caching more data.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5084</id>
      <title>EliasFanoDocIdSet</title>
      <description>DocIdSet in Elias-Fano encoding</description>
      <attachments/>
      <comments>32</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>5086</id>
      <title>RamUsageEstimator causes AWT classes to be loaded by calling ManagementFactory#getPlatformMBeanServer</title>
      <description>Yea, that type of day and that type of title . Since the last update of Java 6 on OS X, I started to see an annoying icon pop up at the doc whenever running elasticsearch. By default, all of our scripts add headless AWT flag so people will probably not encounter it, but, it was strange that I saw it when before I didn't. I started to dig around, and saw that when RamUsageEstimator was being loaded, it was causing AWT classes to be loaded. Further investigation showed that actually for some reason, calling ManagementFactory#getPlatformMBeanServer now with the new Java version causes AWT classes to be loaded (at least on the mac, haven't tested on other platforms yet). There are several ways to try and solve it, for example, by identifying the bug in the JVM itself, but I think that there should be a fix for it in Lucene itself, specifically since there is no need to call #getPlatformMBeanServer to get the hotspot diagnostics one (its a heavy call...). Here is a simple call that will allow to get the hotspot mxbean without using the #getPlatformMBeanServer method, and not causing it to be loaded and loading all those nasty AWT classes: Object getHotSpotMXBean() { try { // Java 6 Class sunMF = Class.forName("sun.management.ManagementFactory"); return sunMF.getMethod("getDiagnosticMXBean").invoke(null); } catch (Throwable t) { // ignore } // potentially Java 7 try { return ManagementFactory.class.getMethod("getPlatformMXBean", Class.class).invoke(null, Class.forName("com.sun.management.HotSpotDiagnosticMXBean")); } catch (Throwable t) { // ignore } return null; }</description>
      <attachments/>
      <comments>24</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5087</id>
      <title>add PostingsHighlighter.getGapSeparator</title>
      <description>Currently this is hardcoded to a space. But in some situations for a multi-valued field (e.g. authors field), its convenient to treat each value discretely. See LUCENE-2603 for example. So for such a field its nice if you can override and specify something else like U+2029 PARAGRAPH SEPARATOR.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5088</id>
      <title>Add term filter</title>
      <description>I think it makes sense add a term filter: There is a TermsFilter, but no TermFilter. I think it is bit a more efficient then wrapping a TermQuery in an QueryWrapperFilter. Allows the usage of DocsEnum.FLAG_NONE.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5090</id>
      <title>SSDVA should detect a mismatch in the SSDVReaderState</title>
      <description>This is trappy today: every time you open a new reader, you must create a new SSDVReaderState (this computes the seg -&gt; global ord mapping), and pass that to SSDVA. But if this gets messed up (e.g. you pass an old SSDVReaderState) it will result in confusing AIOOBE, or silently invalid results.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5091</id>
      <title>Modify SpanNotQuery to act as SpanNotNearQuery too</title>
      <description>With very small modifications, SpanNotQuery can act as a SpanNotNearQuery. To find "a" but not if "b" appears 3 tokens before or 4 tokens after "a": new SpanNotQuery("a", "b", 3, 4) Original constructor still exists and calls SpanNotQuery("a", "b", 0, 0). Patch with tests on way.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5092</id>
      <title>join: don't expect all filters to be FixedBitSet instances</title>
      <description>The join module throws exceptions when the parents filter isn't a FixedBitSet. The reason is that the join module relies on prevSetBit to find the first child document given a parent ID. As suggested by Uwe and Paul Elschot on LUCENE-5081, we could fix it by exposing methods in the iterators to iterate backwards. When the join modules gets an iterator which isn't able to iterate backwards, it would just need to dump its content into another DocIdSet that supports backward iteration, FixedBitSet for example.</description>
      <attachments/>
      <comments>38</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>5093</id>
      <title>nightly-smoke should run some fail fast checks before doing the full smoke tester</title>
      <description>If something like the NOTICES fail the smoke tester, it currently takes 22 minutes to find out on my pretty fast machine. That means testing a fix also takes 22 minutes. It would be nice if some of these types of checks happened right away on the src tree - we should also check the actual artifacts with the same check later - but also have this fail fast path.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5096</id>
      <title>WhitespaceTokenizer supports Java whitespace, should also support Unicode whitespace</title>
      <description>The whitespace tokenizer supports only Java whitespace as defined in http://docs.oracle.com/javase/6/docs/api/java/lang/Character.html#isWhitespace(char) A useful improvement would be to support also Unicode whitespace as defined in the Unicode property list http://www.unicode.org/Public/UCD/latest/ucd/PropList.txt</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5100</id>
      <title>BaseDocIdSetTestCase</title>
      <description>As Robert said on LUCENE-5081, we would benefit from having common testing infrastructure for our DocIdSet implementations.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5101</id>
      <title>make it easier to plugin different bitset implementations to CachingWrapperFilter</title>
      <description>Currently this is possible, but its not so friendly: protected DocIdSet docIdSetToCache(DocIdSet docIdSet, AtomicReader reader) throws IOException { if (docIdSet == null) { // this is better than returning null, as the nonnull result can be cached return EMPTY_DOCIDSET; } else if (docIdSet.isCacheable()) { return docIdSet; } else { final DocIdSetIterator it = docIdSet.iterator(); // null is allowed to be returned by iterator(), // in this case we wrap with the sentinel set, // which is cacheable. if (it == null) { return EMPTY_DOCIDSET; } else { /* INTERESTING PART */ final FixedBitSet bits = new FixedBitSet(reader.maxDoc()); bits.or(it); return bits; /* END INTERESTING PART */ } } } Is there any value to having all this other logic in the protected API? It seems like something thats not useful for a subclass... Maybe this stuff can become final, and "INTERESTING PART" calls a simpler method, something like: protected DocIdSet cacheImpl(DocIdSetIterator iterator, AtomicReader reader) { final FixedBitSet bits = new FixedBitSet(reader.maxDoc()); bits.or(iterator); return bits; }</description>
      <attachments/>
      <comments>26</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5102</id>
      <title>move DocIdBitSet from lucene-core to lucene-test-framework</title>
      <description>I don't see anyone using this except to test a more efficient BitSet</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5109</id>
      <title>EliasFano value index</title>
      <description>Index upper bits of Elias-Fano sequence.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5113</id>
      <title>Allow for packing the pending values of our AppendingLongBuffers</title>
      <description>When working with small arrays, the pending values might require substantial space. So we could allow for packing the pending values in order to save space, the drawback being that this operation will make the buffer read-only.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5114</id>
      <title>remove boolean useCache param from TermsEnum.seekCeil/Exact</title>
      <description>Long ago terms dict had a cache, but it was problematic and we removed it, but the API still has a relic boolean useCache ... I think we should drop it from the API as well.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5115</id>
      <title>Make WAH8DocIdSet compute its cardinality at building time and use it for cost()</title>
      <description>DocIdSetIterator.cost() accuracy can be important for the performance of some queries (eg.ConjunctionScorer). Since WAH8DocIdSet is immutable, we could compute its cardinality at building time and use it for the cost function.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5118</id>
      <title>spatial strategy- add multiplier to makeDistanceValueSource()</title>
      <description>SpatialStrategy has this abstract method: /** * Make a ValueSource returning the distance between the center of the * indexed shape and {@code queryPoint}. If there are multiple indexed shapes * then the closest one is chosen. */ public abstract ValueSource makeDistanceValueSource(Point queryPoint); I'd like to add another argument double multiplier that is internally multiplied to the result per document. It's a convenience over having the user wrap this with another ValueSource, and it'd be faster too. Typical usage would be to add a degrees-to-kilometers multiplier. The current method could be marked deprecated with a default implementation that invokes the new one with a 1.0 multiplier.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5122</id>
      <title>DiskDV probably shouldnt use BlockPackedReader for SortedDV doc-to-ord</title>
      <description>I dont think "blocking" provides any benefit here in general. we can assume the ordinals are essentially random and since SortedDV is single-valued, its probably better to just use the simpler packedints directly? I guess the only case where it would help is if you sorted your segments by that DV field. But that seems kinda wierd/esoteric to sort your index by a deref'ed string value, e.g. I don't think its even supported by SortingMP. For the SortedSet "ord stream", this can exceed 2B values so for now I think it should stay as blockpackedreader. but it could use a large blocksize...</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5124</id>
      <title>fix+document+rename DiskDV to Lucene45</title>
      <description>The idea is that the default implementation should not hold everything in memory, we can have a "Memory" impl for that. I think stuff being all in heap memory is just a relic of FieldCache. In my benchmarking diskdv works well, and its much easier to manage (keep a smaller heap, leave it to the OS, no OOMs etc from merging large FSTs, ...) If someone wants to optimize by forcing everything in memory, they can then use the usual approach (e.g. just use FileSwitchDirectory, or pick "Memory" for even more efficient stuff). Ill keep the issue here for a bit. If we decide to do this, ill work up file format docs and so on. We should also fix a few things that are not great about it (LUCENE-5122) before making it the default.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5125</id>
      <title>Codec classes/packages that do not provide (automatic) file format back compat need to be more explicit about this in javadocs</title>
      <description>rmuir noted in LUCENE-5121... Currently (as documented), we don't provide index back compat for experimental codecs in lucene-codecs.jar. ...but except for a solr wiki page and solrconfig.xml comment, it's extremely non-obvious that any of these codec classes don't provide index backcompat. the codec module overview.html page describes the module as "Collection of useful codec, postings format and terms dictionary implementations" – with no indication that by using these "useful" implementations, the user gives up index backcompat. the package.html files in the individual packages of the codec module (appending, blockterms, bbloom, diskdv, etc...) also say nothing about index backcompat the individual classes in these codecs are mostly labeled with @lucene.experimental but in the resulting javadoc that merely says that "WARNING: This API is experimental and might change in incompatible ways in the next release". Lots of classes in Lucene have this warning on them about their API (including the abstract codec apis themselves in lucene-core: DocValuesFormat, PostingsFormat, etc...) and that annotation (as far back as i can remember) has always only refered to the java API of the labeled class – never to whether using that class ment you were giving up on index format back compat. Given how much effort and work is put into ensuring good index backcompat for default codec, we should be extremely explicit when/if alternative codecs do not support backcompat, so we don't frustrate/confuse users and leave them with the impression that they can never count on index backcompat just because they may not realize they were using an "unsupported" format option because of a blog post they read or advice they got on the mailing list about how to make something faster or use less ram.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5127</id>
      <title>FixedGapTermsIndex should use monotonic compression</title>
      <description>for the addresses in the big in-memory byte[] and disk blocks, we could save a good deal of RAM here. I think this codec just never got upgraded when we added these new packed improvements, but it might be interesting to try to use for the terms data of sorted/sortedset DV implementations. patch works, but has nocommits and currently ignores the divisor. The annoying problem there being that we have the shared interface with "get(int)" for PackedInts.Mutable/Reader, but no equivalent base class for monotonics get(long)... Still its enough that we could benchmark/compare for now.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5130</id>
      <title>fail the build on compilation warnings</title>
      <description>Many modules compile w/o warnings ... we should lock this in and fail the build if warnings are ever added, and try to fix the warnings in existing modules.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5133</id>
      <title>AnalyzingInfixSuggester should return structured highlighted results instead of single String per result</title>
      <description>Today it renders to an HTML string (&lt;b&gt;..&lt;/b&gt; for hits) in protected methods that one can override to change the highlighting, but this is hard/inefficient to use for search servers that want to e.g. return JSON representation of the highlighted result. This is the same issue as LUCENE-4906 (PostingsHighlighter) but for AnalyzingInfixSuggester's highlights instead.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5136</id>
      <title>Improve FacetRequest javadocs</title>
      <description>While working on LUCENE-4985, I noticed that FacetRequest's jdocs are severely outdated. I rewrote them entirely, so prefer to commit them separately than the rest of the changes. Will post a patch shortly.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5138</id>
      <title>Update source file attributes</title>
      <description>Currently we have many java files with executable attribute, while some scripts that generate source files are missing this. Maybe we should clean this up?</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5144</id>
      <title>Nuke FacetRequest.createAggregator</title>
      <description>Aggregator was replaced by FacetsAggregator. FacetRequest has createAggregator() which by default throws an UOE. It was left there until we migrate the aggregators to FacetsAggregator – now all of our requests support FacetsAggregator. Aggregator is used only by StandardFacetsAccumulator, which too needs to vanish, at some point. But it currently it's the only one which handles sampling, complements aggregation and partitions. What I'd like to do is remove FacetRequest.createAggregator and in StandardFacetsAccumulator support only CountFacetRequest and SumScoreFacetRequest, which are the only ones that make sense for sampling and partitions. SumScore does not even support complements (which only work for counting). I'll also rename StandardFA to OldStandardFA. The plan is to eventually implement a SamplingAccumulator, PartitionsAccumulator/Aggregator and ComplementsAggregator, removing that class entirely. Until then ...</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5145</id>
      <title>Added AppendingPackedLongBuffer &amp; extended AbstractAppendingLongBuffer family (customizable compression ratio + bulk retrieval)</title>
      <description>Made acceptableOverheadRatio configurable Added bulk get to AbstractAppendingLongBuffer classes, for faster retrieval. Introduced a new variant, AppendingPackedLongBuffer which solely relies on PackedInts as a back-end. This new class is useful where people have non-negative numbers with a fairly uniform distribution over a fixed (limited) range. Ex. facets ordinals. To distinguish it from AppendingPackedLongBuffer, delta based AppendingLongBuffer was renamed to AppendingDeltaPackedLongBuffer Fixed an Issue with NullReader where it didn't respect it's valueCount in bulk gets.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5147</id>
      <title>Consider returning a Map&lt;FacetRequest,FacetResult&gt; from FacetsAccumulator</title>
      <description>Today the API returns a List which suggests there's an ordering going on. This may be confusing if one uses FacetsAccumulator.create which results in a MultiFacetsAccumulator, and then the order of the results does not correspond to the order of the requests. Rather than trying to enforce ordering, a simple mapping may be better even for consuming apps since they will be able to easily lookup desired results.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5148</id>
      <title>SortedSetDocValues caching / state</title>
      <description>I just spent some time digging into a bug which was due to the fact that SORTED_SET doc values are stateful (setDocument/nextOrd) and are cached per thread. So if you try to get two instances from the same field in the same thread, you will actually get the same instance and won't be able to iterate over ords of two documents in parallel. This is not necessarily a bug, this behavior can be documented, but I think it would be nice if the API could prevent from such mistakes by storing the state in a separate object or cloning the SortedSetDocValues object in SegmentCoreReaders.getSortedSetDocValues? What do you think?</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5149</id>
      <title>CommonTermsQuery should allow minNrShouldMatch for high &amp; low freq terms</title>
      <description>Currently CommonTermsQuery only allows a minShouldMatch for the low frequent query. Yet, we should also allow this for the high frequent part to have better control over scoring. here is an ES issue that is related to this: https://github.com/elasticsearch/elasticsearch/issues/3188</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5150</id>
      <title>WAH8DocIdSet: dense sets compression</title>
      <description>In LUCENE-5101, Paul Elschot mentioned that it would be interesting to be able to encode the inverse set to also compress very dense sets.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5153</id>
      <title>Allow wrapping Reader from AnalyzerWrapper</title>
      <description>It can be useful to allow AnalyzerWrapper extensions to wrap the Reader given to initReader, e.g. with a CharFilter.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5155</id>
      <title>Add OrdinalValueResolver in favor of FacetRequest.getValueOf</title>
      <description>FacetRequest.getValueOf is responsible for resolving an ordinal's value. It is given FacetArrays, and typically does something like arrays.getIntArray()[ord] – for every ordinal! The purpose of this method is to allow special requests, e.g. average, to do some post processing on the values, that couldn't be done during aggregation. I feel that getValueOf is in the wrong place – the calls to getInt/FloatArray are really redundant. Also, if an aggregator maintains some statistics by which it needs to "correct" the aggregated values, it's not trivial to pass it from the aggregator to the request. Therefore I would like to make the following changes: Remove FacetRequest.getValueOf and .getFacetArraysSource Add FacetsAggregator.createOrdinalValueResolver which takes the FacetArrays and has a simple API .valueOf(ordinal). Modify the FacetResultHandlers to use OrdValResolver. This allows an OVR to initialize the right array instance(s) in the ctor, and return the value of the requested ordinal, without doing arrays.getArray() calls. Will post a patch shortly.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5157</id>
      <title>Refactoring MultiDocValues.OrdinalMap to clarify API and internal structure.</title>
      <description>I refactored MultiDocValues.OrdinalMap, removing one unused parameter and renaming some methods to more clearly communicate what they do. Also I renamed subIndex references to segmentIndex.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5158</id>
      <title>Allow StoredFieldVisitor instances to be stateful</title>
      <description>Currently there is no way to build stateful StoredFieldVisitor s. Motivation We would like to optimise our access to stored fields in our indexes by utilising the StoredFieldVisitor.Status.STOP feature to stop processing fields in a document. Unfortunately we have very large indexes, and rebuilding them to have the required field order is not an option. A stateful StoredFieldVisitor could solve this; it could track which fields have been loaded for a document, and then STOP when the fields required have been loaded, regardless of the order they were loaded. Implementation I've added a no-op public void reset() method to the StoredFieldVisitor base class, which gives a StoredFieldVisitor subclass an opportunity to reset its state before the fields of the next document are processed. I've added a call to reset() in all places the StoredFieldVisitor was being used.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5159</id>
      <title>compressed diskdv sorted/sortedset termdictionaries</title>
      <description>Sorted/SortedSet give you ordinal(s) per document, but them separately have a "term dictionary" of all the values. You can do a few operations on these: ord -&gt; term lookup (e.g. retrieving facet labels) term -&gt; ord lookup (reverse lookup: e.g. fieldcacherangefilter) get a term enumerator (e.g. merging, ordinalmap construction) The current implementation for diskdv was the simplest thing that can possibly work: under the hood it just makes a binary DV for these (treating ordinals as document ids). When the terms are fixed length, you can address a term directly with multiplication. When they are variable length though, we have to store a packed ints structure in RAM. This variable length case is overkill and chews up a lot of RAM if you have many unique values. It also chews up a lot of disk since all the values are just concatenated (no sharing).</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5161</id>
      <title>review FSDirectory chunking defaults and test the chunking</title>
      <description>Today there is a loop in SimpleFS/NIOFS: try { do { final int readLength; if (total + chunkSize &gt; len) { readLength = len - total; } else { // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks readLength = chunkSize; } final int i = file.read(b, offset + total, readLength); total += i; } while (total &lt; len); } catch (OutOfMemoryError e) { I bet if you look at the clover report its untested, because its fixed at 100MB for 32-bit users and 2GB for 64-bit users (are these defaults even good?!). Also if you call the setter on a 64-bit machine to change the size, it just totally ignores it. We should remove that, the setter should always work. And we should set it to small values in tests so this loop is actually executed.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5165</id>
      <title>Add SuggestStopFilter</title>
      <description>This is like StopFilter, except if the token is the very last token and there were no non-token characters after it, it keeps the token. This is useful with analyzing suggesters (AnalyzingSuggester, AnalyzingInfixSuggester, FuzzySuggester), where you often want to remove stop words, but not if it's the last word and the user hasn't finished typing it. E.g. "fast a" might complete to "fast amoeba", but if you simply use StopFilter then the a is removed. Really our analysis APIs aren't quite designed to handle a "partial" tokens that suggesters need to work with.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5171</id>
      <title>AnalyzingSuggester and FuzzySuggester should be able to share same FST</title>
      <description>In my code I use both suggesters for the same FST. I use AnalyzerSuggester#store() to create the FST and later on AnalyzingSuggester#load() and FuzzySuggester#load() to use it. This approach works very well but it unnecessarily creates 2 fst instances resulting in 2x memory consumption. It seems that for the time being both suggesters use the same FST format. The following trivial method in AnalyzingSuggester provides the possibility to share the same FST among different instances of AnalyzingSuggester. It has been tested in the above scenario: public boolean shareFstFrom(AnalyzingSuggester instance) { if (instance.fst == null) { return false; } this.fst = instance.fst; this.maxAnalyzedPathsForOneInput = instance.maxAnalyzedPathsForOneInput; this.hasPayloads = instance.hasPayloads; return true; } One could use it like this: analyzingSugg = new AnalyzingSuggester(...); fuzzySugg = new FuzzySuggester(...); analyzingSugg.load(someInputStream); fuzzySugg = analyzingSugg.shareFstFrom(analyzingSugg);</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5173</id>
      <title>Add checkindex piece of LUCENE-5116</title>
      <description>LUCENE-5116 fixes addIndexes(Reader) to never write a 0-document segment (in the case you merge in empty or all-deleted stuff). I considered it just an inconsistency, but it could cause confusing exceptions to real users too if there was a "regression" here. (see solr users list:Split Shard Error - maxValue must be non-negative).</description>
      <attachments/>
      <comments>16</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5175</id>
      <title>Add parameter to lower-bound TF normalization for BM25 (for long documents)</title>
      <description>In the article "When Documents Are Very Long, BM25 Fails!" a fix for the problem is documented. There was a TODO note in BM25Similarity to add this fix. I will attach a patch that implements the fix shortly.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5178</id>
      <title>doc values should expose missing values (or allow configurable defaults)</title>
      <description>DocValues should somehow allow a configurable default per-field. Possible implementations include setting it on the field in the document or registration of an IndexWriter callback. If we don't make the default configurable, then another option is to have DocValues fields keep track of whether a value was indexed for that document or not.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5180</id>
      <title>ShingleFilter should make shingles from trailing holes</title>
      <description>When ShingleFilter hits a hole, it uses _ as the token, e.g. bigrams for "the dog barked", if you have a StopFilter removing the, would be: "_ dog", "dog barked". But if the input ends with a stopword, e.g. "wizard of", ShingleFilter fails to produce "wizard _" due to LUCENE-3849 ... once we fix that I think we should fix ShingleFilter to make shingles for trailing holes too ...</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5181</id>
      <title>Passage knows its own docID</title>
      <description>The new PostingsHighlight package allows for retrieval of term matches from a query if one creates a class that extends PassageFormatter and overrides format(). However, class Passage does not have a docID field, nor is this provided via PassageFormatter.format(). Therefore, it's very difficult to know which Document contains a given Passage. It would suffice for PassageFormatter.format() to be passed the docID as a parameter. From the code in PostingsHighlight, this seems like it would be easy.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5185</id>
      <title>licenses/*.jar.sha1 don't belong in Lucene and Solr binary distributions</title>
      <description>On LUCENE-3945, where external dependency checksum verification was put in place, Chris Hostetter (Unused) wrote: So i propose that we include checksum files in svn and in our source releases that can be used by users to verify that the jars they get from ivy match the jars we tested against. That is, checksum files in binary distributions was not part of the proposal. And in his comment associated with the final patch: 2) fixes the binary releases to exlcude the sha1 files Somewhere between then and now, *.jar.sha1 files snuck back into the Lucene and Solr binary releases, under the licenses/ directory. They should not be there.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5186</id>
      <title>Add CachingWrapperFilter.getFilter()</title>
      <description>There are a couple of use cases I can think of where being able to get the underlying filter out of CachingWrapperFilter would be useful: 1. You might want to introspect the filter to figure out what's in it (the use case we hit.) 2. You might want to serialise the filter since Lucene no longer supports that itself. We currently work around this by subclassing, keeping another copy of the underlying filter reference and implementing a trivial getter, which is an easy workaround, but the trap is that a junior developer could unknowingly create a CachingWrapperFilter without knowing that the BetterCachingWrapperFilter exists, introducing a filter which cannot be introspected.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5187</id>
      <title>Make SlowCompositeReaderWrapper constructor private</title>
      <description>I found a couple of places in the code base that duplicate the logic of SlowCompositeReaderWrapper.wrap. I think SlowCompositeReaderWrapper.wrap (vs. new SlowCompositeReaderWrapper) is what users need so we should probably make SlowCompositeReaderWrapper constructor private to enforce usage of wrap.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5188</id>
      <title>Make CompressingStoredFieldsFormat more friendly to StoredFieldVisitors</title>
      <description>The way CompressingStoredFieldsFormat works is that it first decompresses data and then consults the StoredFieldVisitor. This is a bit wasteful in case documents are big and only the first field of a document is of interest so maybe we could decompress and consult the StoredFieldVicitor in a more streaming fashion.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5189</id>
      <title>Numeric DocValues Updates</title>
      <description>In LUCENE-4258 we started to work on incremental field updates, however the amount of changes are immense and hard to follow/consume. The reason is that we targeted postings, stored fields, DV etc., all from the get go. I'd like to start afresh here, with numeric-dv-field updates only. There are a couple of reasons to that: NumericDV fields should be easier to update, if e.g. we write all the values of all the documents in a segment for the updated field (similar to how livedocs work, and previously norms). It's a fairly contained issue, attempting to handle just one data type to update, yet requires many changes to core code which will also be useful for updating other data types. It has value in and on itself, and we don't need to allow updating all the data types in Lucene at once ... we can do that gradually. I have some working patch already which I'll upload next, explaining the changes.</description>
      <attachments/>
      <comments>119</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>5193</id>
      <title>Add jar-src to build.xml</title>
      <description>I think it's useful if we have a top-level jar-src which generates source jars for all modules. One can basically do that by iterating through the directories and calling 'ant jar-src' already, so this is just a convenient way to do it. Will attach a patch shortly.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5196</id>
      <title>Revive LTC.PREFLEX_IMPERSONATION_IS_ACTIVE</title>
      <description>This constant is currently not used and from what I understand, it was used in the past to test 3.x stuff? Anyway, I want to revive it by renaming it to OLD_FORMAT_IMPERSONATION_IS_ACTIVE (defaults to true) and have the RWCodecs respect it by not supporting e.g. fieldsConsumer() if it's set to false. This will allow tests that want to verify e.g. old formats are not supported work. Currently it's impossible because the RWCodec gets loaded before the non-RW by SPI, and so writing segments in an old format always work. I want to test something like that in LUCENE-5189 and prevents it. Will post a patch soon.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5197</id>
      <title>Add a method to SegmentReader to get the current index heap memory size</title>
      <description>It would be useful to at least estimate the index heap size being used by Lucene. Ideally a method exposing this information at the SegmentReader level.</description>
      <attachments/>
      <comments>29</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>5199</id>
      <title>Improve LuceneTestCase.defaultCodecSupportsDocsWithField to check the actual DocValuesFormat used per-field</title>
      <description>On LUCENE-5178 Han reported the following test failure: [junit4] FAILURE 0.27s | TestRangeAccumulator.testMissingValues &lt;&lt;&lt; [junit4] &gt; Throwable #1: org.junit.ComparisonFailure: expected:&lt;...(0) [junit4] &gt; less than 10 ([8) [junit4] &gt; less than or equal to 10 (]8) [junit4] &gt; over 90 (8) [junit4] &gt; 9...&gt; but was:&lt;...(0) [junit4] &gt; less than 10 ([28) [junit4] &gt; less than or equal to 10 (2]8) [junit4] &gt; over 90 (8) [junit4] &gt; 9...&gt; [junit4] &gt; at __randomizedtesting.SeedInfo.seed([815B6AA86D05329C:EBC638EE498F066D]:0) [junit4] &gt; at org.apache.lucene.facet.range.TestRangeAccumulator.testMissingValues(TestRangeAccumulator.java:670) [junit4] &gt; at java.lang.Thread.run(Thread.java:722) which can be reproduced with tcase=TestRangeAccumulator -Dtests.method=testMissingValues -Dtests.seed=815B6AA86D05329C -Dtests.slow=true -Dtests.postingsformat=Lucene41 -Dtests.locale=ca -Dtests.timezone=Australia/Currie -Dtests.file.encoding=UTF-8 It seems that the Codec that is picked is a Lucene45Codec with Lucene42DVFormat, which does not support docsWithFields for numericDV. We should improve LTC.defaultCodecSupportsDocsWithField to take a list of fields and check that the actual DVF used for each field supports it.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5204</id>
      <title>Make Directory easier to wrap</title>
      <description>We have a few DirectoryWrapper implementations such as RateLimitedDirectoryWrapper and MockDirectoryWrapper. However, the Directory class is not straightforward to wrap since it already has logic for getting and setting the lock factory, so wrappers need to decide whether they should forward lock handling to the delegate or handle it themselves. I would like to move the locking logic out of the Directory class and to have a base FilterDirectory that could be extended by other directory wrapper impls.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5205</id>
      <title>SpanQueryParser with recursion, analysis and syntax very similar to classic QueryParser</title>
      <description>This parser extends QueryParserBase and includes functionality from: Classic QueryParser: most of its syntax SurroundQueryParser: recursive parsing for "near" and "not" clauses. ComplexPhraseQueryParser: can handle "near" queries that include multiterms (wildcard, fuzzy, regex, prefix), AnalyzingQueryParser: has an option to analyze multiterms. At a high level, there's a first pass BooleanQuery/field parser and then a span query parser handles all terminal nodes and phrases. Same as classic syntax: term: test fuzzy: roam~0.8, roam~2 wildcard: te?t, test*, t*st regex: /[mb]oat/ phrase: "jakarta apache" phrase with slop: "jakarta apache"~3 default "or" clause: jakarta apache grouping "or" clause: (jakarta apache) boolean and +/-: (lucene OR apache) NOT jakarta; +lucene +apache -jakarta multiple fields: title:lucene author:hatcher Main additions in SpanQueryParser syntax vs. classic syntax: Can require "in order" for phrases with slop with the ~&gt; operator: "jakarta apache"~&gt;3 Can specify "not near": "fever bieber"!~3,10 :: find "fever" but not if "bieber" appears within 3 words before or 10 words after it. Fully recursive phrasal queries with [ and ]; as in: [[jakarta apache]~3 lucene]~&gt;4 :: find "jakarta" within 3 words of "apache", and that hit has to be within four words before "lucene" Can also use [] for single level phrasal queries instead of " as in: [jakarta apache] Can use "or grouping" clauses in phrasal queries: "apache (lucene solr)"~3 :: find "apache" and then either "lucene" or "solr" within three words. Can use multiterms in phrasal queries: "jakarta~1 ap*che"~2 Did I mention full recursion: [[jakarta~1 ap*che]~2 (solr~ /l[ou]+[cs][en]+/)]~10 :: Find something like "jakarta" within two words of "ap*che" and that hit has to be within ten words of something like "solr" or that "lucene" regex. Can require at least x number of hits at boolean level: "apache AND (lucene solr tika)~2 Can use negative only query: -jakarta :: Find all docs that don't contain "jakarta" Can use an edit distance &gt; 2 for fuzzy query via SlowFuzzyQuery (beware of potential performance issues!). Trivial additions: Can specify prefix length in fuzzy queries: jakarta~1,2 (edit distance =1, prefix =2) Can specifiy Optimal String Alignment (OSA) vs Levenshtein for distance &lt;=2: (jakarta~1 (OSA) vs jakarta~&gt;1(Levenshtein) This parser can be very useful for concordance tasks (see also LUCENE-5317 and LUCENE-5318) and for analytical search. Until LUCENE-2878 is closed, this might have a use for fans of SpanQuery. Most of the documentation is in the javadoc for SpanQueryParser. Any and all feedback is welcome. Thank you. Until this is added to the Lucene project, I've added a standalone lucene-addons repo (with jars compiled for the latest stable build of Lucene) on github.</description>
      <attachments/>
      <comments>156</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>5207</id>
      <title>lucene expressions module</title>
      <description>Expressions are geared at defining an alternative ranking function (e.g. incorporating the text relevance score and other field values/ranking signals). So they are conceptually much more like ElasticSearch's scripting support (http://www.elasticsearch.org/guide/reference/modules/scripting/) than solr's function queries. Some additional notes: In addition to referring to other fields, they can also refer to other expressions, so they can be used as "computed fields". You can rank documents easily by multiple expressions (its a SortField at the end), e.g. Sort by year descending, then some function of score price and time ascending. The provided javascript expression syntax is much more efficient than using a scripting engine, because it does not have dynamic typing (compiles to .class files that work on doubles). Performance is similar to writing a custom FieldComparator yourself, but much easier to do. We have solr integration to contribute in the future, but this is just the standalone lucene part as a start. Since lucene has no schema, it includes an implementation of Bindings (SimpleBindings) that maps variable names to SortField's or other expressions.</description>
      <attachments/>
      <comments>88</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5208</id>
      <title>SnowballFilter to support minTokenLength</title>
      <description>In some cases you don't want the stemmer to consider short tokens. Instead of modifying the snowball code, testing it, compiling it to Java code and the whole hassle, with this patch you can set a minTokenLength.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5209</id>
      <title>Allow the license checker to optionally avoid check sum comparisons on SNAPSHOT dependencies.</title>
      <description>SNAPSHOT's cannot actually be used and released by Lucene/Solr, but we use them downstream in some cases during development - we have to harmonize jars across multiple projects. It would be nice if we could avoid doing the check sum check on SNAPSHOT's, but still do the license check (dev adds any dependency, dev must add license immediately). This first patch adds a new system property called skipSnapshotsChecksum - if you set it to true, SNAPSHOT dependency's will not be check sum compared. I think this change makes the license checker more consumable.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5214</id>
      <title>Add new FreeTextSuggester, to handle "long tail" suggestions</title>
      <description>The current suggesters are all based on a finite space of possible suggestions, i.e. the ones they were built on, so they can only suggest a full suggestion from that space. This means if the current query goes outside of that space then no suggestions will be found. The goal of FreeTextSuggester is to address this, by giving predictions based on an ngram language model, i.e. using the last few tokens from the user's query to predict likely following token. I got the idea from this blog post about Google's suggest: http://googleblog.blogspot.com/2011/04/more-predictions-in-autocomplete.html This is very much still a work in progress, but it seems to be working. I've tested it on the AOL query logs, using an interactive tool from luceneutil to show the suggestions, and it seems to work well. It's fun to use that tool to explore the word associations... I don't think this suggester would be used standalone; rather, I think it'd be a fallback for times when the primary suggester fails to find anything. You can see this behavior on google.com, if you type "the fast and the ", you see entire queries being suggested, but then if the next word you type is "burning" then suddenly you see the suggestions are only based on the last word, not the entire query. It uses ShingleFilter under-the-hood to generate the token ngrams; once LUCENE-5180 is in it will be able to properly handle a user query that ends with stop-words (e.g. "wizard of "), and then stores the ngrams in an FST.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5215</id>
      <title>Add support for FieldInfos generation</title>
      <description>In LUCENE-5189 we've identified few reasons to do that: If you want to update docs' values of field 'foo', where 'foo' exists in the index, but not in a specific segment (sparse DV), we cannot allow that and have to throw a late UOE. If we could rewrite FieldInfos (with generation), this would be possible since we'd also write a new generation of FIS. When we apply NDV updates, we call DVF.fieldsConsumer. Currently the consumer isn't allowed to change FI.attributes because we cannot modify the existing FIS. This is implicit however, and we silently ignore any modified attributes. FieldInfos.gen will allow that too. The idea is to add to SIPC fieldInfosGen, add to each FieldInfo a dvGen and add support for FIS generation in FieldInfosFormat, SegReader etc., like we now do for DocValues. I'll work on a patch. Also on LUCENE-5189, Rob raised a concern about SegmentInfo.attributes that have same limitation – if a Codec modifies them, they are silently being ignored, since we don't gen the .si files. I think we can easily solve that by recording SI.attributes in SegmentInfos, so they are recorded per-commit. But I think it should be handled in a separate issue.</description>
      <attachments/>
      <comments>35</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5216</id>
      <title>Fix SegmentInfo.attributes when updates are involved</title>
      <description>Today, SegmentInfo.attributes are write-once. However, in the presence of field updates (see LUCENE-5189 and LUCENE-5215) this creates an issue, in which if a Codec decides to alter the attributes when updates are applied, they are silently discarded. This is rather a corner case, though one that should be addressed. There were two solutions to address this: Record SI.attributes in SegmentInfos, so they are written per-commit, instead of the .si file. Remove them altogether, as they don't seem to be used anywhere in Lucene code today. If we remove them, we basically don't take away special capability from Codecs, because they can still write the attributes to a separate file, or even the file they record the other data in. This will work even with updates, as long as Codecs respect the given segmentSuffix. If we keep them, I think the simplest solution is to read/write them by SegmentInfos. But if we don't see a good use case, I suggest we remove them, as it's just extra code to maintain. I think we can even risk a backwards break and remove them completely from 4x, though if that's a problem, we can deprecate too. If anyone sees a good usage for them, or better - already uses them, please speak up, so we can make the proper decision.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5219</id>
      <title>Make SynonymFilterFactory format attribute pluggable</title>
      <description>It would be great to allow custom synonym formats to work with SynonymFilterFactory. There is already a comment in the code to make it pluggable.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5224</id>
      <title>org.apache.lucene.analysis.hunspell.HunspellDictionary should implement ICONV and OCONV lines in the affix file</title>
      <description>There are some Hunspell dictionaries that need to emulate Unicode normalization and collation in order to get the correct stem of a word. The original Hunspell provides a way to do this with the ICONV and OCONV lines in the affix file. The Lucene HunspellDictionary ignores these lines right now. Please support these keys in the affix file. This bit of functionality is briefly described in the hunspell man page http://manpages.ubuntu.com/manpages/lucid/man4/hunspell.4.html This functionality is practically required in order to use a Korean dictionary because you want only some of the Jamos of a Hangul character (grapheme cluster) when using stemming. Other languages will find this to be helpful functionality. Here is an example for a .aff file: ICONV 각 각 ... OCONV 각 각 Here is the same example escaped. ICONV \uAC01 \u1100\u1161\u11A8 ... OCONV \u1100\u1161\u11A8 \uAC01</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5225</id>
      <title>ToParentBlockJoinQuery don't accumulate the child doc ids and scores if ToParentBlockJoinCollector is not used</title>
      <description>The BlockJoinScorer temporarily saves the child docids and scores in two arrays (pendingChildDocs/pendingChildScores) for the current block (parent/child docs) being processed. This is only need for ToParentBlockJoinCollector and in the case that this collector isn't used then these two arrays shouldn't be used as well. I've seen cases where only the ToParentBlockJoinQuery is used and there are many child docs (100k and up), in that case these two arrays are a waste of resources.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5226</id>
      <title>optimize sortfield.needsScore on expressions</title>
      <description>To fix LUCENE-5222, it currently returns true with // TODO: optimize. this is needed for e.g. IndexSearcher with executorService search to merge results from the different segments. ideally we should only return this if it really does need it, not if its essentially just a computed field.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5234</id>
      <title>Clarify FieldCache API around the use of NumericDocValues fields</title>
      <description>Spinoff from this thread: http://lucene.markmail.org/thread/wxs6bzf2ul6go4pg. FieldCache (and friends) API javadocs need some improvements.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5235</id>
      <title>throw illegalstate from Tokenizer (instead of NPE/IIOBE) if reset not called</title>
      <description>We added these best effort checks, but it would be much better if we somehow gave a clear exception... this comes up often</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5236</id>
      <title>Use broadword bit selection in EliasFanoDecoder</title>
      <description>Try and speed up decoding</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5248</id>
      <title>Improve the data structure used in ReaderAndLiveDocs to hold the updates</title>
      <description>Currently ReaderAndLiveDocs holds the updates in two structures: Map&lt;String,Map&lt;Integer,Long&gt;&gt; Holds a mapping from each field, to all docs that were updated and their values. This structure is updated when applyDeletes is called, and needs to satisfy several requirements: Un-ordered writes: if a field "f" is updated by two terms, termA and termB, in that order, and termA affects doc=100 and termB doc=2, then the updates are applied in that order, meaning we cannot rely on updates coming in order. Same document may be updated multiple times, either by same term (e.g. several calls to IW.updateNDV) or by different terms. Last update wins. Sequential read: when writing the updates to the Directory (fieldsConsumer), we iterate on the docs in-order and for each one check if it's updated and if not, pull its value from the current DV. A single update may affect several million documents, therefore need to be efficient w.r.t. memory consumption. Map&lt;Integer,Map&lt;String,Long&gt;&gt; Holds a mapping from a document, to all the fields that it was updated in and the updated value for each field. This is used by IW.commitMergedDeletes to apply the updates that came in while the segment was merging. The requirements this structure needs to satisfy are: Access in doc order: this is how commitMergedDeletes works. One-pass: we visit a document once (currently) and so if we can, it's better if we know all the fields in which it was updated. The updates are applied to the merged ReaderAndLiveDocs (where they are stored in the first structure mentioned above). Comments with proposals will follow next.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5249</id>
      <title>All Lucene/Solr modules should use the same dependency versions</title>
      <description>Mark Miller wrote on the dev list: I'd like it for some things if we actually kept the versions somewhere else - for instance, Hadoop dependencies should match across the mr module and the core module. Perhaps we could define versions for dependencies across multiple modules that should probably match, in a prop file or ant file and use sys sub for them in the ivy files. For something like Hadoop, that would also make it simple to use Hadoop 1 rather than 2 with a single sys prop override. Same with some other depenencies.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5251</id>
      <title>New Dictionary Implementation for Suggester consumption</title>
      <description>With the vast array of new suggester, It would be nice to have a dictionary implementation that could feed the suggesters terms, weights and (optionally) payloads from the lucene index. The idea of this dictionary implementation is to grab stored documents from the index and use user-configured fields for terms, weights and payloads. use-case: If you have a document with three fields product_id product_name product_popularity_score then using this implementation would enable you to have a suggester for product_name using the weight of product_popularity_score and return you the payload of product_id, with which you can do further processing on (example: construct a url etc).</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5252</id>
      <title>add NGramSynonymTokenizer</title>
      <description>I'd like to propose that we have another n-gram tokenizer which can process synonyms. That is NGramSynonymTokenizer. Note that in this ticket, the gram size is fixed, i.e. minGramSize = maxGramSize. Today, I think we have the following problems when using SynonymFilter with NGramTokenizer. For purpose of illustration, we have a synonym setting "ABC, DEFG" w/ expand=true and N = 2 (2-gram). There is no consensus (I think how we assign offsets to generated synonym tokens DE, EF and FG when expanding source token AB and BC. If the query pattern looks like ABCY, it cannot be matched even if there is a document "…ABCY…" in index when autoGeneratePhraseQueries set to true, because there is no "CY" token (but "GY" is there) in the index. NGramSynonymTokenizer can solve these problems by providing the following methods. NGramSynonymTokenizer reads synonym settings (synonyms.txt) and it doesn't tokenize registered words. e.g. source text NGramTokenizer+SynonymFilter NGramSynonymTokenizer ABC AB/DE/BC/EF/FG ABC/DEFG The back and forth of the registered words, NGramSynonymTokenizer generates extra tokens w/ posInc=0. e.g. source text NGramTokenizer+SynonymFilter NGramSynonymTokenizer XYZABC123 XY/YZ/ZA/AB/DE/BC/EF/C1/FG/12/23 XY/YZ/Z/ABC/DEFG/1/12/23 In the above sample, "Z" and "1" are the extra tokens.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5253</id>
      <title>add NGramSynonymTokenizer</title>
      <description>I'd like to propose that we have another n-gram tokenizer which can process synonyms. That is NGramSynonymTokenizer. Note that in this ticket, the gram size is fixed, i.e. minGramSize = maxGramSize. Today, I think we have the following problems when using SynonymFilter with NGramTokenizer. For purpose of illustration, we have a synonym setting "ABC, DEFG" w/ expand=true and N = 2 (2-gram). There is no consensus (I think how we assign offsets to generated synonym tokens DE, EF and FG when expanding source token AB and BC. If the query pattern looks like XABC or ABCY, it cannot be matched even if there is a document "…XABCY…" in index when autoGeneratePhraseQueries set to true, because there is no "XA" or "CY" tokens in the index. NGramSynonymTokenizer can solve these problems by providing the following methods. NGramSynonymTokenizer reads synonym settings (synonyms.txt) and it doesn't tokenize registered words. e.g. source text NGramTokenizer+SynonymFilter NGramSynonymTokenizer ABC AB/DE/BC/EF/FG ABC/DEFG The back and forth of the registered words, NGramSynonymTokenizer generates extra tokens w/ posInc=0. e.g. source text NGramTokenizer+SynonymFilter NGramSynonymTokenizer XYZABC123 XY/YZ/ZA/AB/DE/BC/EF/C1/FG/12/23 XY/YZ/Z/ABC/DEFG/1/12/23 In the above sample, "Z" and "1" are the extra tokens.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5255</id>
      <title>Make DocumentsWriter reference final in IW</title>
      <description>the DocumentWriter ref is nulled on close which seems unnecessary altogether. We can just make it final instead.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5257</id>
      <title>Lock down centralized versioning of ivy dependencies</title>
      <description>LUCENE-5249 introduced centralized versioning of 3rd party dependencies and converted all ivy.xml files across Lucene/Solr to use this scheme. But there is nothing preventing people from ignoring this setup and (intentionally or not) introducing non-centralized dependency versions. SOLR-3664 discusses the problem of out-of-sync 3rd party dependencies between Lucene/Solr modules. Centralized versioning makes synchronization problems less likely but not impossible. One fairly simple way to ensure that all modules use the same version of 3rd party deps would be to require that all deps in ivy.xml would have to use the rev="${/org/name}" syntax, via a validation script. The problem remains that there may eventually be a requirement to use different 3rd party libs in different modules. Any form of lockdown here should allow for this possibility. Hoss's suggestion from a conversation on #lucene IRC earlier today: &lt;hoss&gt; perhaps exceptions could be by naming convetion &lt;sarowe&gt; can you give an example? &lt;hoss&gt; ie: variables must match either ${group}/${artifact} or they must match /VERSION_MISTMATCH_EXCEPTION/${group}/${artifact} &lt;sarowe&gt; nice idea no external config required &lt;hoss&gt; right and it has to be real obvious when you are bucking convention &lt;hoss&gt; or better yet: ${group}/${artifact}/VERSION_MISTMATCH_EXCEPTION ... and there is another check that the version file is in ascii order so you are garuntted that it has to be right there in the versions file one line after ${group}/${artifact} &lt;sarowe&gt; i like it &lt;hoss&gt; no change someone updating ${group}/${artifact} won't notice it i suppose really it should be ${group}/${artifact}/VERSION_MISTMATCH_EXCEPTION/${reason} ... since you might have more then one exception per ${group}/${artifact} but now i'm just making things up w/o evn really understanding the conventions you've alreay put in place &lt;sarowe&gt; :) &lt;hoss&gt; you get the idea &lt;sarowe&gt; yes</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5258</id>
      <title>add distance function to expressions/</title>
      <description>Adding this static function makes it really easy to incorporate distance with the score or other signals in arbitrary ways, e.g. score / (1 + log(distance)) or whatever.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5260</id>
      <title>Make older Suggesters more accepting of TermFreqPayloadIterator</title>
      <description>As discussed in https://issues.apache.org/jira/browse/LUCENE-5251, it would be nice to make the older suggesters accepting of TermFreqPayloadIterator and throw an exception if payload is found (if it cannot be used). This will also allow us to nuke most of the other interfaces for BytesRefIterator.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5261</id>
      <title>add simple API to build queries from analysis chain</title>
      <description>Currently this is pretty crazy stuff. Additionally its duplicated in like 3 or 4 places in our codebase (i noticed it doing LUCENE-5259) We can solve that duplication, and make it easy to simply create queries from an analyzer (its been asked on the user list), as well as make it easier to build new queryparsers.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>5265</id>
      <title>Make BlockPackedWriter constructor take an acceptable overhead ratio</title>
      <description>Follow-up of http://search-lucene.com/m/SjmSW1CZYuZ1 MemoryDocValuesFormat takes an acceptable overhead ratio but it is only used when doing table compression. It should be used for all compression methods, especially DELTA_COMPRESSED whose encoding is based on BlockPackedWriter.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5266</id>
      <title>Optimization of the direct PackedInts readers</title>
      <description>Given that the initial focus for PackedInts readers was more on in-memory readers (for storing stuff like the mapping from old to new doc IDs at merging time), I never spent time trying to optimize the direct readers although it could be beneficial now that they are used for disk-based doc values.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5268</id>
      <title>Cutover more postings formats to the inverted "pull" API</title>
      <description>In LUCENE-5123, we added a new, more flexible, "pull" API for writing postings. This API allows the postings format to iterate the fields/terms/postings more than once, and mirrors the API for writing doc values. But that was just the first step (only SimpleText was cutover to the new API). I want to cutover more components, so we can (finally) e.g. play with different encodings depending on the term's postings, such as using a bitset for high freq DOCS_ONLY terms (LUCENE-5052).</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5270</id>
      <title>add Terms.hasFreqs</title>
      <description>While working on LUCENE-5268, I realized we have hasPositions/Offsets/Payloads methods in Terms but not hasFreqs ...</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5271</id>
      <title>A slightly more accurate SloppyMath distance</title>
      <description>SloppyMath, intriduced in LUCENE-5258, uses earth's avg. (according to WGS84) ellipsoid radius as an approximation for computing the "spherical" distance. (The TO_KILOMETERS constant). While this is pretty accurate for long distances (latitude wise) this may introduce some small errors while computing distances close to the equator (as the earth radius there is larger than the avg.) A more accurate approximation would be taking the avg. earth radius at the source and destination points. But computing an ellipsoid radius at any given point is a heavy function, and this distance should be used in a scoring function.. So two optimizations are optional - Pre-compute a table with an earth radius per latitude (the longitude does not affect the radius) Instead of using two point radius avg, figure out the avg. latitude (exactly between the src and dst points) and get its radius.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5274</id>
      <title>Teach fast FastVectorHighlighter to highlight "child fields" with parent fields</title>
      <description>I've been messing around with the FastVectorHighlighter and it looks like I can teach it to highlight matches on "child fields". Like this query: foo:scissors foo_exact:running would highlight foo like this: &lt;em&gt;running&lt;/em&gt; with &lt;em&gt;scissors&lt;/em&gt; Where foo is stored WITH_POSITIONS_OFFSETS and foo_plain is an unstored copy of foo a different analyzer and its own WITH_POSITIONS_OFFSETS. This would make queries that perform weighted matches against different analyzers much more convenient to highlight. I have working code and test cases but they are hacked into Elasticsearch. I'd love to Lucene-ify if you'll take them.</description>
      <attachments/>
      <comments>31</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5277</id>
      <title>Modify FixedBitSet copy constructor to take numBits to allow grow/shrink the new bitset</title>
      <description>FixedBitSet copy constructor is redundant the way it is now – one can call FBS.clone() to achieve that (and indeed, no code in Lucene calls this ctor). I think it will be useful to add a numBits parameter to that method to allow growing/shrinking the new bitset, while copying all relevant bits from the passed one.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5279</id>
      <title>Don't use recursion in DisjunctionSumScorer.countMatches</title>
      <description>I noticed the TODO in there, to not use recursion, so I fixed it to just use a private queue ...</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5280</id>
      <title>Rename TermFreqPayloadIterator -&gt; SuggestionIterator</title>
      <description>The current name is cumbersome, and annoying to change whenever we add something to the iterator (in this case payloads). Since we are breaking it anyway in 4.6, I think we should take the opportunity to find a better name.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5284</id>
      <title>Make it possible to filter classification training by Query</title>
      <description>It's sometimes useful to use only a certain subset of the whole index to train the classifier with. To do that we can pass a Query to be used during the training phase.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5287</id>
      <title>improvements for testing packed ints</title>
      <description>I noticed while working on LUCENE-5266 there is no guarantee every bpv is tested, since the acceptable overhead is always random. It would be good to first ensure all bpvs work, and then have random acceptable overhead. There was also a simple issue with the order of expected/actual for the direct packed reader loop.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5288</id>
      <title>Add ProxBooleanTermQuery, like BooleanQuery but boosting when term occur "close" together (in proximity) in each document</title>
      <description>This is very much a work in progress, tons of nocommits... It adds two classes: ProxBooleanTermQuery: like BooleanQuery (currently, all clauses must be TermQuery, and only Occur.SHOULD is supported), which is essentially a BooleanQuery (same matching/scoring) except for each matching docs the positions are merge-sorted and scored to "boost" the document's score QueryRescorer: simple API to re-score top hits using a different query. Because ProxBooleanTermQuery is so costly, apps would normally run an "ordinary" BooleanQuery across the full index, to get the top few hundred hits, and then rescore using the more costly ProxBooleanTermQuery (or other costly queries). I'm not sure how to actually compute the appropriate prox boost (this is the hard part!!) and I've completely punted on that in the current patch (it's just a hack now), but the patch does all the "mechanics" to merge/visit all the positions in order per hit. Maybe we could do the similar scoring that SpanNearQuery or sloppy PhraseQuery would do, or maybe this paper: http://plg.uwaterloo.ca/~claclark/sigir2006_term_proximity.pdf which Rob also used in LUCENE-4909 to add proximity scoring to PostingsHighlighter. Maybe we need to make it (how the prox boost is computed/folded in) somehow pluggable ...</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5291</id>
      <title>Faster Query-Time Join</title>
      <description>The current implementation of query-time join could be complemented with a much faster one, provided some choices can be made about what to join on. Since join is really a database concept, we found it quite natural to restrict the keys to be integers and be single valued. We found that if it is possible to use integers keys, and having single valued key fields, the speed of join can be improved 50 fold. Proper caching again speeds up about 20 times. I'd like to contribute our code if you agree that it is a useful contribution. That probably depends on what you think of the choices we made about the keys, so that need to be discussed first?</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5292</id>
      <title>Remove code dup in Disjunction/Sum/MaxScorer</title>
      <description>This patch is from Rob (I tweaked a bit, removing the numScorers param to DisjunctionScorer ctor); it eliminates the code dup in the two DisjunctionScorer subclasses by moving the code up to the base class ... I did a quick perf test on Wikipedia and it looks like hotspot seems to not mind (NOTE: I hardwired BS2 for both comp and base): Task QPS base StdDev QPS comp StdDev Pct diff MedPhrase 201.46 (5.9%) 200.06 (6.5%) -0.7% ( -12% - 12%) HighPhrase 4.32 (6.5%) 4.29 (6.8%) -0.6% ( -13% - 13%) LowPhrase 13.21 (1.5%) 13.15 (1.8%) -0.5% ( -3% - 2%) OrNotHighLow 371.05 (2.0%) 369.82 (1.9%) -0.3% ( -4% - 3%) AndHighLow 394.43 (2.3%) 393.16 (2.4%) -0.3% ( -4% - 4%) MedSpanNear 31.08 (3.2%) 31.01 (3.1%) -0.2% ( -6% - 6%) HighSpanNear 8.20 (5.9%) 8.19 (5.5%) -0.2% ( -10% - 11%) OrNotHighHigh 7.62 (3.1%) 7.60 (4.2%) -0.2% ( -7% - 7%) OrHighNotMed 26.76 (3.6%) 26.73 (4.4%) -0.1% ( -7% - 8%) OrNotHighMed 51.30 (1.3%) 51.28 (1.2%) -0.0% ( -2% - 2%) OrHighMed 18.50 (5.0%) 18.49 (5.1%) -0.0% ( -9% - 10%) OrHighLow 14.30 (6.0%) 14.30 (6.1%) 0.0% ( -11% - 12%) OrHighHigh 5.53 (4.7%) 5.53 (4.8%) 0.0% ( -9% - 10%) AndHighMed 34.18 (1.6%) 34.19 (1.7%) 0.0% ( -3% - 3%) OrHighNotLow 19.71 (3.8%) 19.72 (4.4%) 0.0% ( -7% - 8%) MedSloppyPhrase 3.49 (4.0%) 3.50 (3.2%) 0.1% ( -6% - 7%) Fuzzy1 67.11 (2.3%) 67.17 (2.6%) 0.1% ( -4% - 5%) OrHighNotHigh 9.69 (3.1%) 9.70 (4.0%) 0.1% ( -6% - 7%) AndHighHigh 28.19 (1.6%) 28.22 (1.7%) 0.1% ( -3% - 3%) LowSloppyPhrase 43.85 (2.0%) 43.91 (1.7%) 0.1% ( -3% - 3%) LowSpanNear 10.39 (2.9%) 10.41 (3.1%) 0.2% ( -5% - 6%) Wildcard 16.83 (3.1%) 16.85 (3.9%) 0.2% ( -6% - 7%) Respell 48.11 (2.9%) 48.19 (2.9%) 0.2% ( -5% - 6%) Fuzzy2 46.97 (2.6%) 47.10 (2.7%) 0.3% ( -4% - 5%) Prefix3 71.44 (3.8%) 71.66 (4.2%) 0.3% ( -7% - 8%) HighSloppyPhrase 3.41 (5.4%) 3.45 (4.7%) 1.0% ( -8% - 11%) LowTerm 308.11 (1.6%) 311.17 (3.9%) 1.0% ( -4% - 6%) IntNRQ 3.52 (7.7%) 3.57 (8.1%) 1.3% ( -13% - 18%) HighTerm 64.66 (4.3%) 65.53 (7.8%) 1.3% ( -10% - 13%) MedTerm 100.12 (3.2%) 101.53 (6.2%) 1.4% ( -7% - 11%)</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5294</id>
      <title>Suggester Dictionary implementation that takes expressions as term weights</title>
      <description>It would be nice to have a Suggester Dictionary implementation that could compute the weights of the terms consumed by the suggester based on an user-defined expression (using lucene's expression module). It could be an extension of the existing DocumentDictionary (which takes terms, weights and (optionally) payloads from the stored documents in the index). The only exception being that instead of taking the weights for the terms from the specified weight fields, it could compute the weights using an user-defn expression, that uses one or more NumicDocValuesField from the document. Example: let the document have product_id product_name product_popularity product_profit Then this implementation could be used with an expression of "0.2*product_popularity + 0.8*product_profit" to determine the weights of the terms for the corresponding documents (optionally along with a payload (product_id))</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5295</id>
      <title>Allow the license checker to optionally avoid check sum comparisons.</title>
      <description>Similar motivation as LUCENE-5209 and same implementation.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5296</id>
      <title>Add DirectDocValuesFormat</title>
      <description>Indexes values to disk but at search time it loads/accesses the values via simple java arrays (i.e. no compression).</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5297</id>
      <title>Allow rang faceting on any ValueSource</title>
      <description>Today RangeAccumulator assumes the ranges should be read from a NumericDocValues field. Would be good if we can modify it, or introduce a new ValueSourceAccumulator which allows you to range-facet on any ValueSource, e.g. one that is generated from an expression.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5298</id>
      <title>Allow aggregating facets by any ValueSource</title>
      <description>Facets can be aggregated today by counting them, aggregating by summing their association values, or summing the scores of the documents. Applications can write their own FacetsAggregator to compute the value of the facet in other ways. Following the new expressions module, I think it will be interesting to allow aggregating facets by arbitrary expressions, e.g. _score * sqrt(price) where 'price' is an NDV field. I'd like to explore allowing any ValueSource to be passed in and write a ValueSourceFacetsAggregator.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5299</id>
      <title>Refactor Collector API for parallelism</title>
      <description>Motivation We should be able to scale-up better with Solr/Lucene by utilizing multiple CPU cores, and not have to resort to scaling-out by sharding (with all the associated distributed system pitfalls) when the index size does not warrant it. Presently, IndexSearcher has an optional constructor arg for an ExecutorService, which gets used for searching in parallel for call paths where one of the TopDocCollector's is created internally. The per-atomic-reader search happens in parallel and then the TopDocs/TopFieldDocs results are merged with locking around the merge bit. However there are some problems with this approach: If arbitary Collector args come into play, we can't parallelize. Note that even if ultimately results are going to a TopDocCollector it may be wrapped inside e.g. a EarlyTerminatingCollector or TimeLimitingCollector or both. The special-casing with parallelism baked on top does not scale, there are many Collector's that could potentially lend themselves to parallelism, and special-casing means the parallelization has to be re-implemented if a different permutation of collectors is to be used. Proposal A refactoring of collectors that allows for parallelization at the level of the collection protocol. Some requirements that should guide the implementation: easy migration path for collectors that need to remain serial the parallelization should be composable (when collectors wrap other collectors) allow collectors to pick the optimal solution (e.g. there might be memory tradeoffs to be made) by advising the collector about whether a search will be parallelized, so that the serial use-case is not penalized. encourage use of non-blocking constructs and lock-free parallelism, blocking is not advisable for the hot-spot of a search, besides wasting pooled threads.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>5300</id>
      <title>SORTED_SET could use SORTED encoding when the field is actually single-valued</title>
      <description>It would be nice to detect when a SORTED_SET field is single-valued in order to optimize storage.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5301</id>
      <title>All PackedInts APIs should share a common interface for random-access reads</title>
      <description>It would be convenient if all PackedInts had a super-class with the long get(long index) method. Maybe this super-class could even be NumericDocValues so that doc values formats don't need to wrap eg. BlockPackedReader into this kind of construct: final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true); return new LongNumericDocValues() { @Override public long get(long id) { return reader.get(id); } }; Instead, they could just return new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5302</id>
      <title>Make StemmerOverrideMap methods public</title>
      <description>StemmerOverrideFilter is configured with an FST-based map that you can build at construction time from a list of entries. Building this FST offline and loading it directly as a bytestream makes construction a lot quicker, but you can't do that conveniently at the moment as all the methods of StemmerOverrideMap are package-private.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5304</id>
      <title>SingletonSortedSetDocValues should allow for getting back the wrapped instance</title>
      <description>This idea was mentioned by Robert on LUCENE-5300 Some codecs or FieldCache impls use SingletonSortedSetDocValues when a field which is supposed to be multi-valued is actually single-valued. By having a getter on this class to get back the wrapped SortedDocValues instance, we could add more specialization (which often already exists, eg. Solr's DocValuesFacets already have a specialized impl for SortedDocValues).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5306</id>
      <title>Add CompositeReader Support to DocumentExpressionDictionary</title>
      <description>Currently the DocumentExpressionDictionary does not have support for CompositeReader. It would be nice to add that support.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5308</id>
      <title>explore per-dimension fixed-width ordinal encoding</title>
      <description>I've been testing performance of Solr vs Lucene facets, and one area where Solr's "fcs" method shines (low RAM, high faceting perf) is in low-cardinality dimensions. I suspect the gains are because with the field-cache entries the ords are encoded in "column-stride" form, and are private to that dim (vs facet module's shared ord space). So I thought about whether we could do something like this in the facet module ... I.e., if we know certain documents will have a specific set of single-valued dimensions, we can pick an encoding format for the per-doc byte[] "globally" for all such documents, and use private ord space per-dimension to improve compression. The basic idea is to pre-assign up-front (before the segment is written) which bytes belong to which dim. E.g., date takes bytes 0-1 (&lt;= than 65536 unique labels), imageCount takes byte 2 (&lt;= 256 unique labels), username takes bytes 3-6 (&lt;= 16.8 M unique labels), etc. This only works for single-valued dims, and only works if all docs (or at least an identifiable subset?) have all dims. To test this idea, I made a hacked up prototype patch; it has tons of limitations so we clearly can't commit it, but I was able to test full wikipedia en with 6 facet dims (date, username, refCount, imageCount, sectionCount, subSectionCount, subSubSectionCount). Trunk (base) requires 181 MB of net doc values to hold the facet ords, while the patch requires 183 MB. Perf: Report after iter 19: Task QPS base StdDev QPS comp StdDev Pct diff Respell 54.30 (3.1%) 54.02 (2.7%) -0.5% ( -6% - 5%) MedSloppyPhrase 3.58 (5.6%) 3.60 (6.0%) 0.6% ( -10% - 12%) OrNotHighLow 63.58 (6.8%) 64.03 (6.9%) 0.7% ( -12% - 15%) HighSloppyPhrase 3.80 (7.4%) 3.84 (7.1%) 1.1% ( -12% - 16%) LowSpanNear 8.93 (3.5%) 9.09 (4.6%) 1.8% ( -6% - 10%) LowPhrase 12.15 (6.4%) 12.43 (7.2%) 2.3% ( -10% - 17%) AndHighLow 402.54 (1.4%) 425.23 (2.3%) 5.6% ( 1% - 9%) LowSloppyPhrase 39.53 (1.6%) 42.01 (1.9%) 6.3% ( 2% - 9%) MedSpanNear 26.54 (2.8%) 28.39 (3.6%) 7.0% ( 0% - 13%) HighPhrase 4.01 (8.1%) 4.30 (9.7%) 7.4% ( -9% - 27%) Fuzzy2 44.01 (2.3%) 47.43 (1.8%) 7.8% ( 3% - 12%) OrNotHighMed 32.64 (4.7%) 35.22 (5.5%) 7.9% ( -2% - 19%) Fuzzy1 62.24 (2.1%) 67.35 (1.9%) 8.2% ( 4% - 12%) MedPhrase 129.06 (4.9%) 141.14 (6.2%) 9.4% ( -1% - 21%) AndHighMed 27.71 (0.7%) 30.32 (1.1%) 9.4% ( 7% - 11%) HighSpanNear 5.15 (3.5%) 5.63 (4.2%) 9.5% ( 1% - 17%) AndHighHigh 24.98 (0.7%) 27.89 (1.1%) 11.7% ( 9% - 13%) OrNotHighHigh 15.13 (2.0%) 17.90 (2.6%) 18.3% ( 13% - 23%) Wildcard 9.06 (1.4%) 10.85 (2.6%) 19.8% ( 15% - 24%) OrHighNotHigh 8.84 (1.8%) 10.64 (2.6%) 20.3% ( 15% - 25%) OrHighHigh 3.73 (1.6%) 4.51 (2.4%) 20.9% ( 16% - 25%) OrHighLow 5.22 (1.5%) 6.34 (2.5%) 21.4% ( 17% - 25%) OrHighNotLow 8.94 (1.6%) 10.95 (2.5%) 22.5% ( 18% - 26%) Prefix3 27.61 (1.2%) 33.90 (2.3%) 22.8% ( 19% - 26%) OrHighMed 11.72 (1.6%) 14.56 (2.3%) 24.3% ( 20% - 28%) OrHighNotMed 14.74 (1.5%) 18.34 (2.2%) 24.5% ( 20% - 28%) MedTerm 26.37 (1.2%) 32.85 (2.7%) 24.6% ( 20% - 28%) IntNRQ 2.61 (1.2%) 3.25 (3.0%) 24.7% ( 20% - 29%) HighTerm 19.69 (1.3%) 25.33 (3.0%) 28.7% ( 23% - 33%) LowTerm 131.50 (1.3%) 170.49 (3.0%) 29.7% ( 25% - 34%) I think the gains are sizable, and the increase in index size quite minor (in another test with fewer dims I saw the index size get a bit smaller) ... at least for this specific test. However, finding a clean solution here will be tricky...</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5310</id>
      <title>Merge Threads unnecessarily block on SerialMergeScheduler</title>
      <description>I have been working on a high level merge multiplexer that shares threads across different IW instances and I came across the fact that SerialMergeScheduler actually blocks incoming thread is a merge in going on. Yet this blocks threads unnecessarily since we pull the merges in a loop anyway. We should use a tryLock operation instead of syncing the entire method?</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5311</id>
      <title>Make it possible to train / run classification over multiple fields</title>
      <description>It'd be nice to be able to use multiple fields instead of just one for training / running each classifier.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5312</id>
      <title>Block-join-friendly index sorting</title>
      <description>It could be useful to have a block-join-friendly sorter implementation that doesn't break index-time blocks: blocks must not interleave, parents must remain at the end of the blocks</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5313</id>
      <title>Add "preservePositionIncrements" to AnalyzingSuggester and FuzzySuggester constructors</title>
      <description>It would be convenient to have "preservePositionIncrements" in the suggesters constructor, rather than having a setPreservePositionIncrements method. That way it could be nicely used with the factory model already used by Solr.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5314</id>
      <title>Decide on whether the central class of the sorting API should be a sorter or a comparator</title>
      <description>Robert made a good point on LUCENE-5312 that the API currently feels half baked since it exposes Sorter as a central point of the API while all the useful impls are based on a comparator. Initially, I wanted a Sorter to be the central class because it would allow to compute a DocMap eg. to revert the order of the documents in the index without having to actually sort the documents. If you look at Sorter.REVERSE_DOCS, it returns the DocMap that reverts index order in constant time. However, this Sorter-based API doesn't allow for composability although a comparator-based API could. For example, we would like to be able to compose a sorter for block joins based on a sorter for parents and another for children. So maybe the use-cases that are not based on an actual sort are not really important and we could enforce sorting so that sorters could be composable?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5316</id>
      <title>Taxonomy tree traversing improvement</title>
      <description>The taxonomy traversing is done today utilizing the ParallelTaxonomyArrays. In particular, two taxonomy-size int arrays which hold for each ordinal it's (array #1) youngest child and (array #2) older sibling. This is a compact way of holding the tree information in memory, but it's not perfect: Large (8 bytes per ordinal in memory) Exposes internal implementation Utilizing these arrays for tree traversing is not straight forward Lose reference locality while traversing (the array is accessed in increasing only entries, but they may be distant from one another) In NRT, a reopen is always (not worst case) done at O(Taxonomy-size) This issue is about making the traversing more easy, the code more readable, and open it for future improvements (i.e memory footprint and NRT cost) - without changing any of the internals. A later issue(s?) could be opened to address the gaps once this one is done.</description>
      <attachments/>
      <comments>31</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5317</id>
      <title>Concordance/Key Word In Context (KWIC) capability</title>
      <description>This patch enables a Lucene-powered concordance search capability. Concordances are extremely useful for linguists, lawyers and other analysts performing analytic search vs. traditional snippeting/document retrieval tasks. By "analytic search," I mean that the user wants to browse every time a term appears (or at least the topn) in a subset of documents and see the words before and after. Concordance technology is far simpler and less interesting than IR relevance models/methods, but it can be extremely useful for some use cases. Traditional concordance sort orders are available (sort on words before the target, words after, target then words before and target then words after). Under the hood, this is running SpanQuery's getSpans() and reanalyzing to obtain character offsets. There is plenty of room for optimizations and refactoring. Many thanks to my colleague, Jason Robinson, for input on the design of this patch.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5318</id>
      <title>Co-occurrence counts from Concordance</title>
      <description>This patch calculates co-occurrence statistics on search terms within a window of x tokens. This can help in synonym discovery and anywhere else co-occurrence stats have been used. The attached patch depends on LUCENE-5317. Again, many thanks to my colleague, Jason Robinson, for advice in developing this code and for his modifications to this code to make it more Solr-friendly.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5319</id>
      <title>Keywords in concordance windows, Solr Wrapper for LUCENE-5318</title>
      <description>This is a simple RequestHandler wrapper around ConcordanceCooccurSearcher submitted in LUCENE-5318. It reanalyzes the concordance windows and ranks keywords wrt the target value of the concordance search. Does have some minimal support for SolrCloud, including distributed tf*idf. **Following Yonik's Law** This is patch is more of a placeholder for a much more polished draft. Among other things, test scripts and javadocs are forthcoming!</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5320</id>
      <title>Create SearcherTaxonomyManager over Directory</title>
      <description>SearcherTaxonomyManager now only allows working in NRT mode. It could be useful to have an STM which allows reopening a SearcherAndTaxonomy pair over Directories, e.g. for replication. The problem is that if the thread that calls maybeRefresh() is not the one that does the commit(), it could lead to a pair that is not synchronized. Perhaps at first we could have a simple version that works under some assumptions, i.e. that the app does the commit + reopen in the same thread in that order, so that it can be used by such apps + when replicating the indexes, and later we can figure out how to generalize it to work even if commit + reopen are done by separate threads/JVMs. I'll see if SearcherTaxonomyManager can be extended to support it, or a new STM is required.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5321</id>
      <title>Remove Facet42DocValuesFormat</title>
      <description>The new DirectDocValuesFormat is nearly identical to Facet42DVF, except that it stores the addresses in direct int[] rather than PackedInts. On LUCENE-5296 we measured the performance of DirectDVF vs Facet42DVF and it improves perf for some queries and have negligible effect for others, as well as RAM consumption isn't much worse. We should remove Facet42DVF and use DirectDVF instead. I also want to rename Facet46Codec to FacetCodec. There's no need to refactor the class whenever the default codec changes (e.g. from 45 to 46) since it doesn't care about the actual Codec version underneath, it only overrides the DVF used for the facet fields. FacetCodec should take the DVF from the app (so e.g. the facet/ module doesn't depend on codecs/) and be exposed more as a utility Codec rather than a real, versioned, Codec.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5323</id>
      <title>Add sizeInBytes to Suggester.Lookup</title>
      <description>It would be nice to have a sizeInBytes() method added to Suggester.Lookup interface. This would allow users to estimate the size of the in-memory data structure created by various suggester implementation.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5324</id>
      <title>Make AnalyzerWrapper.get(Offset|PositionIncrement)Gap non-final</title>
      <description>It can sometimes be useful to reconfigure the position and offset gaps of an existing analyzer.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5325</id>
      <title>Move ValueSource and FunctionValues under core/</title>
      <description>Spinoff from LUCENE-5298: ValueSource and FunctionValues are abstract APIs which exist under the queries/ module. That causes any module which wants to depend on these APIs (but not necessarily on any of their actual implementations!), to depend on the queries/ module. If we move these APIs under core/, we can eliminate these dependencies and add some mock impls for testing purposes. Quoting Robert from LUCENE-5298: we should eliminate the suggest/ dependencies on expressions and queries, the expressions/ on queries, the grouping/ dependency on queries, the spatial/ dependency on queries, its a mess. To add to that list, facet/ should not depend on queries too.</description>
      <attachments/>
      <comments>36</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>5326</id>
      <title>Add enum facet method to Lucene facet module</title>
      <description>I've been testing Solr facet performance, and the enum method works very well for low cardinality (not many unique values) fields. So I think we should fold a similar option into Lucene's facet module.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5327</id>
      <title>Expose getNumericDocValues and getBinaryDocValues at toplevel reader and searcher levels</title>
      <description>Expose NumericDocValues and BinaryDocValues in both IndexReader and IndexSearcher apis.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5329</id>
      <title>Make DocumentDictionary and co more lenient to dirty documents</title>
      <description>Currently DocumentDictionary errors out whenever any document does not have value for any relevant stored fields. It would be nice to make it lenient and instead ignore the invalid documents. Another "issue" with the DocumentDictionary is that it only allows string fields as suggestions and binary fields as payloads. When exposing these dictionaries to solr (via https://issues.apache.org/jira/browse/SOLR-5378), it is inconvenient for the user to ensure that a suggestion field is a string field and a payload field is a binary field. It would be nice to have the dictionary "just work" whenever a string/binary field is passed to suggestion/payload field. The patch provides one solution to this problem (by accepting string or binary values), though it would be great if there are any other solution to this, without making the DocumentDictionary "too flexible"</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5333</id>
      <title>Support sparse faceting for heterogeneous indices</title>
      <description>In some search apps, e.g. a large e-commerce site, the index can have a mix of wildly different product categories and facet dimensions, and the number of dimensions could be huge. E.g. maybe the index has shirts, computer memory, hard drives, etc., and each of these many categories has different attributes. In such an index, when someone searches for "so dimm", which should match a bunch of laptop memory modules, you can't (easily) know up front which facet dimensions will be important. But, I think this is very easy for the facet module, since ords are stored "row stride" (each doc lists all facet labels it has), we could simply count all facets that the hits actually saw, and then in the end see which ones "got traction" and return facet results for these top dims. I'm not sure what the API would look like, but conceptually this should work very well, because of how the facet module works. You shouldn't have to state up front exactly which facet dimensions to count...</description>
      <attachments/>
      <comments>12</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5334</id>
      <title>Add Namespaces to Expressions Javascript Compiler</title>
      <description>I would like to add the concept of namespaces to the expressions javascript compiler using '.' as the operator. Example of namespace usage in functions: AccurateMath.sqrt(field) FastMath.sqrt(field) Example of namespace usage in variables: location.latitude location.longitude</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5336</id>
      <title>Add a simple QueryParser to parse human-entered queries.</title>
      <description>I would like to add a new simple QueryParser to Lucene that is designed to parse human-entered queries. This parser will operate on an entire entered query using a specified single field or a set of weighted fields (using term boost). All features/operations in this parser can be enabled or disabled depending on what is necessary for the user. A default operator may be specified as either 'MUST' representing 'and' or 'SHOULD' representing 'or.' The features/operations that this parser will include are the following: AND specified as '+' OR specified as '|' NOT specified as '-' PHRASE surrounded by double quotes PREFIX specified as '*' PRECEDENCE surrounded by '(' and ')' WHITESPACE specified as ' ' '\n' '\r' and '\t' will cause the default operator to be used ESCAPE specified as '\' will allow operators to be used in terms The key differences between this parser and other existing parsers will be the following: No exceptions will be thrown, and errors in syntax will be ignored. The parser will do a best-effort interpretation of any query entered. It uses minimal syntax to express queries. All available operators are single characters or pairs of single characters. The parser is hand-written and in a single Java file making it easy to modify.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5337</id>
      <title>Add Payload support to FileDictionary (Suggest) and make it more configurable</title>
      <description>It would be nice to add payload support to FileDictionary, so user can pass in associated payload with suggestion entries. Currently the FileDictionary has a hard-coded field-delimiter (TAB), it would be nice to let the users configure the field delimiter as well.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5339</id>
      <title>Simplify the facet module APIs</title>
      <description>I'd like to explore simplifications to the facet module's APIs: I think the current APIs are complex, and the addition of a new feature (sparse faceting, LUCENE-5333) threatens to add even more classes (e.g., FacetRequestBuilder). I think we can do better. So, I've been prototyping some drastic changes; this is very early/exploratory and I'm not sure where it'll wind up but I think the new approach shows promise. The big changes are: Instead of *FacetRequest/Params/Result, you directly instantiate the classes that do facet counting (currently TaxonomyFacetCounts, RangeFacetCounts or SortedSetDVFacetCounts), passing in the SimpleFacetsCollector, and then you interact with those classes to pull labels + values (topN under a path, sparse, specific labels). At index time, no more FacetIndexingParams/CategoryListParams; instead, you make a new SimpleFacetFields and pass it the field it should store facets + drill downs under. If you want more than one CLI you create more than one instance of SimpleFacetFields. I added a simple schema, where you state which dimensions are hierarchical or multi-valued. From this we decide how to index the ordinals (no more OrdinalPolicy). Sparse faceting is just another method (getAllDims), on both taxonomy &amp; ssdv facet classes. I haven't created a common base class / interface for all of the search-time facet classes, but I think this may be possible/clean, and perhaps useful for drill sideways. All the new classes are under oal.facet.simple.*. Lots of things that don't work yet: drill sideways, complements, associations, sampling, partitions, etc. This is just a start ...</description>
      <attachments/>
      <comments>89</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5348</id>
      <title>Allow to define MLT min doc and term freq for kNN classifier</title>
      <description>Allow to define MLT minDocFreq and minTermFreq for KNearestNeighborClassifier to tweak recall of MoreLikeThis queries.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5349</id>
      <title>Ivy's resolution cache can easily corrupt and cause premature end of file errors.</title>
      <description>If probably know this problem well if you run with a few executors on your jenkins setup. Really annoying.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5350</id>
      <title>Add Context Aware Suggester</title>
      <description>It would be nice to have a Context Aware Suggester (i.e. a suggester that could return suggestions depending on some specified context(s)). Use-cases: location-based suggestions: returns suggestions which 'match' the context of a particular area suggest restaurants names which are in Palo Alto (context -&gt; Palo Alto) category-based suggestions: returns suggestions for items that are only in certain categories/genres (contexts) suggest movies that are of the genre sci-fi and adventure (context -&gt; [sci-fi, adventure])</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5353</id>
      <title>ShingleFilter should have a way to specify FILLER_TOKEN</title>
      <description>Today we have no choice that if pos_inc is &gt; 1 there will be a `_` inserted in between the tokens. We should have the ability to change this character and the char[] that holds it should not be public static since it's mutable.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5354</id>
      <title>Blended score in AnalyzingInfixSuggester</title>
      <description>I'm working on a custom suggester derived from the AnalyzingInfix. I require what is called a "blended score" (//TODO ln.399 in AnalyzingInfixSuggester) to transform the suggestion weights depending on the position of the searched term(s) in the text. Right now, I'm using an easy solution : If I want 10 suggestions, then I search against the current ordered index for the 100 first results and transform the weight : a) by using the term position in the text (found with TermVector and DocsAndPositionsEnum) or b) by multiplying the weight by the score of a SpanQuery that I add when searching and return the updated 10 most weighted suggestions. Since we usually don't need to suggest so many things, the bigger search + rescoring overhead is not so significant but I agree that this is not the most elegant solution. We could include this factor (here the position of the term) directly into the index. So, I can contribute to this if you think it's worth adding it. Do you think I should tweak AnalyzingInfixSuggester, subclass it or create a dedicated class ?</description>
      <attachments/>
      <comments>15</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5355</id>
      <title>Add more support to validate the -Dbootclasspath given for javadocs generate</title>
      <description>When Simon created the nice looking javadocs for LuSolr 4.6, he just copypasted the command line from http://wiki.apache.org/lucene-java/HowToGenerateNiceJavadocs Unfortunately this does not work with AppleJDK6, because it has no rt.jar! The rt.jar file is there in a completely different directory and is named classes.jar. I had a similar problem when I wanted to regenerate the Javadocs on my Linux box, but specified -Dbootclasspath with shell specials (e.g., ~ for homedir). This patch will assist the user and will "validate" the given bootclasspath, so it points to a JAR file that actually contains the runtime. Also to make life easier, instead of -Dbootclasspath you can set -Dbootjdk to the JDK homefolder (same like JAVA_HOME) and ANT will figure out if it is Apple or Oracle or maybe only a JRE. In the meantime, I regenerated the 4.6 Javadocs with correct bootclasspath.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5356</id>
      <title>Morfologik filter can accept custom dictionary resources</title>
      <description>I have little proposal for morfologik lucene module. Current module is tightly coupled with polish DICTIONARY enumeration. But other people (like me) can build own dictionaries to FSA and use it with lucene. You can find proposal in attachment and also example usage in analyzer (SlovakLemmaAnalyzer). It uses dictionary property as String resource from classpath, not enumeration. One change is, that dictionary variable must be set in MofologikFilterFactory (no default value).</description>
      <attachments/>
      <comments>22</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5357</id>
      <title>Upgrade StandardTokenizer &amp; co to latest unicode rules</title>
      <description>besides any change in data, the rules have also changed (regional indicators, better handling for hebrew, etc)</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5360</id>
      <title>Add support for developing in netbeans IDE</title>
      <description>It will be nice to have ant target for building netbeans IDE project definition.</description>
      <attachments/>
      <comments>32</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5362</id>
      <title>IndexReader and friends should check ref count when incrementing</title>
      <description>IndexReader and SegmentCoreReaders blindly increments it's refcount which could already be counted down to 0 which might allow an IndexReader to "rise from the dead" and use an already closed SCR instance. Even if that is caught we should try best effort to raise ACE asap.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5366</id>
      <title>Add autoGeneratePhraseQueries into StandardQueryParser</title>
      <description>classic.QueryParser has an autoGeneratePhraseQueries but flexible.standard.StandardQueryParser does not have it. autoGeneratePhraseQueries feature is useful when using CJKAnalyzer. I wrote a patch that adds this feature into StandardQueryParser. Please apply this patch if you'd like.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5369</id>
      <title>Add an UpperCaseFilter</title>
      <description>We should offer a standard way to force upper-case tokens. I understand that lowercase is safer for general search quality because some uppercase characters can represent multiple lowercase ones. However, having upper-case tokens is often nice for faceting (consider normalizing to standard acronyms)</description>
      <attachments/>
      <comments>13</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5370</id>
      <title>Sorting Facets on CategoryPath (Label)</title>
      <description>Facet support sorting through FacetRequest.SortOrder. This is used in the ResultSortUtils. For my application it would be very nice if the facets can also be sorted on their label. I think this could be accomplished by altering FacetRequest with an extra enum SortType, and two extra Heap in ResultSortUtils which instead of comparing the double value, compare the CategoryPath. What do you think of this idea? Or could the same behaviour be accomplished in a different way already? (btw: I tried building this patch on the trunk of lucene5.0; but I couldn't get the maven build to build correctly. I will try again lateron on the 4.6 branch.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5371</id>
      <title>Range faceting should use O(log(N)) search per hit</title>
      <description>Today, Lucene's dynamic range faceting uses a simple linear search to find which ranges match, but there are known data structures to do this in log(N) time. I played with segment trees and wrote up a blog post here: http://blog.mikemccandless.com/2013/12/fast-range-faceting-using-segment-trees.html O(N) cost is actually OK when number of ranges is smallish, which is typical for facet use cases, but then scales badly if there are many ranges.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5376</id>
      <title>Add a demo search server</title>
      <description>I think it'd be useful to have a "demo" search server for Lucene. Rather than being fully featured, like Solr, it would be minimal, just wrapping the existing Lucene modules to show how you can make use of these features in a server setting. The purpose is to demonstrate how one can build a minimal search server on top of APIs like SearchManager, SearcherLifetimeManager, etc. This is also useful for finding rough edges / issues in Lucene's APIs that make building a server unnecessarily hard. I don't think it should have back compatibility promises (except Lucene's index back compatibility), so it's free to improve as Lucene's APIs change. As a starting point, I'll post what I built for the "eating your own dog food" search app for Lucene's &amp; Solr's jira issues http://jirasearch.mikemccandless.com (blog: http://blog.mikemccandless.com/2013/05/eating-dog-food-with-lucene.html ). It uses Netty to expose basic indexing &amp; searching APIs via JSON, but it's very rough (lots nocommits).</description>
      <attachments/>
      <comments>98</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>5378</id>
      <title>Enable using extended field types with prefix queries for non-default encoded strings</title>
      <description>Enable users to be able to use prefix query with custom field types with non-default encoding/decoding for queries more easily. e.g. having a custom field work with base64 encoded query strings. Currently, the workaround for it is to have the override at getRewriteMethod level. Perhaps having the prefixQuery also use the calling FieldType's readableToIndexed method would work better.</description>
      <attachments/>
      <comments>34</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>5379</id>
      <title>Kurdish Analyzer</title>
      <description>Normalizer+Stemmer+Stopwords for Sorani kurdish (written in the arabic script). The most important piece is the normalization: this varies wildly in practice. The stemmer is a light stemmer, very simple and not aggressive at all. I tested against the pewan test collection, see: http://eng.uok.ac.ir/esmaili/research/klpp/downloads/publications/AICCSA2013.pdf http://eng.uok.ac.ir/esmaili/research/klpp/en/downloads.htm baseline is StandardAnalyzer. short queries (T) TFIDF BM25 I(ne)B2 baseline 0.2355 0.2473 0.2702 patch 0.2930 (+24%) 0.3163 (+28%) 0.3309 (+22%) long queries (D) TFIDF BM25 I(ne)B2 baseline 0.3111 0.3185 0.3547 patch 0.4060 (+31%) 0.4422 (+39%) 0.4800 (+35%)</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5380</id>
      <title>PagingFieldCollector should track previous page hits</title>
      <description>PagingFieldCollector partitions all hits into three buckets: previous page hits, collected (current page) hits, and non-competitive (following page) hits. Total hits and collected hits are tracked, but neither non-competitive hits nor previous page hits are tracked, so previous page hits can't be derived from the total and collected hits.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5383</id>
      <title>fix changes2html to link pull requests</title>
      <description>If someone submits a pull request, i think we should put it in changes.txt in some way similar to the jira issues: e.g. for a JIRA issue we do: * LUCENE-XXXX: Add FooBar. (Joe Contributor via John Committer) changes2html recognizes and expands these to jira issue links. so I think we should be able to do something like: * pull request #xxx: Add FooBar. (Joe Contributor via John Committer) and have it link to the request, too.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5384</id>
      <title>Analysis overview could mention clearAttributes and end</title>
      <description>It would be helpful to tokenizer implementors for the analysis package overview to mention more things. I'll supply a patch.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5386</id>
      <title>Make Tokenizers deliver their final offsets</title>
      <description>Tokenizers must have an implementation of #end() in which they set up the final offset. Currently, nothing enforces this. end() has a useful implementation in TokenStream, so just making it abstract is not attractive. Proposal: add abstract int finalOffset(); to tokenizer, and then make void end() { super.end(); int fo = finalOffset(); offsetAttr.setOffsets(fo, fo); } or something to that effect. Other alternative to be considered depending on how this looks.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5387</id>
      <title>Improve FacetConfig.build</title>
      <description>FacetConfig.build() takes an IndexDocument and returns a new instance of IndexDocument. This forces you to write code like this: Document doc = new Document(); doc.add(new StringField("id", "someID", Store.NO)); doc.add(new FacetField("author", "john")); IndexDocument facetDoc = facetConfig.build(doc); indexWriter.addDocument(facetDoc); Technically, you don't need to declare 'facetDoc', you could just indexWriter.addDocument(facetConfig.build(doc)), but it's weird: After you call facetConfig.build(), you cannot add any more fields to the document (since you get an IndexDoc), so you must call it last. Nothing suggests you should call facetConfig.build() at all - I can already see users trapped by the new API, thinking that adding a FacetField is enough. We should at least document on FacetField that you should call FacetConfig.build(). Nothing suggests that you shouldn't ignore the returned IndexDoc from FC.build() - we should at least document that. I think that if FacetConfig.build() took an IndexDocument but returned a Document, that will at least allow you to call it in whatever stage of the pipeline that you want (after adding all FacetFields though)... I'll post a patch later.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5388</id>
      <title>Eliminate construction over readers for Tokenizer</title>
      <description>In the modern world, Tokenizers are intended to be reusable, with input supplied via #setReader. The constructors that take Reader are a vestige. Worse yet, they invite people to make mistakes in handling the reader that tangle them up with the state machine in Tokenizer. The sensible thing is to eliminate these ctors, and force setReader usage.</description>
      <attachments/>
      <comments>34</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5389</id>
      <title>Even more doc for construction of TokenStream components</title>
      <description>There are more useful things to tell would-be authors of tokenizers. Let's tell them.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5395</id>
      <title>Upgrade Spatial4j to 0.4</title>
      <description>Spatial4j 0.4 should be released the week of January 13th; a snapshot is published. A longer version of the delta from 0.4 is in CHANGES.md A couple notable new features are: Built-in WKT parser without relying on JTS. The older shape string format is deprecated. A binary shape codec for reading &amp; writing the shapes to a byte-stream in a reasonably compact manner.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5402</id>
      <title>Add support for index-time pruning in Document*Dictionary (Suggester)</title>
      <description>It would be nice to be able to prune out entries that the suggester consumes by some query.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5404</id>
      <title>Add support to get number of entries a Suggester Lookup was built with and minor refactorings</title>
      <description>It would be nice to be able to tell the number of entries a suggester lookup was built with. This would let components using lookups to keep some stats regarding how many entries were used to build a lookup. Additionally, Dictionary could use InputIterator rather than the BytesRefIteratator, as most of the implmentations now use it.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5405</id>
      <title>Exception strategy for analysis improved</title>
      <description>SOLR-5623 included some conversation about the dilemmas of exception management and reporting in the analysis chain. I've belatedly become educated about the infostream, and this situation is a job for it. The DocInverterPerField can note exceptions in the analysis chain, log out to the infostream, and then rethrow them as before. No wrapping, no muss, no fuss. There are comments on this JIRA from a more complex prior idea that readers might want to ignore.</description>
      <attachments/>
      <comments>35</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5406</id>
      <title>ShingleAnalyzerWrapper should expose the delegated analyzer as a public final</title>
      <description>I'm sometimes given a ShingleAnalyzerWrapper that I would like to change the shingle size on, so I need to create a new instance. However, I don't always know what the underlying analyzer is and I can't access it b/c it is a protected method on a final class. The solution here is to make the getAnalyzer method public final for the ShingleAnalyzerWrapper.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5408</id>
      <title>SerializedDVStrategy -- match geometries in DocValues</title>
      <description>I've started work on a new SpatialStrategy implementation I'm tentatively calling SerializedDVStrategy. It's similar to the JtsGeoStrategy in Spatial-Solr-Sandbox but a little different in the details – certainly faster. Using Spatial4j 0.4's BinaryCodec, it'll serialize the shape to bytes (for polygons this in internally WKB format) and the strategy will put it in a BinaryDocValuesField. In practice the shape is likely a polygon but it needn't be. Then I'll implement a Filter that returns a DocIdSetIterator that evaluates a given document passed via advance(docid)) to see if the query shape matches a shape in DocValues. It's improper usage for it to be used in a situation where it will evaluate every document id via nextDoc(). And in practice the DocValues format chosen should be a disk resident one since each value tends to be kind of big. This spatial strategy in and of itself has no index; it's O(N) where N is the number of documents that get passed thru it. So it should be placed last in the query/filter tree so that the other queries limit the documents it needs to see. At a minimum, another query/filter to use in conjunction is another SpatialStrategy like RecursivePrefixTreeStrategy. Eventually once the PrefixTree grid encoding has a little bit more metadata, it will be possible to further combine the grid &amp; this strategy in such a way that many documents won't need to be checked against the serialized geometry.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5410</id>
      <title>Add fuzziness support to SimpleQueryParser</title>
      <description>It would be nice to add fuzzy query support to the SimpleQueryParser so that: foo~2 generates a FuzzyQuery with an max edit distance of 2 and: "foo bar"~2 generates a PhraseQuery with a slop of 2.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5411</id>
      <title>Upgrade to released JFlex 1.5.0</title>
      <description>The JFlex 1.5.0 release will be officially announced shortly. The jar is already on Maven Central.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5415</id>
      <title>Support wildcard &amp; co in PostingsHighlighter</title>
      <description>PostingsHighlighter uses the offsets encoded in the postings lists for the terms to find query matches. As such, it isn't really suitable for stuff like wildcards for two reasons: 1. an expensive rewrite against the term dictionary (i think other highlighters share this problem) 2. accumulating data from potentially many terms (e.g. reading many postings) However, we could provide an option for some of these queries to work, but in a different way, that avoids these downsides. Instead we can just grab the Automaton representation of the queries, and match it against the content directly (which won't blow up).</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5417</id>
      <title>Solr function query supports reading multiple values from a field.</title>
      <description>Solr function query is a powerful tool to customize search criterion and ranking function (http://wiki.apache.org/solr/FunctionQuery). However, it cannot effectively benefit from field values from multi-valued field, namely, the field(...) function can only read one value and discard the others. This limitation has been associated with FieldCacheSource, and the fact that FieldCache cannot fetch multiple values from a field, but such constraint has been largely lifted by LUCENE-3354, which allows multiple values to be extracted from one field. Those values in turn can be used as parameters of other functions to yield a single score. I personally find this limitation very unhandy when building a learning-to-rank system that uses many cues and string features. Therefore I would like to post this feature request and (hopefully) work on it myself.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5418</id>
      <title>Don't use .advance on costly (e.g. distance range facets) filters</title>
      <description>If you use a distance filter today (see http://blog.mikemccandless.com/2014/01/geospatial-distance-faceting-using.html ), then drill down on one of those ranges, under the hood Lucene is using .advance on the Filter, which is very costly because we end up computing distance on (possibly many) hits that don't match the query. It's better performance to find the hits matching the Query first, and then check the filter. FilteredQuery can already do this today, when you use its QUERY_FIRST_FILTER_STRATEGY. This essentially accomplishes the same thing as Solr's "post filters" (I think?) but with a far simpler/better/less code approach. E.g., I believe ElasticSearch uses this API when it applies costly filters. Longish term, I think Query/Filter ought to know itself that it's expensive, and cases where such a Query/Filter is MUST'd onto a BooleanQuery (e.g. ConstantScoreQuery), or the Filter is a clause in BooleanFilter, or it's passed to IndexSearcher.search, we should also be "smart" here and not call .advance on such clauses. But that'd be a biggish change ... so for today the "workaround" is the user must carefully construct the FilteredQuery themselves. In the mean time, as another workaround, I want to fix DrillSideways so that when you drill down on such filters it doesn't use .advance; this should give a good speedup for the "normal path" API usage with a costly filter. I'm iterating on the lucene server branch (LUCENE-5376) but once it's working I plan to merge this back to trunk / 4.7.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5419</id>
      <title>Implement Codec for bulk copying of postings</title>
      <description>Based on Michael McCandless's suggestion: We have no codec that implements bulk merging for postings, which would be interesting to pursue: in the append-only case it's possible, and merging of postings is normally by far the most time consuming step of a merge. Source: http://search-lucene.com/m/Kw9ZhaUjhA</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5422</id>
      <title>Postings lists deduplication</title>
      <description>The context: http://markmail.org/thread/tywtrjjcfdbzww6f Robert Muir and I have discussed what Robert eventually named "postings lists deduplication" at Berlin Buzzwords 2013 conference. The idea is to allow multiple terms to point to the same postings list to save space. This can be achieved by new index codec implementation, but this jira is open to other ideas as well. The application / impact of this is positive for synonyms, exact / inexact terms, leading wildcard support via storing reversed term etc. For example, at the moment, when supporting exact (unstemmed) and inexact (stemmed) searches, we store both unstemmed and stemmed variant of a word form and that leads to index bloating. That is why we had to remove the leading wildcard support via reversing a token on index and query time because of the same index size considerations. Comment from Mike McCandless: Neat idea! Would this idea allow a single term to point to (the union of) N other posting lists? It seems like that's necessary e.g. to handle the exact/inexact case. And then, to produce the Docs/AndPositionsEnum you'd need to do the merge sort across those N posting lists? Such a thing might also be do-able as runtime only wrapper around the postings API (FieldsProducer), if you could at runtime do the reverse expansion (e.g. stem -&gt; all of its surface forms). Comment from Robert Muir: I think the exact/inexact is trickier (detecting it would be the hard part), and you are right, another solution might work better. but for the reverse wildcard and synonyms situation, it seems we could even detect it on write if we created some hash of the previous terms postings. if the hash matches for the current term, we know it might be a "duplicate" and would have to actually do the costly check they are the same. maybe there are better ways to do it, but it might be a fun postingformat experiment to try.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5424</id>
      <title>FilteredQuery useRandomAccess() should use cost()</title>
      <description>Now that Lucene's DISI has a cost() method, it's possible for FilteredQuery's RANDOM_ACCESS_FILTER_STRATEGY to use a smarter algorithm in its useRandomAccess() method. In particular, it might examine filterIter.cost() to see if it is greater than the cost returned by weight.scorer().cost() of the query.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5425</id>
      <title>Make creation of FixedBitSet in FacetsCollector overridable</title>
      <description>In FacetsCollector, creation of bits in MatchingDocs are allocated per query. For large indexes where maxDocs are large creating a bitset of maxDoc bits will be expensive and would great a lot of garbage. Attached patch is to allow for this allocation customizable while maintaining current behavior.</description>
      <attachments/>
      <comments>43</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5426</id>
      <title>Make SortedSetDocValuesReaderState customizable</title>
      <description>We have a reader that have a different data structure (in memory) where the cost of computing ordinals per reader open is too expensive in the realtime setting. We are maintaining in memory data structure that supports all functionality and would like to leverage SortedSetDocValuesAccumulator.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5427</id>
      <title>Support for changing FieldCache implementation in Lucene</title>
      <description>Believe in general it would be a nice feature to be able to overwrite the FieldCache-implementation in Lucene. For instance we override with a FieldCache-implementation that basically does not allow anything to be put in the FieldCache caches. We have so much data that it always creates memory issues. But I could imagine other reasons for wanting to override the FieldCache-implementation</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5428</id>
      <title>Make Faceting counting array overridable</title>
      <description>In SortedSetDocValuesFacetCounts, the count array is allocated as an int[] size of number of total values across all facets and that is allocated per query. In the case where number of values are large, large amount of garbage maybe created. Furthermore, the size of the array is dependent on the number of possible values, not number of number values needed for which facets fields are being accumulated for. E.g. if FacetSearchParam indicates counting only one 1 field with 2 values, we are still creating the array for all values across all fields. This patch makes the count array abstract to allow for 1) caching 2) hash counting - which can choose to count only of needed fields. This patch can be further enhanced to create FacetCouter per segment, per field by pass in the ordinal map.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5429</id>
      <title>Run one search across multiple scorers/collectors</title>
      <description>I'm looking into the possibility of running the same search across many scorers, so that decoding postings lists / doing union and intersect are done once, but scoring via Similarity can be done multiple times for each it (and the results collected into separate collectors).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5431</id>
      <title>Add FSLockFactory.toString()</title>
      <description>FSLockFactory doesn't override toString, which causes Dir.toString() to print the class.name@instance. I think it would be better if it printed e.g. the lockDir. I added it but TestCrashCausesCorruptIndex failed because it declares a Directory which doesn't override getLockID(), which returns toString(). I changed that Directory to extend FilterDirectory, and fixed FilterDirectory to override getLockID() to call in.getLockID(). Will attach a patch shortly.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5434</id>
      <title>NRT support for file systems that do no have delete on last close or cannot delete while referenced semantics.</title>
      <description>See SOLR-5693 and our HDFS support - for something like HDFS to work with NRT, we need an ability for near realtime readers to hold references to their files to prevent deletes.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5437</id>
      <title>ASCIIFoldingFilter that emits both unfolded and folded tokens</title>
      <description>I've found myself wanting an ASCIIFoldingFilter that emits both the folded tokens and the original, unfolded tokens.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5438</id>
      <title>add near-real-time replication</title>
      <description>Lucene's replication module makes it easy to incrementally sync index changes from a master index to any number of replicas, and it handles/abstracts all the underlying complexity of holding a time-expiring snapshot, finding which files need copying, syncing more than one index (e.g., taxo + index), etc. But today you must first commit on the master, and then again the replica's copied files are fsync'd, because the code operates on commit points. But this isn't "technically" necessary, and it mixes up durability and fast turnaround time. Long ago we added near-real-time readers to Lucene, for the same reason: you shouldn't have to commit just to see the new index changes. I think we should do the same for replication: allow the new segments to be copied out to replica(s), and new NRT readers to be opened, to fully decouple committing from visibility. This way apps can then separately choose when to replicate (for freshness), and when to commit (for durability). I think for some apps this could be a compelling alternative to the "re-index all documents on each shard" approach that Solr Cloud / ElasticSearch implement today, and it may also mean that the transaction log can remain external to / above the cluster.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5439</id>
      <title>Add Jacoco option for Test Coverage</title>
      <description>Jacoco (http://www.jacoco.org/) is a much cleaner and simpler to use code coverage tool than clover and additionally doesn't require having a third party license since it is open source. It also has nice Jenkins integration tools that make it incredibly easy to see what is and isn't tested. We should convert the Lucene and Solr builds to use Jacoco instead of Clover.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5440</id>
      <title>Add LongFixedBitSet and replace usage of OpenBitSet</title>
      <description>Spinoff from here: http://lucene.markmail.org/thread/35gw3amo53dsqsqj. I wrote a LongFixedBitSet which behaves like FixedBitSet, only allows managing more than 2.1B bits. It overcome some issues I've encountered with OpenBitSet, such as the use of set/fastSet as well the implementation of DocIdSet. I'll post a patch shortly and describe it in more detail.</description>
      <attachments/>
      <comments>32</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5442</id>
      <title>Build system should sanity check transative 3rd party dependencies</title>
      <description>SOLR-5365 is an example of a bug that croped up because we upgraded a 3rd party dep (tika) w/o realizing that the version we upgraded too depended on a newer version of another 3rd party dep (commons-compress) in a comment in SOLR-5365, Jan suggested that it would be nice if there was an easy way to spot problems like this ... i asked steve about it, thinking maybe this is something the maven build could help with, and he mentioned that there is already an ant task to inspect the ivy transative deps in order to generate the maven deps and it could be used to help detect this sort of problem. opening this issue per steve's request as a reminder to look into this possibility.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5448</id>
      <title>Random string generation centralized in _TestUtil</title>
      <description>The random string generators in BaseTokenStreamTestCase have wider applicability and should move in with their cousins.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5449</id>
      <title>Two ancient classes renamed to be less peculiar: _TestHelper and _TestUtil</title>
      <description>_TestUtil and _TestHelper begin with _ for historical reasons that don't apply any longer. Lets eliminate those _'s.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5452</id>
      <title>Combine matches from multiple fields into one with the postings highlighter</title>
      <description>Like you can do with the FVH, it'd be nice to be able combine matches from multiple fields with the postings highlighter. Note that the postings highlighter doesn't do phrase matching and doesn't use term boosts so some of the FVH's field combining features won't work. It'd be nice to get some of them, though.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5453</id>
      <title>some trivial refactoring to postingshighlighter</title>
      <description>As mentioned on LUCENE-5452, its hard to see whats going on in highlightFields (e.g. the termsenum reuse). I think thats just because the code needs to be re-arranged a little bit and a few comments added.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5454</id>
      <title>SortField for SortedSetDV</title>
      <description>Currently, its not possible to sort by a sortedsetdv (e.g. a multi-valued field). So the idea is to provide some comparators that let you do this: by choosing a selector (e.g. min/max/median type stuff) that picks a value from the set as a representative sort value. The implementation is pretty simple, just actually wrap the SortedSet in a SortedDV that does the selection, and re-use the existing TermOrdValComparator logic. One issue is that, with the current sortedset API only 'min' is a viable option because its the only one that can be done in constant time. So this patch adds an optional extension (RandomAccessOrds) for codecs that can support random access. I added this to the default codec, diskdv, and direct, as they can all easily support it. While this could be useful for other purposes (e.g. min/max as valuesource or whatever), i think its best to be optional because it prevents some forms of encoding/compression. Anyway I'm targeting lucene/sandbox with this change...</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5457</id>
      <title>Expose SloppyMath earth diameter table</title>
      <description>LUCENE-5271 introduced a table in order to get approximate values of the diameter of the earth given a latitude. This could be useful for other computations so I think it would be nice to have a method that exposes this table.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5460</id>
      <title>Allow driving a query by sparse filters</title>
      <description>Today if a filter is very sparse we execute the query in sort of a leap-frog manner between the query and filter. If the query is very expensive to compute, and/or matching few docs only too, calling scorer.advance(doc) just to discover the doc it landed on isn't accepted by the filter, is a waste of time. Since Filter is always the "final ruler", I wonder if we had something like boolean DISI.advanceExact(doc) we could use it instead, in some cases. There are many combinations in which I think we'd want to use/not-use this API, and they depend on: Filter's complexity, Filter.cost(), Scorer.cost(), query complexity (span-near, many clauses) etc. I open an issue so we can discuss. DISI.advanceExact(doc) is just a preliminary proposal, to get an API we could experiment with. The default implementation should be fairly easy and straightforward, and we could override where we can offer a more optimized imp.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5467</id>
      <title>Re-add "ant clean-jars" on root level</title>
      <description>Although ivy sync=true seems to work in most cases, sometimes (especially when updating version numbers) the old JAR files are not nuked on resolve. I have no idea about why this is still a bug in ivy, but sync=true does not help in all cases. It would be good to re-add the ant clean-jars target to the top-level build.xml, because this allows you to remove all JAR files and do a re-resolve. The bug occurred to Mark Miller and also to me this morning when Mark updated slf4j versions: If you run ant jar-checksums after updating the version numbers, it creates 2 checksum files (for the old and the new version). The call to ant check-svn-working-copy then does not find a violation, because the old version's checksum is still recreated. It only complains about the new one, but when you add it it will not complain anymore about the old one. The violation is only found by Jenkins, because this one does a fresh svn checkout (or it emulates one), so it starts without any JARs. Because of this, checksum re-creation deletes the old file (still on subversion server) and a violation is detected. Currently I do the JAR file deletion by a find/xargs/rm and then recreate checksums, which finds the problem. The good old ant clean-jars would be back as a last resort if you are hit my this IVY bug when you are updating revision numbers.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5472</id>
      <title>Long terms should generate a RuntimeException, not just infoStream</title>
      <description>As reported on the solr-user list, when a term is greater then 2^15 bytes it is silently ignored at indexing time – a message is logged in to infoStream if enabled, but no error is thrown. seems like we should change this behavior (if nothing else starting in 5.0) to throw an exception.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5474</id>
      <title>Add example for retrieving facet counts without retrieving documents</title>
      <description>In the examples of facetting the FacetsCollector.search() is used. There are use cases where you do not need the documents that match the search. It would be nice if there is an example showing this. Basically, it comes down to using searcher.search(query, null /* Filter */, facetCollector)</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5476</id>
      <title>Facet sampling</title>
      <description>With LUCENE-5339 facet sampling disappeared. When trying to display facet counts on large datasets (&gt;10M documents) counting facets is rather expensive, as all the hits are collected and processed. Sampling greatly reduced this and thus provided a nice speedup. Could it be brought back?</description>
      <attachments/>
      <comments>59</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5477</id>
      <title>add near-real-time suggest building to AnalyzingInfixSuggester</title>
      <description>Because this suggester impl. is just a Lucene index under-the-hood, it should be straightforward to enable near-real-time additions/removals of suggestions.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5478</id>
      <title>Allow CommonTermsQuery to create custom term queries</title>
      <description>currently we create term queries with new TermQuery(..) directly in CommonTermsQuery I'd like to extend the creation of the term query just like you can do that in the the query parser.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5479</id>
      <title>Make default dimension config in FacetConfig adjustable</title>
      <description>Now it is hardcoded to DEFAULT_DIM_CONFIG. This may be useful for most standard approaches. However, I use lots of facets. These facets can be multivalued, I do not know that on beforehand. So what I would like to do is to change the default config to mulitvalued = true. Currently I have a working, but rather ugly workaround that subclasses FacetConfig, like this: CustomFacetConfig.java public class CustomFacetsConfig extends FacetsConfig { public final static DimConfig DEFAULT_D2A_DIM_CONFIG = new DimConfig(); static { DEFAULT_D2A_DIM_CONFIG.multiValued = true; } @Override public synchronized DimConfig getDimConfig( String dimName ) { DimConfig ft = super.getDimConfig( dimName ); if ( DEFAULT_DIM_CONFIG.equals( ft ) ) { return DEFAULT_D2A_DIM_CONFIG; } return ft; } } I created a patch to illustrate what I would like to change. By making a protected method it is easier to create a custom subclass of FacetConfig. Also, maybe there are better way to accomplish my goal (easy default to multivalue?)</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5482</id>
      <title>improve default TurkishAnalyzer</title>
      <description>Add a TokenFilter that strips characters after an apostrophe (including the apostrophe itself).</description>
      <attachments/>
      <comments>28</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5484</id>
      <title>Distinct control of recursion levels for prefix and suffix in Hunspell.</title>
      <description>Currently, there is an option to set recursionCap value to control depth of recursion in Hunspell token filter. This recursion enables to apply allowed affix rule to input token and pass output token(s) as an input tokens recursively. However, the recursionCap does not allow to distinguish between how many prefix and suffix rules were applied. It just counts for total. For example if recursionCap is set to 1 it actually includes all of the following options: 2 prefix rules, 0 suffix rules 1prefix rule, 1 suffix rule 0 prefix rules, 2 suffix rules In some cases it is required to be able to distinguish between prefix rule and suffix rule and have finer control over how many times is each applied. Requested feature should allow setting recursion level separately for prefix and suffix rules. Specific example is the Czech dictionary, where it gives best results if suffix rules are applied only once. Hence recursionCap = 0. But if for input token a prefix rule is applied it does not allow to apply suffix rule and produces a token that is not in root form. And setting recursionCap = 1 produces too many irrelevant tokens that it makes Hunspell token filter unuseful. Good solution to this problem would be tell Hunspell token filter to apply up to 1 prefix rule and up to 1 suffix rule only (meaning never allow to apply 0 prefix rules and 2 suffix rules). Generally, this is probably dependant a lot on how particular dictionary and affix rules are constructed and it might not be considered a generalization but rather an expert feature. (There was some relevant discussion going on in LUCENE-5468)</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5485</id>
      <title>hunspell circumfix support</title>
      <description>when the affix file has CIRCUMFIX &lt;flag&gt;, prefixes and suffixes with this flag really represent a single affix around the word, and must agree. in practice i've only seen this in the german dictionary (i unzipped a ton and looked).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5487</id>
      <title>Can we separate "top scorer" from "sub scorer"?</title>
      <description>This is just an exploratory patch ... still many nocommits, but I think it may be promising. I find the two booleans we pass to Weight.scorer confusing, because they really only apply to whoever will call score(Collector) (just IndexSearcher and BooleanScorer). The params are pointless for the vast majority of scorers, because very, very few query scorers really need to change how top-scoring is done, and those scorers can only score top-level (throw throw UOE from nextDoc/advance). It seems like these two types of scorers should be separately typed.</description>
      <attachments/>
      <comments>40</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5489</id>
      <title>Add query rescoring API</title>
      <description>When costly scoring factors are used during searching, a common approach is to do a cheaper / basic query first, collect the top few hundred hits, and then rescore those hits using the more costly query. It's not clear/simple to do this with Lucene today; I think we should make it easier.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5490</id>
      <title>make LengthFilterFactory's min/max args have sensible defaults</title>
      <description>LengthFilterFactory's min/max args are currently required, but it seems like we could give them sensible defaults and make them optional... min = 0 max = IndexWriter.MAX_TERM_LENGTH</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5499</id>
      <title>EarlyTerminatingSortingCollector shouldnt require exact Sort match</title>
      <description>Today EarlyTerminatingSortingCollector requires that the Sort match exactly at query and at index time. However, now that you can use any Sort (e.g. with multiple sortfields), this should be improved. For example, early termination is fine in the following case: index-time: popularity desc, time desc query-time: popularity desc</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5501</id>
      <title>Out-of-order collection testing</title>
      <description>Collectors have the ability to declare whether or not they support out-of-order collection, but since most scorers score in order this is not well tested.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5511</id>
      <title>Upgrade to SvnKit 1.8.4 for checks</title>
      <description>We had a hack since LUCENE-5385 in our build, because svnkit 1.8.0 - 1.8.3 were not able to read svn 1.7 checkouts. Because of this the user had to choose the right svnkit version when executing ant check-svn-working-copy. Since svnkit 1.8.4 we can read all svn working copy formats again, so our checks will work on any checkout without forcefully choosing svnkit version. This patch removes the extra warnings and error messages and update to 1.8.4.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5515</id>
      <title>Improve TopDocs#merge for pagination</title>
      <description>If TopDocs#merge takes from and size into account it can be optimized to create a hits ScoreDoc array equal to size instead of from+size what is now the case.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5516</id>
      <title>Forward information that trigger a merge to MergeScheduler</title>
      <description>Today we pass information about the merge trigger to the merge policy. Yet, no matter if the MP finds a merge or not we call the MergeScheduler who runs &amp; blocks even if we didn't find a merge. In some cases we don't even want this to happen but inside the MergeScheduler we have no choice to opt out since we don't know what triggered the merge. We should forward the infos we have to the MergeScheduler as well.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5518</id>
      <title>minor hunspell optimizations</title>
      <description>After benchmarking indexing speed on SOLR-3245, I ran a profiler and a couple things stood out. There are other things I want to improve too, but these almost double the speed for many dictionaries. Hunspell supports two-stage affix stripping, but the vast majority of dictionaries don't have any affixes that support it. So we just add a boolean (Dictionary.twoStageAffix) that is false until we see one. We use java.util.regex.Pattern for condition checks. This is slow, I switched to o.a.l.automaton and its much faster, and uses slightly less RAM too.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5519</id>
      <title>Make queueDepth enforcing optional in TopNSearcher</title>
      <description>currently TopNSearcher enforces the maxQueueSize based on rejectedCount + topN. I have a usecase where I just simply don't know the exact limit and I am ok with a top N that is not 100% exact. Yet, if I don't specify the right upper limit for the queue size I get an assertion error when I run tests but the only workaround it to make the queue unbounded which looks odd while it would possibly work just fine. I think it's fair to add an option that just doesn't enforce the limit and if it shoudl be enforced we throw a real exception.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5526</id>
      <title>Forced merges</title>
      <description>IndexWriter.forceMerge allows to force merges if the number of segments is higher than a configured limit. I would like to have the ability to also force a merge if there is a single segment in the directory. This is possible today by closing the IndexWriter, reopening with a custom merge policy such as UpgradeIndexMergePolicy, running a forceMerge and finally reopening again with the original merge policy but this is a bit impractical and if possible I would like to be able to do it without reopening the writer twice.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5527</id>
      <title>Make the Collector API work per-segment</title>
      <description>Spin-off of LUCENE-5299. LUCENE-5229 proposes different changes, some of them being controversial, but there is one of them that I really really like that consists in refactoring the Collector API in order to have a different Collector per segment. The idea is, instead of having a single Collector object that needs to be able to take care of all segments, to have a top-level Collector: public interface Collector { AtomicCollector setNextReader(AtomicReaderContext context) throws IOException; } and a per-AtomicReaderContext collector: public interface AtomicCollector { void setScorer(Scorer scorer) throws IOException; void collect(int doc) throws IOException; boolean acceptsDocsOutOfOrder(); } I think it makes the API clearer since it is now obious setScorer and acceptDocsOutOfOrder need to be called after setNextReader which is otherwise unclear. It also makes things more flexible. For example, a collector could much more easily decide to use different strategies on different segments. In particular, it makes the early-termination collector much cleaner since it can return different atomic collectors implementations depending on whether the current segment is sorted or not. Even if we have lots of collectors all over the place, we could make it easier to migrate by having a Collector that would implement both Collector and AtomicCollector, return this in setNextReader and make current concrete Collector implementations extend this class instead of directly extending Collector.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>12</commenters>
    </issue>
    <issue>
      <id>5528</id>
      <title>Add context to AnalyzingInfixSuggester</title>
      <description>Spinoff from LUCENE-5350.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5529</id>
      <title>Spatial: Small optimization searching on indexed non-point shapes</title>
      <description>When searching for indexed non-point shapes (such as polygons), there are redundant cells which can be skipped at the bottom "detail level" of the search. This won't be a problem once LUCENE-4942 is fixed since there then won't be any but it's easy to fix now. This affects all predicates RecursivePrefixTreeStrategy uses except Contains.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5531</id>
      <title>Allow ComplexPhraseQuery to accept fields</title>
      <description>Breaking out a patch created by Tomas Fernandez Lobbe to from LUCENE-1486 so we can track this better.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5534</id>
      <title>GreekStemmer javadocs</title>
      <description>Just an issue for tracking https://github.com/apache/lucene-solr/pull/43.patch</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5539</id>
      <title>Simplify IndexWriter.commitMergedDeletesAndUpdates</title>
      <description>IW.commitMergedDeletes could use some simplification. For example, if we factor out a holder class for mergedDeletesAndUpdates and docMap, we can factor out a lot of the duplicated logic into a single method. I'll attach a patch shortly.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5542</id>
      <title>Explore making DVConsumer sparse-aware</title>
      <description>Today DVConsumer API requires the caller to pass a value for every document, where null means "this doc has no value". The Codec can then choose how to encode the values, i.e. whether it encodes a 0 for a numeric field, or encodes the sparse docs. In practice, from what I see, we choose to encode the 0s. I wonder if we e.g. added an Iterable&lt;Number&gt; to DVConsumer.addXYZField(), if that would make a better API. The caller only passes &lt;doc,value&gt; pairs and it's up to the Codec to decide how it wants to encode the missing values. Like, if a user's app truly has a sparse NDV, IndexWriter doesn't need to "fill the gaps" artificially. It's the job of the Codec. To be clear, I don't propose to change any Codec implementation in this issue (w.r.t. sparse encoding - yes/no), only change the API to reflect that sparseness. I think that if we'll ever want to encode sparse values, it will be a more convenient API. Thoughts? I volunteer to do this work, but want to get others' opinion before I start.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5543</id>
      <title>Remove Directory.fileExists</title>
      <description>Since 3.0.x/3.6.x (see LUCENE-5541), Lucene has substantially removed its reliance on fileExists to the point where I think we can fully remove it now. Like the other iffy IO methods we've removed over time (touchFile, fileModified, seeking back during write, ...), File.exists is dangerous because a low level IO issue can cause it to return false when it should have returned true. The fewer IO operations we rely on the more reliable/portable Lucene is.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5545</id>
      <title>Add ExpressionRescorer</title>
      <description>In LUCENE-5489 we added QueryRescorer, to rescore first-pass hits using scores from a (usually) more expensive second-pass query. I think we should also add ExpressionRescorer, to compute the second pass score using an arbitrary JS expression.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5548</id>
      <title>Improve flexibility and testability of the classification module</title>
      <description>Lucene classification module's flexibility and capabilities may be improved with the following: make it possible to use them "online" (or provide an online version of them) so that if the underlying index(reader) is updated the classifier doesn't need to be trained again to take into account newly added docs eventually pass a different Analyzer together with the text to be classified (or directly a TokenStream) to specify custom tokenization/filtering. normalize score calculations of existing classifiers provide publicly available dataset based accuracy and speed tests more Lucene based classification algorithms Specific subtasks for each of the above topics should be created to discuss each of them in depth.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5551</id>
      <title>Fix warnings in the ant build</title>
      <description>The build produces a lot of warnings. LUCENE-5130 aims to fail the build on certain warnings, but until we actually eliminate those warnings from the code, failing the build is premature. This issue is concerned with fixing or suppressing warnings reported by the ant build, anything further than that may need one or more separate issues.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5552</id>
      <title>Upgrade JFlex version from 1.5.0 to 1.5.1</title>
      <description>JFlex 1.5.1, just released, fixes a 1.5.0 bug in the JFlex version in generated code: it included the "-SNAPSHOT" suffix.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5554</id>
      <title>Add TermBulkScorer</title>
      <description>Hotspot was unhappy with the changes in LUCENE-5487, e.g.: http://people.apache.org/~mikemccand/lucenebench/OrHighHigh.html But it looks like we can get the performance back by making a dedicated BulkScorer for TermQuery.</description>
      <attachments/>
      <comments>23</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5556</id>
      <title>buildAndPushRelease.py: scp to people.apache.org stalls during transfer</title>
      <description>When I first tried to build the first RC for 4.7.1 using dev-tools/scripts/buildAndPushRelease.py, after preparing both the Lucene and Solr releases, the scp command to transfer the Lucene release tarball to people.apache.org stalled after about 180MB had transferred out of 220MB. After an hour or so, I kill -9'd the scp process and started over. Thankfully, everything went smoothly the second go-around. But I'd like to see if we can reduce the likelihood of this happening again. I did some searching for causes/solutions to this problem and came across this recommendation to use scp's -l option to limit bandwidth in order to prevent stalls: http://www.aixmind.com/?p=1371 - the recommendation there limits bandwidth to 1MB/sec (-l 8192 is 8192Kbit/sec = 8Mbit/sec = 1MB/sec).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5558</id>
      <title>Add TruncateTokenFilter</title>
      <description>I am using this filter as a stemmer for Turkish language. In many academic research (classification, retrieval) it is used and called as Fixed Prefix Stemmer or Simple Truncation Method or F5 in short. Among F3 TO F7, F5 stemmer (length=5) is found to work well for Turkish language in Information Retrieval on Turkish Texts. It is the same work where most of stopwords_tr.txt are acquired. ElasticSearch has truncate filter but it does not respect keyword attribute. And it has a use case similar to TruncateFieldUpdateProcessorFactory Main advantage of F5 stemming is : it does not effected by the meaning loss caused by ascii folding. It is a diacritics-insensitive stemmer and works well with ascii folding. Effects of diacritics on Turkish information retrieval Here is the full field type I use for "diacritics-insensitive search" for Turkish &lt;fieldType name="text_tr_ascii_f5" class="solr.TextField" positionIncrementGap="100"&gt; &lt;analyzer&gt; &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt; &lt;filter class="solr.ApostropheFilterFactory"/&gt; &lt;filter class="solr.TurkishLowerCaseFilterFactory"/&gt; &lt;filter class="solr.ASCIIFoldingFilterFactory"/&gt; &lt;filter class="solr.KeywordRepeatFilterFactory"/&gt; &lt;filter class="solr.TruncateTokenFilterFactory" prefixLength="5"/&gt; &lt;filter class="solr.RemoveDuplicatesTokenFilterFactory"/&gt; &lt;/analyzer&gt; I would like to get community opinions : 1) Any interest in this? 2) keyword attribute should be respected? 3) package name analysis.misc versus analyis.tr 4) name of the class TruncateTokenFilter versus FixedPrefixStemFilter</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5559</id>
      <title>Argument validation for TokenFilters having numeric constructor parameter(s)</title>
      <description>Some TokenFilters have numeric arguments in their constructors. They should throw IllegalArgumentException for negative or meaningless values. Here is some examples that demonstrates invalid/meaningless arguments : &lt;filter class="solr.LimitTokenCountFilterFactory" maxTokenCount="-10" /&gt; &lt;filter class="solr.LengthFilterFactory" min="-5" max="-1" /&gt; &lt;filter class="solr.LimitTokenPositionFilterFactory" maxTokenPosition="-3" /&gt;</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5565</id>
      <title>Remove String based encoding from SpatialPrefixTree/Cell API; just use bytes</title>
      <description>The SpatialPrefixTree/Cell API supports bytes and String encoding/decoding dually. I want to remove the String side to keep the API simpler. Included in this issue, I'd like to make some small refactorings to reduce assumptions the filters make of the underlying encoding such that future encodings can work a in more different ways with less impact on the filters. String encode/decode will exist for the Geohash one for now since GeohashUtils works off of Strings, but Quad could change more easily.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5569</id>
      <title>Rename AtomicReader to LeafReader</title>
      <description>See LUCENE-5527 for more context: several of us seem to prefer Leaf to Atomic. Talking from my experience, I was a bit confused in the beginning that this thing is named AtomicReader, since Atomic is otherwise used in Java in the context of concurrency. So maybe renaming it to Leaf would help remove this confusion and also carry the information that these readers are used as leaves of top-level readers?</description>
      <attachments/>
      <comments>26</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>5579</id>
      <title>Spatial, enhance RPT to differentiate confirmed from non-confirmed hits, then validate with SDV</title>
      <description>If a cell is within the query shape (doesn't straddle the edge), then you can be sure that all documents it matches are a confirmed hit. But if some documents are only on the edge cells, then those documents could be validated against SerializedDVStrategy for precise spatial search. This should be much faster than using RPT and SerializedDVStrategy independently on the same search, particularly when a lot of documents match. Perhaps this'll be a new RPT subclass, or maybe an optional configuration of RPT. This issue is just for the Intersects predicate, which will apply to Disjoint. Until resolved in other issues, the other predicates can be handled in a naive/slow way by creating a filter that combines RPT's filter and SerializedDVStrategy's filter using BitsFilteredDocIdSet. One thing I'm not sure of is how to expose to Lucene-spatial users the underlying functionality such that they can put other query/filters in-between RPT and the SerializedDVStrategy. Maybe that'll be done by simply ensuring the predicate filters have this capability and are public. It would be ideal to implement this capability after the PrefixTree term encoding is modified to differentiate edge leaf-cells from non-edge leaf cells. This distinction will allow the code here to make more confirmed matches.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5580</id>
      <title>Always verify stored fields' checksum on merge</title>
      <description>I have seen a couple of index corruptions over the last months, and most of them happened on stored fields. The explanation might just be that since stored fields are usually most of the index size, they are just more likely to be corrupted due to a hardware/operating-system failure, but it might be as well a sneaky bug on our side. Lucene recently added checksums to index files, and you can enable integrity verification upon merge, but this comes with a cost since you need to read all index files twice instead of once. If you are merging a very large segment and your merges are I/O-bound, this might be noticeable. I would like to implement integrity checks for stored fields on merges on the fly, so that the stored fields files need to be read only once.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5582</id>
      <title>Remove IndexOutput.length and .setLength</title>
      <description>Since we removed seeking from IndexOutput, you can just use .getFilePointer() to get the length. Also, nothing uses .setLength, so I think we should remove it too.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5584</id>
      <title>Allow FST read method to also recycle the output value when traversing FST</title>
      <description>The FST class heavily reuses Arc instances when traversing the FST. The output of an Arc however is not reused. This can especially be important when traversing large portions of a FST and using the ByteSequenceOutputs and CharSequenceOutputs. Those classes create a new byte[] or char[] for every node read (which has an output). In our use case we intersect a lucene Automaton with a FST&lt;BytesRef&gt; much like it is done in org.apache.lucene.search.suggest.analyzing.FSTUtil.intersectPrefixPaths() and since the Automaton and the FST are both rather large tens or even hundreds of thousands of temporary byte array objects are created. One possible solution to the problem would be to change the org.apache.lucene.util.fst.Outputs class to have two additional methods (if you don't want to change the existing methods for compatibility): /** Decode an output value previously written with {@link * #write(Object, DataOutput)} reusing the object passed in if possible */ public abstract T read(DataInput in, T reuse) throws IOException; /** Decode an output value previously written with {@link * #writeFinalOutput(Object, DataOutput)}. By default this * just calls {@link #read(DataInput)}. This tries to reuse the object * passed in if possible */ public T readFinalOutput(DataInput in, T reuse) throws IOException { return read(in, reuse); } The new methods could then be used in the FST in the readNextRealArc() method passing in the output of the reused Arc. For most inputs they could even just invoke the original read(in) method. If you should decide to make that change I'd be happy to supply a patch and/or tests for the feature.</description>
      <attachments/>
      <comments>27</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5588</id>
      <title>We should also fsync the directory when committing</title>
      <description>Since we are on Java 7 now and we already fixed FSDir.sync to use FileChannel (LUCENE-5570), we can also fsync the directory (at least try to do it). Unlike RandomAccessFile, which must be a regular file, FileChannel.open() can also open a directory: http://stackoverflow.com/questions/7694307/using-filechannel-to-fsync-a-directory-with-nio-2</description>
      <attachments/>
      <comments>20</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5591</id>
      <title>ReaderAndUpdates should create a proper IOContext when writing DV updates</title>
      <description>Today we pass IOContext.DEFAULT. If DV updates are used in conjunction w/ NRTCachingDirectory, it means the latter will attempt to write the entire DV field in its RAMDirectory, which could lead to OOM. Would be good if we can build our own FlushInfo, estimating the number of bytes we're about to write. I didn't see off hand a quick way to guesstimate that - I thought to use the current DV's sizeInBytes as an approximation, but I don't see a way to get it, not a direct way at least. Maybe we can use the size of the in-memory updates to guesstimate that amount? Something like sizeOfInMemUpdates * (maxDoc/numUpdatedDocs)? Is it a too wild guess?</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5596</id>
      <title>Support for index/search large numeric field</title>
      <description>Currently if an number is larger than Long.MAX_VALUE, we can't index/search that in lucene as a number. For example, IPv6 address is an 128 bit number, so we can't index that as a numeric field and do numeric range query etc. It would be good to support BigInteger / BigDecimal I've tried use BigInteger for IPv6 in Elasticsearch and that works fine, but there are still lots of things to do https://github.com/elasticsearch/elasticsearch/pull/5758</description>
      <attachments/>
      <comments>26</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5604</id>
      <title>Should we switch BytesRefHash to MurmurHash3?</title>
      <description>MurmurHash3 has better hashing distribution than the current hash function we use for BytesRefHash which is a simple multiplicative function with 31 multiplier (same as Java's String.hashCode, but applied to bytes not chars). Maybe we should switch ...</description>
      <attachments/>
      <comments>35</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>5606</id>
      <title>Add @Monster annotation and run these tests periodically</title>
      <description>We have some awesome very heavy tests, Test2B*, but they are @Ignored and we never run them. I think we should add a new @Monster annotation and set up a Jenkins job with huge test timeout, big Java heap, etc.?</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5608</id>
      <title>SpatialPrefixTree API refactor</title>
      <description>This is a refactor of the SpatialPrefixTree spatial API, in preparation for more SPT implementations on the near horizon. These are fairly internal APIs; SpatialExample.java didn't have to change, nor the Solr adapters, and I doubt ES would have to either. API changes: SpatialPrefixTree &amp; Cell had a fairly significant make-over. The existing implementations for Geohash &amp; Quad have been made to subclass LegacyPrefixTree &amp; LegacyCell shim's, and otherwise had very few changes (performance should be the same). Cell is now an interface. New CellIterator which is an Iterator&lt;Cell&gt;. Includes 3 implementations. PrefixTreeStrategy.simplifyIndexedCells was renamed to pruneLeafyBranches and moved to RPT and made toggle'able with a setter. It's going to be removed in the future but for the time being it remains a useful optimization. RPT's pointsOnly &amp; multiOverlappingIndexedShapes options now have setters. Future: The AbstractVisitingPrefixTreeFilter (used by RPT's Intersects, Within, Disjoint) really should be refactored to use the new CellIterator API as it will reduce the amount of code and should make the code easier to follow since it would be based on a well-knon design-pattern (an iterator). I wish I had done this as a series of commits on a GitHub branch; ah well.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5609</id>
      <title>Should we revisit the default numeric precision step?</title>
      <description>Right now it's 4, for both 8 (long/double) and 4 byte (int/float) numeric fields, but this is a pretty big hit on indexing speed and disk usage, especially for tiny documents, because it creates many (8 or 16) terms for each value. Since we originally set these defaults, a lot has changed... e.g. we now rewrite MTQs per-segment, we have a faster (BlockTree) terms dict, a faster postings format, etc. Index size is important because it limits how much of the index will be hot (fit in the OS's IO cache). And more apps are using Lucene for tiny docs where the overhead of individual fields is sizable. I used the Geonames corpus to run a simple benchmark (all sources are committed to luceneutil). It has 8.6 M tiny docs, each with 23 fields, with these numeric fields: lat/lng (double) modified time, elevation, population (long) dem (int) I tested 4, 8 and 16 precision steps: indexing: PrecStep Size IndexTime 4 1812.7 MB 651.4 sec 8 1203.0 MB 443.2 sec 16 894.3 MB 361.6 sec searching: Field PrecStep QueryTime TermCount geoNameID 4 2872.5 ms 20306 geoNameID 8 2903.3 ms 104856 geoNameID 16 3371.9 ms 5871427 latitude 4 2160.1 ms 36805 latitude 8 2249.0 ms 240655 latitude 16 2725.9 ms 4649273 modified 4 2038.3 ms 13311 modified 8 2029.6 ms 58344 modified 16 2060.5 ms 77763 longitude 4 3468.5 ms 33818 longitude 8 3629.9 ms 214863 longitude 16 4060.9 ms 4532032 Index time is with 1 thread (for identical index structure). The query time is time to run 100 random ranges for that field, averaged over 20 iterations. TermCount is the total number of terms the MTQ rewrote to across all 100 queries / segments, and it gets higher as expected as precStep gets higher, but the search time is not that heavily impacted ... negligible going from 4 to 8, and then some impact from 8 to 16. Maybe we should increase the int/float default precision step to 8 and long/double to 16? Or both to 16?</description>
      <attachments/>
      <comments>22</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5610</id>
      <title>Add Terms min/max</title>
      <description>Having upper/lower bounds on terms could be useful for various optimizations in the future, e.g. to accelerate sorting (if a segment can't compete, don't even search it), and so on. Its pretty obvious how to get the smallest term, but the maximum term for a field is tricky, but worst case you can do it in ~ log(N) time by binary searching term space.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5611</id>
      <title>Simplify the default indexing chain</title>
      <description>I think Lucene's current indexing chain has too many classes / hierarchy / abstractions, making it look much more complex than it really should be, and discouraging users from experimenting/innovating with their own indexing chains. Also, if it were easier to understand/approach, then new developers would more likely try to improve it ... it really should be simpler. So I'm exploring a pared back indexing chain, and have a starting patch that I think is looking ok: it seems more approachable than the current indexing chain, or at least has fewer strange classes. I also thought this could give some speedup for tiny documents (a more common use of Lucene lately), and it looks like, with the evil optimizations, this is a ~25% speedup for Geonames docs. Even without those evil optos it's a bit faster. This is very much a work in progress / nocommits, and there are some behavior changes e.g. the new chain requires all fields to have the same TV options (rather than auto-upgrading all fields by the same name that the current chain does)...</description>
      <attachments/>
      <comments>27</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5620</id>
      <title>LowerCaseFilter.preserveOriginal</title>
      <description>Following closely the model of LUCENE-5437 (which worked on ASCIIFoldingFilter), this patch adds the ability to preserve the original token to LowerCaseFilter. This is useful if you want an all-lowercase search term to match without regard to case, while search terms with uppercase letters match in a case-sensitive manner.</description>
      <attachments/>
      <comments>27</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5621</id>
      <title>remove IndexOutput.flush()</title>
      <description>This method is extraneous: it just makes the API confusing. Its not actually used anywhere by Lucene, so it shouldn't be mandatory on IndexOutput. Maybe it had some use-case before things were append-only, i dont know, but now its time for it to go.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5625</id>
      <title>Update demo documentation with prerequisites and ant target</title>
      <description>It would be great if the Lucene demo mentioned the fact that Apache Ant and Ivy need to be installed and configured to build the demo JARs. This could be listed in a prerequisites section. It might also be a good idea to tell a first time user what ant target to use for building the JARs necessary for the demo.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5627</id>
      <title>Positional joins</title>
      <description>Prototype of analysis and search for labeled fragments</description>
      <attachments/>
      <comments>27</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5629</id>
      <title>Comparing the Version of Lucene , the Analyzer and the similarity function that are being used for indexing and searching.</title>
      <description>We have observed that Lucene does not check if the same Similarity function is used during indexing and searching. The same problem exists for the Analyzer that is used. This may lead to poor or misleading results. So we decided to create an xml file during indexing that will store information such as the Analyzer and the Similarity function that were used as well as the version of Lucene that was used. This xml file will always be available to the users. At search time , we will retrieve this information using SAX parsing and check if the utils used for searching , match those used for indexing. If not , a warning message will be displayed to the user.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5631</id>
      <title>Improve access to archived versions of Lucene and Solr</title>
      <description>When visiting the website to download Lucene or Solr, it is very difficult for people to locate where to download previous versions. The archive link does show up when you click the download link, but the page where it lives is replaced in less than a second by the CGI for picking a download mirror for the current release. There's nothing there for previous versions. At a minimum, we need a link to the download archive that's right below the main Download link. Something else I think we should do (which might actually be an INFRA issue, as this problem exists for other projects too) would be to have the "closer.cgi" page include a link to the archives.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5632</id>
      <title>transition Version constants from LUCENE_MN to LUCENE_M_N</title>
      <description>We should fix this, otherwise the constants will be hard to read (e.g. Version.LUCENE_410, is it 4.1.0 or 4.10 or whatever). I do not want this to be an excuse for an arbitrary 5.0 release that does not have the features expected of a major release</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5633</id>
      <title>NoMergePolicy should have one singleton - NoMergePolicy.INSTANCE</title>
      <description>Currently there are two singletons available - MergePolicy.NO_COMPOUND_FILES and MergePolicy.COMPOUND_FILES and it's confusing to distinguish on compound files when the merge policy never merges segments. We should have one singleton - NoMergePolicy.INSTANCE Post to the relevant discussion - http://mail-archives.apache.org/mod_mbox/lucene-java-user/201404.mbox/%3CCAOdYfZXXyVSf9%2BxYaRhr5v2O4Mc6S2v-qWuT112_CJFYhWTPqw%40mail.gmail.com%3E</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5634</id>
      <title>Reuse TokenStream instances in Field</title>
      <description>If you don't reuse your Doc/Field instances (which is very expert: I suspect few apps do) then there's a lot of garbage created to index each StringField because we make a new StringTokenStream or NumericTokenStream (and their Attributes). We should be able to re-use these instances via a static ThreadLocal...</description>
      <attachments/>
      <comments>20</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5637</id>
      <title>Scaling scale function</title>
      <description>The existing scale() function examines the scores of all documents in the index in order to calculate its scale constant. This does not perform well in solr on very large indexes or with costly scoring mechanisms such as geo distance. I have developed a patch that allows the scale function to only score documents that match the given filters, thus improving performance of the scale function. For test queries involving two scale operations where one was scaling the result of keyword scoring and the other was scaling the result of geo distance scoring on an index with ~2 million documents, query time was improved from ~400 ms with vanilla scale to ~190 ms with new scale. A similar query using no scaling ran in ~90 ms. (Each enhanced scale function added to the query appeared to add about 50 ms of processing) e.g. scaled query - q = scale(keywords, 0, 90) and scale(geo, 0, 10) e.g. unscaled query - q = keywords and geo In both cases fq includes keywords and geo. In order to accomplish this goal I had to introduce a couple of changes: 1) In the indexsearcher.search method where scorers are created and then used to score on a per-atomicreadercontext basis I had to make it so that all scorers would be created before any scoring was done. This was so that the scale function would have an opportunity to observe the entire index before being asked to score something. 2) Introduced a new property to the Bits interface that indicates whether or not the bits provide constant-time access. Why? Read on. 3) FilterSet used to return Null when asked for its bits because it did not have any, it had an iterator. This was an issue when trying to make it so that scale would only score documents matching the filter. Thus a new bits implementation was added (LazyIteratorBackedBits) that could expose an iterator as a Bits implementation. It advances the iterator on-demand when asked about a document and uses an OpenBitSet to keep track of what it has advanced beyond. Thus once the iterator is exhausted it provides constant-time answers like any other Bits. 4) Introduced a function on the ValueSource interface to allow a Bits to be passed in for filtering purposes. This was originally developed against Solr 4.2 but I have ported it to Solr 4.8. There is one failing unit test related to code that has been added in the interim, AnalyzingInfixSuggesterTest.testRandomNRT. I have not been able to figure out why this test fails. All other tests pass. In relation to implementation detail 1) above, the introduction of LeafCollectors in trunk has caused somewhat of an issue. ( LUCENE-5527 ) It seems to no longer be possible to create multiple scorers without immediately scoring on that LeafCollector. This may be related to the encapsulation of the Collector.setNextReader() method which was very useful for this purpose.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5648</id>
      <title>Index/search multi-valued time durations</title>
      <description>If you need to index a date/time duration, then the way to do that is to have a pair of date fields; one for the start and one for the end – pretty straight-forward. But if you need to index a variable number of durations per document, then the options aren't pretty, ranging from denormalization, to joins, to using Lucene spatial with 2D as described here. Ideally it would be easier to index durations, and work in a more optimal way. This issue implements the aforementioned feature using Lucene-spatial with a new single-dimensional SpatialPrefixTree implementation. Unlike the other two SPT implementations, it's not based on floating point numbers. It will have a Date based customization that indexes levels at meaningful quantities like seconds, minutes, hours, etc. The point of that alignment is to make it faster to query across meaningful ranges (i.e. [2000 TO 2014]) and to enable a follow-on issue to facet on the data in a really fast way. I'll expect to have a working patch up this week.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5650</id>
      <title>Enforce read-only access to any path outside the temporary folder via security manager</title>
      <description>The recent refactoring to all the create temp file/dir functions (which is great!) has a minor regression from what existed before. With the old LuceneTestCase.TEMP_DIR, the directory was created if it did not exist. So, if you set java.io.tmpdir to "./temp", then it would create that dir within the per jvm working dir. However, getBaseTempDirForClass() now does asserts that check the dir exists, is a dir, and is writeable. Lucene uses "." as java.io.tmpdir. Then in the test security manager, the per jvm cwd has read/write/execute permissions. However, this allows tests to write to their cwd, which I'm trying to protect against (by setting cwd to read/execute in my test security manager).</description>
      <attachments/>
      <comments>55</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>5664</id>
      <title>New meaning of equal sign in StandardQueryParser</title>
      <description>The StandardSyntaxParser.jj has (undocumented?) support for the &lt;, &lt;=, &gt; and =&gt; operators that generate a TermRangeQueryNode. The equal operator, however, behaves just like the colon and produces a regular Term node instead of a TermRangeQueryNode. I've been using the attached patch in a project where we had to be able to query the exact value of a field and I'm hoping there is interest to apply it upstream. (Note that the colon operator works just as before, producing TermQuery or PhraseQuery nodes.)</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5665</id>
      <title>Remove/deprecate SerialMergeScheduler</title>
      <description>I don't think there's any good case for apps to be using this, and we struggle to modernize it (many patches/iterations with no convergence on LUCENE-5310). I think we should remove it. Apps can use CMS(maxThreadCount=1, maxMergeCount=1) if they must, or they can easily pull SMS's sources into their app if they really need it.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5666</id>
      <title>Add UninvertingReader</title>
      <description>Currently the fieldcache is not pluggable at all. It would be better if everything used the docvalues apis. This would allow people to customize the implementation, extend the classes with custom subclasses with additional stuff, etc etc. FieldCache can be accessed via the docvalues apis, using the FilterReader api.</description>
      <attachments/>
      <comments>47</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>5667</id>
      <title>Optimize common-prefix across all terms in a field</title>
      <description>I tested different UUID sources in Lucene http://blog.mikemccandless.com/2014/05/choosing-fast-unique-identifier-uuid.html and I was surprised to see that Flake IDs were slower than UUID V1. They use the same raw sources of info (timestamp, node id, sequence counter) but Flake ID preserves total order by keeping the timestamp "intact" in the leading 64 bits. I think the reason might be because a Flake ID will typically have a longish common prefix for all docs, and I think we might be able to optimize this in block-tree by storing that common prefix outside of the FST, or maybe just pre-computing the common prefix on init and storing the "effective" start node for the FST.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5669</id>
      <title>Make it easier to index a full binary term</title>
      <description>When I ran performance tests for different UUID fields for http://blog.mikemccandless.com/2014/05/choosing-fast-unique-identifier-uuid.html I found that full binary term, i.e. base / radix 256 using all 256 values of each byte, was typically faster than the "default" base 16/36/etc. encoding for UUIDs. But in Lucene today it's a hassle to index a binary term (I had to go poach BinaryTokenStream from tests). I think we should make this easier so apps are encouraged to use it for their ID-like fields. Maybe we add an indexable option to BinaryField?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5670</id>
      <title>org.apache.lucene.util.fst.FST should skip over outputs it is not interested in</title>
      <description>Currently the FST uses the read(DataInput) method from the Outputs class to skip over outputs it actually is not interested in. For most use cases this just creates some additional objects that are immediately destroyed again. When traversing an FST with non-trivial data however this can easily add up to several excess objects that nobody actually ever read.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5674</id>
      <title>A new token filter: SubSequence</title>
      <description>A new configurable token filter which, given a token breaks it into sub-parts and outputs consecutive sub-sequences of those sub-parts. Useful for, for example, using during indexing to generate variations on domain names, so that "www.google.com" can be found by searching for "google.com", or "www.google.com". Parameters: sepRegexp: A regular expression used split incoming tokens into sub-parts. glue: A string used to concatenate sub-parts together when creating sub-sequences. minLen: Minimum length (in sub-parts) of output sub-sequences maxLen: Maximum length (in sub-parts) of output sub-sequences (0 for unlimited; negative numbers for token length in sub-parts minus specified length) anchor: Anchor.START to output only prefixes, or Anchor.END to output only suffixes, or Anchor.NONE to output any sub-sequence withOriginal: whether to output also the original token EDIT: now includes tests for filter and for factory.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5675</id>
      <title>"ID postings format"</title>
      <description>Today the primary key lookup in lucene is not that great for systems like solr and elasticsearch that have versioning in front of IndexWriter. To some extend BlockTree can "sometimes" help avoid seeks by telling you the term does not exist for a segment. But this technique (based on FST prefix) is fragile. The only other choice today is bloom filters, which use up huge amounts of memory. I don't think we are using everything we know: particularly the version semantics. Instead, if the FST for the terms index used an algebra that represents the max version for any subtree, we might be able to answer that there is no term T with version &lt; V in that segment very efficiently. Also ID fields dont need postings lists, they dont need stats like docfreq/totaltermfreq, etc this stuff is all implicit. As far as API, i think for users to provide "IDs with versions" to such a PF, a start would to set a payload or whatever on the term field to get it thru indexwriter to the codec. And a "consumer" of the codec can just cast the Terms to a subclass that exposes the FST to do this version check efficiently.</description>
      <attachments/>
      <comments>40</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5679</id>
      <title>Consolidate IndexWriter.deleteDocuments()</title>
      <description>Spinoff from here: http://markmail.org/message/7kjlaizqdh7kst4d. We should consolidate the various IW.deleteDocuments().</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5680</id>
      <title>Allow updating multiple DocValues fields atomically</title>
      <description>This has come up on the list (http://markmail.org/message/2wmpvksuwc5t57pg) – it would be good if we can allow updating several doc-values fields, atomically. It will also improve/simplify our tests, where today we index two fields, e.g. the field itself and a control field. In some multi-threaded tests, since we cannot be sure which updates came through first, we limit the test such that each thread updates a different set of fields, otherwise they will collide and it will be hard to verify the index in the end. I was working on a patch and it looks pretty simple to do, will post a patch shortly.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5683</id>
      <title>Improve SegmentReader.getXXXDocValues</title>
      <description>Today we do two hash lookups, where in most cases a single one is enough. E.g. SR.getNumericDocValues initializes the FieldInfo (first lookup in FieldInfos), however if that field was already initialized, we can simply check dvFields.get(). This can be improved in all getXXXDocValues as well as getDocsWithField.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5684</id>
      <title>Add best effort detection when index is removed &amp; recreated before openIfChanged</title>
      <description>I dug into the confusing NPE from the java-user email with subject "[lucene 4.6] NPE when calling IndexReader#openIfChanged". It happens because the app was opening an IndexReader, then rm -rf the entire index, build a new one, then calling reopen. This is invalid usage (the app should instead use IW.deleteAll, or open new IW with OpenMode.CREATE), but I'd like to add a minor best effort check...</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5685</id>
      <title>Add file:// support to crawl.maven.release.dist.sh</title>
      <description>During the release process i always zip up and download the entire voted on RC folder locally, so I can commit the release artifacts. This is just the simplest way to avoid mistakes. Maven publishing is a mystery to me, I just follow the instructions exactly because I'm not totally sure what the directory structure should be that the scripts expect. Currently this means i have to do a large file transfer over the internet again, because the crawl script wont work with a file:// url (the unzipped contents of the release folder i just downloaded). It would be great if it could just use 'cp -r' or something for that, rather than wget, to save another large transfer.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5686</id>
      <title>Fail smoketester if there is LUCENE_XXXX or SOLR_XXXX in Changes.txt</title>
      <description>One of these slipped into the 4.8.1 release in the solr changes. Looking at trunk/4.9 in solr/CHANGES.txt, there are quite a few SOLR_XXXX issues. When the underscore is used, no link to the issue is generated.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5688</id>
      <title>NumericDocValues fields with sparse data can be compressed better</title>
      <description>I ran into this problem where I had a dynamic field in Solr and indexed data into lots of fields. For each field only a few documents had actual values and the remaining documents the default value ( 0 ) got indexed. Now when I merge segments, the index size jumps up. For example I have 10 segments - Each with 1 DV field. When I merge segments into 1 that segment will contain all 10 DV fields with lots if 0s. This was the motivation behind trying to come up with a compression for a use case like this.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5690</id>
      <title>expose sub-Terms from MultiTerms</title>
      <description>MultiTermsEnum and MultiDocsEnum both expose their subs. It would be useful to do the same for MultiTerms.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5693</id>
      <title>don't write deleted documents on flush</title>
      <description>When we flush a new segment, sometimes some documents are "born deleted", e.g. if the app did a IW.deleteDocuments that matched some not-yet-flushed documents. We already compute the liveDocs on flush, but then we continue (wastefully) to send those known-deleted documents to all Codec parts. I started to implement this on LUCENE-5675 but it was too controversial. Also, I expect typically the number of deleted docs is 0, or small, so not writing "born deleted" docs won't be much of a win for most apps. Still it seems silly to write them, consuming IO/CPU in the process, only to consume more IO/CPU later for merging to re-delete them.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5695</id>
      <title>Add DocIdSet.ramBytesUsed</title>
      <description>LUCENE-5463 tried to remove calls to RamUsageEstimator.sizeOf(Object) yet it was not always possible to remove the call when there was no other API to compute the memory usage of a particular class. In particular, this is the case for CachingWrapperFilter.sizeInBytes() that needs to be able to get the memory usage of any cacheable DocIdSet instance. We could add DocIdSet.ramBytesUsed in order to remove the need for RamUsageEstimator. This will also help have bounded filter caches and take the size of the cached doc id sets into account when doing evictions.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5701</id>
      <title>Move core closed listeners to AtomicReader</title>
      <description>Core listeners are very helpful when managing per-segment caches (filters, uninverted doc values, etc.) yet this API is only exposed on SegmentReader. If you want to use it today, you need to do instanceof checks, try to unwrap in case of a FilterAtomicReader and finally fall back to a reader closed listener if every other attempt to get the underlying SegmentReader failed.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5702</id>
      <title>Per-segment comparator API</title>
      <description>As a next step of LUCENE-5527, it would be nice to have per-segment comparators, and maybe even change the default behavior of our top* comparators so that they merge top hits in the end.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5703</id>
      <title>Don't allocate/copy bytes all the time in binary DV producers</title>
      <description>Our binary doc values producers keep on creating new byte[] arrays and copying bytes when a value is requested, which likely doesn't help performance. This has been done because of the way fieldcache consumers used the API, but we should try to fix it in 5.0.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5706</id>
      <title>Don't allow unsetting values through DocValues updates</title>
      <description>Spinoff from LUCENE-5680: we shouldn't allow unsetting values through DocValues updates, at least not until there's a real usecase for it. This will simplify a lot of the internal code, as well make the numeric update API work w/ primitive long instead of Long.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5709</id>
      <title>NorwegianPhoneticFilter</title>
      <description>There has been a steady demand for a Norwegian phonetic normalization filter.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5710</id>
      <title>DefaultIndexingChain swallows useful information from MaxBytesLengthExceededException</title>
      <description>In DefaultIndexingChain, when a MaxBytesLengthExceededException is caught, the original message is discarded, however, the message contains useful information like the size that exceeded the limit. Lucene should make this information included in the newly thrown IllegalArgumentException.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5711</id>
      <title>Pass IW to MergePolicy</title>
      <description>Related to LUCENE-5708 we keep state in the MP holding on to the IW which prevents sharing the MP across index writers. Aside of this we should really not keep state in the MP it should really only select merges without being bound to the index writer.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5712</id>
      <title>Remove Similarity.queryNorm</title>
      <description>This method is a no-op for ranking within one query, causes confusion for users making their own Similarity impls, and isn't necessary for / makes it harder to switch the default to more modern scoring models like BM25.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5714</id>
      <title>Improve tests for BBoxStrategy then port to 4x.</title>
      <description>BBoxStrategy needs better tests before I'm comfortable seeing it in 4x. Specifically it should use random rectangles based validation (ones that may cross the dateline), akin to the other tests. And I think I see an equals/hashcode bug to be fixed in there too. One particular thing I'd like to see added is how to handle a zero-area case for AreaSimilarity. I think an additional feature in which you declare a minimum % area (relative to the query shape) would be good. It should be possible for the user to combine rectangle center-point to query shape center-point distance sorting as well. I think it is but I need to make sure it's possible without having to index a separate center point field. Another possibility (probably not to be addressed here) is a minimum ratio between width/height, perhaps 10%. A long but nearly no height line should not be massively disadvantaged relevancy-wise to an equivalently long diagonal road that has a square bbox.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5717</id>
      <title>Postings highlighter support for multi term queries within filtered and constant score queries</title>
      <description>The automata extraction that is done to make multi term queries work with the postings highlighter does support boolean queries but it should also support other compound queries like filtered and constant score.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5718</id>
      <title>More flexible compound queries (containing mtq) support in postings highlighter</title>
      <description>The postings highlighter currently pulls the automata from multi term queries and doesn't require calling rewrite to make highlighting work. In order to do so it also needs to check whether the query is a compound one and eventually extract its subqueries. This is currently done in the MultiTermHighlighting class and works well but has two potential problems: 1) not all the possible compound queries are necessarily supported as we need to go over each of them one by one (see LUCENE-5717) and this requires keeping the "switch" up-to-date if new queries gets added to lucene 2) it doesn't support custom compound queries but only the set of queries available out-of-the-box I've been thinking about how this can be improved and one of the ideas I came up with is to introduce a generic way to retrieve the subqueries from compound queries, like for instance have a new abstract base class with a getLeaves or getSubQueries method and have all the compound queries extend it. What this method would do is return a flat array of all the leaf queries that the compound query is made of. Not sure whether this would be needed in other places in lucene, but it doesn't seem like a small change and it would definitely affect (or benefit?) more than just the postings highlighter support for multi term queries. In particular the second problem (custom queries) seems hard to solve without a way to expose this info directly from the query though, unless we want to make the MultiTermHighlighting#extractAutomata method extensible in some way. Would like to hear what people think and work on this as soon as we identified which direction we want to take.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5720</id>
      <title>Optimize on disk packed integers part 2</title>
      <description>These are heavily optimized for the in-RAM case (for example FieldCache uses PackedInts.FAST to make it even faster so), but for the docvalues case they are not: we always essentially use COMPACT, we have only one decoder that must solve all the cases, even the complicated ones, we use BlockPackedWriter for all integers (even if they are ordinals), etc.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5721</id>
      <title>Monotonic packed could maybe be faster</title>
      <description>This compression is used in lucene for monotonically increasing offsets, e.g. stored fields index, dv BINARY/SORTED_SET offsets, OrdinalMap (used for merging and faceting dv) and so on. Today this stores a +/- deviation from an expected line of y=mx + b, where b is the minValue for the block and m is the average delta from the previous value. Because it can be negative, we have to do some additional work to zigzag-decode. Can we just instead waste a bit for every value explicitly (lower the minValue by the min delta) so that deltas are always positive and we can have a simpler decode? Maybe If we do this, the new guy should assert that values are actually monotic at write-time. The current one supports "mostly monotic" but do we really need that flexibility anywhere? If so it could always be kept...</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5723</id>
      <title>Performance improvements for FastCharStream</title>
      <description>Hello from the .NET land, A user of ours has identified an optimization opportunity, although minor I think it points to a valid point - we should avoid using exceptions from controlling flow when possible. Here's the original ticket + commits to our codebase. If this looks valid to you too I can go ahead and prepare a PR. https://issues.apache.org/jira/browse/LUCENENET-541 https://github.com/apache/lucene.net/commit/ac8c9fa809110ddb180bf7b2ce93e86270b39ff6 https://git-wip-us.apache.org/repos/asf?p=lucenenet.git;a=blobdiff;f=src/core/QueryParser/QueryParserTokenManager.cs;h=ec09c8e451f7a7d1572fbdce4c7598e362526a7c;hp=17583d20f660fdb6e4aa86105c7574383f965ebe;hb=41ebbc2d;hpb=ac8c9fa809110ddb180bf7b2ce93e86270b39ff6</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5728</id>
      <title>use slice() api in packedints decode</title>
      <description>Today, for example 8-bpv decoder looks like this: in.seek(startPointer + index); return in.readByte() &amp; 0xFF; If instead we take a slice of 'in', we can remove an addition. Its not much, but helps a little. additionally we already (in PackedInts.java) compute the number of bytes, so we could make this an actual slice of the range, which would return an error on abuse instead of garbage data.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5729</id>
      <title>explore random-access methods to IndexInput</title>
      <description>Traditionally lucene access is mostly reading lists of postings and geared at that, but for random-access stuff like docvalues, it just creates overhead. So today we are hacking around it, by doing this random access with seek+readXXX, but this is inefficient (additional checks by the jdk that we dont need). As a hack, I added the following to IndexInput, changed direct packed ints decode to use them, and implemented in MMapDir: byte readByte(long pos) --&gt; ByteBuffer.get(pos) short readShort(long pos) --&gt; ByteBuffer.getShort(pos) int readInt(long pos) --&gt; ByteBuffer.getInt(pos) long readLong(long pos) --&gt; ByteBuffer.getLong(pos) This gives ~30% performance improvement for docvalues (numerics, sorting strings, etc) We should do a few things first before working this (LUCENE-5728: use slice api in decode, pad packed ints so we only have one i/o call ever, etc etc) but I think we need to figure out such an API. It could either be on indexinput like my hack (this is similar to ByteBuffer API with both relative and absolute methods), or we could have a separate API. But i guess arguably IOContext exists to supply hints too, so I dont know which is the way to go.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5730</id>
      <title>FSDirectory.open should return mmap for 64-bit OS X</title>
      <description>Report after iter 19: Task QPS trunk StdDev QPS patch StdDev Pct diff OrHighNotHigh 18.70 (10.3%) 19.79 (8.4%) 5.8% ( -11% - 27%) OrNotHighHigh 25.15 (11.5%) 26.65 (8.0%) 6.0% ( -12% - 28%) OrHighHigh 20.92 (9.9%) 22.22 (7.2%) 6.2% ( -9% - 25%) HighTerm 79.38 (10.8%) 84.42 (13.0%) 6.4% ( -15% - 33%) Prefix3 125.67 (8.6%) 136.21 (10.5%) 8.4% ( -9% - 30%) LowTerm 445.68 (13.7%) 484.84 (13.8%) 8.8% ( -16% - 42%) OrNotHighLow 45.83 (12.9%) 50.22 (8.8%) 9.6% ( -10% - 35%) OrNotHighMed 41.65 (12.0%) 46.05 (12.1%) 10.6% ( -12% - 39%) HighSloppyPhrase 5.82 (8.9%) 6.46 (10.4%) 10.9% ( -7% - 33%) MedPhrase 127.03 (16.3%) 141.19 (12.8%) 11.2% ( -15% - 48%) IntNRQ 4.85 (3.9%) 5.39 (7.4%) 11.2% ( 0% - 23%) MedTerm 101.62 (13.0%) 113.27 (12.2%) 11.5% ( -12% - 42%) OrHighNotLow 69.01 (12.9%) 77.77 (15.3%) 12.7% ( -13% - 47%) AndHighHigh 43.57 (8.1%) 49.24 (8.4%) 13.0% ( -3% - 32%) HighPhrase 9.30 (6.4%) 10.55 (8.4%) 13.4% ( -1% - 30%) OrHighLow 41.49 (10.1%) 47.45 (13.9%) 14.3% ( -8% - 42%) HighSpanNear 62.59 (7.5%) 71.79 (10.1%) 14.7% ( -2% - 34%) OrHighMed 41.74 (12.9%) 48.66 (12.2%) 16.6% ( -7% - 47%) AndHighMed 59.09 (8.2%) 69.27 (8.1%) 17.2% ( 0% - 36%) MedSloppyPhrase 7.14 (6.2%) 8.42 (7.2%) 17.9% ( 4% - 33%) LowPhrase 16.34 (6.3%) 19.34 (5.9%) 18.3% ( 5% - 32%) OrHighNotMed 50.43 (13.0%) 59.81 (13.5%) 18.6% ( -6% - 51%) MedSpanNear 56.50 (12.4%) 69.91 (14.5%) 23.7% ( -2% - 57%) LowSloppyPhrase 43.81 (10.0%) 57.88 (14.5%) 32.1% ( 6% - 62%) Wildcard 20.94 (7.4%) 28.53 (9.6%) 36.3% ( 18% - 57%) LowSpanNear 10.16 (5.4%) 14.05 (10.2%) 38.4% ( 21% - 57%) Fuzzy1 33.04 (8.3%) 58.74 (13.4%) 77.8% ( 51% - 108%) Fuzzy2 21.37 (4.4%) 38.41 (6.7%) 79.7% ( 65% - 95%) Respell 26.68 (7.1%) 50.00 (12.5%) 87.4% ( 63% - 115%) PKLookup 52.16 (12.4%) 101.00 (24.2%) 93.7% ( 50% - 148%) AndHighLow 269.54 (9.2%) 537.98 (16.1%) 99.6% ( 67% - 137%)</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5733</id>
      <title>Minor PackedInts API cleanups</title>
      <description>The PackedInts API has quite some history now and some of its methods are not used anymore, eg. PackedInts.Reader.hasArray. I'd like to remove them.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5735</id>
      <title>Faceting for DateRangePrefixTree</title>
      <description>The newly added DateRangePrefixTree (DRPT) encodes terms in a fashion amenable to faceting by meaningful time buckets. The motivation for this feature is to efficiently populate a calendar bar chart or heat-map. It's not hard if you have date instances like many do but it's challenging for date ranges. Internally this is going to iterate over the terms using seek/next with TermsEnum as appropriate. It should be quite efficient; it won't need any special caches. I should be able to re-use SPT traversal code in AbstractVisitingPrefixTreeFilter. If this goes especially well; the underlying implementation will be re-usable for geospatial heat-map faceting.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5739</id>
      <title>Add zig-zag encoding support to DataInput/DataOutput</title>
      <description>It would be convenient to have support for zig-zag-encoded integers in DataInput/DataOutput. There are not many places that use that feature today but I think it's quite common to need to read/write small signed values.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5740</id>
      <title>Add stripContentOfTags option to HTMLStripCharFilter</title>
      <description>HTMLStripCharFilter should have an option to strip out the sub-content of certain elements. It already does this for SCRIPT &amp; STYLE but it should be configurable to add more. I don't want certain elements to have their contents to be searchable.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5743</id>
      <title>new 4.9 norms format</title>
      <description>Norms can eat up a lot of RAM, since by default its 8 bits per field per document. We rely upon users to omit them to not blow up RAM, but its a constant trap. Previously in 4.2, I tried to compress these by default, but it was too slow. My mistakes were: allowing slow bits per value like bpv=5 that are implemented with expensive operations. trying to wedge norms into the generalized docvalues numeric case not handling "simple" degraded cases like "constant norm" the same norm value for every document. Instead, we can just have a separate norms format that is very careful about what it does, since we understand in general the patterns in the data: uses CONSTANT compression (just writes the single value to metadata) when all values are the same. only compresses to bitsPerValue = 1,2,4 (this also happens often, for very short text fields like person names and other stuff in structured data) otherwise, if you would need 5,6,7,8 bits per value, we just continue to do what we do today, encode as byte[]. Maybe we can improve this later, but this ensures we don't have a performance impact.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5745</id>
      <title>Refactoring AbstractVisitingPrefixTreeFilter code using cellIterator.</title>
      <description>The AbstractVisitingPrefixTreeFilter (used by RPT's Intersects, Within, Disjoint) really should be refactored to use the new CellIterator API as it will reduce the amount of code and should make the code easier to follow since it would be based on a well-known design-pattern (an iterator). It currently uses a VNode and VNode Iterator.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5748</id>
      <title>SORTED_NUMERIC dv type</title>
      <description>Currently for Strings you have SORTED and SORTED_SET, capable of single and multiple values per document respectively. For multi-numerics, there are only a few choices: encode with NumericUtils into byte[]'s and store with SORTED_SET. encode yourself per-document into BINARY. Both of these techniques have problems: SORTED_SET isn't bad if you just want to do basic sorting (e.g. min/max) or faceting counts: most of the bloat in the "terms dict" is compressed away, and it optimizes the case where the data is actually single-valued, but it falls apart performance-wise if you want to do more complex stuff like solr's analytics component or elasticsearch's aggregations: the ordinals just get in your way and cause additional work, deref'ing each to a byte[] and then decoding that back to a number. Worst of all, any mathematical calculations are off because it discards frequency (deduplicates). using your own custom encoding in BINARY removes the unnecessary ordinal dereferencing, but you trade off bad compression and access: you have no real choice but to do something like vInt within each byte[] for the doc, which means even basic sorting (e.g. max) is slow as its not constant time. There is no chance for the codec to optimize things like dates with GCD compression or optimize the single-valued case because its just an opaque byte[]. So I think it would be good to explore a simple long[] type that solves these problems.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5749</id>
      <title>analyzers should be further customizable to allow for better code reuse</title>
      <description>To promote code reuse, the customizability of the analyzers included with Lucene (e.g. EnglishAnalyzer) ought to be further improved. To illustrate, it is currently difficult to specify general stemming behavior without having to modify each and every analyzer class. In our case, we had to change the constructors of every analyzer class to accept an AnalyzerOption argument. The AnalyzerOption class has a getStemStrategy() method. StemStrategy is defined as follows: public enum StemStrategy { AGGRESSIVE, LIGHT, NONE } ; We needed to modify over 20 or so Lucene classes. This is obviously not ideal from a code reuse and maintainability standpoint.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5751</id>
      <title>Bring MemoryDocValues up to speed</title>
      <description>This one has fallen behind... It picks TABLE/GCD even when it won't actually save space or help, writes with BlockpackedWriter even when it won't save space, etc. Instead of comparing PackedInts.bitsRequired, factor in acceptableOverheadRatio too to determine "will save space". Check if blocking will save space along the same lines (otherwise use regular packed ints). Fix a similar bug in Lucene49 codec along these same lines (comparing PackedInts.bitsRequired instead of what would actually be written)</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5752</id>
      <title>Explore light weight Automaton replacement</title>
      <description>This effort started with the patch on LUCENE-4556, to create a "light weight" replacement for the current object-heavy Automaton class (which creates separate State and Transition objects). I took that initial patch much further, and cutover most places in Lucene that use Automaton to LightAutomaton. Tests pass. The core idea of LightAutomaton is all states are ints, and you build up the automaton under the restriction that you add all outgoing transitions one state at a time. This worked well for most operations, but for some (e.g. UTF32ToUTF8!!) it was harder, so I also added a separate builder to add transitions in any order and then in the end they are sorted and added to the real automaton. If this is successful I think we should just replace the current Automaton with LightAutomaton; right now they both exist in my current patch... This is very much a work in progress, and I'm not sure the restrictions the API imposes are "reasonable" (some algos got uglier). But I think it's at least worth exploring/iterating... I'll make a branch and commit my current state.</description>
      <attachments/>
      <comments>29</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5754</id>
      <title>Allow "$" in expression variables</title>
      <description>The current expressions modules only allows ASCII letters, ASCII digits and the underscore in variable names. It is quite common to javascript developers to use also the dollar sign as identifier part (especially at the beginning of an identifier, see the famous "jQuery" $-function). This would allow bindings like "$score". The official ECMAScript spec allows the $: http://www.ecma-international.org/publications/files/ECMA-ST-ARCH/ECMA-262,%203rd%20edition,%20December%201999.pdf (see section 7.6) The other stuff there like unicode escapes and all unicode letter crazyness should be excluded for simplicity. Adding the "$" is just a one-line change in the grammar and does not conflict with anything else, because "$" is not a special character to javascript.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5756</id>
      <title>Implement Accountable from IndexWriter</title>
      <description>We already expose the ram usage inside the FlushPolicy via DWFlushControl and since we now have Accountable I think IW should implement it too to get more info about it's current RAM usage.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5759</id>
      <title>Add PackedInts.unsignedBitsRequired</title>
      <description>Across the code base, we have lots of: long minValue, maxValue; final long delta = maxValue - minValue; final int bitsRequired = delta &lt; 0 64 : Packedints.bitsRequired(delta); Packedints.bitsRequired(delta) doesn't work directly in that case since it expects a positive value. And that is important that it does so in order to get an error instead of silently being super wasteful if a negative value is provided. Yet in some cases such as the one depicted above, the value should be interpreted as an unsigned long. So I propose to add another bitsRequired method that would interpret the value as unsigned.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5760</id>
      <title>Speed up BufferedIndexInput.randomAccessSlice</title>
      <description>Today this uses the default implementation, e.g. for readInt(pos): @Override public int readInt(long pos) throws IOException { slice.seek(pos); return slice.readInt(); } But this causes the bounds to be checked twice. Just like we did for MMap, we can provide a faster implementation that only checks once: yields ~30% speedup.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5767</id>
      <title>OrdinalMap optimizations</title>
      <description>OrdinalMap does its best to store a mapping from segment to global ordinals with as little memory as possible using MonotonicAppendingLongBuffer. In the low-cardinality case, there are things that could be optimized though: on large segments, it's quite likely that the segment ordinals will perfectly match the global ordinals. In that case there is nothing to do, we can just return the segment ordinal as-is. even if they don't, it might be that storing the global ordinals directly in a PackedInts.Mutable only takes slightly more memory while removing the overhead of the monotonic encoding.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5769</id>
      <title>SingletonSortedSet should extend RandomAccessOrds</title>
      <description>The point of this (exposed via DocValues#singleton) is that you can process single-valued data too, with the SortedSet api. But it doesn't support the RandomAccessOrds API... pretty silly since its either missing or has only one value.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5771</id>
      <title>Review semantics of SpatialOperation predicates</title>
      <description>SpatialOperation (which I wish was named SpatialPredicate) is a bunch of predicates – methods that return true/false based on a pair of shapes. Some of them don't seem to be defined in a way consistent with their definitions on ESRI's site: http://edndoc.esri.com/arcsde/9.1/general_topics/understand_spatial_relations.htm (which is linked as a reference, and is in turn equivalent to OGC spec definitions, I believe). Problems: the definitions make no mention of needing to have area or not, yet some of our predicates are defined as to require area on either the indexed or query shape. the definitions make a distinction of the boundary of a shape, yet in Lucene-spatial, there is none. That suggests our predicates are wrongly chosen since there are official predicates that are boundary-neutral – namely "Covers" and "CoveredBy" in lieu of Contains and Within, respectively. If we don't rename our predicates, we should at least support the correct predicates names! Overlaps appears totally wrong. It should be defined as indexedShape.relate(queryShape) == Intersects (and thus not Within or Contains or Disjoint). It's presently defined as the same as Intersects plus the query shape needing area.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5778</id>
      <title>Support hunspell morphological description fields</title>
      <description>Currently hunspell stemmer doesn't support these (particularly the st:XYZ field which signifies a stemming "exception" basically). For example in english "feet" might have "st:foot". These can be encoded two ways, inline into the .dic or aliased via AM entries from the .aff. Unfortunately, our parsing was really lenient and in order to do this properly (e.g. handling words with spaces and morphological fields containing slashes and all that jazz), it had to be cleaned up a bit to follow the hunspell rules. For now, we dont waste space with part of speech and only concern ourselves with the "st:" field and the stemmer uses it transparently. Encoding these exceptions is a little complicated because these exceptions are rarely used, but when they are, they are typically common verbs and stuff (like english 'be'), so we dont want it to be slow. They are also not "per-word" but "per-form", so you could have homonyms with different stems (at least theoretically). On the other hand this is silly stuff particular to these silly languages, so we dont want it to blow up the datastructure for 99% of languages that dont use it. So the way we do it is to just store the exception ID alongside the form ID (this doubles the intsref, which is usually 1). So for e.g. english i think it typically boils down to an extra byte or so in the FST and doesn't blow up. For languages not using this stuff there is no impact.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5779</id>
      <title>Improve BBox AreaSimilarity algorithm to consider lines and points</title>
      <description>GeoPortal's area overlap algorithm didn't consider lines and points; they end up turning the score 0. I've thought about this for a bit and I've come up with an alternative scoring algorithm. (already coded and tested and documented): New Javadocs: /** * The algorithm is implemented as envelope on envelope overlays rather than * complex polygon on complex polygon overlays. * &lt;p/&gt; * &lt;p/&gt; * Spatial relevance scoring algorithm: * &lt;DL&gt; * &lt;DT&gt;queryArea&lt;/DT&gt; &lt;DD&gt;the area of the input query envelope&lt;/DD&gt; * &lt;DT&gt;targetArea&lt;/DT&gt; &lt;DD&gt;the area of the target envelope (per Lucene document)&lt;/DD&gt; * &lt;DT&gt;intersectionArea&lt;/DT&gt; &lt;DD&gt;the area of the intersection between the query and target envelopes&lt;/DD&gt; * &lt;DT&gt;queryTargetProportion&lt;/DT&gt; &lt;DD&gt;A 0-1 factor that divides the score proportion between query and target. * 0.5 is evenly.&lt;/DD&gt; * * &lt;DT&gt;queryRatio&lt;/DT&gt; &lt;DD&gt;intersectionArea / queryArea; (see note)&lt;/DD&gt; * &lt;DT&gt;targetRatio&lt;/DT&gt; &lt;DD&gt;intersectionArea / targetArea; (see note)&lt;/DD&gt; * &lt;DT&gt;queryFactor&lt;/DT&gt; &lt;DD&gt;queryRatio * queryTargetProportion;&lt;/DD&gt; * &lt;DT&gt;targetFactor&lt;/DT&gt; &lt;DD&gt;targetRatio * (1 - queryTargetProportion);&lt;/DD&gt; * &lt;DT&gt;score&lt;/DT&gt; &lt;DD&gt;queryFactor + targetFactor;&lt;/DD&gt; * &lt;/DL&gt; * Note: The actual computation of queryRatio and targetRatio is more complicated so that it considers * points and lines. Lines have the ratio of overlap, and points are either 1.0 or 0.0 depending on wether * it intersects or not. * &lt;p /&gt; * Based on Geoportal's * &lt;a href="http://geoportal.svn.sourceforge.net/svnroot/geoportal/Geoportal/trunk/src/com/esri/gpt/catalog/lucene/SpatialRankingValueSource.java"&gt; * SpatialRankingValueSource&lt;/a&gt; but modified. GeoPortal's algorithm will yield a score of 0 * if either a line or point is compared, and it's doesn't output a 0-1 normalized score (it multiplies the factors). * * @lucene.experimental */</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5782</id>
      <title>Improve OrdinalMap compression by sorting the supplied terms enums</title>
      <description>As mentionned in LUCENE-5780, OrdinalMaps might have much better compression when the terms enums are supplied sorted by descending cardinality. When it is not the case, we could sort the enums and re-map segment indices on top of it.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5783</id>
      <title>smokeTestRelease.py should be more user friendly</title>
      <description>If one runs "python dev-tools/scripts/smokeTestRelease.py" then you get no usage instructions; you get an unhelpful error. It should contain help on how to invoke it. Furthermore, if would be nice if it inferred the svn release version and the distribution version by assuming a conventional URL format. For example: "http://people.apache.org/~rmuir/staging_area/lucene_solr_4_9_0_r1604085/" is 1604085 and 4.9.0. Or, at least remind you that the URL may contain this info.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5785</id>
      <title>White space tokenizer has undocumented limit of 256 characters per token</title>
      <description>The white space tokenizer breaks tokens at 256 characters, which is a hard-wired limit of the character tokenizer abstract class. The limit of 256 is obviously fine for normal, natural language text, but excessively restrictive for semi-structured data. 1. Document the current limit in the Javadoc for the character tokenizer. Add a note to any derived tokenizers (such as the white space tokenizer) that token size is limited as per the character tokenizer. 2. Added the setMaxTokenLength method to the character tokenizer ala the standard tokenizer so that an application can control the limit. This should probably be added to the character tokenizer abstract class, and then other derived tokenizer classes can inherit it. 3. Disallow a token size limit of 0. 4. A limit of -1 would mean no limit. 5. Add a "token limit mode" method - "skip" (what the standard tokenizer does), "break" (current behavior of the white space tokenizer and its derived tokenizers), and "trim" (what I think a lot of people might expect.) 6. Not sure whether to change the current behavior of the character tokenizer (break mode) to fix it to match the standard tokenizer, or to be "trim" mode, which is my choice and likely to be what people might expect. 7. Add matching attributes to the tokenizer factories for Solr, including Solr XML javadoc. At a minimum, this issue should address the documentation problem.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5789</id>
      <title>Fix issue with newsticker and other included stuff on Lucene webpage</title>
      <description>When talking to asfinfra today, they told me, that there is a solutions for the problem we have on the Lucene web page: The index markdown file includes other files like the newsticker markdown. If you update the newsticker, the main webpage is not updated, unless you do a fake commit on the index markdown. The solution is here (Apache Thrift uses this, including example): [18:46] joes4: ThetaPh1: the way to deal with included files is through the %path::dependencies hash [18:46] ThetaPh1: ah there is a solution for that? [18:46] joes4: yes see the thrift cms site [18:46] ThetaPh1: the last time we asked there was only the fake commit on the file that includes [18:46] ThetaPh1: in short: how must the link look like? [18:47] joes4: $dependencies{"path_to_base_file"} = [ @list_of_files_to_be_included ] [18:47] joes4: in path.pm [18:49] ThetaPh1: ok, I'll open a lucene issue to fix this on our website [18:49] ThetaPh1: it mainly affects news ticker, which is a separte markdown [18:49] joes4: https://svn.apache.org/repos/asf/thrift/cms-site/trunk/lib/path.pm [18:49] joes4: they are doing ther state of the art [18:50] joes4: dynamicly generated %dependencies hash</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5792</id>
      <title>Improve our packed *AppendingLongBuffer</title>
      <description>Since these classes are writeteable, they need a buffer in order to stage pending changes for efficiency reasons. The issue is that at read-time, the code then needs, for every call to get to check whether the requested value is in the buffer of pending values or has been packed into main storage, which is inefficient. I would like to fix these APIs to separate the writer from the reader, the latter being immutable.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5793</id>
      <title>Add equals/hashCode to FieldType</title>
      <description>would be nice to have equals and hashCode to FieldType, so one can easily check if they are the same, and for example, reuse existing default implementations of it.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5794</id>
      <title>Add a slow random-access ords wrapper</title>
      <description>Even if you are using an algorithm that requires random access (eg. sorting based on the maximum value), it might still be ok to allow for it occasionally on a codec that doesn't support random access like MemoryDocValuesFormat by having a slow random-access wrapper. This slow wrapper would need to be enabled explicitely. This would allow to have algorithms that are optimized on random-access codecs but still work in the general case.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5797</id>
      <title>improve speed of norms merging</title>
      <description>Today we use the following procedure: track HashSet&lt;Long&gt; uniqueValues, until it exceeds 256 unique values. convert to array, sort and assign ordinals to each one create encoder map (HashMap&lt;Long,Integer&gt;) to encode each value. This results in each value being hashed twice... but the vast majority of the time people will just be using single-byte norms and a simple array is enough for that range.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5798</id>
      <title>minor optimizations to MultiDocs(AndPositions)Enum.reset()</title>
      <description>This method is called by merging for each term, potentially many times, but only returning a few docs for each invocation (e.g. imagine high cardinality fields, unique id fields, normal zipf distribution on full text). Today we create a new EnumWithSlice[] array and new EnumWithSlice entry for each term, but this creates a fair amount of unnecessary garbage: instead we can just make this array up-front as size subReaderCount and reuse it.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5799</id>
      <title>speed up DocValuesConsumer.mergeNumericField</title>
      <description>This method (used for both numeric docvalues and norms) is a little slow: does some boxing for no good reason (can just use a boolean instead) checks docsWithField always, instead of only when value == 0. This can cause unnecessary i/o.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5802</id>
      <title>SpatialStrategy DocValues &amp; FieldType customizability</title>
      <description>The SpatialStrategy API is a simple facade to index spatial data and query by it in a consistent way, hiding the implementation. For indexing data, it has one applicable method: public abstract Field[] createIndexableFields(Shape shape); The base abstraction provides no further configuration. BBoxStrategy and PointVectorStrategy have a way to set the precisionStep of the underlying Double trie fields. But none have a way to use Floats, and none have a way to specify the use of DocValues (and which type). Perhaps there are other plausible nobs to turn. It is actually more than just indexing since at search time it may have to change accordingly (e.g. search difference between Float &amp; Double). PrefixTreeStrategy is likely to soon deprecate/remove any applicability here (see LUCENE-5692). If there is no change that could reasonably be made to SpatialStrategy itself, what is the pattern that BBoxStrategy and others should use?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5803</id>
      <title>Add another AnalyzerWrapper class that does not have its own cache, so delegate-only wrappers don't create thread local resources several times</title>
      <description>This is a followup issue for the following Elasticsearch issue: https://github.com/elasticsearch/elasticsearch/pull/6714 Basically the problem is the following: Elasticsearch has a pool of Analyzers that are used for analysis in several indexes Each index uses a different PerFieldAnalyzerWrapper PerFieldAnalyzerWrapper uses PER_FIELD_REUSE_STRATEGY. Because of this it caches the tokenstreams for every field. If there are many fields, this are a lot. In addition, the underlying analyzers may also cache tokenstreams and other PerFieldAnalyzerWrappers do the same, although the delegate Analyzer can always return the same components. We should add similar code to Elasticsearch's directly to Lucene: If the delegating Analyzer just delegates per Field or just wraps CharFilters around the Reader, there is no need to cache the TokenStreamComponents a second time in the delegating Analyzers. This is only needed, if the delegating Analyzers adds additional TokenFilters (like ShingleAnalyzerWrapper). We should name this new class DelegatingAnalyzerWrapper extends AnalyzerWrapper. The wrapComponents method must be final, because we are not allowed to add additional TokenFilters, but unlike ES, we don't need to disallow wrapping with CharFilters. Internally this class uses a private ReuseStrategy that just delegates to the underlying analyzer. It does not matter here if the strategy of the delegate is global or per field, this is private to the delegate.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5810</id>
      <title>NRTCachingDirecotory should subclass FilterDirectory</title>
      <description>For consistency NRTCachingDirecotory should subclass FilterDirectory</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5813</id>
      <title>Directory should implement Accountable</title>
      <description>Follow-up of LUCENE-5812.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5815</id>
      <title>Add TermAutomatonQuery, for proximity matching that generalizes MultiPhraseQuery/SpanNearQuery</title>
      <description>I created a new query, called TermAutomatonQuery, that's a proximity query to generalize MultiPhraseQuery/SpanNearQuery: it lets you construct an arbitrary automaton whose transitions are whole terms, and then find all documents that the automaton matches. This is different from a "normal" automaton whose transitions are usually bytes/characters within a term/s. So, if the automaton has just 1 transition, it's just an expensive TermQuery. If you have two transitions in sequence, it's a phrase query of two terms. You can express synonyms by using transitions that overlap one another but the automaton doesn't have to be a "sausage" (as MultiPhraseQuery requires) i.e. it "respects" posLength (at query time). It also allows "any" transitions, to match any term, so you can do sloppy matching and span-like queries, e.g. find "lucene" and "python" with up to 3 other terms in between. I also added a class to convert a TokenStream directly to the automaton for this query, preserving posLength. (Of course, the index can't store posLength, so the matching won't be fully correct if any indexed tokens has posLength != 1). But if you do query-time-only synonyms then the matching should finally be correct. I haven't tested performance but I suspect it's quite slowish ... its cost is O(sum-totalTF) of all terms "used" in the automaton. There are some optimizations we could do, e.g. detecting that some terms in the automaton can be upgraded to MUST (right now they are all effectively SHOULD). I'm not sure how it should assign scores (punted on that for now), but the matching seems to be working.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5816</id>
      <title>ToParentBlockJoinQuery deothogonalization</title>
      <description>For now ToParentBlockJoinQuery accepts only child documents. Before (LUCENE-4968) passing parent document to TPBJQ lead to undefined behavior and garbage in results, unfortunately it also affects TPBJQ.advance(). After pointed patch IllegalStateException is thrown when this occurs. So we must always take parent-child relations into account while writing queries. At most of time it is necessary when writing a query, but sometimes, filters can be independent of data model (for example, ACL filters: +TPBJQ +allowed:user). TPBJQ shall returns parent doc if parent doc is passed to TPBJQ.advance() or returned from childScorer.advance(). This change doesn't break anything: results will be absolutely the same for parent-child orthogonal queries. In few words: Document matching parent filter should be parent of itself.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5819</id>
      <title>Add block tree postings format that supports term ords</title>
      <description>BlockTree is our default terms dictionary today, but it doesn't support term ords, which is an optional API in the postings format to retrieve the ordinal for the currently seek'd term, and also later seek by that ordinal e.g. to lookup the term. This can possibly be useful for e.g. faceting, and maybe at some point we can share the postings terms dict with the one used by sorted/set DV for cases when app wants to invert and facet on a given field. The older (3.x) block terms dict can easily support ords, and we have a Lucene41OrdsPF in test-framework, but it's not as fast / compact as block-tree, and doesn't (can't easily) implement an optimized intersect, but it could be for fields we'd want to facet on, these tradeoffs don't matter. It's nice to have options...</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5820</id>
      <title>SuggestStopFilter should have a factory</title>
      <description>While trying to use the new Suggester in Solr I realized that SuggestStopFilter did not have a factory. We should add one.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5821</id>
      <title>SuggestStopFilter should have a factory</title>
      <description>While trying to use the new Suggester in Solr I realized that SuggestStopFilter did not have a factory. We should add one.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5823</id>
      <title>recognize hunspell FULLSTRIP option in affix file</title>
      <description>With LUCENE-5818 we fixed stripping to be correct (ensuring it doesnt strip the entire word before applying an affix). This is usually true, but there is an option in the affix file to allow this. Its used by several languages (french, latvian, swedish, etc) FULLSTRIP With FULLSTRIP, affix rules can strip full words, not only one less characters, before adding the affixes</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5825</id>
      <title>Allowing the benchmarking algorithm to choose PostingsFormat</title>
      <description>The algorithm file for benchmarking should allow PostingsFormat to be configurable.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5826</id>
      <title>Support proper hunspell case handling and related options</title>
      <description>When ignoreCase=false, we should accept title-cased/upper-cased forms just like hunspell -m. Furthermore there are some options around this: LANG: can turn on alternate casing for turkish/azeri KEEPCASE: can prevent acceptance of title/upper cased forms for words While we are here setting up the same logic anyway, add support for similar options: NEEDAFFIX/PSEUDOROOT: form is invalid without being affixed ONLYINCOMPOUND: form/affixes only make sense inside compounds. This stuff is unrelated to the ignoreCase=true option. If you use that option though, it does use correct alternate casing for tr_TR/az_AZ now though. I didn't yet implement CHECKSHARPS because it seems more complicated, I have to figure out what the logic there should be first.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5834</id>
      <title>Make empty doc values impls singletons</title>
      <description>Making these empty instances singletons would allow to use unwrapSingleton to check if they are single-valued.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5835</id>
      <title>Add sortMissingLast support to TermValComparator</title>
      <description>It would be nice to allow to configure the behavior on missing values for this comparator, similarly to what TermOrdValComparator does.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5837</id>
      <title>Only check docsWithField when necessary in numeric comparators</title>
      <description>Our numeric comparators have branches to deal with missing values. However there are some cases when checking docs that have a field is not useful: if all docs have a value if no docs have a value if the missing value is 0</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5841</id>
      <title>Remove FST.Builder.FreezeTail interface</title>
      <description>The FST Builder has a crazy-hairy interface called FreezeTail, which is only used by BlockTreeTermsWriter to find appropriate prefixes (i.e. containing enough terms or sub-blocks) to write term blocks. But this is really a silly abuse ... it's cleaner and likely faster/less GC for BTTW to compute this itself just by tracking the term ordinal where each prefix started in the pending terms/blocks. The code is also insanely hairy, and this is at least a baby step to try to make it a bit simpler. This also makes it very hard to experiment with different formats at write-time because you have to get your new formats working through this strange FreezeTail.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5847</id>
      <title>Improved implementation of language models in lucene</title>
      <description>The current implementation of language models in lucene is based on the paper "A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval" by Zhai and Lafferty ('01). Specifically, LMDiricheltSimilarity and LMJelinikMercerSimilarity use a normalized smoothed score for a matching term in a document, as suggested in the above mentioned paper. However, lucene doesn't assign a score to query terms that do not appear in a matched document. According to the "pure" LM approach, these terms should be assigned with a collection probability "background score". If one uses the Jelinik Mercer smoothing method, the final result list produced by lucene is rank equivalent to the one that would have been created by a full LM implementation. However, this is not the case for Dirichlet smoothing method, because the background score is document dependent. Documents in which not all query terms appear, are missing the document-dependant background score for the missing terms. This component affects the final ranking of documents in the list. Since LM is a baseline method in many works in the IR research field, I attach a patch that implements a full LM in lucene. The basic issue that should be addressed here is assigning a document with a score that depends on all the query terms, collection statistics and the document length. The general idea of what I did is adding a new getBackGroundScore(int docID) method to similarity, scorer and bulkScorer. Than, when a collector assigns a score to a document (score = scorer.score()) I added the backgound score (score=scorer.score()+scorer.background(doc)) that is assigned by the similarity class used for ranking. The patch also includes a correction of the document length such that it will be the real document length and not the encoded one. It is required for the full LM implementation.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5856</id>
      <title>remove useless &amp; 0x3f from *BitSet.get and company</title>
      <description>java specification says: If the promoted type of the left-hand operand is long, then only the six lowest-order bits of the right-hand operand are used as the shift distance. It is as if the right-hand operand were subjected to a bitwise logical AND operator &amp; (§15.22.1) with the mask value 0x3f (0b111111). The shift distance actually used is therefore always in the range 0 to 63, inclusive. and x86 works the same way. if we remove this, we just see less instructions with printassembly...</description>
      <attachments/>
      <comments>8</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5857</id>
      <title>make genLookaheadCountLimit configurable, add genLookbackCountLimit parameter</title>
      <description>The problem we saw was that an error-in-final-commit during solr-shutdown led to a corrupted segments_.... file which prevented subsequent solr-start, details below. this change: adds genLookbackCountLimit similar to the existing genLookaheadCountLimit makes both parameters configurable (initial values defaulted to existing behaviour) error-in-final-commit result: segments_2fi5 file present and zero-bytes long (https://issues.apache.org/jira/i#browse/SOLR-6296 concerns root cause) segments_2fi4 file absent on disk (speculation: 2fi4 was result of in-memory segment merging) segments_2fi3 file present and usable (but not found by existing looking-logic) segments.gen file referenced 2fi3</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5860</id>
      <title>Use Terms.getMin/Max to speed up range queries/filters</title>
      <description>As of LUCENE-5610, Lucene's Terms API now exposes min and max terms in each field. I think we can use this in our term/numeric range query/filters to avoid visiting a given segment by detecting up front that the terms in the segment don't overlap with the query's range. Even though block tree avoids disk seeks in certain cases when the term cannot exist on-disk, I think this change would further avoid disk seeks in additional cases because the min/max term has more/different information than the in-memory FST terms index.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5861</id>
      <title>CachingTokenFilter should use ArrayList not LinkedList</title>
      <description>CachingTokenFilter, to my surprise, puts each new AttributeSource.State onto a LinkedList. I think it should be an ArrayList. On large fields that get analyzed, there can be a ton of State objects to cache. I also observe that State is itself a linked list of other State objects. Perhaps we could take this one step further and do parallel arrays of AttributeImpl, thereby bypassing State.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5865</id>
      <title>create fork of analyzers module without Version</title>
      <description>Since this is obviously too controversial to fix, we can just add alternatives that don't have this messy api, e.g. under analyzers-simple. These won't have Version. They don't need factories, because they are actually geared at being usable for lucene users. Once nice thing is, this way the problem can be fixed in 4.10</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5867</id>
      <title>Add BooleanSimilarity</title>
      <description>This can be used when the user doesn't want tf/idf scoring for some reason. The idea is that the score is just query_time_boost * index_time_boost, no queryNorm/IDF/TF/lengthNorm...</description>
      <attachments/>
      <comments>11</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5868</id>
      <title>JoinUtil support for NUMERIC docValues fields</title>
      <description>while polishing SOLR-6234 I found that JoinUtil can't join int dv fields at least. I plan to provide test/patch. It might be important, because Solr's join can do that. Please vote if you care!</description>
      <attachments/>
      <comments>29</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5870</id>
      <title>Simplify StoredFieldsVisitor</title>
      <description>StoredFieldVisitor has a visitor method for 4 numeric types: int, long, float and double. We should remove this specialization and just have a method that takes a java.lang.Number.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5875</id>
      <title>Default page/block sizes in the FST package can cause OOMs</title>
      <description>We are building some fairly big FSTs (the biggest one having about 500M terms with an average of 20 characters per term) and that works very well so far. The problem is just that we can use neither the "doShareSuffix" nor the "doPackFST" option from the builder since both would cause us to get exceptions. One beeing an OOM and the other an IllegalArgumentException for a negative array size in ArrayUtil. The thing here is that we in theory still have far more than enough memory available but it seems that java for some reason cannot allocate byte or long arrays of the size the NodeHash needs (maybe fragmentation?). Reducing the constant in the NodeHash from 1&lt;&lt;30 to e.g. 27 seems to fix the issue mostly. Could e.g. the Builder pass through its bytesPageBits to the NodeHash or could we get a custom parameter for that? The other problem we run into was a NegativeArraySizeException when we try to pack the FST. It seems that we overflowed to 0x80000000. Unfortunately I accidentally overwrote that exception but I remember it was triggered by the GrowableWriter for the inCounts in line 728 of the FST. If it helps I can try to reproduce it.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5877</id>
      <title>SpanQuery subclass fields should be protected and final</title>
      <description>I needed access to SpanNearQuery.collectPayloads but annoyingly found it to be private, forcing my subclass to store a copy. For that matter, I think it's good practice for these fields to be declared final.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5878</id>
      <title>Add utility to convert document terms to doubles</title>
      <description>It would be good to have a way to create double vectors/arrays from doc terms (using frequencies), this would allow to visualize / plot documents better and also better interact with other classification / learning frameworks (e.g. for comparison).</description>
      <attachments/>
      <comments>6</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5879</id>
      <title>Add auto-prefix terms to block tree terms dict</title>
      <description>This cool idea to generalize numeric/trie fields came from Adrien: Today, when we index a numeric field (LongField, etc.) we pre-compute (via NumericTokenStream) outside of indexer/codec which prefix terms should be indexed. But this can be inefficient: you set a static precisionStep, and always add those prefix terms regardless of how the terms in the field are actually distributed. Yet typically in real world applications the terms have a non-random distribution. So, it should be better if instead the terms dict decides where it makes sense to insert prefix terms, based on how dense the terms are in each region of term space. This way we can speed up query time for both term (e.g. infix suggester) and numeric ranges, and it should let us use less index space and get faster range queries. This would also mean that min/maxTerm for a numeric field would now be correct, vs today where the externally computed prefix terms are placed after the full precision terms, causing hairy code like NumericUtils.getMaxInt/Long. So optos like LUCENE-5860 become feasible. The terms dict can also do tricks not possible if you must live on top of its APIs, e.g. to handle the adversary/over-constrained case when a given prefix has too many terms following it but finer prefixes have too few (what block tree calls "floor term blocks").</description>
      <attachments/>
      <comments>78</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5882</id>
      <title>add 4.10 docvaluesformat</title>
      <description>We can improve the current format in a few ways: speed up Sorted/SortedSet byte[] lookup by structuring the term blocks differently (allow random access, more efficient bulk i/o) speed up reverse lookup by adding a reverse index (small: just every 1024'th term with useless suffixes removed). use slice API for access to access to binary content, too.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5883</id>
      <title>Move MergePolicy to LiveIndexWriterConfig</title>
      <description>Since LUCENE-5711, MergePolicy is no longer wired to an IndexWriter instance. Therefore it can be moved to be a live setting on IndexWriter, which will allow apps to plug-in an MP on a live IW instance, without closing/reopening the writer. See for example LUCENE-5526 - instead of adding MP to forceMerge, apps could change the MP before calling forceMerge, with e.g. an UpgradeIndexMergePolicy. I think we can also make MergeScheduler a live setting, though I currently don't see the benefits of doing that, so I'd rather not touch it now.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5884</id>
      <title>Speed up FST.ramBytesUsed</title>
      <description>It's a little heavy now, relying too much on reflection (RUE.shallowSizeOf)... and we do this up to 128 times per FST (= per indexed field, per segment, for the terms index).</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5887</id>
      <title>Remove horrible WeakIdentityMap caching in AttributeFactory, AttributeSource and VirtualMethod</title>
      <description>Especially the use case in AttributeFactory is horrible: Because of ClassLoader issues we cannot hold strong references (see LUCENE-5640 for explanation), we need WeakIdentityMap&lt;Class, WeakReference&lt;someVal&gt;&gt;. You could say: let's use a strong value for stuff like MethodHandles (used in AttributeFactory), but because those have a strong reference to the class, our reference to key would be strong, so garbage collector can no longer unload the class. This is why we use the WeakReference also on the value. The problem is if the value is something like a MethodHandle, which itsself has hard reference to (so it gets garbage collected). Then the cache is useless. In DefaultAttributeFactory I decided, to make methodhandles strong references, but then I needed to restrict it to our own classloader, otherwise we would have strong references to foreign classloaders. Since Java 7 there is java.lang.ClassValue, that fixes the following JVM bug: http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6389107 See also: http://stackoverflow.com/questions/7444420/classvalue-in-java-7 In fact internally, there is a also a WeakReference/WeakHashMap used, but only as fallback - and its only one globally, used by many other JVM internals, too. By default it has a very fast path and the call to ClassValue.get() is incredibly fast. This should therefore also improve AttributeFactory alltogether. Next to AttributeFactory, I also improved the Interfaces cache of AttributeSource (this one assigns an array of Attribute interfaces to an AttributeImpl). The other one is VirtualMethod (assigns its own implementationDistance for every seen subclass). This also removes almost all uses of WeakIdentityMap, the remaining one is the ByteBuffer stuff in MMapDirectory. Unfortunately I have still no idea how to remove that one...</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5889</id>
      <title>AnalyzingInfixSuggester should expose commit()</title>
      <description>There is no way short of close() for a user of AnalyzingInfixSuggester to cause it to commit() its underlying index: only refresh() is provided. But callers might want to ensure the index is flushed to disk without closing.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>5893</id>
      <title>FreeTextSuggester can now use Files.createTempDirectory()</title>
      <description>Came across the TODO in the code and now it's possible to use Files.createTempDirectory since 4x is also on Java 7.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5894</id>
      <title>refactor bulk merge logic</title>
      <description>Today its only usable really by stored fields/term vectors, has hardcoded logic in SegmentMerger specific to certain impls, etc. It would be better if this was generalized to terms/postings/norms/docvalues as well. Bulk merge is boring, the real idea is to allow codecs to do more: e.g. with this patch they could do streaming checksum validation, or prevent the loading of "latent" norms, or other things we cannot do today.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5895</id>
      <title>Add per-segment and per-commit id to help replication</title>
      <description>It would be useful if Lucene recorded a unique id for each segment written and each commit point. This way, file-based replicators can use this to "know" whether the segment/commit they are looking at on a source machine and dest machine are in fact that same. I know this would have been very useful when I was playing with NRT replication (LUCENE-5438).</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5909</id>
      <title>Run smoketester on Java 8</title>
      <description>In the past, when we were on Java 6, we ran the Smoketester on Java 6 and Java 7. As Java 8 is now officially released and supported, smoketester should now use and require JAVA8_HOME. For the nightly-smoke tests I have to install the openjdk8 FreeBSD package, but that should not be a problem.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5911</id>
      <title>Make MemoryIndex thread-safe for queries</title>
      <description>We want to be able to run multiple queries at once over a MemoryIndex in luwak (see https://github.com/flaxsearch/luwak/commit/49a8fba5764020c2f0e4dc29d80d93abb0231191), but this isn't possible with the current implementation. However, looking at the code, it seems that it would be relatively simple to make MemoryIndex thread-safe for reads/queries.</description>
      <attachments/>
      <comments>29</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>5912</id>
      <title>Non-NRT directory readers don't reuse segments maintained IndexWriter's segment reader pool</title>
      <description>Currently, if you attempt to open a reader into an index at a specific commit point, it will always behave as though it's opening a completely new index - even if one were to use the DirectoryReader.openIfChanged(DirectoryReader, IndexCommit) API, and pass in an NRT reader instance. What should ideally happen here is that the SegmentReader pool managed by IndexWriter linked to the NRT reader gets reused for the commit point open as much as possible, to avoid wasting heap space. The problem becomes evident when looking at the code in DirectoryReader: protected DirectoryReader doOpenIfChanged(final IndexCommit commit) throws IOException { ensureOpen(); // If we were obtained by writer.getReader(), re-ask the // writer to get a new reader. if (writer != null) { return doOpenFromWriter(commit); } else { return doOpenNoWriter(commit); } } private DirectoryReader doOpenFromWriter(IndexCommit commit) throws IOException { if (commit != null) { return doOpenFromCommit(commit); } ...... Looks like the fact that a commit point is being re-opened trumps the presence of the associated IndexWriter.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5914</id>
      <title>More options for stored fields compression</title>
      <description>Since we added codec-level compression in Lucene 4.1 I think I got about the same amount of users complaining that compression was too aggressive and that compression was too light. I think it is due to the fact that we have users that are doing very different things with Lucene. For example if you have a small index that fits in the filesystem cache (or is close to), then you might never pay for actual disk seeks and in such a case the fact that the current stored fields format needs to over-decompress data can sensibly slow search down on cheap queries. On the other hand, it is more and more common to use Lucene for things like log analytics, and in that case you have huge amounts of data for which you don't care much about stored fields performance. However it is very frustrating to notice that the data that you store takes several times less space when you gzip it compared to your index although Lucene claims to compress stored fields. For that reason, I think it would be nice to have some kind of options that would allow to trade speed for compression in the default codec.</description>
      <attachments/>
      <comments>39</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>5925</id>
      <title>Use rename instead of segments_N fallback / segments.gen etc</title>
      <description>Our commit logic is strange, we write corrupted commit points and only on the last phase of commit do we "correct them". This means the logic to get the latest commit is always scary and awkward, since it must deal with partial commits, and try to determine if it should fall back to segments_N-1 or actually relay an exception. This logic is incomplete/sheisty as we, e.g. i think we only fall back so far (at most one). If we somehow screw up in all this logic do the wrong thing, then we lose data (e.g. LUCENE-4870 wiped entire index because of TooManyOpenFiles). We now require java 7, i think we should expore instead writing pending_segments_N and then in finishCommit() doing an atomic rename to segments_N. We could then remove all the complex fallback logic completely, since we no longer have to deal with "ignoring partial commits", instead simply delivering any exception we get when trying to read the commit and sleep better at night. In java 7, we have the apis for this (ATOMIC_MOVE).</description>
      <attachments/>
      <comments>20</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5926</id>
      <title>ResourceLoaderAware classes should probably throw IllegalStateException if you attempt to use them w/o calling inform()</title>
      <description>Mailing list user reported an NPE from SnowballPorterFilterFactory.create that was ultimately caused by not realizing that they had to call SnowballPorterFilterFactory.inform. We should consider updating all ResourceLoaderAware classes so that if you attempt to use them w/o calling inform, you consistently get an IllegalStateException w/meaningful error. ie, in the case of SnowballPorterFilterFactory, something like... @Override public TokenFilter create(TokenStream input) { if (null == stemClass) { throw new IllegalStateException("inform(ResourceLoader) method must be called before using this factory"); } ...</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5930</id>
      <title>IntelliJ config: drop resource-only modules, add module groups, and add module for lucene/backward-codecs</title>
      <description>The number of intellij modules is getting out of hand. Intellij supports marking subdirectories within a module as source/resources/tests/test-resources. I think we should consolidate these modules so we have just one per lucene module. Is there some reason I'm missing that this was not done in the first place?</description>
      <attachments/>
      <comments>24</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5933</id>
      <title>Add FilterSpans to allow easily wrapping a Spans impl</title>
      <description>I found this useful while working with spans recently. It's simple and straightforward. I'll add a patch shortly.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5937</id>
      <title>Can we fix TestIW.testThreadInterruptDeadlock to capture IW infoStream</title>
      <description>I already tried the "obvious" approach but this quickly OOMEs since IW just prints too much stuff ...</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5938</id>
      <title>New DocIdSet implementation with random write access</title>
      <description>We have a great cost API that is supposed to help make decisions about how to best execute queries. However, due to the fact that several of our filter implementations (eg. TermsFilter and BooleanFilter) return FixedBitSets, either we use the cost API and make bad decisions, or need to fall back to heuristics which are not as good such as RandomAccessFilterStrategy.useRandomAccess which decides that random access should be used if the first doc in the set is less than 100. On the other hand, we also have some nice compressed and cacheable DocIdSet implementation but we cannot make use of them because TermsFilter requires a DocIdSet that has random write access, and FixedBitSet is the only DocIdSet that we have that supports random access. I think it would be nice to replace FixedBitSet in those filters with another DocIdSet that would also support random write access but would have a better cost?</description>
      <attachments/>
      <comments>19</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>5941</id>
      <title>IndexWriter.forceMerge documentation error</title>
      <description>IndexWriter.forceMerge documents that it requires up to 3X FREE space in order to run successfully. We even go further with it and test it in TestIWForceMerge.testForceMergeTempSpaceUsage(). But I think that's wrong. I cannot think of a situation where we consume 3X additional space during merge: 1X - that's the source segments to be merged 2X - that's the result non-CFS merged segment 3X - that's the CFS creation At no point do we publish the non-CFS merged segment, therefore the merge, as I understand it, only consumes up to 2X additional space during that merge. And anyway, we only require 2X of additional space of the largest merge (or total batch of running merges, depends on your MergeScheduler), not the whole index size. This is an important observation, since if you e.g. have a 500GB index, users shouldn't think they need to reserve an additional 1TB for merging, since most of their big segments won't be merged by default anyway (TieredMP defaults to 5GB largest segment). I'll post a patch which fixes the documentation and the test. If anyone can think of a scenario where we consume up to 3X additional space, please chime, and I'll only modify IW.forceMerge documentation to explain that.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5944</id>
      <title>move trunk to 6.x, create branch_5x</title>
      <description>In order to actually add real features (as opposed to just spending 24/7 fixing bugs and back compat), I need a trunk that doesn't have the back compat handcuffs. In the meantime, we should rename the current trunk (which is totally tied down in back compat already, without even a single release!) to branch_5x while you guys (i wont be doing any back compat anymore) figure out what you want to do with the back compat policly. Here is the proposal what to do in this issue: http://mail-archives.apache.org/mod_mbox/lucene-dev/201409.mbox/%3CCAOdYfZUpAbYp-omdw=ngJSdzBKVHn2ZYdoBZvj1gDxK+LRT1SQ@mail.gmail.com%3E</description>
      <attachments/>
      <comments>38</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>5951</id>
      <title>Detect when index is on SSD and set dynamic defaults</title>
      <description>E.g. ConcurrentMergeScheduler should default maxMergeThreads to 3 if it's on SSD and 1 if it's on spinning disks. I think the new NIO2 APIs can let us figure out which device we are mounted on, and from there maybe we can do os-specific stuff e.g. look at /sys/block/dev/queue/rotational to see if it's spinning storage or not ...</description>
      <attachments/>
      <comments>40</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>5953</id>
      <title>Refactor LockFactory usage in Directory</title>
      <description>We should remove the setters for the LockFactory from Directory and make the field final. It is a bug to change the LockFactory after creating a directory, because you may break locking (if locks are currently held). The LockFactory should be passed on ctor only. The other suggestion: Should LockFactory have a directory at all? We moved away from having the lock separately from the index directory. This is no longer a supported configuration (since approx Lucene 2.9 or 3.0). I would like to remove the directory from LockFactory and make it part of the Directory only.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5956</id>
      <title>Add runnable index upgrader</title>
      <description>As a spinoff from discussion in LUCENE-5940, I'd like to add a new module "lucene-upgrader", move IndexUpgrader to this, and add embed older versions of lucene (just enough to upgrade indexes) in the built version of the module's jar. This would be runnable from the command line with something like: java -jar lucene-upgrader-4.11.0.jar /path/to/index</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5957</id>
      <title>Add option for tests to not randomize codec</title>
      <description>This is particularly useful when creating the backcompat indexes, since it is a pain to figure out which codec you need to specify to avoid being randomized. Something like -Dtests.codec=default could simply bypass the randomization of the codec.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>5959</id>
      <title>Optimized memory management in Automaton.Builder.finish()</title>
      <description>Reworked Automaton.Builder.finish() to not allocate memory stepwise. Added growTransitions(int numTransitions) to be able to resize the transistions array just once.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5960</id>
      <title>Avoid grow of Set in AnalyzingSuggester.topoSortStates(Automaton)</title>
      <description>Converted "visited" to a BitSet and sized it correctly in AnalyzingSuggester.topoSortStates(Automaton). This avoids dynamic resizing of the set.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5962</id>
      <title>patch creation tool (rename and improve diffSources.py)</title>
      <description>The script diffSources.py is used for creating patches for feature branches. I think the name createPatch.py would be more apt. It also only works with certain file types and is one of the only python scripts written for python 2. I'd like to rename this script, upgrade it to python 3, and fix it to work with all files that git/svn would not ignore.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5963</id>
      <title>Improved AnalyzingSuggester.replaceSep()</title>
      <description>Reworked AnalyzingSuggester.replaceSep() to use Automaton.Builder instead of Automaton. This avoids most of the unnecessary allocation of memory via grow*().</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5964</id>
      <title>Update READ_BEFORE_REGENERATING.txt</title>
      <description>Reading the file READ_BEFORE_REGENERATING.txt from analysis/common/src/java/org/apache/lucene/analysis/standard tells me to use jflex trunk. ant regenerate already uses ivy to get current jflex (1.6) which should be used - does the text still apply or is it obsolete?</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5966</id>
      <title>How to migrate from numeric fields to auto-prefix terms</title>
      <description>In LUCENE-5879 we are adding auto-prefix terms to the default terms dict, which is generalized from numeric fields and offers faster performance while using less indexing space and about the same indexing time. But there are many users out there with indices already created containing numeric fields ... so ideally we have some simple way for such users to switch over to auto-prefix terms. Robert has a good plan (copied from LUCENE-5879): Here are some thoughts. keep current trie "Encoding" for terms, it just uses precision step=Inf and lets the term dictionary do it automatically. create a filteratomicreader, that for a previous trie encoded field, removes "fake" terms on merge. Users could continue to use NumericRangeQuery just with the infinite precision step, and it will always work, just execute slower for old segments as it doesnt take advantage of the trie terms that are not yet merged away. One issue to making it really nice, is that lucene doesnt know for sure that a field is numeric, so it cannot be "full-auto". Apps would have to use their schema or whatever to wrap with this reader in their merge policy. Maybe we could provide some sugar for this, such as a wrapping merge policy that takes a list of field names that are numeric, or sugar to pass this to IWC in IndexUpgrader to force it, and so on.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>5968</id>
      <title>Improve error message when 'ant beast' is run on top-level modules</title>
      <description>As discussed at http://markmail.org/thread/c5y63pmvpgyrmct5 'ant beast' currently gives confusing error messages when run on top-level modules, this makes it clear that it should only be run within a module.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5969</id>
      <title>Add Lucene50Codec</title>
      <description>Spinoff from LUCENE-5952: Fix .si to write Version as 3 ints, not a String that requires parsing at read time. Lucene42TermVectorsFormat should not use the same codecName as Lucene41StoredFieldsFormat It would also be nice if we had a "bumpCodecVersion" script so rolling a new codec is not so daunting.</description>
      <attachments/>
      <comments>105</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5971</id>
      <title>Separate backcompat creation script from adding version</title>
      <description>The recently created bumpVersion.py attempts to create a new backcompat index if the default codec has changed. However, we now want to create a backcompat index for every released version, instead of just when there is a change to the default codec. We should have a separate script which creates the backcompat indexes. It can even work directly on the released artifacts (by pulling down from mirrors once released), so that there is no possibility for generating the index from an incorrect svn/git checkout.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>5972</id>
      <title>Index too old/new is not a corruption</title>
      <description>IndexFormatTooOldException and IndexFormatTooNewException both extend from CorruptIndexException. But this is not a corruption, it is simply an unsupported version of an index. They should just extend IOException.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5979</id>
      <title>Use the cost API instead of a heuristic on the first document in FilteredQuery to decide on whether to use random access</title>
      <description>Now that some major filters such as TermsFilter and MultiTermQueryWrapperFilter return DocIdSets that have a better cost, we should switch to the cost API.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>5983</id>
      <title>RoaringDocIdSet</title>
      <description>Robert pointed me to this paper: http://arxiv.org/pdf/1402.6407v4 that describes an interesting way to build doc id sets: The bit space is divided into blocks of 2^16 bits so that you can store the bits which are set either in a short[] (2 bytes per doc ID) or in a FixedBitSet. The choice is easy, if less than 2^12 bits are set, then the short[] representation is more compact otherwise a FixedBitSet would be more compact. It's quite similar to the way that Solr builds DocSets in SolrIndexSearcher.getDocSet(DocsEnumState).</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5984</id>
      <title>Remove ChainedFilter</title>
      <description>I would like to suggest removing ChainedFilter. It is currently only used in Solr's CurrencyField but could easily be replaced with a BooleanFilter and my understanding of this filter is that it can generally be replaced with a BooleanFilter. So let's drop it and suggest using BooleanFilter instead?</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>5989</id>
      <title>Allow StringField to take BytesRef value, to index a single binary token</title>
      <description>5 years ago (LUCENE-1458) we "enabled" fully binary terms in the lowest levels of Lucene (the codec APIs) yet today, actually adding an arbitrary byte[] binary term during indexing is far from simple: you must make a custom Field with a custom TokenStream and a custom TermToBytesRefAttribute, as far as I know. This is supremely expert, I wonder if anyone out there has succeeded in doing so? I think we should make indexing a single byte[] as simple as indexing a single String. This is a pre-cursor for issues like LUCENE-5596 (encoding IPv6 address as byte[16]) and LUCENE-5879 (encoding native numeric values in their simple binary form).</description>
      <attachments/>
      <comments>21</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>5994</id>
      <title>Rename "reverse" sort parameter to "descending"</title>
      <description>This always causes me confusion every time I look at sorting. What exactly is "natural" order? I can think about it to figure it out (1, 2, 3... is natural), but since the parameter/variable is called "reversed" it always requires looking back at the docs (which then state that the flag is reversing the natural order). I think it would be much clearer to use the ascending/descending terminology. Although I think ascending is better, so that the default would be true, it probably is better to keep the semantics of the boolean the same, thus naming it descending.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6000</id>
      <title>Remove StandardTokenizerInterface</title>
      <description>This interface existed for backcompat, so that each impl had at least some common minimal interface, and could be used by StandardTokenizer. However, in LUCENE-5999 backcompat for standard tokenizer was implemented using separate named classes. We should remove this interface, as it no longer serves a purpose.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6006</id>
      <title>Replace FieldInfo.normsType with FieldInfo.hasNorms boolean</title>
      <description>I came across this precursor while working on LUCENE-6005: I think FieldInfo.normsType can only be null (field did not index norms) or DocValuesType.NUMERIC (it did). I'd like to simplify to just boolean hasNorms. This is a strange boolean, though: in theory it should be derived from indexed &amp;&amp; omitNorms == false, but we have it for the exceptions case where every document in a segment hit an exception and never added norms. I think this is the only reason it exists? (In theory, such cases should result in 100% deleted segments, which IW should then drop ... but seems dangerous to "rely" on that). So I changed the indexing chain to just fill in the default (0) norms for all documents in such exceptional cases; this way going forward (starting with 5.0 indices) we really don't need this hasNorms. But we still need it for pre-5.0 indices...</description>
      <attachments/>
      <comments>17</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6009</id>
      <title>Remove redundant == NO_MORE_DOCS checks</title>
      <description>When I added some new doc id sets, I took inspiration from FixedBitSet which had protection for bad users of its iterator by basically doing if (doc == NO_MORE_DOCS || ++doc &gt;= numBits) { return doc = NO_MORE_DOCS; } in order to not overflow the integer doc when the iterator was already exhausted. However, DocIdSetIterator clearly states that the behaviours of nextDoc() and advance() are undefined when the iterator is already exhausted so we do not need such protection. For the record, I don't expect better performance from this change since the branch is highly predictable (benchmarks seem to support this). At best it might just help inlining by making methods smaller.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6010</id>
      <title>Remove dead code</title>
      <description>For instance OpenBitSet is not used anymore in our code. Let's take advantage of the major version bump to remove unused code.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6013</id>
      <title>Remove IndexableFieldType.indexed()</title>
      <description>Like LUCENE-6006, here's another pre-cursor for LUCENE-6005 ... because I think it's important to nail down Lucene's low-schema (FieldType/FieldInfos) semantics before adding a high-schema. IndexableFieldType.indexed() is redundant with IndexableFieldType.indexOptions() != null, so we should remove it, codecs shouldn't have to write/read it, high-schema should not configure it, etc. Similarly, the FieldInfo.indexed bit is redundant, so I removed it, but I left the sugar API (FieldInfo.isIndexed) and implement it as just checking IndexOptions != null.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6017</id>
      <title>Specialize FixedBitSet.xor(DocIdSetIterator) like and/or/andNot</title>
      <description>As Uwe noticed on LUCENE-5441, FixedBitSet.xor(DocIdSetIterator) doesn't have the same optimization when the provided iterator is a FixedBitSetIterator as other bulk methods (and, or, ...).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6020</id>
      <title>Jenkins should randomize -Dtests.asserts=false|true</title>
      <description>Spinoff from LUCENE-6019. It defaults to true, but we should sometimes pass false.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6021</id>
      <title>Make FixedBitSet and SparseFixedBitSet share a wider common interface</title>
      <description>Today, the only common interfaces that these two classes share are Bits and Accountable. I would like to add a BitSet base class that would be both extended by FixedBitSet and SparseFixedBitSet. The idea is to share more code between these two impls and make them interchangeable for more use-cases so that we could just use one or the other based on the density of the data that we are working on.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6022</id>
      <title>DocValuesDocIdSet: check deleted docs before doc values</title>
      <description>When live documents are not null, DocValuesDocIdSet checks if doc values match the document before the live docs. Given that checking if doc values match could involve a heavy computation (eg. geo distance) and that the default codec has live docs in memory but doc values on disk, I think it makes more sense to check live docs first?</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6024</id>
      <title>Improve oal.util.BitSet's bulk and/or/and_not</title>
      <description>LUCENE-6021 introduced oal.util.BitSet with default impls taken from FixedBitSet. However, these default impls could be more efficient (and eg. perform an actual leap frog for AND and AND_NOT). Additionally, SparseFixedBitSet could benefit from some specialization.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6025</id>
      <title>Add BitSet.prevSetBit</title>
      <description>This would allow the join module to work with any BitSet as opposed to only FixedBitSet.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6030</id>
      <title>Add norms patched compression which uses table for most common values</title>
      <description>We have added the PATCHED norms sub format in lucene 50, which uses a bitset to mark documents that have the most common value (when &gt;97% of the documents have that value). This works well for fields that have a predominant value length, and then a small number of docs with some other random values. But another common case is having a handful of very common value lengths, like with a title field. We can use a table (see TABLE_COMPRESSION) to store the most common values, and save an oridinal for the "other" case, at which point we can lookup in the secondary patch table.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6031</id>
      <title>TokenSources optimization, avoid sort</title>
      <description>TokenSources.java, in the highlight module, is a facade that returns a TokenStream for a field by either un-inverting &amp; converting the TermVector Terms, or by text re-analysis if TermVectors are unavailable or don't have the right options. TokenSources is used by the default highlighter, which is the most accurate highlighter we've got. When documents are large (say hundreds of kilobytes on up), I found that most of the highlighter's activity was up-front spent un-inverting &amp; converting the term vector to a TokenStream, not on the actual/real highlighting that follows. Much of that time was on a huge sort of hundreds of thousands of Tokens. Time was also spent doing lots of String conversion and char copying, and it used a lot of memory, too. In this patch, I overhauled TokenStreamFromTermPositionVector.java, and I removed similar logic in TokenSources that was used in circumstances when positions weren't available but offsets were. This class can un-invert term vectors that have positions and/or offsets (at least one). It doesn't sort. It places Tokens directly into an array of tokens directly indexed by position. When positions aren't available, the startOffset/8 is a substitute. I've got a more light-weight Token inner class used in place of the former and deprecated Token that ultimately forms a linked-list when the process is done. There is no string conversion; character copying is minimized. The Token array is GC'ed after initialization, it's only needed during construction. Misc: It implements reset() efficiently so it need not be wrapped in CachingTokenFilter (I'll supply a patch later on this). It only fetches payloads if you ask for them by adding the attribute (the default highlighter won't add the attribute). It exposes the underlying TermVector terms via a getter too, which is needed by another patch to follow later. A key assumption is that the position increment gap or first position isn't gigantic, as that will create wasted space and the linked-list formation ultimately has to visit all the slots. We also assume that there aren't a ton of tokens at the same position, since inserting new tokens in sorted order is O(N^2) where 'N' is the average co-occurring token length. My performance testing using Lucene's benchmark module on a megabyte document showed &gt;5x speedup, in conjunction with some other patches to be posted separately. This patch made the most difference.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6033</id>
      <title>Add CachingTokenFilter.isCached and switch LinkedList to ArrayList</title>
      <description>CachingTokenFilter could use a simple boolean isCached() method implemented as-such: /** If the underlying token stream was consumed and cached */ public boolean isCached() { return cache != null; } It's useful for the highlighting code to remove its wrapping of CachingTokenFilter if after handing-off to parts of its framework it turns out that it wasn't used. Furthermore, use an ArrayList, not a LinkedList. ArrayList is leaner when the token count is high, and this class doesn't manipulate the list in a way that might favor LL. A separate patch will come that actually uses this method.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6034</id>
      <title>MemoryIndex should be able to wrap TermVector Terms</title>
      <description>The default highlighter has a "WeightedSpanTermExtractor" that uses MemoryIndex for certain queries – basically phrases, SpanQueries, and the like. For lots of text, this aspect of highlighting is time consuming and consumes a fair amount of memory. What also consumes memory is that it wraps the tokenStream in CachingTokenFilter in this case. But if the underlying TokenStream is actually from TokenSources (wrapping TermVector Terms), this is all needless! Furthermore, MemoryIndex doesn't support payloads. The patch here has 3 aspects to it: Internal refactoring to MemoryIndex to simplify it by maintaining the fields in a sorted state using a TreeMap. The ramifications of this led to reduced LOC for this file, even with the other features I added. It also puts the FieldInfo on the Info, and thus there's one less data structure to keep around. I suppose if there are a huge variety of fields in MemoryIndex, the aggregated N*Log(N) field lookup could add up, but that seems very unlikely. I also brought in the MemoryIndexNormDocValues as a simple anonymous inner class - it's super-simple after all, not worth having in a separate file. New MemoryIndex.addField(String fieldName, Terms) method. In this case, MemoryIndex is providing the supporting wrappers around the underlying Terms so that it appears as an Index. In so doing, MemoryIndex supports payloads for such fields. WeightedSpanTermExtractor now detects TokenSources' wrapping of Terms and it supplies this to MemoryIndex.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6039</id>
      <title>Add IndexOptions.NO and DocValuesType.NO, instead of null</title>
      <description>Idea from Simon: it seems dangerous for IndexOptions and DocValuesType via Indexable/FieldType and FieldInfo that we use null to mean it's not indexed or has no doc values. We should instead have an explicit choice (IndexOptions.NO, DocValuesType.NO) in the enum?</description>
      <attachments/>
      <comments>16</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6040</id>
      <title>Speedup broadword bit selection</title>
      <description>Use table lookup instead of some broadword manipulations</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6041</id>
      <title>remove sugar FieldInfo.isIndexed and .hasDocValues</title>
      <description>Follow-on from LUCENE-6039; these two booleans don't really exist: they are just sugar to check for IndexOptions.NO and DocValuesType.NO. I think for the low-level schema API in Lucene we should not expose such sugar: callers should have to be explicit.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6045</id>
      <title>Refator classifier APIs to work better with multi threading</title>
      <description>In https://issues.apache.org/jira/browse/LUCENE-4345?focusedCommentId=13454729&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13454729 Simon Willnauer pointed out that the current Classifier API doesn't work well in multi threading environments: The interface you defined has some problems with respect to Multi-Threading IMO. The interface itself suggests that this class is stateful and you have to call methods in a certain order and at the same you need to make sure that it is not published for read access before training is done. I think it would be wise to pass in all needed objects as constructor arguments and make the references final so it can be shared across threads and add an interface that represents the trained model computed offline? In this case it doesn't really matter but in the future it might make sense. We can also skip the model interface entirely and remove the training method until we have some impls that really need to be trained. I missed that at that point but I think for 6.0 it would be wise to rearrange the API to address that properly.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6053</id>
      <title>Serbian Analyzer</title>
      <description>This is analyzer for Serbian language, so far consisting only of a normalizer. Serbian language uses both Cyrillic and Latin alphabet, so the normalizer works with both alphabets. In the future, I'll see to add stopwords, stemmer and so on.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6057</id>
      <title>Clarify the Sort(SortField...) constructor)</title>
      <description>I don't really know which version this affects, but I clarified the documentation of the Sort(SortField...) constructor to ease the understanding for new users. Pull Request: https://github.com/apache/lucene-solr/pull/20</description>
      <attachments/>
      <comments>16</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6058</id>
      <title>Changes to Beider-Morse Encoder in latest commons-codec 1.10 release</title>
      <description>We have just recently released commons-codec 1.10 and I wanted to inform you that there have been slight changes to the Beider Morse encoder. This might result in slightly different encodings (see CODEC-187). At least one unit test has to be adjusted when updating. The results of the Beider Morse encoder are now identical to the reference implementation of the algorithm (v3.4, http://stevemorse.org/census/soundex.html), it might be necessary to inform users before upgrading as they might have to re-index to get consistent results.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6059</id>
      <title>Add Daitch-Mokotoff Soundex phonetic filter from latest commons-codec 1.10 release</title>
      <description>The latest commons-codec release (1.10) has added a new phonetic encoder: Daitch-Mokotoff Soundex.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6063</id>
      <title>Allow overriding ConcurrentMergeScheduler's denial-of-service protection</title>
      <description>In LUCENE-5310 we explored improving CMS/SMS sharing/concurrency, but the issue never "converged", so I want to break out one small part of it here: the ability to override CMS's default "aggressive" denial-of-service protection where it forcefully stalls the incoming threads that are responsible for creating too many segments. More advanced applications can more gracefully handle the "too many merges" by e.g. slowing down the incoming indexing rate at a higher level.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6066</id>
      <title>Collector that manages diversity in search results</title>
      <description>This issue provides a new collector for situations where a client doesn't want more than N matches for any given key (e.g. no more than 5 products from any one retailer in a marketplace). In these circumstances a document that was previously thought of as competitive during collection has to be removed from the final PQ and replaced with another doc (eg a retailer who already has 5 matches in the PQ receives a 6th match which is better than his previous ones). This requires a new remove method on the existing PriorityQueue class.</description>
      <attachments/>
      <comments>31</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6077</id>
      <title>Add a filter cache</title>
      <description>Lucene already has filter caching abilities through CachingWrapperFilter, but CachingWrapperFilter requires you to know which filters you want to cache up-front. Caching filters is not trivial. If you cache too aggressively, then you slow things down since you need to iterate over all documents that match the filter in order to load it into an in-memory cacheable DocIdSet. On the other hand, if you don't cache at all, you are potentially missing interesting speed-ups on frequently-used filters. Something that would be nice would be to have a generic filter cache that would track usage for individual filters and make the decision to cache or not a filter on a given segments based on usage statistics and various heuristics, such as: the overhead to cache the filter (for instance some filters produce DocIdSets that are already cacheable) the cost to build the DocIdSet (the getDocIdSet method is very expensive on some filters such as MultiTermQueryWrapperFilter that potentially need to merge lots of postings lists) the segment we are searching on (flush segments will likely be merged right away so it's probably not worth building a cache on such segments)</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6083</id>
      <title>Span containing/contained queries</title>
      <description>SpanContainingQuery reducing a spans to where it is containing another spans. SpanContainedQuery reducing a spans to where it is contained in another spans.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6084</id>
      <title>Add reasonable IndexOutput.toString</title>
      <description>In LUCENE-3539 we fixed IndexInput.toString to always include the resourceDescription. I think we should do the same for IndexOutput? I don't think Lucene currently uses/relies on IndexOutput.toString, but e.g. at least Elasticsearch does, and likely others, so I think it can only help if you can see which path is open by this IndexOutput.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6086</id>
      <title>Minor optimizations to the encoding of numerics in stored fields</title>
      <description>LUCENE-5914 explored minor optimizations for numeric encoding in stored fields, I think we should try to push them to the default format.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6087</id>
      <title>SearcherManager should accept foreign DirectoryReader on init</title>
      <description>Today you init SearcherManager either with a dir or an IndexWriter, but since we have a useful FilterDirectoryReader class to wrap the sub-readers, it's useful for apps to also pass their own wrapped DirectoryReader and have SearcherManager reopen from that.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6088</id>
      <title>Make TermsFilter implement Accountable</title>
      <description>Terms filters can sometimes be massive. Having their memory usage exposed can be useful eg. for the FilterCache.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6090</id>
      <title>don't wrap Deflater in CompressionMode</title>
      <description>This outputs zlib header/footer and computes adler32 for each block. The space is nothing, but the adler32 computation on encode/decode has a cost, and we already have our own checksum. Since we currently compress/decompress at merge, this reduces the overall time of merging stored fields with deflate vs lz4, from 1.8x to 1.5x, reducing some of the pain.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6095</id>
      <title>Remove RamUsageTester.IdentityHashSet</title>
      <description>RamUsageTester has an implementation of an identity hash set. I am not sure why it is there but now that RamUsageTester can only be used in the context of tests (ie. memory usage or speed are less of a concern) I think we can switch to Collections.newSetFromMap(new IdentityHashMap&lt;&gt;())?</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6096</id>
      <title>Improve FilterCache.ramBytesUsed</title>
      <description>I worked on some tests to improve ram usage estimation of LRUFilterCache and found some issues: 1. It currently only takes into account filters and DocIdSets, not the internal overhead of the various hash tables and linked lists that are being maintained. If the filter cache mostly stores sparse doc id sets (or even worse DocIdSet.EMPTY instances that require 0 bytes since it's a singleton) then the memory usage can be significantly under-estimated. 2. It treats all filters as if they were singletons. The filter cache is essentially a Map&lt;SegmentCoreReaders, Map&lt;Filter, DocIdSet&gt;&gt; and it treats filters as if all filters that are equal are the same instance when it comes to memory usage. But this is not the case actually since the map on the second level uses the first-seen filter as a key, which might be different on different segments because of merging.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6099</id>
      <title>Add FilterDirectory.unwrap and FilterDirectoryReader.unwrap</title>
      <description>We already have the useful FilterLeafReader.unwrap, but not for these other filter classes....</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6100</id>
      <title>Further tuning of Lucene50Codec(BEST_COMPRESSION)</title>
      <description>Currently this codec has two options: BEST_SPEED and BEST_COMPRESSION. But in the case of highly compressible data, the ratio for BEST_COMPRESSION is not much over BEST_SPEED, because they share the same underlying format which is not optimized for this here. block size is currently 24576 (32kb sliding window size minus 8kb "grace" to avoid going over it). And we compress this in a stateless manner, each block is its own stream and they dont share preset dictionary or anything. So we have a lot of waste in many cases, since zlib has to reboot itself, then we generally throw away 1/4 of the window and start over. I ran some experiments with highly compressible logs data: method time indexing(ms) time merging(ms) fdt fdx BEST_SPEED 101,729 15,638 372,845,282 406,964 BEST_COMPRESSION 114,364 23,474 269,387,347 275.909 patch (60KB) 105,533 18,914 237,284,342 117,639 The other experiments I ran were: method time indexing(ms) time merging(ms) fdt fdx crappy preset 130,854 38,095 234,603,971 274,500 64KB 107,256 21,570 236,004,297 111,135 crappy preset+64KB 121,503 30,030 222,422,924 110,751 For 'crappy preset' I just use arbitrary first 32KB bytes of original data as a preset dictionary for every block. This is effective, but slow because of some unnecessary overhead involved (like computing adler32 over and over of the preset dict for each block). However, this overhead is reduced with larger block sizes, and still offers benefits, so maybe in the future we can do it (especially e.g. if its per-chunk and we can bulk merge chunks without recompressing, etc). For 64KB, we measure removing the "grace" completely so it spills to another block each time. The proposed smaller "grace" amount still offers cpu savings, so I think we should keep it. But its not terrible if you go over.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6101</id>
      <title>Allow the SlowCompositeReaderWrapper and MultiDocValues to accept acceptableOverheadRatio for OrdinalMap</title>
      <description>There are use cases where it's useful to be able to trade off performance and memory when accessing top level DocValues ordinals. This issue adds methods to the SlowCompositeReaderWrapper and MultiDocValues that allow an acceptableOverheadRatio to be passed in and applied to the OrdinalMapping. If people are ok with this approach I will add tests and performance benchmarks.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6104</id>
      <title>simplify internals of Lucene50NormsProducer</title>
      <description>This is tracking additional data structures, and has a lot of complexity, when we could just refactor the internal structure to be a bit cleaner. as a bonus, its less memory overhead, but a more thorough memory tree: it works like the docvalues one now.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6106</id>
      <title>Improve FilterCachingPolicy statistics computation</title>
      <description>Currently FilterCachingPolicy.onCache is supposed to be called every time that FilterCache.onCache is used. However, this does not necessarily reflect how much a filter is used. For instance you can call cache and not use the filter, or call cache once and then use it a hundred times. It would be more useful to know how many times a filter has been used on a top level reader, and I think we can do this by doing something like below in the caching wrapper filter? @Override public DocIdSet getDocIdSet(LeafReaderContext context, Bits acceptDocs) throws IOException { if (context.ord == 0) { // increment counter } }</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6107</id>
      <title>Add statistics to LRUFilterCache</title>
      <description>It would be useful to have statistics about the usage of the filter cache to figure out whether the cache is useful at all and to help tune filter caching policies.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6115</id>
      <title>Add getMergeInstance to CompressingStoredFieldsReader</title>
      <description>CompressingStoredFieldsReader is currently terrible at merging with different codecs or wrapped readers since it does not keep state. So if you want to get 5 documents that come from the same block, it means that you will have to decode the block header and decompress 5 times. It has some optimizations so that if you want to get the 2nd doc of the block then it will stop decompressing soon after the 2nd document, but it doesn't help much with merging since we want all documents. We should implement getMergeInstance and have a different behaviour when merging by decompressing everything up-front and then reusing for all documents of the block.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6116</id>
      <title>Simplify RoaringDocIdSet.Iterator constructor</title>
      <description>The constructor duplicates the logic from firstDocFromNextBlock whichs looks for the next block that contains at least one document.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6118</id>
      <title>Improve efficiency of the history structure for filter caching</title>
      <description>The filter caching uses a ring buffer that tracks frequencies of the hashcodes of the most-recently used filters. However it is based on an ArrayDeque&lt;Integer&gt; and a HashMap&lt;Integer&gt; which keep on (un)wrapping ints. Since the data-structure is very simple, we could try to do something better...</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6119</id>
      <title>Add auto-io-throttle to ConcurrentMergeScheduler</title>
      <description>This method returns number of "incoming" bytes IW has written since it was opened, excluding merging. It tracks flushed segments, new commits (segments_N), incoming files/segments by addIndexes, newly written live docs / doc values updates files. It's an easy statistic for IW to track and should be useful to help applications more intelligently set defaults for IO throttling (RateLimiter). For example, an application that does hardly any indexing but finally triggered a large merge can afford to heavily throttle that large merge so it won't interfere with ongoing searches. But an application that's causing IW to write new bytes at 50 MB/sec must set a correspondingly higher IO throttling otherwise merges will clearly fall behind.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6121</id>
      <title>Fix CachingTokenFilter to propagate reset() the first time</title>
      <description>CachingTokenFilter should have been propagating reset() but only the first time and thus you would then use CachingTokenFilter in a more normal way – wrap it and call reset() then increment in a loop, etc., instead of knowing you need to reset() on what it wraps but not this token filter itself. That's weird. It's ab-normal for a TokenFilter to never propagate reset, so every user of CachingTokenFilter to date has worked around this by calling reset() on the underlying input instead of the final wrapping token filter (CachingTokenFilter in this case).</description>
      <attachments/>
      <comments>22</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6128</id>
      <title>Add stricter ant/build option for developers?</title>
      <description>Can we add a build option (-Dpendantic=true) that is more anal? E.g. on backporting a change from trunk (java 8) to 5.x (java 7) I very rarely remember to "use java 7" to compile/test. Sure, we set javac.source/target to 1.7, but that's not perfect (e.g. an interface w/ a default method as of java 8). With this option set we could do things like require you are using java 7 on 5.x, fail on warnings, etc. Casual users/developers can then still use java 8 to run tests, etc. (the option would default to off).</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6129</id>
      <title>Assert that terms enums, docs enums and doc values are not leaked across threads</title>
      <description>Terms enums, docs enums and doc values should only be consumed in the thread that they have been acquired in. We could add checks to Asserting* to make sure they never leak?</description>
      <attachments/>
      <comments>11</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6130</id>
      <title>Solr partial search</title>
      <description>I am using SOLR version 4.7. when i search result with multiple combination of keyword. solr should give only matched result, Currently it is giving matched result as well as individual single keyword result. EXample: "black diamond Luxurman". when i search result from this keyword. SOLR gives result by first result from all three combine keyword then it is giving result follow by black, diamond and Luxurman keyword. I am wanting solr give result from only combine word and not break the word. Let me know. Thanks Arvind</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6142</id>
      <title>Faster merging with Lucene41StoredFieldsFormat</title>
      <description>This format is slow at merging with foreign codecs so we could implement getMergeInstance on it too (like on LUCENE-6115) to make the upgrade to the 5.0 codec easier.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6145</id>
      <title>Make EarlyTerminatingSortingCollector smarter about when it can early terminate</title>
      <description>Today EarlyTerminatingSortingCollector only early-terminates if the sort order matches exactly the index-time sort order. It should also early-terminate when the sort order is a prefix of the index-time sort order.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6148</id>
      <title>Accountable.getChildResources should return a Collection</title>
      <description>Since the child resources must be a snapshot, their size has to be known anyway so returning a collection instead of an iterable would make consumption easier without introducing limitations.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6149</id>
      <title>Infix suggesters' highlighting, allTermsRequired options are hardwired and not configurable for non-contextual lookup</title>
      <description>Highlighting and allTermsRequired are hardwired in AnalyzingInfixSuggester for non-contextual lookup (via Lookup) see true, true below: AnalyzingInfixSuggester.java (extends Lookup.java) public List&lt;LookupResult&gt; lookup(CharSequence key, Set&lt;BytesRef&gt; contexts, boolean onlyMorePopular, int num) throws IOException { return lookup(key, contexts, num, true, true); } /** Lookup, without any context. */ public List&lt;LookupResult&gt; lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException { return lookup(key, null, num, allTermsRequired, doHighlight); } Lookup.java public List&lt;LookupResult&gt; lookup(CharSequence key, boolean onlyMorePopular, int num) throws IOException { return lookup(key, null, onlyMorePopular, num); } The above means the majority of the current infix suggester lookup always return highlighted results with allTermsRequired in effect. There is no way to change this despite the options and improvement of LUCENE-6050, made to incorporate Boolean lookup clauses (MUST/SHOULD). This shortcoming has also been reported in SOLR-6648. The suggesters (AnalyzingInfixSuggester, BlendedInfixSuggester) should provide a proper mechanism to set defaults for highlighting and "allTermsRequired", e.g. in constructors (and in Solr factories, thus configurable via solrconfig.xml).</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6155</id>
      <title>Payload support for MemoryIndex</title>
      <description>MemoryIndex could be enhanced to support payloads. It should be optional, defaulting to false.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6157</id>
      <title>Add the ability to compute fine-grained statistics on the filter cache</title>
      <description>The filter cache exposes some useful statistics about its usage, eg. hit count, eviction count, etc. In some cases it could be useful to give users the ability to compute finer-grained statistics though, for example by breaking up statistics by segment, index or by type of filter.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6160</id>
      <title>Add whitespace to temp test folder names</title>
      <description>Currently we rely too much on the fact that Uwe Schindler likes to run lucene tests with c:\Users\Uwe Schindler\... to find bugs. Instead we should just turn one of the dashes into a space so problems fail everywhere i think?</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6163</id>
      <title>Use NGramPhraseQuery via QueryBuilder</title>
      <description>This is derived from SOLR-3055. To gain performance improvement of LUCENE-3426, QueryBuilder should generate NGramPhraseQuery to optimize n-gram based phrase query. (especially good for CJK languages, but the benefit is not limited to those.)</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6167</id>
      <title>Speed up SortingMergePolicy by string</title>
      <description>Building the sorted docmaps can take a nontrivial amount of time, for String we currently don't do a very good job.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6172</id>
      <title>Improve the in-order / out-of-order collection decision process</title>
      <description>Today the logic is the following: IndexSearcher looks if the weight can score out-of-order Depending on the value it creates the appropriate top docs/field collector I think this has several issues: Only IndexSearcher can actually make the decision correctly, and it only works for top docs/field collectors. If you want to make a multi collector in order to have both facets and top docs, then you're clueless about whether you should create a top docs collector that supports out-of-order collection It is quite fragile: you need to make sure that Weight.scoresDocsOutOfOrder and Weight.bulkScorer agree on when they can score out-of-order. Some queries like BooleanQuery duplicate the logic and other queries like FilteredQuery just always return true to avoid complexity. This is inefficient as this means that IndexSearcher will create a collector that supports out-of-order collection while the common case actually scores documents in order (leap frog between the query and the filter). Instead I would like to take advantage of the new collection API to make out-of-order scoring an implementation detail of the bulk scorers. My current idea is as follows: remove Weight.scoresDocsOutOfOrder change Collector.getLeafCollector(LeafReaderContext) to Collector.getLeafCollector(LeafReaderContext, boolean canScoreOutOfOrder) This new boolean in Collector.getLeafCollector tells the collector that the scorer supports out-of-order scoring. So by returning a leaf collector that supports out-of-order collection, things will be faster. The new logic would be the following. First Weights cannot tell whether they support out-of-order scoring or not. However when a weight knows it supports out-of-order scoring, it will pass canScoreOutOfOrder=true when getting the leaf collector. If the returned collector accepts documents out of order, then the weight will return an out-of order scorer. Otherwise, an in-order scorer is returned.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6174</id>
      <title>Improve "ant eclipse" to select right JRE for building</title>
      <description>Whenever I run "ant eclipse" the setting choosing the right JVM is lost and has to be reassigned in the project properties. In fact the classpath generator writes a new classpath file (as it should), but this onl ycontains the "default" entry: &lt;classpathentry kind="con" path="org.eclipse.jdt.launching.JRE_CONTAINER"/&gt; Instead it should preserve something like: &lt;classpathentry kind="con" path="org.eclipse.jdt.launching.JRE_CONTAINER/org.eclipse.jdt.internal.debug.ui.launcher.StandardVMType/jdk1.8.0_25"/&gt; We can either path this by a Ant property via command line or user can do this with "lucene/build.properties" or per user. An alternative would be to generate the name "jdk1.8.0_25" by guessing from ANT's "java.home". If this name does not exist in eclipse it would produce an error and user would need to add the correct JDK. I currently have the problem that my Eclipse uses Java 7 by default and whenever I rebuild the eclipse project, the change to Java 8 in trunk is gone. When this is fixed, I could easily/automatically have the "right" JDK used by eclipse for trunk (Java 8) and branch_5x (Java 7).</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6176</id>
      <title>Modify FSIndexOutput in FSDirectory to open output steam for Write and Read</title>
      <description>The FSIndexOutput, in FSDirecotry, opens the output file stream for Write/Append (W/A), but no Read. This is an issue when Windos wites to remote files. For local storage files the Windows cache manager is part of the kernel and can read from the file even if it is opened for W/A only (and it needs to read the current content of the page). When accessing remote files, like SMB shares, the cache manager is restricted to the access mode requested from the remote system. In this case since it is W/A every write, even a single byte, is a roundtrip to the remote storage server. Openning the output file stream for Write and Read, which does not impact other functionality, allows Windows to cache the individual Lucene writes regadless of their size</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6177</id>
      <title>Add CustomAnalyzer - a builder that creates Analyzers from the factory classes</title>
      <description>I prepared some "generic Analyzer class CustomAnalyzer, that makes it easy to build analyzers like in Solr or Elasticsearch. Under the hood it uses the factory classes. The class is made like a builder: Analyzer ana = CustomAnalyzer.builder(Path.get("/path/to/config/dir")) .withTokenizer("standard") .addTokenFilter("standard") .addTokenFilter("lowercase") .addTokenFilter("stop", "ignoreCase", "false", "words", "stopwords.txt", "format", "wordset") .build(); It is possible to give the resource loader (used by stopwords and similar). By default it tries to load stuff from context classloader (without any class as reference so paths must be absolute - this is the behaviour ClasspathResourseLoader defaults to). In addition you can give a Lucene MatchVersion, by default it would use Version.LATEST (once LUCENE-5900 is completely fixed).</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6178</id>
      <title>don't score MUST_NOT clauses with BooleanScorer</title>
      <description>Its similar to the conjunction case: we should just use BS2 since it has advance(). Even in the dense case I think its currently better since it avoids calling score() in cases where BS1 calls it redundantly.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6179</id>
      <title>Remove out-of-order scoring</title>
      <description>Out-of-order currently adds complexity that I would like to remove. Here is a selection of issues that come from out-of-order scoring. lots of specializations with collectors: we have two versions of every top score/field collector depending on whether it should support out-of-order collection or not it feels like it should be an implementation detail of our bulk scorers but it also makes our APIs more complicated, eg. LeafCollector.acceptsDocsOutOfOrder if you create a TopFieldCollector, how do you know if you should pass docsScoredInOrder=true or false? To make the decision, you actually need to know whether your query supports out-of-order scoring while the API is on Weight. I initially wanted to keep it and improve the decision process in LUCENE-6172 but I'm not sure it's the right approach as it would require to make the API even more complicated... hence the suggestion to remove out-of-order scoring completely.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6180</id>
      <title>Make BitsSlice public (not package-private)</title>
      <description>org.apache.lucene.index.BitsSlice is a simple and generic utility class. I've found a need to use and it'd be nice if it were public but it's currently package-private. It should go to the util package, and the constructor taking ReaderSlice cold be removed. In Java 8, it'd be neat to have a Bits.subSlice default method simply calling-out to this but no big deal.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6181</id>
      <title>Move spatial pointsOnly from RPT to superclass - PrefixTreeStrategy</title>
      <description>The 'points only' hint should be at PrefixTreeStrategy, not at RPT. The Term strategy subclass may not use it (yet) but conveys intent and prevents a needless cast in faceting on PrefixTreeStrategy generally in a separate issue. The attached patch also adds some getters for good measure.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6182</id>
      <title>Spatial VisitorTemplate.visitScanned needn't be abstract</title>
      <description>visitScanned can be implemented, allowing subclasses to specialize if desired. protected void visitScanned(Cell cell) throws IOException { if (queryShape.relate(cell.getShape()).intersects()) { if (cell.isLeaf()) { visitLeaf(cell); } else { visit(cell); } } } Then I can remove Intersect's impl, and remove the one prefix-tree faceting. Additionally, I noticed collectDocs(FixBitSet) can be improved to take BitSet and call bitSet.or(docsEnum)</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6183</id>
      <title>Avoid re-compression on stored fields merge</title>
      <description>We removed this optimization before, it didnt really work right because it required things to be "aligned". But I think we can do it simpler and safer. This recompression is a big cpu hog in merge, and limits our options compression-wise (especially ones like LZ4-HC that are only slower at write-time).</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6184</id>
      <title>BooleanScorer should better deal with sparse clauses</title>
      <description>The way that BooleanScorer works looks like this: for each (window of 2048 docs) { for each (optional scorer) { scorer.score(window) } } This is not efficient for very sparse clauses (doc freq much lower than maxDoc/2048) since we keep on scoring windows of documents that do not match anything. BooleanScorer2 currently performs better in those cases.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6185</id>
      <title>Fix IndexSearcher with threads to not collect documents out of order</title>
      <description>When created with an executor, IndexSearcher searches all leaves in a different task and eventually merges the results when all tasks are completed. However, this merging logic involves a TopFieldCollector which is collected out-of-order. I think it should just use TopDocs.merge?</description>
      <attachments/>
      <comments>14</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6187</id>
      <title>explore symmetic docvalues pull API</title>
      <description>Currently the DocValuesConsumer and NormsConsumer have a streaming pull API based on Iterable. addNumericField(FieldInfo field, Iterable&lt;Number&gt; values) ... addSortedSetField(FieldInfo field, Iterable&lt;BytesRef&gt; values, Iterable&lt;Number&gt; docToOrdCount, Iterable&lt;Number&gt; ords) I think this was a good initial approach, but it has a few downsides: for more complex structures (sorted/sortedset/sortednumeric) the codec must awkwardly handle multiple streams and sometimes inefficiently do extra passes. thousands of lines of XXXDocValues &lt;-&gt; Iterable bridge handling in merge code (when MultiDocValues already knows how to merge multiple subs) missing values represented as null is awkward, complicated and a little trappy on the consumer. I think we should explore changing it to look more like postings: addNumericField(FieldInfo field, NumericDocValues values, Bits docsWithField) addSortedSetField(FieldInfo field, SortedSetDocValues values, Bits docsWithField) I don't think it would be hard on the implementation: e.g. when I look at IndexWriter it seems like these would even be simpler code than the current iterators (e.g. for numerics its already got a NumericDocValues and a Bits docsWithField, the current iterable stuff is just "extra" bridge code like merging). My main concern is if it makes things easier on the codec impls or not. I think we have to try it out to see. We could test it out on trunk with just NormsConsumer.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6188</id>
      <title>Remove HTML verification from checkJavaDocs.py</title>
      <description>Currently, the broken HTML verification in checkJavaDocs.py has issues in some cases (see SOLR-6902). On looking further to fix it with the html.parser package instead, noticed that there is broken HTML verification already present (using html.parser!)in checkJavadocLinks.py anyway which takes care of validation, and probably jTidy does it as well, going by the output (haven't verified it). Given this, the validation in checkJavaDocs.py doesn't seem to add any further value, so here's a patch to just nuke it instead.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6190</id>
      <title>spatial pointsOnly flag shouldn't force predicate to Intersects</title>
      <description>In the process of testing the pointsOnly flag, I realized RPT's optimization to force the predicate to Intersects from Within|Contains isn't sound. In the case of Within, this is only valid if there is one point per document but not multiple (since all points on a doc need to intersect the query shape), and for Contains it was simply wrong. Note that the strategy has no multi-valued hint or some-such. If it did, then if !multiValued &amp;&amp; pointsOnly, then Within could be changed to Intersects. Regardless, swapping the predicate can be done at a higher level (Solr/ES).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6191</id>
      <title>Spatial 2D faceting (heatmaps)</title>
      <description>Lucene spatial's PrefixTree (grid) based strategies index data in a way highly amenable to faceting on grids cells to compute a so-called heatmap. The underlying code in this patch uses the PrefixTreeFacetCounter utility class which was recently refactored out of faceting for NumberRangePrefixTree LUCENE-5735. At a low level, the terms (== grid cells) are navigated per-segment, forward only with TermsEnum.seek, so it's pretty quick and furthermore requires no extra caches &amp; no docvalues. Ideally you should use QuadPrefixTree (or Flex once it comes out) to maximize the number grid levels which in turn maximizes the fidelity of choices when you ask for a grid covering a region. Conveniently, the provided capability returns the data in a 2-D grid of counts, so the caller needn't know a thing about how the data is encoded in the prefix tree. Well almost... at this point they need to provide a grid level, but I'll soon provide a means of deriving the grid level based on a min/max cell count. I recommend QuadPrefixTree with geo=false so that you can provide a square world-bounds (360x360 degrees), which means square grid cells which are more desirable to display than rectangular cells.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6195</id>
      <title>Use pegdown to generate some documentation files</title>
      <description>Spinning this out of SOLR-6870 for Uwe's request to do a similar pegdown'ing of Lucene's site files.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6196</id>
      <title>Include geo3d package, along with Lucene integration to make it useful</title>
      <description>I would like to explore contributing a geo3d package to Lucene. This can be used in conjunction with Lucene search, both for generating geohashes (via spatial4j) for complex geographic shapes, as well as limiting results resulting from those queries to those results within the exact shape in highly performant ways. The package uses 3d planar geometry to do its magic, which basically limits computation necessary to determine membership (once a shape has been initialized, of course) to only multiplications and additions, which makes it feasible to construct a performant BoostSource-based filter for geographic shapes. The math is somewhat more involved when generating geohashes, but is still more than fast enough to do a good job.</description>
      <attachments/>
      <comments>105</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>6198</id>
      <title>two phase intersection</title>
      <description>Currently some scorers have to do a lot of per-document work to determine if a document is a match. The simplest example is a phrase scorer, but there are others (spans, sloppy phrase, geospatial, etc). Imagine a conjunction with two MUST clauses, one that is a term that matches all odd documents, another that is a phrase matching all even documents. Today this conjunction will be very expensive, because the zig-zag intersection is reading a ton of useless positions. The same problem happens with filteredQuery and anything else that acts like a conjunction.</description>
      <attachments/>
      <comments>52</comments>
      <commenters>13</commenters>
    </issue>
    <issue>
      <id>6199</id>
      <title>Reduce per-field heap usage for indexed fields</title>
      <description>Lucene uses a non-trivial baseline bytes of heap for each indexed field, and I know it's abusive for an app to create 100K indexed fields but I still think we can and should make some effort to reduce heap usage per unique field? E.g. in block tree we store 3 BytesRefs per field, when 3 byte[]s would do...</description>
      <attachments/>
      <comments>19</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6201</id>
      <title>MinShouldMatchSumScorer should advance less and score lazily</title>
      <description>MinShouldMatchSumScorer currently computes the score eagerly, even on documents that do not eventually match if it cannot find minShouldMatch matches on the same document.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6209</id>
      <title>IndexWriter should confess when it stalls flushes</title>
      <description>You tell IW how much RAM it's allowed to use to hold recently indexed documents before they must be written to disk. IW is willing to use up to 2X that amount for in-progress flushes. If the in-progress flushes go over that limit, then IW will stall them, hijacking indexing threads and having them wait until the in-progress flushes are below 2X indexing buffer size again. This is important back-pressure e.g. if you are indexing on a machine with many cores but slowish IO. Often when I profile an indexing heavy use case, even on fast IO (SSD) boxes, I see the methods associated with this back-pressure taking unexpected time ... yet IW never logs when it stalls/unstalls flushing. I think it should.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6212</id>
      <title>Remove IndexWriter's per-document analyzer add/updateDocument APIs</title>
      <description>IndexWriter already takes an analyzer up-front (via IndexWriterConfig), but it also allows you to specify a different one for each add/updateDocument. I think this is quite dangerous/trappy since it means you can easily index tokens for that document that don't match at search-time based on the search-time analyzer. I think we should remove this trap in 5.0.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>10</commenters>
    </issue>
    <issue>
      <id>6215</id>
      <title>Move NumberRangePrefixTreeStrategy to correct package</title>
      <description>I completely overlooked that NumberRangePrefixTreeStrategy is in the wrong package. It's at the top level of the spatial package org.apache.lucene.spatial instead of being in the prefix sub-package. Doh! As soon as LUCENE-5735 finishes up, I'll do an 'svn move', and then add a same-named subclass to where it is now extending the real one, and deprecate it of course.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6216</id>
      <title>Make it easier to modify Japanese token attributes downstream</title>
      <description>Japanese-specific token attributes such as PartOfSpeechAttribute, BaseFormAttribute, etc. get their values from a org.apache.lucene.analysis.ja.Token through a setToken() method. This makes it cumbersome to change these token attributes later on in the analysis chain since the Token instances are difficult to instantiate (sort of read-only objects). I've ran into this issue in LUCENE-3922 (JapaneseNumberFilter) where it would be appropriate to update token attributes to also reflect Japanese number normalization. I think it might be more practical to allow setting a specific value for these token attributes directly rather than through a Token since it makes the APIs simpler, allows for easier changing attributes downstream, and also supporting additional dictionaries easier. The drawback with the approach that I can think of is a performance hit as we will miss out on the inherent lazy retrieval of these token attributes from the Token object (and the underlying dictionary/buffer). I'd like to do some testing to better understand the performance impact of this change. Happy to hear your thoughts on this.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6219</id>
      <title>Remove ScoringNoMaxScoreCollector</title>
      <description>ScoringNoMaxScoreCollector does like NonScoringCollector except that it computes the score for all competitive hits in order to have the score set on the final top hits. This is inefficient since there are potentially many more competitive documents than the requested number of hits, especially if the index order is not random. We could instead compute the top hits with NonScoringCollector and then compute scores on these top hits only. Another nice side-effect is that it would help get rid of a TopFieldCollector specialization.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6220</id>
      <title>Move needsScores from Weight.scorer to Query.createWeight</title>
      <description>Whether scores are needed is currently a Scorer-level property while it should actually be a Weight thing I think?</description>
      <attachments/>
      <comments>19</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6222</id>
      <title>Remove TermFilter</title>
      <description>It used to be better than a QueryWrapperFilter(TermQuery) by not decoding freqs but it is not the case anymore since LUCENE-6218</description>
      <attachments/>
      <comments>10</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6225</id>
      <title>Clarify documentation of clone() in IndexInput</title>
      <description>Here is a snippet from IndexInput's documentation: The original instance must take care that cloned instances throw AlreadyClosedException when the original one is closed. But concrete implementations don't throw this AlreadyClosedException (this would break the contract on Closeable). For example, see NIOFSDirectory: public void close() throws IOException { if (!isClone) { channel.close(); } } What trapped me was that the abstract class IndexInput overrides the default implementation of clone(), but doesn't do anything useful... I guess you could make it final and provide the tracking for cloned instances in this class rather than reimplementing it everywhere else (isCloned() would be a superclass method then too). Thoughts?</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6226</id>
      <title>Add interval iterators to Scorer</title>
      <description>This change will allow Scorers to expose which positions within a document they have matched, via a new IntervalIterator interface. Consumers get the iterator by calling intervals() on the Scorer, then call reset(docId) whenever the scorer has advanced and nextInterval() to iterate through positions. Once all matching intervals on the current document have been exhausted, nextInterval() returns false.</description>
      <attachments/>
      <comments>44</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6227</id>
      <title>Add BooleanClause.Occur.FILTER</title>
      <description>Now that we have weight-level control of whether scoring is needed or not, we could add a new clause type to BooleanQuery. It would behave like MUST exept that it would not participate in scoring. Why do we need it given that we already have FilteredQuery? The idea is that by having a single query that performs conjunctions, we could potentially take better decisions. It's not ready to replace FilteredQuery yet as FilteredQuery has handling of random-access filters that BooleanQuery doesn't, but it's a first step towards that direction and eventually FilteredQuery would just rewrite to a BooleanQuery. I've been calling this new clause type FILTER so far, but feel free to propose a better name.</description>
      <attachments/>
      <comments>29</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>6228</id>
      <title>Do not expose full-fledged scorers in LeafCollector.setScorer</title>
      <description>Currently LeafCollector.setScorer takes a Scorer, which I don't like because several methods should never be called in the context of a Collector (like nextDoc or advance). I think it's even more trappy for methods that might seem to work in some particular cases but will not work in the general case, like getChildren which will not work if you have a specialized BulkScorer or iterating over positions which will not work if you are in a MultiCollector and another leaf collector consumes positions too. So I think we should restrict what can be seen from a collector to avoid such traps.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6229</id>
      <title>Remove Scorer.getChildren?</title>
      <description>This API is used in a single place in our code base: ToParentBlockJoinCollector. In addition, the usage is a bit buggy given that using this API from a collector only works if setScorer is called with an actual Scorer (and not eg. FakeScorer or BooleanScorer like you would get in disjunctions) so it needs a custom IndexSearcher that does not use the BulkScorer API.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>6232</id>
      <title>Replace ValueSource context Map with a more concrete data type</title>
      <description>Inspired by LUCENE-3973 The context object used by ValueSource and friends is a raw Map that provides no type safety guarantees. In our current state, there are lots of warnings about unchecked casts, raw types, and generally unsafe code from the compiler's perspective. There are several common patterns and types of Objects that we store in the context. It would be beneficial to instead use a class with typed methods for get/set of Scorer, Weights, etc.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6234</id>
      <title>harden smokeTestRelease.py's expectations regarding CWD</title>
      <description>smokeTestRelease.py changed at some point to require/expect that you run it from the root of the checkout of the branch you are smoke testing, per rernst's comments in mail... That part of the script expects you run from the root of a checkout. It runs the backcompat tests, and scrapes the test output to check all are tested. The context was that i (out of habit from older versions of this script) was trying to do this in my personal tmp space (expecting the script to create it's RC specific tmp dir in the CWD)... hossman@frisbee:~/tmp$ python3.2 ~/lucene/branch_5_0/dev-tools/scripts/smokeTestRelease.py http://people.apache.org/~anshum/staging_area/lucene-solr-5.0.0-RC2-rev1658469 If smokeTestRelease.py is going to have expectations about it's CWD, then it should either enforce those expectations (ie: do a simple assert in main() that the CWD meets some expectations, and/or check "$0") or it should use "$0" to figure out where the root svn checkout is and cd to that directory itself.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6244</id>
      <title>Approximations on disjunctions</title>
      <description>Like we just did on exact phrases and conjunctions, we should also support approximations on disjunctions in order to apply "matches()" lazily.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6250</id>
      <title>Approximations on Spans</title>
      <description>Approximate spans using existing conjunction/disjunction approximations</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6254</id>
      <title>Dictionary-based lemmatizer</title>
      <description>The only way to achieve lemmatization today is to use the SynonymFilterFactory. The available stemmers are also inaccurate since they are only following simplistic rules. A dictionary-based lemmatizer will be more precise because it has the opportunity to know the part of speech. Thus it provides a more precise method to stem words compared to other dictionary-based stemmers such as Hunspell. This is my effort to develop such a lemmatizer for Apache Lucene. The documentation is temporarily placed here: http://folk.uio.no/erlendfg/solr/lemmatizer.html</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6257</id>
      <title>Remove javadocs from releases (except for publishing)</title>
      <description>In LUCENE-6247, one idea discussed to decrease the size of release artifacts was to remove javadocs from the binary release. Anyone needing javadocs offline can download the source distribution and generate the javadocs. I also think we should investigate removing javadocs jars from maven. I did a quick test, and getting the source in intellij seemed sufficient to show javadocs. However, this test was far from scientific, so if someone knows for sure whether a separate javadocs jar is truly necessary, please say so. Regardless of the outcome of the two ideas above, we would continue building, validating and making the javadocs available online.</description>
      <attachments/>
      <comments>22</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6258</id>
      <title>Cut binary releases down to a single format</title>
      <description>In LUCENE-6247, one idea discussed to decrease the size of release artifacts was removing either tgz or zip from our binary releases. The source releases are already only in tgz. I think we should do the same for binary releases. I looked at a number of other Apache projects, and the results are mixed, but there definitely are many major projects (hadoop, couchdb, cassandra, cordova) that only release tgz. Anyone who can deal with running using Lucene or Solr should have the skills necessary to extract an archive in either format, so in this way I think either format is fine, but I think matching what we release source in has a nice look.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6259</id>
      <title>Remove dependencies from binary releases</title>
      <description>In LUCENE-6247, one idea discussed to decrease the size of release artifacts was to remove the inclusion of dependencies from binary releases. These jar files increase the size of the binary releases a lot, and the size is mostly in a couple modules (eg benchmark and spatial). I think most people consume lucene through maven. For those that do use the binary release, we can still make pulling the dependencies for these modules easy. We can add a generated README file in each module that has dependencies, with instructions indicating they need to download the dependencies, and then give the list of jar files they need to download, with exact links to maven (which we can extract from ivy?).</description>
      <attachments/>
      <comments>16</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6260</id>
      <title>Simplify ExactPhraseScorer</title>
      <description>ExactPhraseScorer tries to intersect positions using windows of 4096 documents. In LUCENE-2410 it was reported that it helped a lot but I tried again on wikibig with a simpler impl that does advance one position at a time and the performance difference was only of a few percents. I'm guessing that maybe other changes (eg. the new postings format?) do not make this behaviour as useful as it used to be?</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6261</id>
      <title>TwoPhaseDocIdSetIterator.matches should be called at most once per doc ID</title>
      <description>We did not document it but TwoPhaseDocIdSetIterator.matches should only be called once per doc ID. In some cases, running it twice might not work (eg. phrases since the positions would already be consumed when we try to call matches() on the second time) and in the general case it would be a performance bug if this method is called several times anyway.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6262</id>
      <title>No need to wrap with ConstantWeight when needsScores is false</title>
      <description>Today ConstantScoreQuery always wraps the inner weight into a ConstantWeight, but this is only necessary if scores are needed.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6263</id>
      <title>Automatically wrap with ScoreCachingWrapperScorer when several collectors need scores</title>
      <description>Now that we know which collectors need scores, we could take advantage of this information in order to automatically wrap with ScoreCachingWrapperScorer when it would help.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6268</id>
      <title>Replace doc values filters with queries having approximations</title>
      <description>We should use approximations in order to deal with queries/filters that have slow iterators such as doc-values based queries/filters.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6272</id>
      <title>Scorer should not extend PostingsEnum</title>
      <description>Scorer currently has to implement a whole bunch of methods that are never called. The only method that Scorer uses in addition to the methods on DocIdSetIterator is freq(), and as currently implemented this means different things on different Scorers: TermScorer returns its underlying termfreq MinShouldMatchScorer returns how many of its subscorers are matching {Exact|Sloppy} PhraseScorer returns how many phrases it has found on a document In addition, freq() is never actually called on TermScorer, and it's only used in explain() on the phrase scorers. We should make Scorer extend DocIdSetIterator instead. In place of freq(), Scorer would have a coord() method that by default returns 1, and for boolean scorers returns how many subscorers are matching.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6274</id>
      <title>SloppyPhrase approximations</title>
      <description>This scorer still does not support the approximations api to defer positions matching.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6275</id>
      <title>SloppyPhraseScorer should use ConjunctionDISI</title>
      <description>Currently, this guy has his own little built-in algorithm, which doesn't seem optimal to me. It might be better if it reused ConjunctionDISI like ExactPhraseScorer does.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6276</id>
      <title>Add matchCost() api to TwoPhaseDocIdSetIterator</title>
      <description>We could add a method like TwoPhaseDISI.matchCost() defined as something like estimate of nanoseconds or similar. ConjunctionScorer could use this method to sort its 'twoPhaseIterators' array so that cheaper ones are called first. Today it has no idea if one scorer is a simple phrase scorer on a short field vs another that might do some geo calculation or more expensive stuff. PhraseScorers could implement this based on index statistics (e.g. totalTermFreq/maxDoc)</description>
      <attachments/>
      <comments>50</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6277</id>
      <title>Allow Ivy resolutionCacheDir to be overriden by system property.</title>
      <description>This makes it simpler to run tests in parallel (https://gist.github.com/markrmiller/dbdb792216dc98b018ad) without making any tweaks.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6278</id>
      <title>Rationalise Scorer.freq()</title>
      <description>At the moment, Scorer.freq() does different things depending on the Scorer implementation: TermScorer and the phrase scorers return the frequency of that term or phrase in the current document. TermScorer.freq() is not actually called anywhere (apart from in a couple of tests), and XPhraseScorer.freq() is only called in PhraseWeight.explain() The various Boolean scorers return the number of matching subscorers, and are used for coord calculations. I think this is confusing. I propose that we instead add a new coord() method to Scorer that by default returns 1, and that is overridden by the boolean scorers; and that we just remove freq() entirely. PhraseWeight.explain() can call a package-private method on XPhraseScorer.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6283</id>
      <title>More Like This: skip terms, like Fields and lenient defaults</title>
      <description>added skip terms: list of terms not to be considered as interesting added like on Fields and Terms objects made defaults more lenient</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6294</id>
      <title>Generalize how IndexSearcher parallelizes collection execution</title>
      <description>IndexSearcher takes an ExecutorService that can be used to parallelize collection execution. This is useful if you want to trade throughput for latency. However, this executor service will only be used if you search for top docs. In that case, we will create one collector per slide and call TopDocs.merge in the end. If you use search(Query, Collector), the executor service will never be used. But there are other collectors that could work the same way as top docs collectors, eg. TotalHitCountCollector. And maybe also some of our users' collectors. So maybe IndexSearcher could expose a generic way to take advantage of the executor service?</description>
      <attachments/>
      <comments>13</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6295</id>
      <title>Remove Query.rewrite?</title>
      <description>Queries are a bit hard to consume because of their complicated workflow: your first need to rewrite before creating a weight. Maybe we could simplify it and make query rewriting part of creating the weight? If a user would like to have access to the rewritten query, he could still call Query.createWeight(searcher, needsScores).getQuery() instead of Query.rewrite?</description>
      <attachments/>
      <comments>10</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6296</id>
      <title>BBoxFieldType should not store values for subfields (by default)</title>
      <description>When the bbox field creates the subfields, it uses the schema for 'double' and 'boolean' types. As is, we can specify these field types as indexed, not stored – but that is a bit trappy. Lets add a property to the field definition: storeSubFields="false" and register the subfields appropriatly</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6302</id>
      <title>Adding Date Math support to Lucene Expressions module</title>
      <description>Lucene Expressions are great, but they don't allow for date math. More specifically, they don't allow to infer date parts from a numeric representation of a date stamp, nor they allow to parse strings representations to dates. Some of the features requested here easy to implement via ValueSource implementation (and potentially minor changes to the lexer definition) , some are more involved. I'll be happy if we could get half of those in, and will be happy to work on a PR for the parts we can agree on. The items we will be happy to have: A now() function (with or without TZ support) to return a current long date/time value as numeric, that we could use against indexed datetime fields (which are infact numerics) Parsing methods - to allow to express datetime as strings, and / or read it from stored fields and parse it from there. Parse errors would render a value of zero. Given a numeric value, allow to specify it is a date value and then infer date parts - e.g. Date(1424963520).Year == 2015, or Date(now()) - Date(1424963520).Year. Basically methods which return numerics but internally create and use Date objects.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6304</id>
      <title>Add MatchNoDocsQuery that matches no documents</title>
      <description>As a followup to LUCENE-6298, it would be nice to have an explicit MatchNoDocsQuery to indicate that no documents should be matched. This would hopefully be a better indicator than a BooleanQuery with no clauses or (even worse) null.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6307</id>
      <title>Rename SegmentInfo.docCount -&gt; .maxDoc</title>
      <description>We already have maxDoc and numDocs, I think it's crazy we have a 3rd one docCount. We should just rename to maxDoc.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6313</id>
      <title>Add filter support to queryparser</title>
      <description>Now that Query has the capabilities of Filter, we should let users add clauses of type Occur.FILTER from the query language.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6315</id>
      <title>Simplify custom Term iterator used to resolve deletions</title>
      <description>In LUCENE-6161 we added yet-another-term-iterator ... this patch tries to simplify that by re-using the existing BytesRefIterator instead.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6319</id>
      <title>Delegating OneMerge</title>
      <description>In trying to integrate SortingMergePolicy into ElasticSearch, I ran into an issue where the custom merge logic was being stripped out by IndexUpgraderMergeSpecification. Related issue here: https://github.com/elasticsearch/elasticsearch/issues/9731 In an endeavor to fix this, I attempted to create a DelegatingOneMerge that could be used to chain the different MergePolicies together. I quickly discovered this to be impossible, due to the direct member variable access of OneMerge by IndexWriter and other classes. It would be great if this variable access could be privatized and the consuming classes modified to use the appropriate getters and setters. Here's an example DelegatingOneMerge and modified OneMerge. https://gist.github.com/ebradshaw/e0b74e9e8d4976ab9e0a https://gist.github.com/ebradshaw/d72116a014f226076303 The downside here is that this would require an API change, as there are three public variables in OneMerge: estimatedMergeBytes, segments and totalDocCount. These would have to be moved behind public getters. Without this change, I'm not sure how we could get the SortingMergePolicy working in ES, but if anyone has any other suggestions I'm all ears! Thanks!</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6332</id>
      <title>join query scanning "toField" docValue</title>
      <description>I want to contribute the subj which should do something like ..WHERE EXISTS (SELECT 1 FROM fromSearcher.search(fromQuery) WHERE fromField=toField). It turns out, that it can be returned by the current method createJoinQuery(...ScoreMode.None) at first, it should run fromQuery first, collect fromField into BytesRefHash by TermsCollector, like it's done now then it should return query with TwoPhase Scorer which obtains toField docValue on matches() and check term for existence in BytesRefHash Do you think it's ever useful? if you do, I can bake a patch. Anyway, suggest the better API eg separate method, or enum and actual name!</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6333</id>
      <title>Clean up overridden .equals and .hashCode methods in Query subclasses</title>
      <description>As a followup to LUCENE-6304, all classes that subclass Query and override the equals and hashCode methods should call super.equals/hashCode and, when possible, not override the methods at all. For example, TermQuery.hashCode overrides the Query.hashCode, but will be exactly the same code once LUCENE-6304 is merged.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6339</id>
      <title>[suggest] Near real time Document Suggester</title>
      <description>The idea is to index documents with one or more SuggestField(s) and be able to suggest documents with a SuggestField value that matches a given key. A SuggestField can be assigned a numeric weight to be used to score the suggestion at query time. Document suggestion can be done on an indexed SuggestField. The document suggester can filter out deleted documents in near real-time. The suggester can filter out documents based on a Filter (note: may change to a non-scoring query?) at query time. A custom postings format (CompletionPostingsFormat) is used to index SuggestField(s) and perform document suggestions. Usage // hook up custom postings format // indexAnalyzer for SuggestField Analyzer analyzer = ... IndexWriterConfig config = new IndexWriterConfig(analyzer); Codec codec = new Lucene50Codec() { PostingsFormat completionPostingsFormat = new Completion50PostingsFormat(); @Override public PostingsFormat getPostingsFormatForField(String field) { if (isSuggestField(field)) { return completionPostingsFormat; } return super.getPostingsFormatForField(field); } }; config.setCodec(codec); IndexWriter writer = new IndexWriter(dir, config); // index some documents with suggestions Document doc = new Document(); doc.add(new SuggestField("suggest_title", "title1", 2)); doc.add(new SuggestField("suggest_name", "name1", 3)); writer.addDocument(doc) ... // open an nrt reader for the directory DirectoryReader reader = DirectoryReader.open(writer, false); // SuggestIndexSearcher is a thin wrapper over IndexSearcher // queryAnalyzer will be used to analyze the query string SuggestIndexSearcher indexSearcher = new SuggestIndexSearcher(reader, queryAnalyzer); // suggest 10 documents for "titl" on "suggest_title" field TopSuggestDocs suggest = indexSearcher.suggest("suggest_title", "titl", 10); Indexing Index analyzer set through IndexWriterConfig SuggestField(String name, String value, long weight) Query Query analyzer set through SuggestIndexSearcher. Hits are collected in descending order of the suggestion's weight // full options for TopSuggestDocs (TopDocs) TopSuggestDocs suggest(String field, CharSequence key, int num, Filter filter) // full options for Collector // note: only collects does not score void suggest(String field, CharSequence key, int num, Filter filter, TopSuggestDocsCollector collector) Analyzer CompletionAnalyzer can be used instead to wrap another analyzer to tune suggest field only parameters. CompletionAnalyzer(Analyzer analyzer, boolean preserveSep, boolean preservePositionIncrements, int maxGraphExpansions)</description>
      <attachments/>
      <comments>35</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6341</id>
      <title>add CheckIndex -fast option</title>
      <description>CheckIndex is great for testing and when tracking down lucene bugs. But in cases where users just want to verify their index files are OK, it is very slow and expensive. I think we should add a -fast option, that only opens the reader and calls checkIntegrity(). This means all files are the correct files (identifiers match) and have the correct CRC32 checksums. For our 10M doc wikipedia index, this is the difference between a 2 second check and a 2 minute check.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6342</id>
      <title>add some missing sanity checks for old codecs</title>
      <description>We can beef up the FieldInfosReaders and the StoredFieldsReader a bit here.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6349</id>
      <title>Call hashCode outside of sync in UsageTrackingQueryCachingPolicy.onUse?</title>
      <description>The Query.hashCode call might be somewhat expensive? Can/should we call it up front outside of the sync block?</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6352</id>
      <title>Add global ordinal based query time join</title>
      <description>Global ordinal based query time join as an alternative to the current query time join. The implementation is faster for subsequent joins between reopens, but requires an OrdinalMap to be built. This join has certain restrictions and requirements: A document can only refer to on other document. (but can be referred by one or more documents) A type field must exist on all documents and each document must be categorized to a type. This is to distingues between the "from" and "to" side. There must be a single sorted doc values field use by both the "from" and "to" documents. By encoding join into a single doc values field it is trival to build an ordinals map from it.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6354</id>
      <title>Add minChildren and maxChildren options to ToParentBlockJoinQuery</title>
      <description>This effectively allows to ignore parent documents with too few children documents via the minChildren option or too many matching children documents via the maxChildren option.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6355</id>
      <title>Add verbose IndexWriter logging for writing field infos</title>
      <description>SegmentMerger should also write the amount of time it takes to write the field infos during a merge. This makes it much easier to determine the contributing times for the total merge time.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6360</id>
      <title>TermsQuery should rewrite to a ConstantScoreQuery over a BooleanQuery when there are few terms</title>
      <description>TermsQuery helps when there are lot of terms from which you would like to compute the union, but it is a bit harmful when you have few terms since it cannot really skip: it always consumes all documents matching the underlying terms. It would certainly help to rewrite this query to a ConstantScoreQuery over a BooleanQuery when there are few terms in order to have actual skip support. As usual the hard part is probably to figure out the threshold.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6361</id>
      <title>Optimized AnalyzinSuggester#topoSortStates()</title>
      <description>Optimized implementation of AnalyzinSuggester#topoSortStates().</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6362</id>
      <title>Differentiate within/approx SpatialPrefixTree leaf cells</title>
      <description>This patch enables indexed cells to be differentiated between those that are completely within the shape that was indexed, and those on the edge (intersects, not within) but reached a precision threshold. The point of this differentiation is to be used by the two-phase optimized predicates in LUCENE-5579 to be able to avoid an expensive geometry check for accuracy, in more cases than it is able to without this differentiation. In particular, if the query shape is within an indexed shape, then this cell differentiation will enable a predicate to observe that without doing a geometry check. note: patch here is moved from LUCENE-5579 as this is really an add-on</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6364</id>
      <title>DocValues should support BoolField</title>
      <description>DocValues supports numerics and strings, but it currently does not support booleans. Please add this support. Here is the error message you get if you try to use DocValues with a BoolField. ERROR - 2015-03-18 00:49:54.041; org.apache.solr.common.SolrException; null:org.apache.solr.common.SolrException: SolrCore 'test' is not available due to init failure: Could not load conf for core test: F ield type boolean{class=org.apache.solr.schema.BoolField,analyzer=org.apache.solr.schema.FieldType$DefaultAnalyzer,args={sortMissingLast=true, class=solr.BoolField}} does not support doc values. Schema fi le is /Users/kosborn/solr/server/solr/test/conf/schema.xml</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6365</id>
      <title>Optimized iteration of finite strings</title>
      <description>Replaced Operations.getFiniteStrings() by an optimized FiniteStringIterator. Benefits: Avoid huge hash set of finite strings. Avoid massive object/array creation during processing. "Downside": Iteration order changed, so when iterating with a limit, the result may differ slightly. Old: emit current node, if accept / recurse. New: recurse / emit current node, if accept. The old method Operations.getFiniteStrings() still exists, because it eases the tests. It is now implemented by use of the new FiniteStringIterator.</description>
      <attachments/>
      <comments>44</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>6367</id>
      <title>Can PrefixQuery subclass AutomatonQuery?</title>
      <description>Spinoff/blocker for LUCENE-5879. It seems like PrefixQuery should "simply" be an AutomatonQuery rather than specializing its own TermsEnum ... with maybe some performance improvements to ByteRunAutomaton.run to short-circuit once it's in a "sink state", AutomatonTermsEnum could be just as fast as PrefixTermsEnum. If we can do this it will make LUCENE-5879 simpler.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6371</id>
      <title>Improve Spans payload collection</title>
      <description>Spin off from LUCENE-6308, see the comments there from around 23 March 2015.</description>
      <attachments/>
      <comments>66</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>6372</id>
      <title>Simplify hashCode/equals for SpanQuery subclasses</title>
      <description>Spin off from LUCENE-6308, see the comments there from around 23 March 2015.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6373</id>
      <title>Complete two phase doc id iteration support for Spans</title>
      <description>Spin off from LUCENE-6308, see comments there from about 23 March 2015.</description>
      <attachments/>
      <comments>33</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6374</id>
      <title>remove unused BitUtil code</title>
      <description>There is some dead unused and untested code here.. we should nuke it.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6375</id>
      <title>Inconsistent interpretation of maxDocCharsToAnalyze in Highlighter &amp; WeightedSpanTermExtractor</title>
      <description>Way back in LUCENE-2939, the default/standard Highlighter's WeightedSpanTermExtractor (referenced by QueryScorer, used by Highlighter.java) got a performance feature maxDocCharsToAnalyze to set a limit on how much text to process when looking for phrase queries and wildcards (and some other advanced query types). Highlighter itself also has a limit by the same name. They are not interpreted the same way! Highlighter loops over tokens and halts early if the token's start offset &gt;= maxDocCharsToAnalyze. In this light, it's almost as if the input string was truncated to be this length, but a bit beyond to the next tokenization boundary. The PostingsHighlighter also has a configurable limit it calls "maxLength" (or contentLength) that is conceptually similar but implemented differently because it doesn't tokenize; but it does have the inverted start &amp; end offsets to check if it's reached the end with respect to this configured limit. FYI Solr's hl.maxAnalyzedChars is supplied as a configured input to both highlighters in this manner; the FastVectorHighlighter doesn't have a limit. Highlighter propagates it's configured maxAnalyzedChars to QueryScorer which in turn propagates it to WeightedSpanTermExtractor. WSTE doesn't interpret this the same way as Highlighter or PostingsHighlighter. It uses an OffsetLimitTokenFilter which accumulates the deltas in start &amp; end offsets of each token it sees. That is: int offsetLength = offsetAttrib.endOffset() - offsetAttrib.startOffset(); offsetCount += offsetLength; So if you've got analysis which produces a lot of posInc-0 tokens (as I do), you will likely hit this limit earlier than when Highlighter will. Or if you have very few tokens with tons of whitespace then WSTE will index terms that will never be highlighted. This isn't a big deal but it should be fixed. This filter should simply examine if the startOffset is &gt;= a configured limit and return false from it's incrementToken if so.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6376</id>
      <title>Spatial PointVectorStrategy should use DocValues</title>
      <description>PointVectorStrategy.createIndexableFields should be using DocValues, like BBoxStrategy does. Without this, UninvertingReader is required.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6377</id>
      <title>Pass previous reader to SearcherFactory</title>
      <description>SearcherFactory is often used as advertised for warming segments for newly flushed segments or for searchers that are opened for the first time (generally where merge warmers don't apply). To make this simpler we should pass the previous reader to the factory as well to know what needs to be warmed.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6379</id>
      <title>IndexWriter's delete-by-query should optimize/specialize MatchAllDocsQuery</title>
      <description>We can short-circuit this to just IW.deleteAll (Solr already does so I think). This also has the nice side effect of clearing Lucene's low-schema (FieldInfos).</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6382</id>
      <title>Don't allow position = Integer.MAX_VALUE going forward</title>
      <description>Spinoff from LUCENE-6308, where Integer.MAX_VALUE position is now used as a sentinel during position iteration to indicate that there are no more positions. Where IW now detects int overflow of position, it should now also detect == Integer.MAX_VALUE. And CI should note corruption if a segment's version is &gt;= 5.2 and has Integer.MAX_VALUE position.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6388</id>
      <title>Optimize SpanNearQuery</title>
      <description>After the big spans overhaul in LUCENE-6308, we can speed up SpanNearQuery a little more: SpanNearQuery defaults to collectPayloads=true, but this requires a slower implementation, for an uncommon case. Use the faster no-payloads impl if the field doesn't actually have any payloads. Use a simple array of Spans rather than List in NearSpans classes. This is iterated over often in the logic.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6389</id>
      <title>Add ScoreMode.Min</title>
      <description>In addition to the existing score modes add 'min' score mode that aggregates the lowest child score to the parent hit.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6391</id>
      <title>Give SpanScorer two-phase iterator support.</title>
      <description>Fix SpanScorer to use any two-phase iterator support of the underlying Spans. This means e.g. a spans in a booleanquery, or a spans with a filter can be faster. In order to do this, we have to clean up this class a little bit: forward most methods directly to the underlying spans. ensure positions are only iterated at most once.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6392</id>
      <title>Add offset limit to Highlighter's TokenStreamFromTermVector</title>
      <description>The Highlighter's TokenStreamFromTermVector utility, typically accessed via TokenSources, should have the ability to filter out tokens beyond a configured offset. There is a TODO there already, and this issue addresses it. New methods in TokenSources now propagate a limit. This patch also includes some memory saving optimizations, to be described shortly.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6398</id>
      <title>Usability and documentation issues found trying to build an RC using dev-tools/scripts/buildAndPushRelease.py</title>
      <description>This ticket is to address usability and documentation problems found while trying to do the 5.1 RC. Overall, it's a powerful process, but want to make it a bit easier for newbies doing it the first time. Here's the start: 1) dev-tools/scripts/buildAndPushRelease.py Prompts you for the following: Enter GPG keystore password: It's really asking for the passphrase the key you're using to sign the release artifacts, so it seems like it should prompt for something like: Enter secret passphrase for key: 2. At the end of a release build, the scripts says: Next set the PYTHON_EXEC env var and you can run the smoker tester: $PYTHON_EXEC dev-tools/scripts/buildAndPushRelease.py URL Running that command verbatim, you get an error: usage: buildAndPushRelease.py [-h] [--no-prepare] [--push-remote USERNAME] [--push-local PATH] [--sign KEYID] [--rc-num NUM] [--smoke-test PATH] checkout_path buildAndPushRelease.py: error: Root path is not a valid lucene-solr checkout [~/dev/lw/projects/lucene_solr_5_1]$ $PYTHON_EXEC dev-tools/scripts/buildAndPushRelease.py --smoke-test file:///Users/timpotter/dev/lw/releases/5.1rc1/lucene-solr-5.1.0-RC1-rev1671632 usage: buildAndPushRelease.py [-h] [--no-prepare] [--push-remote USERNAME] [--push-local PATH] [--sign KEYID] [--rc-num NUM] [--smoke-test PATH] checkout_path buildAndPushRelease.py: error: the following arguments are required: checkout_path So we need to fix the example to display what is actually needed to run. Will add more to the docs as I go thru the process, but wanted to get a ticket in place for making improvements.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6399</id>
      <title>Benchmark's AbstractQueryMaker.resetInputs should call setConfig</title>
      <description>DocMaker.resetInput() will call setConfig. QueryMaker should too for the same reason – so that it can respond to properties that change per round. DocMaker is concrete but QueryMaker is an interface, but this behavior can be put into AbstractQueryMaker. I found this as some benchmarking was driving me crazy as I couldn't get spatial benchmark queries to see changes I had!</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6410</id>
      <title>Remove unused "reuse" param to Terms.iterator</title>
      <description>Terms.iterator takes a reuse param but no impls in fact reuse it. I think callers can just hang onto a TermsEnum and reuse themselves instead?</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6412</id>
      <title>Merge SpanTermQuery into TermQuery</title>
      <description>Having a separate SpanTermQuery doesn't actually gain us anything now, and it's trivial enough to make TermQuery extend SpanQuery copy the getSpans() and getField() impls over from STQ.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6413</id>
      <title>Test runner should report the number of suites completed/ remaining</title>
      <description>Pretty much as suggested by Shawn on the dev list. Suite: org.apache.lucene.util.TestNamedSPILoader Completed [2/414] on J3 in 0.59s, 3 tests Suite: org.apache.lucene.util.TestBitDocIdSetBuilder Completed [3/414] on J0 in 0.29s, 2 tests Suite: org.apache.lucene.index.TestStressIndexing2 Completed [4/414] on J1 in 2.06s, 3 tests The number of individual tests cannot be printed (it's not available globally to the runner until the suite is actually executed). There is no ETA remaining for similar reasons (the variance on each suite's execution time is unpredictable).</description>
      <attachments/>
      <comments>6</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6414</id>
      <title>let AbstractSpatialFieldType set the ctx directly</title>
      <description>Right now AbstractSpatialFieldType needs to use create a new SpatialContext for each type using the factory. refactor thigs so that we can set the ctx directly</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6417</id>
      <title>Upgrade ANTLR to version 4.5</title>
      <description>I would like to upgrade ANTLR from 3.5 to 4.5. This version adds several features that will improve the existing grammars. The main improvement would be the allowance of left-hand recursion in grammar rules which will reduce the number of rules significantly for expressions. This change will require some code refactoring to the existing expressions work.</description>
      <attachments/>
      <comments>38</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6420</id>
      <title>Update forbiddenapis to 1.8</title>
      <description>Update forbidden-apis plugin to 1.8: Initial support for Java 9 including JIGSAW Errors are now reported sorted by line numbers and correctly grouped (synthetic methods/lambdas) Package-level forbids: Deny all classes from a package: org.hatedpkg.** (also other globs work) In addition to file-level excludes, forbiddenapis now supports fine granular excludes using Java annotations. You can use the one shipped, but define your own, e.g. inside Lucene and pass its name to forbidden (e.g. using a glob: **.SuppressForbidden would any annotation in any package to suppress errors). Annotation need to be on class level, no runtime annotation required. This will for now only update the dependency and remove the additional forbid by Shalin Shekhar Mangar for MessageFormat (which is now shipped with forbidden). But we should review and for example suppress forbidden failures in command line tools using @SuppressForbidden (or similar annotation). The discussion is open, I can make a patch.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6422</id>
      <title>Add PackedQuadPrefixTree</title>
      <description>This task introduces a PackedQuadPrefixTree that includes two things: 1. A packed 8 byte representation for a QuadCell, including more efficient implementations of the SPT API than the existing QuadPrefixTree or GeoHashPrefixTree. 2. An alternative implementation to RPT's "pruneLeafyBranches" that streams the cells without buffering them all, which is way more memory efficient. However pruning is limited to the target detail level, where it accomplishes the most good. Future improvements over this approach may include the generation of the packed cells using an AutoPrefixAutomaton</description>
      <attachments/>
      <comments>32</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6423</id>
      <title>New LimitTokenOffsetFilter</title>
      <description>It would be nice to have a token filter that limited based on the offset. I suggest the start offset. It should be named LimitTokenOffsetFilter to have a name similar to the other LimitToken*Filter choices, including a "consumeAllTokens" option. I plan to use this filter in LUCENE-6392 (to limit tokens from analyzed text for applicable methods in TokenSources) and in LUCENE-6375.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6428</id>
      <title>Shore up IOUtils.spins a bit</title>
      <description>Some minor improvements to the spooky hacks it uses today: Use FileStore.name() instead of getBlockDevice parsing FileStore.toString to extract the second part in (..). Fix TestIOUtils.MockFileStore to extend FileStore directly not FilterFileStore so no delegate is required Require that we find " (" when parsing out the mount point</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6434</id>
      <title>simplify extrasfs more</title>
      <description>As Dawid mentions on LUCENE-6431, we can do all conditions once in the ctor, since it will not change at the very least.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6438</id>
      <title>Improve clean-jars when dealing with symbolic links.</title>
      <description>Ever since I started seeing jars in the lib folders use symbolic links on linux I've run into jar problems when working with an old checkout or switching branches on a git checkout. You would normally expect ant clean-jars to help, but it didn't and led to some headaches and random bs. Turns out, clean-jars is not properly removing all symbolic links for me. I've seen two cases - symbolic links to jars that are not removed and broken symbolic links to jars. I can get rid of the symbolic links with the following: &lt;target name="clean-jars" description="Remove all JAR files from lib folders in the checkout"&gt; &lt;delete failonerror="true" removeNotFollowedSymlinks="true"&gt; &lt;fileset dir="." followsymlinks="false"&gt; But that doesn't work with the broken links. I guess you can remove those with the Ant Symlink task, but it seems only specifically one at a time which is not that useful.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6441</id>
      <title>Change default formatting settings to break lines at 120 characters</title>
      <description>Our eclipse settings default to break lines at 80 characters. This issue changes them to break lines at 120 characters.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6445</id>
      <title>Highlighter TokenSources simplification; just one getAnyTokenStream()</title>
      <description>The Highlighter "TokenSources" class has quite a few utility methods pertaining to getting a TokenStream from either term vectors or analyzed text. I think it's too much: some go to term vectors, some don't. But if you don't want to go to term vectors, then it's quite easy for the caller to invoke the Analyzer for the field value, and to get that field value. Some methods return null, some never null; I forget which at a glance. Some methods read the Document (to get a field value) from the IndexReader, some don't. Furthermore, it's not an ideal place to get the doc since your app might be using an IndexSearcher with a document cache (e.g. SolrIndexSearcher). None of the methods accept a Fields instance from term vectors as a parameter. Based on how Lucene's term vector format works, this is a performance trap if you don't re-use an instance across fields on the document that you're highlighting.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6450</id>
      <title>Add simple encoded GeoPointField type to core</title>
      <description>At the moment all spatial capabilities, including basic point based indexing and querying, require the lucene-spatial module. The spatial module, designed to handle all things geo, requires dependency overhead (s4j, jts) to provide spatial rigor for even the most simplistic spatial search use-cases (e.g., lat/lon bounding box, point in poly, distance search). This feature trims the overhead by adding a new GeoPointField type to core along with GeoBoundingBoxQuery and GeoPolygonQuery classes to the .search package. This field is intended as a straightforward lightweight type for the most basic geo point use-cases without the overhead. The field uses simple bit twiddling operations (currently morton hashing) to encode lat/lon into a single long term. The queries leverage simple multi-phase filtering that starts by leveraging NumericRangeQuery to reduce candidate terms deferring the more expensive mathematics to the smaller candidate sets.</description>
      <attachments/>
      <comments>47</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>6451</id>
      <title>Support non-static methods in the Javascript compiler</title>
      <description>Allow methods such as date.getMonth() or string.getOrdinal() to be added in the same way expression variables are now (forwarded to the bindings for processing). This change will only allow non-static methods that have zero arguments due to current limitations in the architecture, and to keep the change simple.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6453</id>
      <title>Specialize SpanPositionQueue similar to DisiPriorityQueue</title>
      <description>Inline the position comparison function</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6456</id>
      <title>Don't cache queries that are too large for the query cache</title>
      <description>We have a default query cache with a reasonable size: 32MB. However if you happen to have a large index (eg. 1B docs), this might be too small even to store a single cached filter. In such cases we should not even try to cache instead of generating cache entries and trashing them immediately.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6457</id>
      <title>Create mixed format bwc index for major releases in addBackcompatIndexes.py</title>
      <description>An important edge case to test in backcompat tests is a segment infos file written from the current major version, but still containing commits from the previous major version. Right now it a very manual process to create one of these indexes. We should add this to the bwc index creation script, to be run when generating bwc indexes for a major release (X.0.0).</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6458</id>
      <title>MultiTermQuery's FILTER rewrite method should support skipping whenever possible</title>
      <description>Today MultiTermQuery's FILTER rewrite always builds a bit set fom all matching terms. This means that we need to consume the entire postings lists of all matching terms. Instead we should try to execute like regular disjunctions when there are few terms.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6459</id>
      <title>[suggest] Query Interface for suggest API</title>
      <description>This patch factors out common indexing/search API used by the recently introduced NRTSuggester. The motivation is to provide a query interface for FST-based fields (SuggestField and ContextSuggestField) to enable suggestion scoring and more powerful automaton queries. Previously, only prefix ‘queries’ with index-time weights were supported but we can also support: Prefix queries expressed as regular expressions: get suggestions that match multiple prefixes Example: star[wa|tr] matches starwars and startrek Fuzzy Prefix queries supporting scoring: get typo tolerant suggestions scored by how close they are to the query prefix Example: querying for seper will score separate higher then superstitious Context Queries: get suggestions boosted and/or filtered based on their indexed contexts (meta data) Boost example: get typo tolerant suggestions on song names with prefix like a roling boosting songs with genre rock and indie Filter example: get suggestion on all file names starting with finan only for user1 and user2 Suggest API SuggestIndexSearcher searcher = new SuggestIndexSearcher(reader); CompletionQuery query = ... TopSuggestDocs suggest = searcher.suggest(query, num); CompletionQuery CompletionQuery is used to query SuggestField and ContextSuggestField. A CompletionQuery produces a CompletionWeight, which allows CompletionQuery implementations to pass in an automaton that will be intersected with a FST and allows boosting and meta data extraction from the intersected partial paths. A CompletionWeight produces a CompletionScorer. A CompletionScorer executes a Top N search against the FST with the provided automaton, scoring and filtering all matched paths. PrefixCompletionQuery Return documents with values that match the prefix of an analyzed term text Documents are sorted according to their suggest field weight. PrefixCompletionQuery(Analyzer analyzer, Term term) RegexCompletionQuery Return documents with values that match the prefix of a regular expression Documents are sorted according to their suggest field weight. RegexCompletionQuery(Term term) FuzzyCompletionQuery Return documents with values that has prefixes within a specified edit distance of an analyzed term text. Documents are ‘boosted’ by the number of matching prefix letters of the suggestion with respect to the original term text. FuzzyCompletionQuery(Analyzer analyzer, Term term) Scoring suggestion_weight * boost where suggestion_weight and boost are all integers. boost = # of prefix characters matched ContextQuery Return documents that match a CompletionQuery filtered and/or boosted by provided context(s). ContextQuery(CompletionQuery query) contextQuery.addContext(CharSequence context, int boost, boolean exact) NOTE: ContextQuery should be used with ContextSuggestField to query suggestions boosted and/or filtered by contexts. Running ContextQuery against a SuggestField will error out. Scoring suggestion_weight * context_boost where suggestion_weight and context_boost are all integers When used with FuzzyCompletionQuery, suggestion_weight * (context_boost + fuzzy_boost) Context Suggest Field To use ContextQuery, use ContextSuggestField instead of SuggestField. Any CompletionQuery can be used with ContextSuggestField, the default behaviour is to return suggestions from all contexts. Context for every completion hit can be accessed through SuggestScoreDoc#context. ContextSuggestField(String name, Collection&lt;CharSequence&gt; contexts, String value, int weight)</description>
      <attachments/>
      <comments>23</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6460</id>
      <title>TermsQuery should rewrite to BooleanQuery if &lt; 50 terms</title>
      <description>If there aren't many terms in a TermsQuery (perhaps 50), it should be faster for TermsQuery to rewrite to a BooleanQuery so that there is disjunction/skipping. Above some number of terms, there is overhead in BQ/DisjunctionScorer's PriorityQueue.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6461</id>
      <title>TwoPhaseCommitTool should suppress not discard exceptions from rollback.</title>
      <description>This is just generally nice to do, since we are on java 7 we should try to always preserve any context like this.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6462</id>
      <title>Latin Stemmer for lucene</title>
      <description>In the latest lucene package there is no stemmer for Latin language. I have a stemmer for latin language which is a rule based program based on the grammar and rules of Latin</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6463</id>
      <title>Share more logic between our approximated queries</title>
      <description>We have several queries that support approximations, and in particular the ones based on random-access (doc values terms/range, FieldValueFilter, ...) duplicate some logic.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6464</id>
      <title>Allow possibility to group contexts in AnalyzingInfixSuggester.loockup()</title>
      <description>This is an enhancement to LUCENE-6050 LUCENE-6050 added lookup(CharSequence key, Map&lt;BytesRef, BooleanClause.Occur&gt; contextInfo, int num, boolean allTermsRequired, boolean doHighlight) which allowed to do something like (A OR B AND C OR D ...) In our use-case, we realise that we need grouping i.e (A OR B) AND (C OR D) AND (...) In other words, we need the intersection of multiple contexts. The attached patch allows to pass in a varargs of map, each one representing the each group. Looks a bit heavy IMHO. This is an initial patch. The question to Michael McCandless and jane chang is: is it better to expose a FilteredQuery/Query and let the user build their own query instead of passing a map? Exposing a filteredQuery will probably give the best flexibility to the end-users.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6466</id>
      <title>Move SpanQuery.getSpans() to SpanWeight</title>
      <description>SpanQuery.getSpans() should only be called on rewritten queries, so it seems to make more sense to have this being called from SpanWeight</description>
      <attachments/>
      <comments>28</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6467</id>
      <title>Simplify Query.equals()</title>
      <description>Remove this == other test in Query.equals().</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6472</id>
      <title>Add min and max document options to global ordinal join</title>
      <description>This feature allows to only match "to" documents that have at least between min and max matching "from" documents.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6473</id>
      <title>Make Spans an interface</title>
      <description>Spans is currently an abstract class, extending DocIdSetIterator. This restricts what we can do with implementations of Spans. For example, in LUCENE-6371, it would be useful to have PayloadSpan classes that extend existing Spans implementations, but that also implement a PayloadSpans interface that extends Spans. This isn't possible if Spans is not an interface itself.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6476</id>
      <title>Split logic from TermContext.register</title>
      <description>TermContext.register currently does two different things: it stores a reference to an existing TermState in order to be able to seek back to the term efficiently and it accumulates statistics for scoring. However sometimes you would like to provide fake statistics in order to eg. blend frequencies (LUCENE-329) so we could decouple these two operations in order to make it easier?</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6477</id>
      <title>Add BKD tree for spatial shape query intersecting indexed points</title>
      <description>I'd like to explore using dedicated spatial trees for faster shape intersection filters than postings-based implementations. I implemented the tree data structure from https://www.cs.duke.edu/~pankaj/publications/papers/bkd-sstd.pdf The idea is simple: it builds a full binary tree, partitioning 2D space, alternately on lat and then lon, into smaller and smaller rectangles until a leaf has &lt;= N (default 1024) points. It cannot index shapes (just points), and can then do fast shape intersection queries. Multi-valued fields are supported. I only implemented the "point is contained in this bounding box" query for now, but I think polygon shape querying should be easy to implement using the same approach from LUCENE-6450. For indexing, you add BKDPointField (takes lat, lon) to your doc, and must set up your Codec use BKDTreeDocValuesFormat for that field. This DV format wraps Lucene50DVFormat, but then builds the disk-based BKD tree structure on the side. BKDPointInBBoxQuery then requires this DVFormat, and casts it to gain access to the tree. I quantize each incoming double lat/lon to 32 bits precision (so 64 bits per point) = ~9 milli-meter lon precision at the equator, I think.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6479</id>
      <title>Create utility to generate Classifier's confusion matrix</title>
      <description>In order to debug and compare accuracy of Classifiers it's often useful to print the related confusion matrix so it'd be good to provide such an utility class/method.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6480</id>
      <title>Extend Simple GeoPointField Type to 3d</title>
      <description>LUCENE-6450 proposes a simple GeoPointField type to lucene core. This field uses 64bit encoding of 2 dimensional points to construct sorted term representations of GeoPoints (aka: GeoHashing). This feature investigates adding support for encoding 3 dimensional GeoPoints, either by extending GeoPointField to a Geo3DPointField or adding an additional 3d constructor.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6481</id>
      <title>Improve GeoPointField type to only visit high precision boundary terms</title>
      <description>Current GeoPointField LUCENE-6450 computes a set of ranges along the space-filling curve that represent a provided bounding box. This determines which terms to visit in the terms dictionary and which to skip. This is suboptimal for large bounding boxes as we may end up visiting all terms (which could be quite large). This incremental improvement is to improve GeoPointField to only visit high precision terms in boundary ranges and use the postings list for ranges that are completely within the target bounding box. A separate improvement is to switch over to auto-prefix and build an Automaton representing the bounding box. That can be tracked in a separate issue.</description>
      <attachments/>
      <comments>44</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6484</id>
      <title>Remove EliasFanoDocIdSet</title>
      <description>EliasFanoDocIdSet is currently unused, remove it.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6485</id>
      <title>Add a custom separator break iterator</title>
      <description>Lucene currently includes a WholeBreakIterator used to highlight entire fields using the postings highlighter, without breaking their content into sentences. I would like to contribute a CustomSeparatorBreakIterator that breaks when a custom char separator is found in the text. This can be used for instance when wanting to highlight entire fields, value per value. One can subclass PostingsHighlighter and have getMultiValueSeparator return a control character, like U+0000 , then use the custom break iterator to break on U+0000 so that one snippet per value will be generated.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6487</id>
      <title>Add WGS84 capability to geo3d support</title>
      <description>WGS84 compatibility has been requested for geo3d. This involves working with an ellipsoid rather than a unit sphere. The general formula for an ellipsoid is: x^2/a^2 + y^2/b^2 + z^2/c^2 = 1</description>
      <attachments/>
      <comments>41</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6492</id>
      <title>New SpatialStrategy abstraction that doesn't require Spatial4j</title>
      <description>This issue is about coming up with a new (or updated) SpatialStrategy abstraction. The primary goal is to remove a dependency on Spatial4j. Some SpatialStrategies will not use/require the abstractions in Spatial4j. Strategies that support more complex/interesting cases may require Spatial4j still.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6494</id>
      <title>Make PayloadSpanUtil apply to other postings information</title>
      <description>With the addition of SpanCollectors, we can now get arbitrary postings information from SpanQueries. PayloadSpanUtil does some rewriting to convert non-span queries into SpanQueries so that it can collect payloads. It would be good to make this more generic, so that we can collect any postings information from any query (without having to make invasive changes to already optimized Scorers, etc).</description>
      <attachments/>
      <comments>31</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6497</id>
      <title>Allow subclasses of FieldType to check frozen state</title>
      <description>checkIfFrozen() is currently private. We should this protected, so subclasses of FieldType can add additional state that is protected by freezing.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6501</id>
      <title>Flatten subreader structure in ParallelCompositeReader</title>
      <description>The current implementation of ParallelCompositeReader reassembles the whole subreader structure of the wrapped reader with ParallelLeafReader and ParallelCompositeReader. This leads to bugs like described in LUCENE-6500. This reaches back to the time when this reader was reimplemented for the first time shortly before release of 4.0. Shortly afterwards, we completely changed our search infrastructure to just call leaves() and working with them. The method getSequentialSubReaders was made protected, just to be implemented by subclasses (like this one). But no external code can ever call it. Also the search API just rely on the baseId in relation to the top-level reader (to correctly present document ids). The structure is completely unimportant. This issue will therefore simplify ParallelCompositeReader to just fetch all LeafReaders and build a flat structure of ParallelLeafReaders from it. This also has the nice side-effect, that only the parallel leaf readers must be equally sized, not their structure. This issue will solve LUCENE-6500 as a side effect. I just opened a new issue for discussion and to have this listed as "feature" and not bug. In general, we could also hide the ParallelLeafReader class and make it an implementation detail. ParallelCompositeReader would be the only entry point -&gt; because people could pass any IndexReader structure in, a single AtomicReader would just produce a CompositeReader with one leaf. We could then also rename it back to ParallelReader (like it was in pre Lucene4).</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6502</id>
      <title>Spatial Geo3d RectIntersectionTestHelper fails too often</title>
      <description>The RectIntersectionTestHelper requires a minimum number of occurrences of each relation type before it passes, and there are a minimum number of attempts. But this can be a bit much, and too often it can cause a spurious test failure that isn't really a bug. Instead, it should simply try to find at least one of every case in a minimum number of tries. This would solve this bug today: http://jenkins.thetaphi.de/job/Lucene-Solr-trunk-Linux/12825/</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6504</id>
      <title>implement norms with random access API</title>
      <description>We added this api in LUCENE-5729 but we never explored implementing norms with it. These are generally the largest consumer of heap memory and often a real hassle for users.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6513</id>
      <title>Allow limits on SpanMultiTermQueryWrapper expansion</title>
      <description>SpanMultiTermQueryWrapper currently rewrites to a SpanOrQuery with as many clauses as there are matching terms. It would be nice to be able to limit this in a slightly nicer way than using TopTerms, which for most queries just translates to a lexicographical ordering.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6515</id>
      <title>Suggest API cleanup</title>
      <description>This is a spin off from LUCENE-6459. Mainly to clean up the new APIs (e.g. ContextSuggestField) and remove any unused code.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6519</id>
      <title>BKD polygon queries should avoid per-hit filtering when cell is fully enclosed</title>
      <description>In LUCENE-6481, Nicholas Knize added methods to test for the relationship between an axis-aligned rect vs the query polygon, e.g. is the rect fully contained by the polygon, overlaps its boundaries, or fully outside the polygon. I think we should also use those methods to speed up BKDPointInPolygonQuery, to decide on recursively visiting the tree, how to handle the leaf blocks under internal nodes.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6524</id>
      <title>Create an IndexWriter from an already opened NRT or non-NRT reader</title>
      <description>I'd like to add a new ctor to IndexWriter, letting you start from an already opened NRT or non-NRT DirectoryReader. I think this is a long missing API in Lucene today, and we've talked in the past about different ways to fix it e.g. factoring out a shared reader pool between writer and reader. One use-case, which I hit in LUCENE-5376: if you have a read-only index, so you've opened a non-NRT DirectoryReader to search it, and then you want to "upgrade" to a read/write index, we don't handle that very gracefully now because you are forced to open 2X the SegmentReaders. But with this API, IW populates its reader pool with the incoming SegmentReaders so they are shared on any subsequent NRT reopens / segment merging / deletes applying, etc. Another (more expert) use case is allowing rollback to an NRT-point. Today, you can only rollback to a commit point (segments_N). But an NRT reader also reflects a valid "point in time" view of the index (it just doesn't have a segments_N file, and its ref'd files are not fsync'd), so with this change you can close your old writer, open a new one from this NRT point, and revert all changes that had been done after the NRT reader was opened from the old writer.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6530</id>
      <title>Use Java 7 ProcessBuilder.inheritIO() instead of own ThreadPumper</title>
      <description>In some tests wie spawn separate processes (TestIndexWriterOnJRECrash and Solr's IPTables). To capture stdin/stdout/stderr we spawn several threads that pump those to stdout/stderr. Since Java 7 there is ProcessBuilder.inheritIO() that does this for us without any additional threads. We should use this instead. Fix is easy, just remove some stuff I did the same already for my Codec classloader deadlock test, so this is just a followup for the other tests. Patch is attached and can be committed to trunk and 5.x.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6531</id>
      <title>Make PhraseQuery immutable</title>
      <description>Mutable queries are an issue for automatic filter caching since modifying a query after it has been put into the cache will corrupt the cache. We should make all queries immutable (up to the boost) to avoid this issue.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6532</id>
      <title>Add GeoPointDistanceQuery for GeoPointField type</title>
      <description>LUCENE-6481 adds GeoPointField w/ GeoPointInBBox and GeoPointInPolygon queries. This feature adds GeoPointDistanceQuery to support point radius queries.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6536</id>
      <title>Migrate HDFSDirectory from solr to lucene-hadoop</title>
      <description>I am currently working on a search engine that is throughput orientated and works entirely in apache-spark. As part of this, I need a directory implementation that can operate on HDFS directly. This got me thinking, can I take the one that was worked on so hard for solr hadoop. As such I migrated the HDFS and blockcache directories out to a lucene-hadoop module. Having done this work, I am not sure if it is actually a good change, it feels a bit messy, and I dont like how the Metrics class gets extended and abused. Thoughts anyone</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6539</id>
      <title>Add DocValuesNumbersQuery, like DocValuesTermsQuery but works only with long values</title>
      <description>This query accepts any document where any of the provided set of longs was indexed into the specified field as a numeric DV field (NumericDocValuesField or SortedNumericDocValuesField). You can use it instead of DocValuesTermsQuery when you have field values that can be represented as longs. Like DocValuesTermsQuery, this is slowish in general, since it doesn't use an inverted data structure, but in certain cases (many terms/numbers and fewish matching hits) it should be faster than using TermsQuery because it's done as a "post filter" when other (faster) query clauses are MUST'd with it. In such cases it should also be faster than DocValuesTermsQuery since it skips having to resolve terms -&gt; ords.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6540</id>
      <title>Add BKDPointDistanceQuery</title>
      <description>LUCENE-6532 adds the supporting mathematics for point-distance computation based on the ellipsoid (using Vincenty's Direct and Inverse solutions). This feature adds BKDPointDistance query function to LUCENE-6477 for finding all documents that match the provided distance criteria from a given geo point. This should out perform other solutions since we can stop traversing the BKD tree once we've found an internal node that matches the given criteria.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6544</id>
      <title>Geo3d cleanup: Regularize path and polygon construction, plus consider adding ellipsoid surface distance method</title>
      <description>Geo3d's way of constructing polygons and paths differs in that in one case you construct points and the other you feed lat/lon values directly to the builder. Probably both should be supported for both kinds of entity. Also it may be useful to have an accurate point-point ellipsoidal distance function. This is expensive and would be an addition to the arc distance we currently compute. It would probably be called "surface distance".</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6545</id>
      <title>optimize DocTermOrds in cases where the underlying TermEnum being wraped supports ord()</title>
      <description>Prior to LUCENE-6529, DocTermOrds had an optimization when the TermEnum of the field being Uninverted already supported ord(). This optimization was removed in LUCENE-6529 (see r1684704) because it was found to produce incorrect results for numeric fields that had a precisionStep. This issue is to track the possibility of re-adding a correct version of this optimization.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6547</id>
      <title>Add dateline crossing support to GeoPointInBBox and GeoPointDistance Queries</title>
      <description>The current GeoPointInBBoxQuery only supports bounding boxes that are within the standard -180:180 longitudinal bounds. While its perfectly fine to require users to split dateline crossing bounding boxes in two, GeoPointDistanceQuery should support distance queries that cross the dateline. Since morton encoding doesn't support unwinding this issue will add dateline crossing to GeoPointInBBoxQuery and GeoPointDistanceQuery classes.</description>
      <attachments/>
      <comments>26</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6548</id>
      <title>Optimize BlockTree's Terms.intersect a bit for "very finite" automata</title>
      <description>I've been digging into why BlockTree's Terms.intersect impl is slower for a "very finite" union-of-terms test over random IDs (LUCENE-3893) and I found a few performance improvements that at least for that one use case gets a ~11% speedup.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6551</id>
      <title>Add CMS.getAutoIOThrottle getter</title>
      <description>Currently you can enable and disable CMS's auto IO throttle but you can't get it! Silly.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6552</id>
      <title>Add OneMerge.getMergeInfo</title>
      <description>So you can get the SegmentCommitInfo for the newly merged segment.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6553</id>
      <title>Simplify how we handle deleted docs in read APIs</title>
      <description>Today, all scorers and postings formats need to be able to handle deleted documents. I suspect that the reason is that we want to be able to make sure to not perform costly operations on documents that are deleted. For instance if you run a phrase query, reading positions on a document which is deleted is useless. I suspect this is also a source of inefficiencies since in some cases we apply deleted documents several times: for instance conjunctions apply deleted docs to every sub scorer. However, with the new two-phase iteration API, we have a way to make sure that we never run expensive operations on deleted documents: we could first iterate over the approximation, then check that the document is not deleted, and finally confirm the match. Since approximations are cheap, applying deleted docs after them would not be an issue. I would like to explore removing the "Bits acceptDocs" parameter from TermsEnum.postings, Weight.scorer, SpanWeight.getSpans and Weight.BulkScorer, and add it to BulkScorer.score. This way, bulk scorers would be the only API which would need to know how to apply deleted docs, which I think would be more manageable since we only have 3 or 4 impls. And DefaultBulkScorer would be implemented the way described above: first advance the approximation, then check deleted docs, then confirm the match, then collect. Of course that's only in the case the scorer supports approximations, if it does not, it means it is cheap so we can directly iterate the scorer and check deleted docs on top.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6554</id>
      <title>ToBlockJoinFieldComparator wrapping is illegal</title>
      <description>The following test case triggers an AssertionError: public void testMissingValues() throws IOException { final Directory dir = newDirectory(); final RandomIndexWriter w = new RandomIndexWriter(random(), dir, newIndexWriterConfig(new MockAnalyzer(random())) .setMergePolicy(NoMergePolicy.INSTANCE)); w.addDocument(new Document()); w.getReader().close(); w.addDocument(new Document()); IndexReader reader = w.getReader(); w.close(); IndexSearcher searcher = newSearcher(reader); // all docs are parent BitDocIdSetFilter parentFilter = new BitDocIdSetCachingWrapperFilter(new QueryWrapperFilter(new MatchAllDocsQuery())); BitDocIdSetFilter childFilter = new BitDocIdSetCachingWrapperFilter(new QueryWrapperFilter(new MatchNoDocsQuery())); ToParentBlockJoinSortField sortField = new ToParentBlockJoinSortField( "some_random_field", SortField.Type.STRING, false, parentFilter, childFilter ); Sort sort = new Sort(sortField); TopFieldDocs topDocs = searcher.search(new MatchAllDocsQuery(), 1, sort); searcher.getIndexReader().close(); dir.close(); } java.lang.AssertionError at __randomizedtesting.SeedInfo.seed([E9D45D81F597AE4B:83490FC7D11D9ABA]:0) at org.apache.lucene.search.FieldComparator$TermOrdValComparator.setBottom(FieldComparator.java:800) at org.apache.lucene.search.FieldComparator$TermOrdValComparator.getLeafComparator(FieldComparator.java:783) at org.apache.lucene.search.join.ToParentBlockJoinFieldComparator.doSetNextReader(ToParentBlockJoinFieldComparator.java:83) at org.apache.lucene.search.SimpleFieldComparator.getLeafComparator(SimpleFieldComparator.java:36) at org.apache.lucene.search.FieldValueHitQueue.getComparators(FieldValueHitQueue.java:183) at org.apache.lucene.search.TopFieldCollector$NonScoringCollector.getLeafCollector(TopFieldCollector.java:141) at org.apache.lucene.search.FilterCollector.getLeafCollector(FilterCollector.java:40) at org.apache.lucene.search.AssertingCollector.getLeafCollector(AssertingCollector.java:48) at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:611) at org.apache.lucene.search.AssertingIndexSearcher.search(AssertingIndexSearcher.java:92) at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:424) at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:543) at org.apache.lucene.search.IndexSearcher.searchAfter(IndexSearcher.java:528) at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:455) at org.apache.lucene.search.join.TestBlockJoinSorting.testMissingValues(TestBlockJoinSorting.java:347) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1627) at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:836) at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:872) at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:886) at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50) at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46) at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49) at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65) at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48) at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365) at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798) at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458) at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:845) at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:747) at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:781) at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:792) at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46) at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42) at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39) at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39) at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54) at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48) at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65) at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55) at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365) at java.lang.Thread.run(Thread.java:745) The reason is that when a parent document does not have children, ToParentBlockJoinComparator simply omits to forward calls to copy to the wrapped comparator. So the wrapped comparator ends up with allocated slots that have 0 as an ordinal (the default value in an array) and a null value, which is illegal since 0 is a legal ordinal which can't map to null. This can't be fixed without adding new methods to the already crazy comparator API, so I think there is nothing we can do but remove this comparator. It would be possible to achieve the same functionnality by implementing something similar to SortedNumericSelector, except that it would have to select across several docs instead of values.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6560</id>
      <title>Handle "crosses dateline" cases in BKDPointInBBoxQuery</title>
      <description>Just like LUCENE-6547 but for BKD bbox queries ...</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6561</id>
      <title>Add a TermContextCache to IndexSearcher</title>
      <description>TermContexts can be quite expensive to build, and if you have fairly complex queries that re-use the same terms you can end up spending a lot of time re-building the same ones over and over again. It would be nice to be able to cache them on an IndexSearcher, so that they can be re-used.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6562</id>
      <title>GeoPointInBBoxQuery should compute ranges once, not per-segment</title>
      <description>We tried to do this in the original issue, using the same shard attributes that FuzzyQuery did, but it's not quite working...</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6567</id>
      <title>No need for out-of-order payload checks in SpanPayloadCheckQuery</title>
      <description>Since LUCENE-6537, all composite Spans implementations collect their payloads in-order, so we don't need special logic for that case anymore.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6569</id>
      <title>MultiFunction.anyExists - creating FunctionValues[] objects for every document</title>
      <description>In the class org.apache.lucene.queries.function.valuesource.MultiFunction there is the following method signature (line 52) public static boolean allExists(int doc, FunctionValues... values) this method is called from the class org.apache.lucene.queries.function.valuesource.DualFloatFunction (line 68) public boolean exists(int doc) { return MultiFunction.allExists(doc, aVals, bVals); } Because MultiFunction.allExists uses Java varargs syntax ("...") a new FunctionValues[] object will be created every time this call takes place. The problem is that the call takes place in a document level function, which means that it will create new objects in the heap for every document in the query results. for example if you use the following boost function (where ds and dc1 are both TrieDateField) bf=min(ms(ds,dc1),604800000) You will get extra objects created for each document in the result set, which has a big impact on performance and memory usage if you are searching a large result set.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6575</id>
      <title>Improve API of PhraseQuery.Builder</title>
      <description>From LUCENE-6531 In current PhraseQuery.Builder API. User must retype field again and again : PhraseQuery.Builder builder = new PhraseQuery.Builder(); builder.add(new Term("lyrics", "when"), 1); builder.add(new Term("lyrics", "believe"), 3); Cleaner API : PhraseQuery.Builder builder = new PhraseQuery.Builder("lyrics"); builder.add("when", 1); builder.add("believe", 3);</description>
      <attachments/>
      <comments>31</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6580</id>
      <title>Allow defined-width gaps in SpanNearQuery</title>
      <description>SpanNearQuery is not quite an exact Spans replacement for PhraseQuery at the moment, because while you can ask for an overall slop in an ordered match, you can't specify exactly where the gaps should appear.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6581</id>
      <title>Adjust PostingsHighlighter API to allow access to Passage[]</title>
      <description>I need to access the offset of the terms in the document. I believe that Passage holds this data, but the PostingsHighlighter API does not expose a method returning Passage[] - only String[]. Would it be possible/be a good idea to expose a method returning Passage[].</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6585</id>
      <title>Make ConjunctionDISI flatten sub ConjunctionDISI instances</title>
      <description>Today ConjunctionDISI wraps some sub (two-phase) iterators. I would like to improve it by flattening sub iterators when they implement ConjunctionDISI. In practice, this would make "+A +(+B +C)" be executed more like "+A +B +C" (only in terms of matching, scoring would not change). My motivation for this is that if we don't flatten and are unlucky, we can sometimes hit some worst cases. For instance consider the 3 following postings lists (sorted by increasing cost): A: 1, 1001, 2001, 3001, ... C: 0, 2, 4, 6, 8, 10, 12, 14, ... B: 1, 3, 5, 7, 9, 11, 13, 15, ... If we run "+A +B +C", then everything works fine, we use A as a lead, and advance B 1000 by 1000 to find the next match (if any). However if we run "+A +(+B +C)", then we would iterate B and C 2 by 2 over the entire doc ID space when trying to find the first match which occurs on or after A:1. This is an extreme example which is unlikely to happen in practice, but flattening would also help a bit on some more common cases. For instance imagine that A, B and C have respective costs of 100, 10 and 1000. If you search for "+A +(+B +C)", then we will use the most costly iterator (C) to confirm matches of B (the least costly iterator, used as a lead) while it would have been more efficient to confirm matches of B with A first, since A is less costly than C.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6587</id>
      <title>Move explain() to Scorer</title>
      <description>At the moment, the explanation API is on Weight, rather than on Scorer. This has a number of disadvantages: It means that Weights need to know about the scoring algorithms of their child scorers, which results in a leaky API (for example, the SloppyPhraseScorer has a package-private sloppyFreq() method which is only used by PhraseWeight.explain(), and SpanScorer has a similar public method that is again only called by explanation functions) It leads to lots of duplicated code - more or less every Weight.explain() method creates a Scorer, advances to the appropriate doc, and checks for a match It's very slow, because we create a new Scorer for every document I'd like to try moving explain() directly to Scorer. We can keep the old slow IndexSearcher.explain() API, but in addition explanations could now be generated efficiently in a Collector.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6594</id>
      <title>Add java.time forbidden-apis to trunk (update to forbiddenapis 2.0)</title>
      <description>The current version of forbiddenapis misses to add the new java.time APIs of Java 8, Some of the methods use default Locale or default Timezone. Until a new version of forbidden-apis is released, I would like to add those APIs to the Trunk (Java 8) base.txt signatures list. We already have some code in trunk using java.time (IndexWriter logging), so we should take care of that.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6607</id>
      <title>Move geo3d to Lucene's sandbox module</title>
      <description>Geo3d is a powerful low-level geo API, recording places on the earth's surface in the index in three dimensions (as 3 separate numbers) and offering fast shape intersection/distance testing at search time. Karl Wright originally contributed this in LUCENE-6196, and we put it in spatial module, but I think a more natural place for it, for now anyway, is Lucene's sandbox module: it's very new, its APIs/abstractions are very much in flux (and the higher standards for abstractions in the spatial module cause disagreements: LUCENE-6578), Karl Wright and others could iterate faster on changes in sandbox, etc. This would also un-block issues like LUCENE-6480, allowing GeoPointField and BKD trees to also use geo3d.</description>
      <attachments/>
      <comments>44</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6609</id>
      <title>FieldCacheSource (or it's subclasses) should override getSortField</title>
      <description>ValueSource defines the following method... public SortField getSortField(boolean reverse) { return new ValueSourceSortField(reverse); } ...where ValueSourceSortField builds up a ValueSourceComparator containing a double[] based on the FunctionValues of the original ValueSource. meanwhile, the abstract FieldCacheSource exists as a base implementation for classes like IntFieldSource and DoubleFieldSource which wrap a ValueSource around DocValues for the specified field. But neither FieldCacheSource nor any of it's subclasses override the getSortField(boolean) method – so attempting to sort on something like an IntFieldSource winds up using a bunch of ram to build that double[] to give users a less accurate sort (because of casting) then if they just sorted directly on the field. is there any good reason why FieldCacheSource subclases like IntFieldSource shouldn't all override getSortField with something like... public SortField getSortField(boolean reverse) { return new SortField(field, Type.INT, reverse); } ?</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6615</id>
      <title>IOUtils.spins should be moved to Directory.hasFastRandomAccess</title>
      <description>A spinoff from the discussion in LUCENE-6614.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6616</id>
      <title>IndexWriter should list files once on init, and IFD should not suppress FNFE</title>
      <description>Some nice ideas Robert Muir had for cleaning up IW/IFD on init ...</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6617</id>
      <title>Reduce FST's ram usage</title>
      <description>Spinoff from LUCENE-6199, pulling out just the FST RAM reduction changes. The FST data structure tries to be a RAM efficient representation of a sorted map, but there are a few things I think we can do to trim it even more: Don't store arc and node count: this is available from the Builder if you really want to do something with it. Don't use the "paged" byte store unless the FST is huge; just use a single byte[] Some members like lastFrozenNode, reusedBytesPerArc, allowArrayArcs are only used during building, so we should move them to the Builder We don't need to cache NO_OUTPUT: we can ask the Outputs impl for it</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6624</id>
      <title>provide a BookendFilter to make the "exact match against an entire (tokenized) field value" usecase easy</title>
      <description>A question that seems to pop up every now and then is how to require an "exact match" against "an entire field value" even while using some sort of analysis feature (ie: stopwords, or lowercasing, or whitespace normalization). In other words: instead of a literal, byte for byte, "exact match" (eg: new StringField(f, val, Store.NO) at index time; new TermQuery(new Term(f, val)) at query time) some folks want to use some Tokenizer and TokenFilter but then require that a "PhraseQuery" (or SpanNearQuery) on the input matches the entire field value, w/o any terms left over. Example: they want a (phrase) queries like "The Quick Brown Dog" and "quick BROWN dog" to both match a document indexed with a field value "The Quick Brown Dog." because their analyzer tokenizes both the query &amp; the field value into quick | brown | dog (standard tokenizer + stopword &amp; lowercase filters) – BUT – on the other hand they don't want either of those phrase queries to match a document with a field value of "I Love the Quick Brown Dog" because that field value includes additional terms not covered by the query. A suggestion i've seen for years in response to this type of question is that folks can "inject marker tokens" at the begining and end of both the field values &amp; query, and then (as long as there is no "slop" on the phrase queries) they should get the matches they expect. The hackish way to do this being to just prepend and append some strings that won' be found in their data and won't be striped out by their tokenizer or any token filters (eg: new TextField(f, "VAL_START_XYZABC " + val + " VAL_END_XYZABC", Store.NO) at index time; queryBuilder.createPhraseQuery(f, "VAL_START_XYZABC " + val + " VAL_END_XYZABC") at query time). Unless i'm missing something, it should be fairly trivial to write a "BookendFilter" that that does this automatically for users: the first time incrementToken() is called, produce a synthetic "start" token with some CharTermAttribute that is uses a non-printing unicode sequence (overridable by user config) after that, all calls to incrementToken() proxy to the wrapped stream until it's exhausted after that, when incrementToken() is called, produce a synthetic "end" token with some CharTermAttribute that is uses a non-printing unicode sequence (overridable by user config) both synthetic tokens should have KeywordAttribute == true ...At index time the sythetic tokens will be indexed as terms, and if the same analyzer is used at query time to build a PhraseQuery those terms will be the first and last terms in the PhraseQuery.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6631</id>
      <title>Lucene Document Classification</title>
      <description>Currently the Lucene Classification module supports the classification for an input text using the Lucene index as a trained model. This improvement is adding to the module a set of components to provide Document classification ( where the Document is a Lucene document ). All selected fields from the Document will have their part in the classification ( including the use of the proper Analyzer per field).</description>
      <attachments/>
      <comments>20</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6632</id>
      <title>Geo3d: More accurate way of computing circle planes</title>
      <description>The Geo3d code that computes circle planes in GeoPath and GeoCircle is less accurate the smaller the circle. There's a better way of computing this.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6635</id>
      <title>Provide Programmatic Access to CheckIndex: checkFields()</title>
      <description>Would like to have programmatic access to CheckIndex.checkFields() field-by-field. This allows the developer to check different fields in different threads concurrently.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6638</id>
      <title>Factor graph flattening out of SynonymFilter</title>
      <description>Spinoff from LUCENE-6582. SynonymFilter is very hairy, and has known nearly-impossible-to-fix bugs: it produces the wrong graph, both accepting too many phrases and not enough phrases, because it never creates new positions. This makes improvements like LUCENE-6582, to fix some of its bugs, unnecessarily hard. I'd like to pull out the graph flattening into its own token filter, and I think I have a starting patch that works. I started with the "sausagizer" stage on the branch from LUCENE-5012, but changed the approach so that it should not have so many adversarial cases. I think this should make SynonymFilter quite a bit simpler, hopefully to the point where we can just fix its bugs already.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6641</id>
      <title>Idea CodeSyle should be enriched</title>
      <description>Currently Idea CodeStyle has been fixed for latest intelljIdea version but it is not complete. For example it does not contain spaces management ( space within method params, space between operators ext )for the Java language ( and maybe for the other languages involved as well ) . We should define a complete standard ( as some inconsistencies are in the committed code as well) .</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6644</id>
      <title>Symlinked checkout folder causes access denied exceptions</title>
      <description>Had to make some space on my drive and moved certain repositories to a different volume (via windows junction). This causes exceptions from the security manager if the tests are run from the original location (which resolves to a different path). Don't have any thoughts about this (whether it should be fixed or how), just wanted to make note it's the case. [junit4] &gt; Throwable #1: java.security.AccessControlException: access denied ("java.io.FilePermission" "D:\Work\lu cene\trunk\solr\build\solr-solrj\test\J1\temp\solr.common.util.TestJavaBinCodec_25A5FF5CB51DB333-001" "write") [junit4] &gt; at __randomizedtesting.SeedInfo.seed([25A5FF5CB51DB333]:0) [junit4] &gt; at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457) [junit4] &gt; at java.security.AccessController.checkPermission(AccessController.java:884) [junit4] &gt; at java.lang.SecurityManager.checkPermission(SecurityManager.java:549) [junit4] &gt; at java.lang.SecurityManager.checkWrite(SecurityManager.java:979) [junit4] &gt; at sun.nio.fs.WindowsPath.checkWrite(WindowsPath.java:799) [junit4] &gt; at sun.nio.fs.WindowsFileSystemProvider.createDirectory(WindowsFileSystemProvider.java:491) [junit4] &gt; at org.apache.lucene.mockfile.FilterFileSystemProvider.createDirectory(FilterFileSystemProvider. java:133) [junit4] &gt; at org.apache.lucene.mockfile.FilterFileSystemProvider.createDirectory(FilterFileSystemProvider. java:133) [junit4] &gt; at org.apache.lucene.mockfile.FilterFileSystemProvider.createDirectory(FilterFileSystemProvider. java:133) [junit4] &gt; at org.apache.lucene.mockfile.FilterFileSystemProvider.createDirectory(FilterFileSystemProvider. java:133) [junit4] &gt; at java.nio.file.Files.createDirectory(Files.java:674) [junit4] &gt; at org.apache.lucene.util.LuceneTestCase.createTempDir(LuceneTestCase.java:2584) [junit4] &gt; at org.apache.solr.SolrTestCaseJ4.beforeClass(SolrTestCaseJ4.java:201) [junit4] &gt; at java.lang.Thread.run(Thread.java:745)</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6645</id>
      <title>BKD tree queries should use BitDocIdSet.Builder</title>
      <description>When I was iterating on BKD tree originally I remember trying to use this builder (which makes a sparse bit set at first and then upgrades to dense if enough bits get set) and being disappointed with its performance. I wound up just making a FixedBitSet every time, but this is obviously wasteful for small queries. It could be the perf was poor because I was always .or'ing in DISIs that had 512 - 1024 hits each time (the size of each leaf cell in the BKD tree)? I also had to make my own DISI wrapper around each leaf cell... maybe that was the source of the slowness, not sure. I also sort of wondered whether the SmallDocSet in spatial module (backed by a SentinelIntSet) might be faster ... though it'd need to be sorted in the and after building before returning to Lucene.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6647</id>
      <title>Add GeoHash String Utilities to core GeoUtils</title>
      <description>GeoPointField uses morton encoding to efficiently pack lat/lon values into a single long. GeoHashing effectively does the same thing but uses base 32 encoding to represent this long value as a "human readable" string. Many user applications already use the string representation of the hash. This issue simply adds the base32 string representation of the already computed morton code.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6651</id>
      <title>Remove private field reflection (setAccessible) in AttributeImpl#reflectWith</title>
      <description>In AttributeImpl we currently have a "default" implementation of reflectWith (which is used by toString() and other methods) that uses reflection to list all private fields of the implementation class and reports them to the AttributeReflector (used by Solr and Elasticsearch to show analysis output). Unfortunately this default implementation needs to access private fields of a subclass, which does not work without doing Field#setAccessible(true). And this is done without AccessController#doPrivileged()! There are 2 solutions to solve this: Reimplement the whole thing with MethodHandles. MethodHandles allow to access private fields, if you have a MethodHandles.Lookup object created from inside the subclass. The idea is to add a protected constructor taking a Lookup object (must come from same class). This Lookup object is then used to build methodHandles that can be executed to report the fields. Backside: We have to require subclasses that want this "automatic" reflection to pass a Lookup object in ctor's super(MethodHandles.lookup()) call. This breaks backwards for implementors of AttributeImpls The second idea is to remove the whole reflectWith default impl and make the method abstract. This would require a bit more work in tons of AttributeImpl classes, but you already have to implement something like this for equals/hashCode, so its just listing all fields. This would of couse break backwards, too. So my plan would be to implement the missing methods everywhere (as if it were abstract), but keep the default implementation in 5.x. We just would do AccessController.doPrivileged().</description>
      <attachments/>
      <comments>21</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6653</id>
      <title>Cleanup TermToBytesRefAttribute</title>
      <description>While working on LUCENE-6652, I figured out that there were so many test with wrongly implemented TermsToBytesRefAttribute. In addition, the whole concept back from Lucene 4.0 was no longer correct: We don't return the hash code anymore; it is calculated by BytesRefHash The interface is horrible to use. It tends to reuse the BytesRef instance but the whole thing is not correct. Instead we should remove the fillBytesRef() method from the interface and let getBytesRef() populate and return the BytesRef. It does not matter if the attribute reuses the BytesRef or returns a new one. It just get consumed like a standard CharTermAttribute. You get a BytesRef and can use it until you call incrementToken(). As the TermsToBytesRefAttribute is marked experimental, I see no reason why we should not change the semantics to be more easy to understand and behave like all other attributes. I will add a note to the backwards incompatible changes in Lucene 5.3.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6654</id>
      <title>KNearestNeighborClassifier not taking in consideration Class ranking</title>
      <description>Currently the KNN Classifier assign the score for a ClassificationResult, based only on the frequency of the class in the top K results. This is conceptually a simplification. Actually the ranking must take a part. If not this can happen : Top 4 1) Class1 2) Class1 3) Class2 4) Class2 As a result of this Top 4 , both the classes will have the same score. But the expected result is that Class1 has a better score, as the MLT score the documents accordingly.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6661</id>
      <title>Allow queries to opt out of caching</title>
      <description>Some queries have out-of-band dependencies that make them incompatible with caching, it'd be great if they could opt out of the new fancy query/filter cache in IndexSearcher. This affects DrillSidewaysQuery and any user-provided custom Query implementations.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6664</id>
      <title>Replace SynonymFilter with SynonymGraphFilter</title>
      <description>Spinoff from LUCENE-6582. I created a new SynonymGraphFilter (to replace the current buggy SynonymFilter), that produces correct graphs (does no "graph flattening" itself). I think this makes it simpler. This means you must add the FlattenGraphFilter yourself, if you are applying synonyms during indexing. Index-time syn expansion is a necessarily "lossy" graph transformation when multi-token (input or output) synonyms are applied, because the index does not store posLength, so there will always be phrase queries that should match but do not, and then phrase queries that should not match but do. http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html goes into detail about this. However, with this new SynonymGraphFilter, if instead you do synonym expansion at query time (and don't do the flattening), and you use TermAutomatonQuery (future: somehow integrated into a query parser), or maybe just "enumerate all paths and make union of PhraseQuery", you should get 100% correct matches (not sure about "proper" scoring though...). This new syn filter still cannot consume an arbitrary graph.</description>
      <attachments/>
      <comments>45</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>6668</id>
      <title>Optimize SortedSet/SortedNumeric storage for the few unique sets use-case</title>
      <description>Robert suggested this idea: if there are few unique sets of values, we could build a lookup table and then map each doc to an ord in this table, just like we already do for table compression for numerics. I think this is especially compelling given that SortedSet/SortedNumeric are our two only doc values types that use O(maxDoc) memory because of the offsets map. When this new strategy is used, memory usage could be bounded to a constant.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6676</id>
      <title>Remove isActive boolean from DWPTP.ThreadState</title>
      <description>I think this boolean is redundant with DocumentsWriter.closed, and the boolean means ThreadState can confusingly be in 3 different states.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6685</id>
      <title>GeoPointInBBox/Distance queries should have safeguards</title>
      <description>These queries build a big list of term ranges, where the size of the list is in proportion to how many cells of the space filling curve are "crossed" by the perimeter of the query (I think?). This can easily be 100s of MBs for a big enough query ... not to mention slow to enumerate (we still do this again for each segment). I think the queries should have safeguards, much like we have maxDeterminizedStates for Automaton based queries, to prevent accidental OOMEs. But I think longer term we should either change the ranges to be enumerated on-demand and never stored in entirety (like NumericRangeTermsEnum), or change the query so it has a fixed budget of how many cells it's allowed to visit and then within a crossing cell it uses doc values to post-filter.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6686</id>
      <title>Improve InforStream API</title>
      <description>Currently, We use InfoStream in duplicated ways. For example if (infoStream.isEnabled("IW")) { infoStream.message("IW", "init: loaded commit \"" + commit.getSegmentsFileName() + "\""); } Can we change the API of InfoStream to infoStream.messageIfEnabled("component","message");</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6690</id>
      <title>Speed up MultiTermsEnum.next()</title>
      <description>OrdinalMap is very useful when computing top terms on a multi-index segment. However I've seen it being occasionally slow to build, which was either making facets (when the ordinals map is computed lazily) or reopen (when computed eagerly) slow. So out of curiosity, I tried to profile ordinal map building on a simple index: 10M random strings of length between 0 and 20 stored as a SORTED doc values field. The index has 19 segments. The bottleneck was MultiTermsEnum.next() (by far) due to lots of BytesRef comparisons (UTF8SortedAsUnicodeComparator). MultiTermsEnum stores sub enums in two different places: top: a simple array containing all enums on the current term queue: a queue for enums that are not exhausted yet but beyond the current term. A non-exhausted enum is in exactly one of these data-structures. When moving to the next term, MultiTermsEnum advances all enums in top, then adds them to queue and finally, pops all enum that are on the same term back into top. We could save reorderings of the priority queue by not removing entries from the priority queue and then calling updateTop to advance enums which are on the current term. This is already what we do for disjunctions of doc IDs in DISIPriorityQueue. On the index described above and current trunk, building an OrdinalMap has to call UTF8SortedAsUnicodeComparator.compare 80114820 times and runs in 1.9 s. With the change, it calls UTF8SortedAsUnicodeComparator.compare 36900694 times, BytesRef.equals 16297638 times and runs in 1.4s (~26% faster).</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6694</id>
      <title>Snowball stemmer for Lithuanian language</title>
      <description>Snowball stemmer for Lithuanian language.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6695</id>
      <title>BlendedTermQuery</title>
      <description>It is sometimes desirable to ignore differences between index statistics of several terms so that they produce the same scores, for instance if you resolve synonyms at search time or if you want to search across several fields. Elasticsearch has been using this approach for its multi_match query for some time now. We already blend statistics in TopTermsBlendedFreqScoringRewrite (used by FuzzyQuery) but it could be helpful to have a dedicated query to choose manually which terms to blend stats from.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6697</id>
      <title>Use 1D KD tree for alternative to postings based numeric range filters</title>
      <description>Today Lucene uses postings to index a numeric value at multiple precision levels for fast range searching. It's somewhat costly: each numeric value is indexed with multiple terms (4 terms by default) ... I think a dedicated 1D BKD tree should be more compact and perform better. It should also easily generalize beyond 64 bits to arbitrary byte[], e.g. for LUCENE-5596, but I haven't explored that here. A 1D BKD tree just sorts all values, and then indexes adjacent leaf blocks of size 512-1024 (by default) values per block, and their docIDs, into a fully balanced binary tree. Building the range filter is then just a recursive walk through this tree. It's the same structure we use for 2D lat/lon BKD tree, just with 1D instead. I implemented it as a DocValuesFormat that also writes the numeric tree on the side.</description>
      <attachments/>
      <comments>33</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6698</id>
      <title>Add BKDDistanceQuery</title>
      <description>Our BKD tree impl should be very fast at doing "distance from lat/lon center point &lt; X" query. I haven't started this ... Nicholas Knize expressed interest in working on it.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6699</id>
      <title>Integrate lat/lon BKD and spatial3d</title>
      <description>I'm opening this for discussion, because I'm not yet sure how to do this integration, because of my ignorance about spatial in general and spatial3d in particular Our BKD tree impl is very fast at doing lat/lon shape intersection (bbox, polygon, soon distance: LUCENE-6698) against previously indexed points. I think to integrate with spatial3d, we would first need to record lat/lon/z into doc values. Somewhere I saw discussion about how we could stuff all 3 into a single long value with acceptable precision loss? Or, we could use BinaryDocValues? We need all 3 dims available to do the fast per-hit query time filtering. But, second: what do we index into the BKD tree? Can we "just" index earth surface lat/lon, and then at query time is spatial3d able to give me an enclosing "surface lat/lon" bbox for a 3d shape? Or ... must we index all 3 dimensions into the BKD tree (seems like this could be somewhat wasteful)?</description>
      <attachments/>
      <comments>220</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6702</id>
      <title>[suggest] Make Context Query and Field extensible</title>
      <description>ContextSuggestField indexes context information along with suggestions, which can be used to filter and/or boost suggestions using ContextQuery. It would be useful to make ContextSuggestField extensible such that subclasses can inject context values at index-time, without having to specify the contexts in its ctor. ContextQuery can be made extensible by allowing sub-classes to override how context automaton is created from provided query contexts. Currently, ContextQuery uses a context value of "*" to consider all context values, It makes sense to have a dedicated addAllContexts() instead.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6706</id>
      <title>Support Payload scoring for all SpanQueries</title>
      <description>I need a way to have payloads influence the score of SpanOrQuery's.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6708</id>
      <title>TopFieldCollector sometimes calls Scorer.score() several times on the same doc</title>
      <description>If the sort spec includes a sort field that needs scores, and if trackDocScores or trackMaxScore is set, then TopFieldCollectors may compute the score several times on the same document, once to check whether the hit is competitive, and once to update maxScore or to set the score on the ScoreDoc.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6709</id>
      <title>Add RandomAccessOrds support to UninvertedReader's SortedSetDocValues impl (DocTermOrds.Iterator)</title>
      <description>UninvertedReader's SortedSetDocValues impl doesn't implement the RandomAccessOrds API, so it can't be used with SortedSetSelector.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6710</id>
      <title>GeoPointField should use full 64 bit encoding</title>
      <description>Current GeoPointField uses 31 bits for each lat and long, respectively. This causes a precision error for the maximum bounds for 2D mapping applications (e.g., instead of maximum of 180, 90 the max value handled is 179.999999, 89.999999). This issue improves precision for the full 2D map boundaries by using the full 32 bit range for lat/lon values, resulting in a morton hash using the full 64 bit range.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6712</id>
      <title>GeoPointField should cut over to DocValues for boundary filtering</title>
      <description>Currently GeoPointField queries only use the Terms Dictionary for ranges that fall within and on the boundary of the query shape. For boundary ranges the full precision terms are iterated, for within ranges the postings list is used. Instead of iterating full precision terms for boundary ranges, this enhancement cuts over to DocValues for post-filtering boundary terms. This allows us to increase precisionStep for GeoPointField thereby reducing the number of terms and the size of the index. This enhancement should also provide a boost in query performance since visiting more docs and fewer terms should be more efficient than visiting fewer docs and more terms.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6713</id>
      <title>TooComplexToDeterminizeException claims to be serializable but actually isn't?</title>
      <description>This custom exception class, added in LUCENE-6046, claims to be Serializable since in inherits from Throwable yet if you try to serialize it you'll hit runtime exceptions because its members don't implement Serializable. We intentionally pushed Java serialization requirements out of Lucene a while back (LUCENE-2908), but maybe for custom exception classes which unfortunately necessarily claim to implement Serializable we need to do something? We could just mark the members transient here, but that would mean when you unserialize you get null members on the other end, e.g. you would no longer know which RegExp was problematic ...</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6714</id>
      <title>Expose version and resource description in CorruptIndexException and friends</title>
      <description>It would be nice to access the minVersion, maxVersion in IndexTooNewException and IndexTooOldException as well as the resoruce description in CorruptIndexException programmatically. I'd love to use this to support better serialization on top of those exception as well as structured responses from the individual values.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6716</id>
      <title>Improve SpanPayloadCheckQuery API</title>
      <description>SpanPayloadCheckQuery currently takes a Collection&lt;byte[]&gt; to check its payloads against. This is suboptimal a) because payloads internally use BytesRef rather than byte[] and b) Collection is unordered, but the implementation does actually care about the order in which the payloads appear. We should change the constructor to take a List&lt;BytesRef&gt; instead.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6717</id>
      <title>TermAutomatonQuery should be two-phased</title>
      <description>TermAutomatonQuery (still in sandbox) is a simple way to get accurate query-time multi-token synonyms using the new SynonymGraphFilter from LUCENE-6664. It already has a utility class to directly translate an incoming TokenStream into a corresponding query. However the query is likely quite slow because it always iterates positions for all terms in the automaton. I think one simple approach is to walk the automaton and find the subset of terms (if any) that appear in common to all paths, and then approximate with ConjunctionDISI like PhraseQuery does. Such a subset doesn't always exist for an automaton (i.e. it could be empty), so the logic would have to be conditional... And I think there are more complex approximations we could make, but using ConjunctionDISI seems like a simple start.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6724</id>
      <title>Add support for computing GeoHash neighbors to GeoHashUtils</title>
      <description>This simple feature adds the ability to compute the geohash neighbor(s) at a given level, from a provided geohash. Such a utility is beneficial for simple grid faceting/aggregations and geohash based bounding box queries.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6729</id>
      <title>Upgrade ASM version to 5.0.4 (expressions / Solr's TIKA)</title>
      <description>The expressions module currently uses ASM 4 for generating class files. We should upgrade to ASM 5.0.4. In addition, with that version we can create Java 8 class files on trunk. There is a clash with Apache TIKA, which uses the same old 4.x version. But ASM 5 is still compatible to ASM 4, if you pass the old ASM version to your own visitors. But on long term TIKA should upgrade, too, because currently it cannot parse Java 8 class files. ASM 5.0 also fixes some bugs with Java 7 class files, so we should really upgrade.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6732</id>
      <title>Improve validate-source-patterns in build.xml (e.g., detect invalid license headers!!)</title>
      <description>Today I enabled warnings analysis on Policeman Jenkins. This scans the build log for warnings by javac and reports them in statistics, together with source file dumps. When doing that I found out that someone added again a lot of "invalid" license headers using /** instead a simple comment. This causes javadocs warnings under some circumstances, because /** is start of javadocs and not a license comment. I then tried to fix the validate-source-patterns to detect this, but due to a bug in ANT, the &lt;containsregexp/&gt; filter is applied per line (although it has multiline matching capabilities!!!). So I rewrote our checker to run with groovy. This also has some good parts: it tells you wwhat was broken, otherwise you just know there is an error, but not whats wrong (tab, nocommit,...) its much faster (multiple &lt;containsregexp/&gt; read file over and over, this one reads file one time into a string and then applies all regular expressions).</description>
      <attachments/>
      <comments>28</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6737</id>
      <title>Add DecimalDigitFilter</title>
      <description>TokenFilter that folds all unicode digits (http://unicode.org/cldr/utility/list-unicodeset.jsp?a=[:General_Category=Decimal_Number:]) to 0-9. Historically a lot of the impacted analyzers couldn't even tokenize numbers at all, but now they use standardtokenizer for numbers/alphanum tokens. But its usually the case you will find e.g. a mix of both ascii digits and "native" digits, and today that makes searching difficult. Note this only impacts decimal digits, hence the name DecimalDigitFilter. So no processing of chinese numerals or anything crazy like that.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6740</id>
      <title>Reduce warnings emitted by javac in Java 7/Java 8</title>
      <description>This is a overview issue about improvements to reduce warnings during the build. Since we moved trunk to Java 8 and branch_5x to Java 7, there were introduced some additional warnings, mostly in generated code. ANTLR4 does automatically add the needed @SuppressWarnings, but jflex does not. There are also some rawtypes warnings, which changed in Java 7 (before Java 7 there were only "unsafe" warnings, now you need both. I enabled the Warnings tracker in Policeman Jenkins, we can therefore very easily check the warnings: e.g. on trunk: http://jenkins.thetaphi.de/job/Lucene-Solr-trunk-Linux/ (see diagram on the right), you can click on it, e.g. http://jenkins.thetaphi.de/job/Lucene-Solr-trunk-Linux/warnings4</description>
      <attachments/>
      <comments>20</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6743</id>
      <title>Allow Ivy lockStrategy to be overridden by system property.</title>
      <description>The current hard code lock strategy is imperfect and can fail under parallel load. With Ivy 2.4 there is a better option in artifact-lock-nio. We should allow the lock strategy to be overrideen like the resolutionCacheDir.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6746</id>
      <title>Compound queries should create sub weights through IndexSearcher</title>
      <description>By creating sub weights through IndexSearcher, we give IndexSearcher a chance to add a caching wrapper. We were already doing it for BooleanQuery and ConstantScoreQuery but forgot to also modify DisjunctionMaxQuery, BoostingQuery and BoostedQuery.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6747</id>
      <title>FingerprintFilter - a TokenFilter for clustering/linking purposes</title>
      <description>A TokenFilter that emits a single token which is a sorted, de-duplicated set of the input tokens. This approach to normalizing text is used in tools like OpenRefine[1] and elsewhere [2] to help in clustering or linking texts. The implementation proposed here has a an upper limit on the size of the combined token which is output. [1] https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth [2] https://rajmak.wordpress.com/2013/04/27/clustering-text-map-reduce-in-python/</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6754</id>
      <title>Optimize IndexSearcher.count for simple queries</title>
      <description>IndexSearcher.count currently always create a collector to compute the number of hits, but it could optimize some queries like MatchAllDocsQuery or TermQuery.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6756</id>
      <title>Give MatchAllDocsQuery a dedicated BulkScorer</title>
      <description>MatchAllDocsQuery currently uses the default BulkScorer, which creates a Scorer and iterates over matching doc IDs up to NO_MORE_DOCS. I tried to build a dedicated BulkScorer, which seemed to help remove abstractions as it helped improve throughput by a ~2x factor with simple collectors.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6759</id>
      <title>Integrate lat/long BKD and spatial 3d, part 2</title>
      <description>This is just a continuation of LUCENE-6699, which became too big.</description>
      <attachments/>
      <comments>115</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6762</id>
      <title>CheckIndex cannot "fix" indexes that have individual segments with missing or corrupt .si files because sanity checks will fail trying to read the index initially.</title>
      <description>Seems like we should still be able to partially recover by dropping these segments with CheckIndex.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6764</id>
      <title>Payloads should be compressed</title>
      <description>I think we should at least try to do something simple, eg. deduplicate or apply simple LZ77 compression. For instance if you use enclosing html tags to give different weights to individual terms, there might be lots of repetitions as there are not that many unique html tags.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6766</id>
      <title>Make index sorting a first-class citizen</title>
      <description>Today index sorting is a very expert feature. You need to use a custom merge policy, custom collectors, etc. I would like to explore making it a first-class citizen so that: the sort order could be configured on IndexWriterConfig segments would record the sort order that was used to write them IndexSearcher could automatically early terminate when computing top docs on a sort order that is a prefix of the sort order of a segment (and if the user is not interested in totalHits).</description>
      <attachments/>
      <comments>40</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6770</id>
      <title>FSDirectory ctor should use getAbsolutePath instead of getRealPath for directory</title>
      <description>After upgrade from 4.1 to 5.2.1 I found that one of our test failed. Appeared the guilty was FSDirectory that converts given Path to Path.getRealPath. As result the test will fail: Path p = Paths.get("/var/lucene_store"); FSDirectory d = new FSDirectory(p); assertEquals(p.toString(), d.getDirectory().toString()); It because /var/lucene_store is a symlink and Path directory =path.getRealPath(); resolves it to /private/var/lucene_store I think this is bad design decision because "direcrory" isn't just internal state but is exposed in a public interface and "getDirectory()" is widely used to initialize other components. It should use paths.getAbsolutePath() instead. build and "ant test" were successful after fix.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6773</id>
      <title>Always flatten nested conjunctions</title>
      <description>LUCENE-6585 started the work to flatten nested conjunctions, but this only works with approximations. Otherwise a ConjunctionScorer is passed to ConjunctionDISI.intersect, and is not flattened since it is not an instance of ConjunctionDISI.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6775</id>
      <title>Improve MorfologikFilterFactory to allow arbitrary dictionaries from ResourceLoader</title>
      <description>Followup issue for LUCENE-6774: The filter ctor already allows to pass any dictionary to the filter, but you have no chance to configure this through the Factory (CustomAnalyzer/Solr/Elasticsearch/...). This will add 2 parameters to the factory (exclusive with the dictionary string specifying language, default "pl"), to load FSA (dictionary) and corresponding property file (metadata/featureData). This dictionary could be placed, e.g. in Solr's conf dir and loaded, because this would be done via ResourceLoader. Alternatively the language could still be passed, but must be part of JAR file distribution. Currently this defaults to "pl" at the moment and plain Lucene does not allow more, unless you add own JAR files. So practically, the parameter is useless for a pure, uncustomized Lucene-Impl.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6777</id>
      <title>Switch GeoPointTermsEnum range list to use a reusable BytesRef</title>
      <description>GeoPointTermsEnum currently constructs a BytesRef for every computed range, then sorts on this BytesRef. This adds an unnecessary memory overhead since the TermsEnum only requires BytesRef on calls to nextSeekTerm and accept and the ranges only need to be sorted by their long representation. This issue adds the following two improvements: 1. Lazily compute the BytesRef on demand only when its needed 2. Add a single, transient BytesRef to GeoPointTermsEnum This will further cut back on heap usage when constructing ranges across every segment.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6778</id>
      <title>Add GeoPointDistanceRangeQuery support for GeoPointField types</title>
      <description>GeoPointDistanceQuery currently handles a single point distance. This improvement adds a GeoPointDistanceRangeQuery for supporting use cases such as: find all points between 10km and 20km of a known location.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6779</id>
      <title>Reduce memory allocated by CompressingStoredFieldsWriter to write large strings</title>
      <description>In SOLR-7927, I am trying to reduce the memory required to index very large documents (between 10 to 100MB) and one of the places which allocate a lot of heap is the UTF8 encoding in CompressingStoredFieldsWriter. The same problem existed in JavaBinCodec and we reduced its memory allocation by falling back to a double pass approach in SOLR-7971 when the utf8 size of the string is greater than 64KB. I propose to make the same changes to CompressingStoredFieldsWriter as we made to JavaBinCodec in SOLR-7971.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6784</id>
      <title>Enable query caching by default</title>
      <description>Now that our main queries have become immutable, I would like to revisit enabling the query cache by default.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6785</id>
      <title>Consider merging Query.rewrite() into Query.createWeight()</title>
      <description>Prompted by the discussion on LUCENE-6590. Query.rewrite() is a bit of an oddity. You call it to create a query for a specific IndexSearcher, and to ensure that you get a query implementation that has a working createWeight() method. However, Weight itself already encapsulates the notion of a per-searcher query. You also need to repeatedly call rewrite() until the query has stopped rewriting itself, which is a bit trappy - there are a few places (in highlighting code for example) that just call rewrite() once, rather than looping round as IndexSearcher.rewrite() does. Most queries don't need to be called multiple times, however, so this seems a bit redundant. And the ones that do currently return un-rewritten queries can be changed simply enough to rewrite them. Finally, in pretty much every case I can find in the codebase, rewrite() is called purely as a prelude to createWeight(). This means, in the case of for example large BooleanQueries, we end up cloning the whole query structure, only to throw it away immediately. I'd like to try removing rewrite() entirely, and merging the logic into createWeight(), simplifying the API and removing the trap where code only calls rewrite once. What do people think?</description>
      <attachments/>
      <comments>15</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6786</id>
      <title>Remove IndexFileDeleter.refresh(String)</title>
      <description>Today IW has try/finally around places (flush, merge) that write per-segment files and on exception uses IFD.refresh(String) to remove any newly created but now unreferenced files. But, since merge exceptions are now tragic, and IFD.refresh() (not taking a segment name) will be called, doing the same thing, I think it's safe to remove the per-segment refresh. This makes IW a little bit simpler ... and it means at least one fewer Directory.listAll while handling merge exceptions.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6787</id>
      <title>BooleanQuery should be able to drop duplicate non-scoring clauses</title>
      <description>Pulling out of the discussion on LUCENE-6305. BooleanQuery could drop duplicate non-scoring (MUST_NOT, FILTER) clauses.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6789</id>
      <title>change IndexSearcher default similarity to BM25</title>
      <description>Since Lucene 4.0, the statistics needed for this are always present, so we can make the change without any degradation. I think the change should be a 6.0 change only: it will prevent any surprises. DefaultSimilarity is renamed to ClassicSimilarity to prevent confusion. No indexing change is needed as we use the same norm format, its just a runtime switch. Users can just do IndexSearcher.setSimilarity(new ClassicSimilarity()) to get the old behavior. I did not change solr's default here, I think that should be a separate issue, since it has more concerns: e.g. factories in configuration files and so on. One issue was the generation of synonym queries (posinc=0) by QueryBuilder (used by parsers). This is kind of a corner case (query-time synonyms), but we should make it nicer. The current code in trunk disables coord, which makes no sense for anything but the vector space impl. Instead, this patch adds a SynonymQuery which treats occurrences of any term as a single pseudoterm. With english wordnet as a query-time synonym dict, this query gives 12% improvement in MAP for title queries on BM25, and 2% with Classic (not significant). So its a better generic approach for synonyms that works with all scoring models. I wanted to use BlendedTermQuery, but it seems to have problems at a glance, it tries to "take on the world", it has problems like not working with distributed scoring (doesn't consult indexsearcher for stats). Anyway this one is a different, simpler approach, which only works for a single field, and which calls tf(sum) a single time.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6794</id>
      <title>Fix TestSecurityManager to work with IDEs</title>
      <description>Actually both these IDEs work, they just emit confusing noise when it tries to System.exit.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6798</id>
      <title>Add geo3d support for zero-width circles</title>
      <description>The current GeoCircleFactory can't make zero-width circles. We should fix that.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6800</id>
      <title>Regularize how Geo3d XYZSolids are managed, as was done for GeoCircle</title>
      <description>The instantiation of XYZSolid-family objects is currently done through a factory method in GeoAreaFactory. For consistency, there should be the following: underlying interface called XYZSolid, which extends GeoArea a XYZSolidFactory factory class, that instantiates XYZSolid objects reference to XYZSolidFactory.makeXYZSolid() from within GeoAreaFactory</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6805</id>
      <title>Add a general purpose readonly interace to BitSet</title>
      <description>BitSet has many more readonly methods compared to Bits. Similarly, BitSet has many more write methods compared to MutableBits. This Jira issue is to add a new ImmutableBits interface to BitSet that includes all ready only methods of BitSet.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6810</id>
      <title>Upgrade to Spatial4j 0.5</title>
      <description>Spatial4j 0.5 was released a few days ago. There are some bug fixes, most of which were surfaced via the tests here. It also publishes the test jar (thanks Nicholas Knize for that one) and with that there are a couple test utilities here I can remove. https://github.com/locationtech/spatial4j/blob/master/CHANGES.md</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6815</id>
      <title>Should DisjunctionScorer advance more lazily?</title>
      <description>Today if you call DisjunctionScorer.advance(X), it will try to advance all sub scorers to X. However, if DisjunctionScorer is being intersected with another scorer (which is almost always the case as we use BooleanScorer for top-level disjunctions), we could stop as soon as we find one matching sub scorer, and only advance the remaining sub scorers when freq() or score() is called.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6818</id>
      <title>Implementing Divergence from Independence (DFI) Term-Weighting for Lucene/Solr</title>
      <description>As explained in the write-up, many state-of-the-art ranking model implementations are added to Apache Lucene. This issue aims to include DFI model, which is the non-parametric counterpart of the Divergence from Randomness (DFR) framework. DFI is both parameter-free and non-parametric: parameter-free: it does not require any parameter tuning or training. non-parametric: it does not make any assumptions about word frequency distributions on document collections. It is highly recommended not to remove stopwords (very common terms: the, of, and, to, a, in, for, is, on, that, etc) with this similarity. For more information see: A nonparametric term weighting method for information retrieval based on measuring the divergence from independence</description>
      <attachments/>
      <comments>13</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6824</id>
      <title>TermAutomatonQuery should rewrite to a simpler query when possible</title>
      <description>Spinoff from LUCENE-6664. I think TermAutomatonQuery would be easier to integrate into query parsers if you could simply use it always and it would rewrite to simpler / faster queries when possible. This way, when a query parser is confronted with a phrase query requested by the user, it can just make a TermAutomatonQuery and run that. But the non-explicit phrase query case is still tricky...</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6825</id>
      <title>Add multidimensional byte[] indexing support to Lucene</title>
      <description>I think we should graduate the low-level block KD-tree data structure from sandbox into Lucene's core? This can be used for very fast 1D range filtering for numerics, removing the 8 byte (long/double) limit we have today, so e.g. we could efficiently support BigInteger, BigDecimal, IPv6 addresses, etc. It can also be used for &gt; 1D use cases, like 2D (lat/lon) and 3D (x/y/z with geo3d) geo shape intersection searches. The idea here is to add a new part of the Codec API (DimensionalFormat maybe?) that can do low-level N-dim point indexing and at runtime exposes only an "intersect" method. It should give sizable performance gains (smaller index, faster searching) over what we have today, and even over what auto-prefix with efficient numeric terms would do. There are many steps here ... and I think adding this is analogous to how we added FSTs, where we first added low level data structure support and then gradually cutover the places that benefit from an FST. So for the first step, I'd like to just add the low-level block KD-tree impl into oal.util.bkd, but make a couple improvements over what we have now in sandbox: Use byte[] as the value not int (@rjernst's good idea!) Generalize it to arbitrary dimensions vs. specialized/forked 1D, 2D, 3D cases we have now This is already hard enough After that we can build the DimensionalFormat on top, then cutover existing specialized block KD-trees. We also need to fix OfflineSorter to use Directory API so we don't fill up /tmp when building a block KD-tree. A block KD-tree is at heart an inverted data structure, like postings, but is also similar to auto-prefix in that it "picks" proper N-dimensional "terms" (leaf blocks) to index based on how the specific data being indexed is distributed. I think this is a big part of why it's so fast, i.e. in contrast to today where we statically slice up the space into the same terms regardless of the data (trie shifting, morton codes, geohash, hilbert curves, etc.) I'm marking this as trunk only for now... as we iterate we can see if it could maybe go back to 5.x...</description>
      <attachments/>
      <comments>34</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6828</id>
      <title>Speed up requests for many rows</title>
      <description>Standard relevance ranked searches for top-X results uses the HitQueue class to keep track of the highest scoring documents. The HitQueue is a binary heap of ScoreDocs and is pre-filled with sentinel objects upon creation. Binary heaps of Objects in Java does not scale well: The HitQueue uses 28 bytes/element and memory access is scattered due to the binary heap algorithm and the use of Objects. To make matters worse, the use of sentinel objects means that even if only a tiny number of documents matches, the full amount of Objects is still allocated. As long as the HitQueue is small (&lt; 1000), it performs very well. If top-1M results are requested, it performs poorly and leaves 1M ScoreDocs to be garbage collected. An alternative is to replace the ScoreDocs with a single array of packed longs, each long holding the score and the document ID. This strategy requires only 8 bytes/element and is a lot lighter on the GC. Some preliminary tests has been done and published at https://sbdevel.wordpress.com/2015/10/05/speeding-up-core-search/ These indicate that a long[]-backed implementation is at least 3x faster than vanilla HitDocs for top-1M requests. For smaller requests, such as top-10, the packed version also seems competitive, when the amount of matched documents exceeds 1M. This needs to be investigated further. Going forward with this idea requires some refactoring as Lucene is currently hardwired to the abstract PriorityQueue. Before attempting this, it seems prudent to discuss whether speeding up large top-X requests has any value? Paging seems an obvious contender for requesting large result sets, but I guess the two could work in tandem, opening up for efficient large pages.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6829</id>
      <title>OfflineSorter should use Directory API</title>
      <description>I think this is a blocker for LUCENE-6825, because the block KD-tree makes heavy use of OfflineSorter and we don't want to fill up tmp space ... This should be a straightforward cutover, but there are some challenges, e.g. the test was failing because virus checker blocked deleting of files.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6830</id>
      <title>Upgrade ANTLR to version 4.5.1</title>
      <description>Simple upgrade to ANTLR 4.5.1 which includes numerous bug fixes: https://github.com/antlr/antlr4/releases/tag/4.5.1 Note this does not change the grammar itself, only small pieces of the generated code.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6833</id>
      <title>Upgrade morfologik to version 2.0.1, simplify MorfologikFilter's dictionary lookup</title>
      <description>This is a follow-up to Uwe's work on LUCENE-6774. This patch updates the code to use Morfologik stemming version 2.0.1, which removes the "automatic" lookup of classpath-relative dictionary resources in favor of an explicit InputStream or URL. So the user code is explicitly responsible to provide these resources, reacting to missing files, etc. There were no other "default" dictionaries in Morfologik other than the Polish dictionary so I also cleaned up the filter code from a number of attributes that were, to me, confusing. MorfologikFilterFactory now accepts an (optional) dictionary attribute which contains an explicit name of the dictionary resource to load. The resource is loaded with a ResourceLoader passed to the inform(..) method, so the final location depends on the resource loader. There is no way to load the dictionary and metadata separately (this isn't at all useful). If the dictionary attribute is missing, the filter loads the Polish dictionary by default (since most people would be using Morfologik for stemming Polish anyway). This patch is not backward compatible, but it attempts to provide useful feedback on initialization: if the removed attributes were used, it points at this JIRA issue, so it should be clear what to change and how.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6835</id>
      <title>Directory.deleteFile should "own" retrying deletions on Windows</title>
      <description>Rob's idea: Today, we have hairy logic in IndexFileDeleter to deal with Windows file systems that cannot delete still open files. And with LUCENE-6829, where OfflineSorter now must deal with the situation too ... I worked around it by fixing all tests to disable the virus checker. I think it makes more sense to push this "platform specific problem" lower in the stack, into Directory? I.e., its deleteFile method would catch the access denied, and then retry the deletion later. Then we could re-enable virus checker on all these tests, simplify IndexFileDeleter, etc. Maybe in the future we could further push this down, into WindowsDirectory, and fix FSDirectory.open to return WindowsDirectory on windows ...</description>
      <attachments/>
      <comments>26</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6837</id>
      <title>Add N-best output capability to JapaneseTokenizer</title>
      <description>Japanese morphological analyzers often generate mis-segmented tokens. N-best output reduces the impact of mis-segmentation on search result. N-best output is more meaningful than character N-gram, and it increases hit count too. If you use N-best output, you can get decompounded tokens (ex: "シニアソフトウェアエンジニア" =&gt; {"シニア", "シニアソフトウェアエンジニア", "ソフトウェア", "エンジニア"} ) and overwrapped tokens (ex: "数学部長谷川" =&gt; {"数学", "部", "部長", "長谷川", "谷川"} ), depending on the dictionary and N-best parameter settings.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6840</id>
      <title>Put ord indexes of doc values on disk</title>
      <description>Currently we still load monotonic blocks into memory to map doc ids to an offset on disk. Since these data structures are usually consumed sequentially I would like to investigate putting them to disk.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6845</id>
      <title>Merge Spans and SpanScorer</title>
      <description>SpanScorer and Spans currently share the burden of scoring span queries, with SpanScorer delegating to Spans for most operations. Spans is essentially a Scorer, just with the ability to iterate through positions as well, and no SimScorer to use for scoring. This seems overly complicated. We should merge the two classes into one.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6849</id>
      <title>Add IndexWriter API to write segment(s) without refreshing them</title>
      <description>Today, the only way to have IndexWriter free up some heap is to invoke refresh or flush or close it, but these are all quite costly, and do much more than simply "move bytes to disk". I think we should add a simple API, e.g. "move the biggest in-memory segment to disk" to 1) give more granularity (there could be multiple in-memory segments), and 2) only move bytes to disk (not refresh, not fsync, etc.). This way apps that want to be more careful on how heap is used can have more control.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6850</id>
      <title>BooleanWeight should not use BS1 when there is a single non-null clause</title>
      <description>When a disjunction has a single non-null scorer, we still use BS1 for bulk-scoring, which first collects matches into a bit set and then calls the collector. This is inefficient: we should just call the inner bulk scorer directly and wrap the scorer to apply the coord factor (like BooleanTopLevelScorers.BoostedScorer does).</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6852</id>
      <title>Add DimensionalFormat to Codec</title>
      <description>This is phase 2 for adding dimensional indexing in Lucene, so we can (eventually) do efficient numeric range filtering, BigInteger/Decimal and IPv6 support, and "point in shape" spatial searching (2D or 3D). It's the follow-on from LUCENE-6825 (phase 1). This issue "just" adds DimensionalFormat (and Reader/Writer) to Codec and the IndexReader hierarchy, to IndexWriter and merging, and to document API (DimensionalField). I also implemented dimensional support for SimpleTextCodec, and added a test case showing that you can in fact use SimpleTextCodec to do multidimensional shape intersection (seems to pass a couple times!). Phase 3 will be adding support to the default codec as well ("just" wrapping BKDWriter/Reader), phase 4 is then fixing places that use the sandbox/spatial3d BKD tree to use the codec instead and maybe exposing sugar for numerics, things like BigInteger/Decimal, etc. There are many nocommits still, but I think it's close-ish ... I'll commit to a branch and iterate there.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6854</id>
      <title>Provide extraction of more metrics from confusion matrix</title>
      <description>ConfusionMatrix only provides a general accuracy measure while it'd be good to be able to extract more metrics from it, for specific classes, like precision, recall, f-measure, etc.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6861</id>
      <title>Default codec should support dimensional values</title>
      <description>This is phase 3, follow-on from LUCENE-6852, to add dimensional values support to Lucene, so we can index large "numeric" values like BigInteger, BigDecimal, IPv6, and multi-dimensional things like 2d and 3d geo. I created a new Lucene60Codec, implemented DimensionalFormat for it, and a new FieldInfosFormat (to write/read the dimension settings for each field), and moved Lucene54Codec to backwards-codecs. I also fixed CheckIndex to do basic testing of the dimensional values.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6863</id>
      <title>Store sparse doc values more efficiently</title>
      <description>For both NUMERIC fields and ordinals of SORTED fields, we store data in a dense way. As a consequence, if you have only 1000 documents out of 1B that have a value, and 8 bits are required to store those 1000 numbers, we will not require 1KB of storage, but 1GB. I suspect this mostly happens in abuse cases, but still it's a pity that we explode storage requirements. We could try to detect sparsity and compress accordingly.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6871</id>
      <title>Move SpanQueries out of .spans package</title>
      <description>SpanQueries are now essentially the same as a standard query, restricted to a single field and with an extra scorer type returned by getSpans(). There are a number of existing queries that fit this contract, including TermQuery and PhraseQuery, and it should be possible to make them SpanQueries as well without impacting their existing performance. However, we can't do this while SpanQuery and its associated Weight and Spans classes are in their own package. I'd like to remove the o.a.l.search.spans package entirely, in a few stages: 1) Move SpanQuery, SpanWeight, Spans, SpanCollector and FilterSpans to o.a.l.search 2) Remove SpanTermQuery and merge its functionality into TermQuery 3) Move SpanNear, SpanNot, SpanOr and SpanMultiTermQueryWrapper to o.a.l.search 4) Move the remaining SpanQueries to the queries package Then we can look at, eg, making PhraseQuery a SpanQuery, removing SpanMTQWrapper and making MultiTermQuery a SpanQuery, etc.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6874</id>
      <title>WhitespaceTokenizer should tokenize on NBSP</title>
      <description>WhitespaceTokenizer uses Character.isWhitespace to decide what is whitespace. Here's a pertinent excerpt: It is a Unicode space character (SPACE_SEPARATOR, LINE_SEPARATOR, or PARAGRAPH_SEPARATOR) but is not also a non-breaking space ('\u00A0', '\u2007', '\u202F') Perhaps Character.isWhitespace should have been called isLineBreakableWhitespace? I think WhitespaceTokenizer should tokenize on this. I am aware it's easy to work around but why leave this trap in by default?</description>
      <attachments/>
      <comments>66</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>6875</id>
      <title>New Serbian Filter</title>
      <description>This is a new Serbian filter that works with regular Latin text (the current filter works with "bald" Latin). I described in detail what does it do and why is it necessary at the wiki.</description>
      <attachments/>
      <comments>27</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6877</id>
      <title>eclipse generated try/catch discards exception</title>
      <description>The (horrible) eclipse default is something like this: // TODO: autogenerated stub e.printStackTrace(); The current eclipse config does this, which is better, but loses the original exc: throw new RuntimeException() But it should be: throw new RuntimeException(${exception_var})</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6878</id>
      <title>TopDocs.merge should use updateTop instead of pop / add</title>
      <description>The function TopDocs.merge uses PriorityQueue in a pattern: pop, update value (ref.hitIndex++), add. JavaDocs for PriorityQueue.updateTop say that using this function instead should be at least twice as fast.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6879</id>
      <title>Allow to define custom CharTokenizer using Java 8 Lambdas/Method references</title>
      <description>As a followup from LUCENE-6874, I thought about how to generate custom CharTokenizers wthout subclassing. I had this quite often and I was a bit annoyed, that you had to create a subclass every time. This issue is using the pattern like ThreadLocal or many collection methods in Java 8: You have the (abstract) base class and you define a factory method named fromXxxPredicate (like ThreadLocal.withInitial(() -&gt; value). public static CharTokenizer fromTokenCharPredicate(java.util.function.IntPredicate predicate) This would allow to define a new CharTokenizer with a single line statement using any predicate: // long variant with lambda: Tokenizer tok = CharTokenizer.fromTokenCharPredicate(c -&gt; !UCharacter.isUWhiteSpace(c)); // method reference for separator char predicate + normalization by uppercasing: Tokenizer tok = CharTokenizer.fromSeparatorCharPredicate(UCharacter::isUWhiteSpace, Character::toUpperCase); // method reference to custom function: private boolean myTestFunction(int c) { return (cracy condition); } Tokenizer tok = CharTokenizer.fromTokenCharPredicate(this::myTestFunction); I know this would not help Solr users that want to define the Tokenizer in a config file, but for real Lucene users this Java 8-like way would be easy and elegant to use. It is fast as hell, as it is just a reference to a method and Java 8 is optimized for that. The inverted factories fromSeparatorCharPredicate() are provided to allow quick definition without lambdas using method references. In lots of cases, like WhitespaceTokenizer, predicates are on the separator chars (isWhitespace(int), so using the 2nd set of factories you can define them without the counter-intuitive negation. Internally it just uses Predicate#negate(). The factories also allow to give the normalization function, e.g. to Lowercase, you may just give Character::toLowerCase as IntUnaryOperator reference.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6880</id>
      <title>Add document oriented collector for NRTSuggester</title>
      <description>Currently NRTSuggester collects completions iteratively as they are accepted by the TopNSearcher, implying that a document can be collected more than once. In case of indexing a completion with multiple context values, the completion leads to num_context paths in the underlying FST for the same docId and gets collected num_context times, when a query matches all its contexts. Ideally, a document-oriented collector will collect top N documents instead of top N completions by handling the docId deduplication while collecting the completions. This could be used to collect n unique documents that matched a completion query.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6881</id>
      <title>Cutover all BKD tree implementations to the codec</title>
      <description>This is phase 4 for enabling indexing dimensional values in Lucene ... follow-on from LUCENE-6861. This issue removes the 3 pre-existing specialized experimental BKD implementations (BKD* in sandbox module for 2D lat/lon geo, BKD3D* in spatial3d module for 3D x/y/z geo, and range tree in sandbox module) and instead switches over to having the codec index the dimensional values.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6884</id>
      <title>Analyzer.tokenStream() shouldn't throw IOException</title>
      <description>I'm guessing that in the past, calling Analyzer.tokenStream() would call TokenStream.reset() somewhere downstream, meaning that we had to deal with IOExceptions. However, tokenstreams are created entirely lazily now, so this is unnecessary.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6889</id>
      <title>BooleanQuery.rewrite could easily optimize some simple cases</title>
      <description>Follow-up of SOLR-8251: APIs and user interfaces sometimes encourage to write BooleanQuery instances that are not optimal, for instance a typical case that happens often with Solr/Elasticsearch is to send a request that has a MatchAllDocsQuery as a query and some filter, which could be executed more efficiently by directly wrapping the filter into a ConstantScoreQuery. Here are some ideas of rewrite operations that BooleanQuery could perform: remove FILTER clauses when they are also a MUST clause rewrite queries of the form "+: #filter" to a ConstantScoreQuery(filter) rewrite to a MatchNoDocsQuery when a clause that is a MUST or FILTER clause is also a MUST_NOT clause</description>
      <attachments/>
      <comments>15</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6890</id>
      <title>Specialize 1D dimensional values intersection</title>
      <description>I tried implementing the same specialization we had before LUCENE-6881 for the 1D case, but after testing it, I don't think it's worth it. I'll upload the patch here for posterity (tests pass), but net/net it adds non-trivial code complexity in exchange for minor (5.39 sec -&gt; 5.25 sec for 225 queries) query gains. Maybe in the future someone could improve this so it's more compelling... but I don't think the tradeoff is worth it today. Furthermore, the optimization 1) requires an API change, and 2) is not even admissible in the current patch, since the query could be a union of multiple disjoint ranges when the optimization assumes it's just a single range. The gist of the idea is to locate the start leaf block and end leaf block, make an informed estimate of the expected result set size, and then do a linear scan of the leaf blocks, vs the recursion and "grow per leaf block" we do today. I think the conclusion is that this used to be more sizable win, but DocIdSetBuilder has improved so that it is plenty fast without "upfront" growing, which is nice Or maybe my benchmark is bogus I'll commit the minor code comment / TODOs / test improvements from the patch ...</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6891</id>
      <title>Lucene60DimensionalFormat should use block prefix coding when writing values</title>
      <description>Today we write the whole value for every doc in one leaf block in the BKD tree, but that's crazy because the whole point of that leaf block is all the docs inside it have values that are very close together. So I changed this to write the common prefix for the whole block up front in each block. This requires more index-time and search-time work, but gives nice index size reductions: On the 2D (London, UK) lat/lon benchmark: Indexing time was a wee bit slower (743 -&gt; 747 seconds) Index size was ~11% smaller (704 MB -&gt; 630 MB) Query time was ~7% slower (2.84 sec -&gt; 3.05 sec) Heap usage is the same On the 1D (just "lat" from the above test) benchmark: Indexing time was a wee bit slower (363 -&gt; 364 sec) Index size was ~23% smaller (472 MB -&gt; 363 MB) Query time was a wee bit slower (5.39 -&gt; 5.41 sec) Heap usage is the same Index time can be a bit slower since there are two passes now per leaf block (first to find the common prefix per dimension, and second pass must then strip those prefixes). Query time is slower because there's more work per hit that needs value filtering, i.e. collating the suffixes onto the prefixes, per dimension. This affects 2D much more than 1D because 1D has fewer leaf blocks that need filtering (typically 0, 1 or 2, unless there are many duplicate values in the index). I suspect the index size savings is use-case dependent, e.g. if you index a bunch of ipv4 addresses along with a few ipv6 addresses, you'd probably see sizable savings. I also suspect the more docs you index, the greater the savings, because the cells will generally be smaller. Net/net I think the opto is worth it, even if it slows query time a bit.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6894</id>
      <title>Improve DISI.cost() by assuming independence for match probabilities</title>
      <description>The DocIdSetIterator.cost() method returns an estimation of the number of matching docs. Currently conjunctions use the minimum cost, and disjunctions use the sum of the costs, and both are too high. The probability of a match is estimated by dividing available cost() by the number of docs in a segment. The conjunction probability is then the product of the inputs, and the disjunction probability follows from De Morgan's rule: "not (A and B)" is the same as "(not A) or (not B)" with the probability for "not" computed as 1 minus the input probability. The independence that is assumed is normally not there. However, the cost() results are only used to order the input DISIs/Scorers for optimization, and for that I expect this assumption to work nicely.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6898</id>
      <title>Avoid reading last stored field value when StoredFieldVisitor.Status.NO</title>
      <description>CompressingStoredFieldsReader.visitDocument (line 597) loops through the fields in the input while consulting the StoredFieldVisitor on what to do. There is a small optimization that could be done on the last loop iteration. If the visitor returns Status.NO then it should be treated as equivalent to Status.STOP. As it is now, it will call skipField() which reads needless bytes from the DataInput that won't be used. With this optimization in place, it is advisable to put the largest text field last in sequence – something the user or search platform (e.g. ES/Solr) could do.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6899</id>
      <title>Upgrade randomizedtesting to 2.3.1</title>
      <description>This has a bunch of internal and some external improvements, overview here: https://github.com/randomizedtesting/randomizedtesting/releases</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6900</id>
      <title>Grouping sortWithinGroup should use Sort.RELEVANCE to indicate that, not null</title>
      <description>In AbstractSecondPassGroupingCollector, withinGroupSort uses a value of null to indicate a relevance sort. I think it's nicer to use Sort.RELEVANCE for this – after all it's how the groupSort variable is handled. This choice is also seen in GroupingSearch; likely some other collaborators too. Martijn van Groningen is there some wisdom in the current choice that escapes me? If not I'll post a patch.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6901</id>
      <title>Optimize 1D dimensional value indexing</title>
      <description>Dimensional values give a smaller index, and faster search times, for indexing ordered byte[] values across one or more dimensions, vs our existing approaches, but the indexing time is substantially slower. Since the 1D case is so important/common (numeric fields, range query) I think it's worth optimizing its indexing time. It should also be possible to optimize the N &gt; 1 dimensions case too, but it's more complex ... we can postpone that. So for the 1D case, I changed the merge method to do a merge sort (like postings) of the already sorted segments dimensional values, instead of simply re-indexing all values from the incoming segments, and this was a big speedup. I also changed from InPlaceMergeSorter to IntroSorter (this is what postings use, and it's faster but still safe) and this was another good speedup, which should also help the &gt; 1D cases. Finally, I added a BKDReader.verify method (currently it's dark: NOT called) that walks the index and then check that every value in each leaf block does in fact fall within what the index expected/claimed. This is useful for finding bugs! Maybe we can cleanly fold it into CheckIndex somehow later.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6902</id>
      <title>Fail fsync immediately</title>
      <description>While analysing a build issue in Elasticsearch I stumpled upon org.apache.lucene.util.IOUtils.fsync. It has a retry loop in fsync whenever an IOException occurs. However, there are lots of instances where a retry is not useful, e.g. when a channel has been closed, a ClosedChannelException is thrown and IOUtils#fsync still tries to fsync multiple times on the closed channel. After bringing the issue to Robert's attention, he even opted for removing the retry logic entirely for fsyncing. Please find attached a patch that removes the retry logic.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6903</id>
      <title>Enhance WordDelimiterFilter to skip operation on tokens marked as keywords.</title>
      <description>We have a github pull request for a feature to allow WDF to skip operation when the keyword marker filter has marked tokens. https://github.com/apache/lucene-solr/pull/210 This will also add a new configuration parameter called "splitKeywordTokens" that will default to false in 6.0 and true in earlier versions.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6904</id>
      <title>Rewrite max bound GeoPointInBBoxQuery to FieldValueQuery</title>
      <description>This simple improvement rewrites a GeoPointInBBoxQuery to a FieldValueQuery when the min/max lon/lat (or x/y) values are set to their limits (e.g., -180, 180, -90, 90). This bypasses all unnecessary GeoPointTermsEnum logic.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6909</id>
      <title>Improve concurrency for FacetsConfig</title>
      <description>The design of org.apache.lucene.facet.FacetsConfig encourages reuse of a single instance across multiple threads, yet the current synchronization model is too strict as it doesn't allow for concurrent read operations. I'll attach a trivial patch which removes the contention point.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6912</id>
      <title>Grouping's Collectors should have smart needsScores()</title>
      <description>The Grouping module has numerous Collector implementations, and only a couple perhaps override needsScore() with an optimal choice. Lets do this for all of them.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6913</id>
      <title>Standard/Classic/UAX tokenizers could be more ram efficient</title>
      <description>These tokenizers map codepoints to character classes with the following datastructure (loaded in clinit): private static char [] zzUnpackCMap(String packed) { char [] map = new char[0x110000]; This requires 2MB RAM for each tokenizer class (in trunk 6MB if all 3 classes are loaded, in branch_5x 10MB since there are 2 additional backwards compat classes). On the other hand, none of our tokenizers actually use a huge number of character classes, so char is overkill: e.g. this map can safely be a byte [] and we can save half the memory. Perhaps it could make these tokenizers faster too.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6916</id>
      <title>BaseDirectoryTestCase should use try-with-resources for its Directories</title>
      <description>I'm playing around with writing a nio2 FileSystem implementation for HDFS that will work with Directory, and it currently leaks threads everywhere because if a BaseDirectoryTestCase test fails it doesn't close its Directory. This obviously won't be a problem if everything passes, but it will probably be a while before that's true and it makes iterative development a bit of a pain.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6917</id>
      <title>Deprecate and rename NumericField/RangeQuery to LegacyNumeric</title>
      <description>DimensionalValues seems to be better across the board (indexing time, indexing size, search-speed, search-time heap required) than NumericField, at least in my testing so far. I think for 6.0 we should move IntField, LongField, FloatField, DoubleField and NumericRangeQuery to backward-codecs, and rename with Legacy prefix?</description>
      <attachments/>
      <comments>20</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6919</id>
      <title>Change the Scorer API to expose an iterator instead of extending DocIdSetIterator</title>
      <description>I was working on trying to address the performance regression on LUCENE-6815 but this is hard to do without introducing specialization of DisjunctionScorer which I'd like to avoid at all costs. I think the performance regression would be easy to address without specialization if Scorers were changed to return an iterator instead of extending DocIdSetIterator. So conceptually the API would move from class Scorer extends DocIdSetIterator { } to class Scorer { DocIdSetIterator iterator(); } This would help me because then if none of the sub clauses support two-phase iteration, DisjunctionScorer could directly return the approximation as an iterator instead of having to check if twoPhase == null at every iteration. Such an approach could also help remove some method calls. For instance TermScorer.nextDoc calls PostingsEnum.nextDoc but with this change TermScorer.iterator() could return the PostingsEnum and TermScorer would not even appear in stack traces when scoring. I hacked a patch to see how much that would help and luceneutil seems to like the change: TaskQPS baseline StdDev QPS patch StdDev Pct diff Fuzzy1 88.54 (15.7%) 86.73 (16.6%) -2.0% ( -29% - 35%) AndHighLow 698.98 (4.1%) 691.11 (5.1%) -1.1% ( -9% - 8%) Fuzzy2 26.47 (11.2%) 26.28 (10.3%) -0.7% ( -19% - 23%) MedSpanNear 141.03 (3.3%) 140.51 (3.2%) -0.4% ( -6% - 6%) HighPhrase 60.66 (2.6%) 60.48 (3.3%) -0.3% ( -5% - 5%) LowSpanNear 29.25 (2.4%) 29.21 (2.1%) -0.1% ( -4% - 4%) MedPhrase 28.32 (1.9%) 28.28 (2.0%) -0.1% ( -3% - 3%) LowPhrase 17.31 (2.1%) 17.29 (2.6%) -0.1% ( -4% - 4%) HighSloppyPhrase 10.93 (6.0%) 10.92 (6.0%) -0.1% ( -11% - 12%) MedSloppyPhrase 72.21 (2.2%) 72.27 (1.8%) 0.1% ( -3% - 4%) Respell 57.35 (3.2%) 57.41 (3.4%) 0.1% ( -6% - 6%) HighSpanNear 26.71 (3.0%) 26.75 (2.5%) 0.1% ( -5% - 5%) OrNotHighLow 803.46 (3.4%) 807.03 (4.2%) 0.4% ( -6% - 8%) LowSloppyPhrase 88.02 (3.4%) 88.77 (2.5%) 0.8% ( -4% - 7%) OrNotHighMed 200.45 (2.7%) 203.83 (2.5%) 1.7% ( -3% - 7%) OrHighHigh 38.98 (7.9%) 40.30 (6.6%) 3.4% ( -10% - 19%) HighTerm 92.53 (5.3%) 95.94 (5.8%) 3.7% ( -7% - 15%) OrHighMed 53.80 (7.7%) 55.79 (6.6%) 3.7% ( -9% - 19%) AndHighMed 266.69 (1.7%) 277.15 (2.5%) 3.9% ( 0% - 8%) Prefix3 44.68 (5.4%) 46.60 (7.0%) 4.3% ( -7% - 17%) MedTerm 261.52 (4.9%) 273.52 (5.4%) 4.6% ( -5% - 15%) Wildcard 42.39 (6.1%) 44.35 (7.8%) 4.6% ( -8% - 19%) IntNRQ 10.46 (7.0%) 10.99 (9.5%) 5.0% ( -10% - 23%) OrNotHighHigh 67.15 (4.6%) 70.65 (4.5%) 5.2% ( -3% - 15%) OrHighNotHigh 43.07 (5.1%) 45.36 (5.4%) 5.3% ( -4% - 16%) OrHighLow 64.19 (6.4%) 67.72 (5.5%) 5.5% ( -6% - 18%) AndHighHigh 64.17 (2.3%) 67.87 (2.1%) 5.8% ( 1% - 10%) LowTerm 642.94 (10.9%) 681.48 (8.5%) 6.0% ( -12% - 28%) OrHighNotMed 12.68 (6.9%) 13.51 (6.6%) 6.5% ( -6% - 21%) OrHighNotLow 54.69 (6.8%) 58.25 (7.0%) 6.5% ( -6% - 21%)</description>
      <attachments/>
      <comments>16</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>6920</id>
      <title>Simplify callable function checks in Expression module</title>
      <description>The expressions module allows to specify custom functions. It does some checks to ensure that the compiled Expression works correctly and does not produce linkage errors. It also checks parameters and return type to be doubles. There are two problems with the current approach: the check gets classloaders of the method's declaring class. This fails if a security manager forbids access to bootstrap classes (e.g., java.lang.Math) the code only checks if method or declaring class are public, but not if it is really reachable. This may not be the case in Java 9 (different module without exports,...) This issue will use MethodHandles to do the accessibility checks (it uses MethodHandles.publicLookup() to resolve the given reflected method). If that fails, our compiled code cannot acess it. If module system prevents access, this is also checked. To fix the issue with classloaders, it uses a trick: It calls Class.forName() with the classloader we use to compile our expression. If that does not return the same class as the declared method, it also fails compilation. This prevents NoClassDefFoundException on executing the expression. All tests pass.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6921</id>
      <title>Fix SPIClassIterator#isParentClassLoader to don't require extra permissions</title>
      <description>This is not really a big issue, because most setups use a "good" context classloader. The context classloader is required by the Java ServiceProvider standard to look for META-INF classes. To work around issues in some setups, the analysis and codec SPIs also scan our own classloader, if it is not a parent of the context one. This check requires permissions, if we are not a parent. This will fix the parent check to simply return false (and enforce classpath rescan) if we don't have enough permissions. This is the right thing to do, because if we have no permissions, we are also not a parent!</description>
      <attachments/>
      <comments>13</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6922</id>
      <title>Improve svn to git workaround script</title>
      <description>As the git-svn mirror for Lucene/Solr will be turned off near the end of 2015, try and improve the workaround script to become more usable.</description>
      <attachments/>
      <comments>29</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6926</id>
      <title>Take matchCost into account for MUST_NOT clauses</title>
      <description>ReqExclScorer potentially has two TwoPhaseIterators to check: the one for the positive clause and the one for the negative clause. It should leverage the match cost API to check the least costly one first.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6928</id>
      <title>Better deal with sparse norms</title>
      <description>Now that norms are disk-based (since 5.3) we're seeing similar issues as we were having with doc values in case of sparse fields. We could implement a similar approach to what was done in LUCENE-6863 in order to keep things fast in the dense case yet reduce disk requirements in the sparse case.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6930</id>
      <title>Decouple GeoPointField from NumericType</title>
      <description>GeoPointField currently relies on NumericTokenStream to create prefix terms for a GeoPoint using the precision step defined in GeoPointField. At search time GeoPointTermsEnum recurses to a max precision that is computed by the Query parameters. This max precision is never the full precision, so creating and indexing the full precision terms is useless and wasteful (it was always a side effect of just using indexing logic from the Numeric type). Furthermore, since the numerical logic always stored high precision terms first, the recursion in GeoPointTermsEnum required transient memory for storing ranges. By moving the trie logic to its own GeoPointTokenStream and reversing the term order (such that lower resolution terms are first), the GeoPointTermsEnum can naturally traverse, enabling on-demand creation of PrefixTerms. This will be done in a separate issue.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6939</id>
      <title>BlendedInfixSuggester to support exponential reciprocal BlenderType</title>
      <description>The orignal BlendedInfixSuggester introduced in LUCENE-5354 has support for: BlenderType.POSITION_LINEAR and BlenderType.POSITION_RECIPROCAL . These are used to score documents based on the position of the matched token i.e the closer is the matched term to the beginning, the higher score you get. In some use cases, we need a more aggressive scoring based on the position. That's where the exponential reciprocal comes into play i.e coef = 1/Math.pow(position+1, exponent) where the exponent is a configurable variable.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6940</id>
      <title>Bulk scoring could speed up MUST_NOT clauses</title>
      <description>Today when you have MUST_NOT clauses, the ReqExclScorer is used and needs to check the excluded clauses on every iteration. I suspect we could speed things up by having a BulkScorer that would advance the excluded clause first and then tell the required clause to bulk score up to the next excluded document.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6944</id>
      <title>BooleanWeight.bulkScorer should not build any sub bulk scorer if there are required/prohibited clauses</title>
      <description>BooleanWeight.bulkScorer creates a sub bulk scorer for all clauses until it meets a clause that is not optional (the only kind of clause it can deal with). However the Weight.bulkScorer method is sometimes costly, so BooleanWeight.bulkScorer should first inspect all clauses to see if any of them is not optional to avoid creating costly bulk scorers to only trash them later.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6951</id>
      <title>GeoPointInPolygonQuery can be improved</title>
      <description>GeoRelationutils uses a basic algebraic approach for computing if (and where) a rectangle crosses a polygon by checking the line segments of both the polygon and rectangle. The current suboptimal line crossing approach can be greatly improved by exploiting the orientation of the lines and endpoints. If the endpoints of one line are on different "sides" of the line segment then the two may cross.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6952</id>
      <title>All Filter* delegating classes should be abstract</title>
      <description>I think it's confusing that FilterLeafReader (and it's Filter* inner classes) are not abstract. By making them abstract, we clarify to users how to use them by virtue of them being abstract. It seems only a couple tests directly instantiate them. This applies to other Filter* classes as well.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6958</id>
      <title>Improve CustomAnalyzer to also allow to specify factory directly (for compile-time safety)</title>
      <description>Currently CustomAnalyzer only allows to specify the SPI names of factories. As the fluent builder pattern is mostly used inside Java code, it is better for type safety to optionally also specify the factory class directly (using compile-time safe patterns like .withTokenizer(WhitespaceTokenizerFactory.class)). With the string names, you get the error only at runtime. Of course this does not help with wrong, spelled parameter names, but it also has the side effect that you can click on the class name in your code to get javadocs with the parameter names. This issue will add this functionality and update the docs/example. Thanks to Shai Erera for suggesting this!</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6961</id>
      <title>Improve Exception handling in AnalysisFactory/SPI loader</title>
      <description>Currently the AnalysisSPILoader used by AbstractAnalysisFactory uses a catch Exception block when invoking the constructor. If the constructor throws stuff like IllegalArgumentExceptions or similar, this is hidden inside InvocationTargetException, which gets wrapped in IllegalArgumentException. This is not useful. This patch will: Only catch ReflectiveOperationException If it is InvocationTargetException it will rethrow the cause, if it is unchecked. Otherwise it will wrap in RuntimeException If the constructor cannot be called at all (reflective access denied, method not found,...) UOE is thrown with explaining message. This patch will be required by next version of LUCENE-6958.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6962</id>
      <title>Add per-dimension min/max to dimensional values</title>
      <description>It can be useful for apps to know the min/max value for a given field for each dimension, to give the global bounding box. E.g. an app can know that a given range filter excludes all documents in a segment/index and skip searching it.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6964</id>
      <title>Small changes in expressions module</title>
      <description>This patch is not really worth an issue, but it has 2 small changes in expression's JavaScriptCompiler: On the OpenJDK mailinglists from time to time people want to have "nice stack traces" (this got worse with Lambda expressions). Since Java 8u60 they now hide some stack frames automatically. Under discussion was to use the "synthetic" bytecode attribute for that. They did not do that; instead they have a new annotation (@LambdaForm.Hidden). In any case, I will remove the synthetic annotation from the generated class file, because it is not really applicable (and also brings nothing): "A class member that does not appear in the source code must be marked using a Synthetic attribute, or else it must have its ACC_SYNTHETIC flag set. The only exceptions to this requirement are compiler-generated methods which are not considered implementation artifacts, namely the instance initialization method representing a default constructor of the Java programming language (§2.9), the class initialization method (§2.9), and the Enum.values() and Enum.valueOf() methods.". In our case the expressions class has source code (just not Java), so we should not make it synthetic. So there is also no risk that the stack frames get hidden in future (if OpenJDK thinks different). The code has currently some string-based method signatures. They tend to get broken because we have no compile-time checks. I replaced those with compile-time class constants using Java 7's MethodType to create the descriptor. For me the code is more readable. The change is a no-op as its constants only.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6965</id>
      <title>Expression's JavascriptCompiler to throw ParseExceptions with bad function names or arity</title>
      <description>Currently JavascriptCompiler will throw IllegalArgumentException for bad function names (or functions that don't exist) and for bad arity. I can see why this was done this way, but I believe ParseException would also be correct and it would be better since that's the exception clients will be prepared to receive.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6966</id>
      <title>Contribution: Codec for index-level encryption</title>
      <description>We would like to contribute a codec that enables the encryption of sensitive data in the index that has been developed as part of an engagement with a customer. We think that this could be of interest for the community. Below is a description of the project. Introduction In comparison with approaches where all data is encrypted (e.g., file system encryption, index output / directory encryption), encryption at a codec level enables more fine-grained control on which block of data is encrypted. This is more efficient since less data has to be encrypted. This also gives more flexibility such as the ability to select which field to encrypt. Some of the requirements for this project were: The performance impact of the encryption should be reasonable. The user can choose which field to encrypt. Key management: During the life cycle of the index, the user can provide a new version of his encryption key. Multiple key versions should co-exist in one index. What is supported ? Block tree terms index and dictionary Compressed stored fields format Compressed term vectors format Doc values format (prototype based on an encrypted index output) - this will be submitted as a separated patch Index upgrader: command to upgrade all the index segments with the latest key version available. How it is implemented ? Key Management One index segment is encrypted with a single key version. An index can have multiple segments, each one encrypted using a different key version. The key version for a segment is stored in the segment info. The provided codec is abstract, and a subclass is responsible in providing an implementation of the cipher factory. The cipher factory is responsible of the creation of a cipher instance based on a given key version. Encryption Model The encryption model is based on AES/CBC with padding. Initialisation vector (IV) is reused for performance reason, but only on a per format and per segment basis. While IV reuse is usually considered a bad practice, the CBC mode is somehow resilient to IV reuse. The only "leak" of information that this could lead to is being able to know that two encrypted blocks of data starts with the same prefix. However, it is unlikely that two data blocks in an index segment will start with the same data: Stored Fields Format: Each encrypted data block is a compressed block (~4kb) of one or more documents. It is unlikely that two compressed blocks start with the same data prefix. Term Vectors: Each encrypted data block is a compressed block (~4kb) of terms and payloads from one or more documents. It is unlikely that two compressed blocks start with the same data prefix. Term Dictionary Index: The term dictionary index is encoded and encrypted in one single data block. Term Dictionary Data: Each data block of the term dictionary encodes a set of suffixes. It is unlikely to have two dictionary data blocks sharing the same prefix within the same segment. DocValues: A DocValues file will be composed of multiple encrypted data blocks. It is unlikely to have two data blocks sharing the same prefix within the same segment (each one will encodes a list of values associated to a field). To the best of our knowledge, this model should be safe. However, it would be good if someone with security expertise in the community could review and validate it. Performance We report here a performance benchmark we did on an early prototype based on Lucene 4.x. The benchmark was performed on the Wikipedia dataset where all the fields (id, title, body, date) were encrypted. Only the block tree terms and compressed stored fields format were tested at that time. Indexing The indexing throughput slightly decreased and is roughly 15% less than with the base Lucene. The merge time slightly increased by 35%. There was no significant difference in term of index size. Query Throughput With respect to query throughput, we observed no significant impact on the following queries: Term query, boolean query, phrase query, numeric range query. We observed the following performance impact for queries that needs to scan a larger portion of the term dictionary: prefix query: decrease of ~25% wildcard query (e.g., “fu*r”): decrease of ~60% fuzzy query (distance 1): decrease of ~40% fuzzy query (distance 2): decrease of ~80% We can see that the decrease of performance is relative to the size of the dictionary scan. Document Retrieval We observed a decrease of performance that is relative to the size of the set of documents to be retrieved: ~20% when retrieving a medium set of documents (100) ~30/40% when retrieving a large set of documents (1000) Known Limitations compressed stored field do not keep order of fields since non-encrypted and encrypted fields are stored in separated blocks. the current implementation of the cipher factory does not enforce the use of AES/CBC. We are planning to add this to the final version of the patch. the current implementation does not change the IV per segment. We are planning to add this to the final version of the patch. the current implementation of compressed stored fields decrypts a full compressed block even if a small portion is decompressed (high impact when storing very small documents). We are planning to add this optimisation to the final version of the patch. The overall document retrieval performance might increase with this optimisation. The codec has been implemented as a contrib. Given that most of the classes were final, we had to copy most of the original code from the extended formats. At a later stage, we could think of opening some of these classes to extend them properly in order to reduce code duplication and simplify code maintenance.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>6968</id>
      <title>LSH Filter</title>
      <description>I'm planning to implement LSH. Which support query like this Find similar documents that have 0.8 or higher similar score with a given document. Similarity measurement can be cosine, jaccard, euclid.. For example. Given following corpus 1. Solr is an open source search engine based on Lucene 2. Solr is an open source enterprise search engine based on Lucene 3. Solr is an popular open source enterprise search engine based on Lucene 4. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java We wanna find documents that have 0.6 score in jaccard measurement with this doc Solr is an open source search engine It will return only docs 1,2 and 3 (MoreLikeThis will also return doc 4)</description>
      <attachments/>
      <comments>38</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>6971</id>
      <title>Remove StorableField, StoredDocument</title>
      <description>I think this has proven to be an awkward/forced separation, e.g. that doc values are handled as {{StorableField}}s. For the 5.x release we had just "kicked the can down the road" by pushing this change off of the branch, making backporting sometimes hard, but I think for 6.x we should just remove it and put the document API back to what we have in 5.x.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6973</id>
      <title>Improve TeeSinkTokenFilter</title>
      <description>TeeSinkTokenFilter can be improved in several ways, as it's written today: The most major one is removing SinkFilter which just doesn't work and is confusing. E.g., if you set a SinkFilter which filters tokens, the attributes on the stream such as PositionIncrementAttribute are not updated. Also, if you update any attribute on the stream, you affect other SinkStreams ... It's best if we remove this confusing class, and let consumers reuse existing TokenFilters by chaining them to the sink stream. After we do that, we can make all the cached states a single (immutable) list, which is shared between all the sink streams, so we don't need to keep many references around, and also deal with WeakReference. Besides that there are some other minor improvements to the code that will come after we clean up this class. From a backwards-compatibility standpoint, I don't think that SinkFilter is actually used anywhere (since it just ... confusing and doesn't work as expected), and therefore I believe it won't affect anyone. If however someone did implement a SinkFilter, it should be trivial to convert it to a TokenFilter and chain it to the SinkStream.</description>
      <attachments/>
      <comments>40</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6975</id>
      <title>Add dimensional "equals" query to match docs containing precisely a given value</title>
      <description>Today, you can make a dimensional range query using e.g. DimensionalRangeQuery.new1DIntRange, etc., plus a direct ctor for "expert" (2D, 3D, etc.) usages, but matching a single value is awkward and users ask about it from time to time. We could maybe rename DimensionalRangeQuery to DimensionalQuery and add new "factories" like newIntEqualsQuery or something. Or, we could make new classes, DimensionalIntEqualsQuery etc., and you get to use ordinary constructors? Or something else?</description>
      <attachments/>
      <comments>13</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>6978</id>
      <title>Make LuceneTestCase use language tags instead of parsing locales by hand</title>
      <description>Since we are on Java 7, the JDK supports standardized language tags as identifiers for Locales. Previous versions of JDK were missing a constructor from Locale#toString() back to a locale, so we had our own, which was broken several times after the JDK changed their Locale internals. This patch will do the following: When printing the reproduce line, it will use Locale#getLanguageTag(), so you can identify the locale in standardized form. Most notable change is (next to more flexibility around asian languages) the change away from undescores. So it prints "en-US", not "en_US". The code that parses a locale uses Locale's Builder and sets the language tag. This will fail if the tag is invalid! A trap is Locale#forLanguageTag, because this one silently returns root locale if unparseable... The random locale is choosen from all language tags, which are extracted from the JDK as a String[] array. I would also like to place Locale#forLanguageTag on the forbidden list and disallow directly calling Locale#toString(), the latter is legacy API (according to Java 7 Javadocs). This would fail code that calls toString() directly, e.g. when formatting stuff like "my Locale: " + locale. Of course we cannot catch all bad uses.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6979</id>
      <title>Tokenizer input state detection should reset state before throwing</title>
      <description>The Tokenizer will helpfully let you know that you're using it wrong in certain cases, like if you forget to close it. However it does this in a way that happens lazily (after the fact) but worse it keeps the state of the Tokenizer in a cranky state (i.e. if you try to use it again, you'll get an exception again). What makes this issue insidious is that Tokenizers are re-used via a ReuseStrategy in a ThreadLocal. So once you hit this bug, you're thread is, in a word, "poisoned". And what makes the stack trace a real head-scratcher is that it is not of the original "guilty" party that didn't close; it's likely some other caller, perhaps an indexing thread who isn't going to misuse the TokenStream, or at least hasn't yet. The error message could make that clearer.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6980</id>
      <title>Default to applying deletes when opening NRT reader from writer</title>
      <description>Today, DirectoryReader.open, etc., all require you to pass a supremely expert boolean applyDeletes. I think the vast majority of users should just default this to true.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>6982</id>
      <title>throw IAE on illegal bm25 parameter values</title>
      <description>Rather than strange behavior, we should throw an exception if these values are wrong. This can often detect if the two values got "mixed up", because typical values of k1 are &gt; 1 (e.g. 1.2, 2.0), but b is only defined between 0 and 1.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6986</id>
      <title>Add more DFI independence measures</title>
      <description>Since LUCENE-6818 we have DFISimilarity which implements normalized chi-squared distance. But there are other alternatives (as described in http://trec.nist.gov/pubs/trec21/papers/irra.web.nb.pdf): normalized chi-squared: "can be used for tasks that require high precision, against both short and long queries" standardized: "good at tasks that require high recall and high precision, especially against short queries composed of a few words as in the case of Internet searches" saturated: "for tasks that require high recall against long queries" I think we should just provide the three independence measures, and let the user choose. Similar to how we do DFR/IB/etc.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6988</id>
      <title>IndexableField.tokenStream() doesn't need to throw IOException</title>
      <description>No implementations actually throw IOException, and in general TokenStream construction doesn't use IO, only consumption.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>6990</id>
      <title>Mongo 3.x Directory support</title>
      <description>I would like to setup the lucene store to a MongoDB installation. It already exists an old project which gives this possiblity: https://github.com/rstiller/mongo-lucene Unfortunately this project is stuck to the old version of Mondo DB and does not run anymore. Newest version of MongoDB now uses new classes like GridFSBuckets and MongoDatabase (replacing the old classes GridFS and MongoDB). Here an example of usage: http://mongodb.github.io/mongo-java-driver/3.2/driver/reference/gridfs/ Can you please implement a MongoDBDirectory? Because I am not sure which team (Lucene or MongoDB one) can better take care of this request, I am opening a copy of this request to the other team too (see external issue URL). Best regards</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6992</id>
      <title>Add sugar methods to allow creating a MemoryIndex from a Document or set of IndexableFields</title>
      <description>This came up on the mailing list a few days ago - it's not as obvious as it should be how to add arbitrary IndexableFields to a MemoryIndex, and a few sugar methods will make this a lot simpler.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6993</id>
      <title>Update UAX29URLEmailTokenizer TLDs to latest list, and upgrade all JFlex-based tokenizers to support Unicode 8.0</title>
      <description>We did this once before in LUCENE-5357, but it might be time to update the list of TLDs again. Comparing our old list with a new list indicates 800+ new domains, so it would be nice to include them. Also the JFlex tokenizer grammars should be upgraded to support Unicode 8.0.</description>
      <attachments/>
      <comments>53</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>6995</id>
      <title>Add branch change trigger to common-build.xml to keep sane build on GIT branch change</title>
      <description>Currently build systems like Ant or Maven cannot correctly handle the following typical Git workflow: You do some stuff, run some tests You quickly switch the branch to something else (e.g., master -&gt; branch_5x) If you then run tests or compile, you get crazy error messages because all the up-to-date checking does not work. To fix you have to run "ant clean" on root folder. The reason for this behaviour is how switching branches works: "it just replaces your working copy with another verson and different file modification dates (in most cases earlier ones!). This breaks Ant's and Javac'c uptodate checking. This patch will add some check to common-build (which is done before "resolve" through a dependency of "init" task) to keep track of current branch: It copies .git/HEAD into the build folder as "reference point". Before compiling (on init), it does a file compare of current HEAD file with the clone in build dir. If it does not yet exists (clean checkout) or if it is identical, it just proceeds If the file contents changed, it will trigger "ant clean" on the LuSolr root folder automatically and print message. The status is passed down to sub-builds using *.uptodate property, so the check is only done once. Robert an I tried this out in serveral combinations, works fine. Any comments?</description>
      <attachments/>
      <comments>19</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>6996</id>
      <title>Deconflict GeoPointField and BKD LatLonField Encoding, Decoding, and Tolerance</title>
      <description>Follow up to LUCENE-6956, BKD based LatLonField and postings based GeoPointField should use a consistent lat/lon encoding/decoding method to ensure consistent round-off error.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>6997</id>
      <title>Graduate GeoUtils and postings based GeoPointField from sandbox...</title>
      <description>GeoPointField is a lightweight dependency-free postings based geo field currently in sandbox. It has evolved into a very fast lightweight geo option that heavily leverages the optimized performance of the postings structure. It was originally intended to graduate to core but this does not seem appropriate given the variety of "built on postings" term encoding options (e.g., see LUCENE-6930). Additionally, the Geo*Utils classes are dependency free lightweight relational approximation utilities used by both GeoPointField and the BKD based LatLonField and can also be applied to benefit the lucene-spatial module. These classes have been evolving and baking for some time and are at a maturity level qualifying for promotion from sandbox. This will allow support for experimental encoding methods with (minimal) backwards compatibility - something sandbox does not allow. Since GeoPoint classes are dependency free, all GeoPointField and support and utility classes currently in sandbox would be promoted to the spatial3d package. (possibly a separate issue to rename spatial3d to spatialcore or spatiallite?) Such that for basic lightweight Geo support one would only need a handful of lucene jars. By simply adding the lucene-spatial module and its dependency jars users can obtain more advanced geospatial support (heatmap facets, full shape relations, etc).</description>
      <attachments/>
      <comments>42</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7007</id>
      <title>Reduce block-tree GC/CPU cost when flushing or merging postings</title>
      <description>Writing postings is a GC and CPU heavy operation now, in part because of how block tree recursively builds up the tree structure, by creating many tiny FSTs which it inefficiently merges together as it walks up the tree eventually to the root block. So I tried a quick prototype (patch attached) to use a less-RAM-efficient, but much fewer tiny FST related objects, when writing postings. But in some quick indexing performance tests (luceneutil), it makes no measurable improvements to indexing performance. So I'm putting my patch up here for posterity ... I don't intend to commit it unless we can iterate it further. It adds code complexity, it's not committable as-is (we need to conditionalize it so it sometimes does use FSTs, for segments with many terms), etc.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7010</id>
      <title>Create MergePolicyWrapper</title>
      <description>We currently have two MergePolicy implementations that are wrappers around another MP: SortingMergePolicy and UpgradeIndexMergePolicy. A MergePolicyWrapper will simplify building additional such wrapping MPs by delegating all method calls to the wrapped instance, and allowing implementations to override only what they need. Also, this issue removes the final modifier from MP public methods so that they can be delegated properly. See LUCENE-7008 for a test failure that uncovered this issue.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7012</id>
      <title>Change Lucene/Solr IDE configs to place new Java files' license header before the package declaration</title>
      <description>Today the license header is placed after the package declaration. This results in inconsistency in our files. In some the license is put after the package and before the imports, in others it appears after the imports but before class declaration and in others it appears in the middle of the imports. Also, when you e.g. "Organize Imports" in eclipse, if the header is located in the middle of imports, it's sometimes completely removed from the file. This issue is about changing the "New Java Files" template in eclipse so that the license header is placed before the package declaration. This ensures that "Organize Imports" doesn't mess with it, as well for new files (created in eclipse), we will start getting some consistency. If we also want to handle all current files by moving the license header before the package, we can do so (I volunteer), but since it's a big change, will likely do it over multiple commits and no need for an issue. BTW, if anyone knows of a tool that can automate that, great. I found that Apache has a perl script but it seems to only fix the header to the new format, yet not moving it in the file.</description>
      <attachments/>
      <comments>29</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7013</id>
      <title>Move license header before package declaration in all *.java files</title>
      <description>In LUCENE-7012 we committed a change to the IDE templates to place the license header before the package declaration in new Java files. I wrote a simple Python script which moves the header before the package declaration. To be on the safe side, if a .java file does not already start with the license header or with package org.apache, it doesn't modify it and asks for manual intervention. It runs quite fast, so I don't mind running and committing one module at a time.</description>
      <attachments/>
      <comments>42</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>7014</id>
      <title>Use TimeUnit.TARGETUNIT.convert() to convert between time units</title>
      <description>Re-phrased from Steve Rowe's comment : System.nanoTime(), which is guaranteed to be monotonic, is now used to recored elapsed times. In several places, conversion from nano seconds to some target unit (e.g. seconds, milli seconds) is performed using hard-coded conversion constants, which is prone to mistakes. It would be nice to use TimeUnit.TARGETUNIT.convert(sourceDuration, TimeUnit.SOURCEUNIT) instead.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7015</id>
      <title>Refactor spatial module to spatial-extras</title>
      <description>Follow on to LUCENE-6997: non GeoPoint* classes need to be refactored from existing spatial module to a new spatial-extras module. All dev-tools, build and project files will be updated to correctly reference and build the new module.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7020</id>
      <title>TieredMergePolicy - cascade maxMergeAtOnce setting to maxMergeAtOnceExplicit</title>
      <description>SOLR-8621 covers improvements in configuring a merge policy in Solr. Discussions on that issue brought up the fact that if large values are configured for maxMergeAtOnce and segmentsPerTier, but maxMergeAtOnceExplicit is not changed, then doing a forceMerge is likely to not work as expected. When I first configured maxMergeAtOnce and segmentsPerTier to 35 in Solr, I saw an optimize (forceMerge) fully rewrite most of the index twice in order to achieve a single segment, because there were approximately 80 segments in the index before the optimize, and maxMergeAtOnceExplicit defaults to 30. On advice given via the solr-user mailing list, I configured maxMergeAtOnceExplicit to 105 and have not had that problem since. I propose that setting maxMergeAtOnce should also set maxMergeAtOnceExplicit to three times the new value – unless the setMaxMergeAtOnceExplicit method has been invoked, indicating that the user wishes to set that value themselves.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7029</id>
      <title>We need a better way to retrieve many documents at same time</title>
      <description>My use case involve that in some situations I need to get a lot of documents from the index. Most of my documents are small (less than 2KB). I use API: IndexReader.document(id, visitor), but I noticed it keeps decompressing same block over and over again. If we have an api that let's me get many documents at once, I could do the same job way faster. After some experiment I realized I can get 2x+ gains on performance with this simple feature. Disclaimer: I'm a newbie to lucene project. I truly appreciate hard work by all contributors for this awesome library. I would be more than happy to contribute back and get my hands dirty. Any help and pointers is more than welcomed.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7035</id>
      <title>upgrade icu4j to latest (unicode 8)</title>
      <description>See LUCENE-6993. We want to bring all these tokenizers up to date. The icu part can be done independently.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7036</id>
      <title>nio.Paths and nio.Files package are used in StringHelper, but they are restricted in many infrastructure and platforms</title>
      <description>nio.Paths and nio.Files package are used in StringHelper, but they are restricted in many infrastructure and platforms like Google App Engine. The use of Paths and Fiiles are not related to the main function of Lucene. It's better to provide an interface to store system properties instead of using File API in String Helper directly.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7041</id>
      <title>Upgrade to Apache Tika 1.12</title>
      <description>We recently released Apache Tika 1.12. In order to use the fixes provided within the Tika.translate API I propose to upgrade Tika from 1.7 --&gt; 1.12 in lucene/ivy-versions.properties. Patch coming up.</description>
      <attachments/>
      <comments>32</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7042</id>
      <title>more cleanup for Point encodings</title>
      <description>Followup of LUCENE-7039. I started consolidating encode/decode functions into XYZField, but its still pretty messy. Specifically, duplicated code because indexer wants packed byte[] and queries want byte[][] and so on. We can do a bit more cleanup: by defining everything in terms of encodeDimension() and decodeDimension(). This way, the logic is only in one place. Also the new NumericUtils methods here are confusing I think. Its scary how they take a "dimensional index" when parsing from a byte[]. I think they should just take 'offset' and not try to be fancy, its easier to understand.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7043</id>
      <title>add BigIntegerPoint and InetAddressPoint to sandbox</title>
      <description>We can now index 128-bit quantities, but still, making Fields for these is a bit of work: you have to do some numericutils sign-extension/sign-flipping magic for BigInteger quantities, deal with ipv4-mapped ipv6 addresses, etc. We can just provide some simple field types that also have static factory methods for exact match/ranges. The BigIntegerPoint is N-dimensional, so acts just like any other primitive, except its bigger (e.g. long long). InetAddressPoint is 1-dimensional by nature: of course you can have multiple values per field, thats different. Since we prefix-compress values, we can just map IPv4 addresses to IPv6 space and it works for both types. This is consistent with what InetAddress does itself anyway.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7045</id>
      <title>remove all encode/decode hooks from PointRangeQuery</title>
      <description>In LUCENE-7043 we added several new point types and just gave them static methods to generate exact/range/prefix/whatever queries. I think we should do the same in general, e.g. for IntPoint: This field defines static factory methods for creating common queries: &lt;ul&gt; &lt;li&gt;{@link #newExactQuery newExactQuery()} for matching an exact 1D point. &lt;li&gt;{@link #newRangeQuery newRangeQuery()} for matching a 1D range. &lt;li&gt;{@link #newMultiRangeQuery newMultiRangeQuery()} for matching points/ranges in n-dimensional space. &lt;/ul&gt; Then each Point can have types that make sense for it: e.g. multi-dimensional range queries don't make sense at all for IP addresses, but prefix query does, and polygon/distance queries make sense for LatLonPoint and so on (that one is not refactored yet here, followup!)</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7047</id>
      <title>Add Geo3DPoint.newShapeQuery</title>
      <description>This just fixes geo3d to match the other points APIs.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7048</id>
      <title>Add XXXPoint.newSetQuery to match a set of points</title>
      <description>This is the analog of TermsQuery for dimensional points, to (relatively) efficiently match any docs whose point value is in the specified set.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7049</id>
      <title>merge eats CPU much when there are many deleteByQuery</title>
      <description>When adding very many delete&gt;&lt;query&gt; Then we got CPU spike in merge thread that blocks indexing process Considerations Despite adding too many &lt;delete&gt;&lt;query&gt; is odd itself, I suppose the code can more efficient. See sampling snapshots attached.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7050</id>
      <title>Improve the query cache heuristic to detect costly queries</title>
      <description>Term queries, phrase queries and their combinations through boolean queries should not be cached too agressively since they can efficiently make use of skip lists. However we also have a number of queries that in practice need to visit all matches anyway like PrefixQuery, TermsQuery, PointInSetQuery, PointRangeQuery, so caching them more agressively can help avoid computing all documents that match in the whole index again and again.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7051</id>
      <title>Remove the "estimate match count" optimization from point queries</title>
      <description>Point queries try to estimate the number of matches in the visitor so that the doc id set that they build does not have to do it by itself. However, this is incorrect in the multi-valued case and does not seem to buy much (if any) in terms of performance?</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7052</id>
      <title>BytesRefHash.sort should always sort in unicode code point order</title>
      <description>Today BytesRefHash.sort takes a custom Comparator but we always pass it BytesRef.getUTF8SortedAsUnicodeComparator().</description>
      <attachments/>
      <comments>20</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>7053</id>
      <title>Remove deprecated BytesRef#getUTF8SortedAsUTF16Comparator(); remove natural comparator in favour of Java 8 one</title>
      <description>Followup from LUCENE-7052: This removes the legacy, deprecated getUTF8SortedAsUTF16Comparator() in the BytesRef class. I know originally we added the different comparators to be able to allow the index term dict to be sorted in different order. This never proved to be useful, as many Lucene queries rely on the default order. The only codec that used another byte order internally was the Lucene 3 one (but it used the unicode spaghetti algorithm to reorder its term enums at runtime). This patch also removes the BytesRef-Comparator completely and just implements compareTo. So all code can rely on natural ordering. This patch also cleans up other usages of natural order comparators, e.g. in ArrayUtil, because Java 8 natively provides a comparator.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7054</id>
      <title>add newDistanceQuery to sandbox LatLonPoint</title>
      <description>This field is currently missing a distance radius query: this is a common use case.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7055</id>
      <title>Better execution path for costly queries</title>
      <description>In Lucene 5.0, we improved the execution path for queries that run costly operations on a per-document basis, like phrase queries or doc values queries. But we have another class of costly queries, that return fine iterators, but these iterators are very expensive to build. This is typically the case for queries that leverage DocIdSetBuilder, like TermsQuery, multi-term queries or the new point queries. Intersecting such queries with a selective query is very inefficient since these queries build a doc id set of matching documents for the entire index. Is there something we could do to improve the execution path for these queries? One idea that comes to mind is that most of these queries could also run on doc values, so maybe we could come up with something that would help decide how to run a query based on other parts of the query? (Just thinking out loud, other ideas are very welcome)</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7056</id>
      <title>Separate Geo3DPoint into another package from the rest of Geo3D</title>
      <description>The original description follows; it's greater in scope than the new title: This is a proposal for the "spatial3d" module to be purely about the shape/geometry implementations it has. In Lucene 5 that's actually all it has. In Lucene 6 at the moment its ~76 files have 2 classes that I think should go elsewhere: Geo3DPoint and PointInGeo3DShapeQuery. Specifically lucene-spatial-extras (which doesn't quite exist yet so lucene-spatial) would be a suitable place due to the dependency. Eventually I see this module migrating elsewhere be it on its own or a part of something else more spatial-ish. Even if that never comes to pass, non-Lucene users who want to use this module for it's geometry annoyingly have to exclude the Lucene dependencies that are there because this module also contains these two classes. In a comment I'll suggest some specifics.</description>
      <attachments/>
      <comments>41</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7058</id>
      <title>Add getters for the properties of several Query implementations</title>
      <description>Hi! At Hibernate Search, we are currently working on an Elasticsearch backend (aside from the existing Lucene backend). As part of this effort, to provide a smooth migration path, we need to be able to rewrite the Lucene queries as Elasticsearch queries. We know it will be neither perfect or comprehensive but we want it to be the best possible experience. It works well in many cases but several implementations of Query don't have the necessary getters to be able to extract the information from the Query. The attached patch add getters to several implementations of Query. It would be nice if it could be applied. Any chance it could be applied to the next point release too? (probably not but I'd better ask).</description>
      <attachments/>
      <comments>11</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>7060</id>
      <title>Update Spatial4j 0.5 to 0.6 (includes change in package)</title>
      <description>Spatial4j 0.6 was released the 26th of February and I want to upgrade to it. The most impactful change is that the package moves from com.spatial4j.core to org.locationtech.spatial4j. For that reason, it would be ideal to do this for Lucene 6.0 https://github.com/locationtech/spatial4j/blob/master/CHANGES.md</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7062</id>
      <title>PointValues should expose additional stats</title>
      <description>There are two useful stats exposed by postings that are easy to also expose in points: size, which is the total number of points indexed, and getDocCount, which is the total number of documents that have at least one point. Like the other point values API fixes, I think we should do this for 6.0, since otherwise if we add it later we'll have to annoyingly handle the "-1" back compat case.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7064</id>
      <title>Make MultiPhraseQuery immutable</title>
      <description>See LUCENE-6531 Mutable queries are an issue for automatic filter caching since modifying a query after it has been put into the cache will corrupt the cache. We should make all queries immutable (up to the boost) to avoid this issue.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7066</id>
      <title>Optimize the "everything matches" case for point range queries</title>
      <description>Now that points have a getDocCount statistic, we could optimize execution of range queries in the case that the range's lower bound is less that the field's min value and the range's upper bound is greater than the field's max value. I would expect such an optimization to kick in frequently for users who store time series in their indices as it is quite frequent for ranges to cover entire segments or even entire indices.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7068</id>
      <title>Retrieve ranks</title>
      <description>Join TopDocs by docs, keep the result ranks.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7069</id>
      <title>Add LatLonPoint.nearest to find closest indexed point to a given query point</title>
      <description>KD trees (used by Lucene's new dimensional points) excel at finding "nearest neighbors" to a given query point ... I think we should add this to Lucene's sandbox as: public static Document nearest(IndexReader r, String field, double lat, double lon) throws IOException I only implemented the 1 nearest neighbor for starters ... I think we can easily generalize this in the future to K nearest. It could also be generalized to more than 2 dimensions, but for now I'm making the class package private in sandbox for just the geo2d (lat/lon) use case. I don't think this should go into 6.0.0, but should go into 6.1: it's a new feature, and we need to wrap up and ship 6.0.0 already</description>
      <attachments/>
      <comments>23</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7071</id>
      <title>Can we reeduce excessive byte[] copying in OfflineSorter?</title>
      <description>OfflineSorter, which dimensional points uses heavily in the &gt; 1D case, works by reading one partition, a set of N unsorted values, from disk and sorting it in memory and writing it out again. The sort invokes a provided Comparator on two BytesRef values, each of which is fully copied from the ByteBlockPool, when it could often reference a slice from the pool instead. Another byte[] copy happens when iterating through the sorted values. This is an optimization ... I'm targeting 6.1.0 not 6.0.0!</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7072</id>
      <title>Geo3dPointField should always use WGS84 planet model</title>
      <description>Right now it takes any PlanetModel, but I think that's dangerous. First, WGS84 is the most accurate one we have, and second, the planet model is secretly baked into the index via the "planetMax" normalization. I think for Lucene we should only expose WGS84 ...</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7074</id>
      <title>Grouping using custom logic</title>
      <description>My company would like to enable customized grouping in Lucene/Solr. Our need is to allow grouping by hamming distance between two text field values. We started implementing this idea in Lucene through editing several classes (Grouping, AbstractTermFirstPassGroupingCollector and AbstractTermSecondPassGroupingCollector) so that they receive custom logic to use for grouping.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7075</id>
      <title>Clean up LegacyNumericUtils usage.</title>
      <description>Tons of code is still on the deprecated LegacyNumericUtils. We will never be able to remove these or even move them to somewhere better (like the backwards jar) if we don't clean this up!</description>
      <attachments/>
      <comments>21</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7078</id>
      <title>Make remaining mutable Queries immutable</title>
      <description>See LUCENE-6531 Mutable queries are an issue for automatic filter caching since modifying a query after it has been put into the cache will corrupt the cache. We should make all queries immutable (up to the boost) to avoid this issue. Since they are part of the public API I would suggest splitting them in an immutable class and a builder like was done for most other Queries before releasing an official 6.x version I did a quick scan through all derived classes of Query and I compiled the following list (ignoring sources in test or contrib folders) Some of them are already marked as experimental (but should perhaps receive the "official" @lucene.experimental tag ?) For some it's possibly not an issue since they should never end up in a filter cache (like MoreLikeThisQuery ?), but then a comment specifying the exception to the rule should perhaps be added. lucene/queries: org.apache.lucene.queries.CommonTermsQuery org.apache.lucene.queries.CustomScoreQuery (marked as @lucene.experimental) org.apache.lucene.queries.mlt.MoreLikeThisQuery lucene/suggest: org.apache.lucene.search.suggest.document.ContextQuery (marked as @lucene.experimental) lucene/facet: org.apache.lucene.facet.DrillDownQuery (marked as @lucene.experimental)</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7081</id>
      <title>Docvalues terms dict should sometimes prefix-compress fixed-length data.</title>
      <description>For Sorted/SortedSet types, we encode ordinals and a term dictionary (similar to old lucene 3 term dictionary). Originally we had no prefix compression, so we "save space" in the fixed-width case by avoiding addressing, we can just use multiplication: https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/codecs/lucene54/Lucene54DocValuesConsumer.java#L423-L425 But it means no compression whatsoever of the actual bytes, even if values are enormous, I don't think its necessarily a good tradeoff. The lack of prefix compression can become much more magnified now that we have fixed width 128-bit point types in the sandbox...</description>
      <attachments/>
      <comments>8</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7091</id>
      <title>Add doc values support to MemoryIndex</title>
      <description>Sometimes queries executed via the MemoryIndex require certain things to be stored as doc values. Today this isn't possible because the memory index doesn't support this and these queries silently return no results.</description>
      <attachments/>
      <comments>25</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7092</id>
      <title>Point range factory methods for excluded bounds</title>
      <description>I am playing with using the new points API with elasticsearch and one challenge is to generate range queries whose bounds are excluded, which is something that was very easy with the previous numerics implementation. It is easy to do externally with ints, but becomes tricky with floats or ip addresses. Maybe we should have factory methods that take 2 additional booleans to allow the bounds to be excluded?</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7097</id>
      <title>Can we increase the stack depth before Introsorter switches to heapsort?</title>
      <description>Introsort is a "safe" quicksort: it uses quicksort but detects when an adversary is at work and cuts over to heapsort at that point. The description at https://en.wikipedia.org/wiki/Introsort shows the cutover as 2X log_2(N) but our impl (IntroSorter) currently uses just log_2. So I tested using 2X log_2 instead, and I see a decent (~5.6%, from 98.2 sec to 92.7 sec) speedup in the time for offline sorter to sort when doing the force merge of 6.1 LatLonPoints from the London UK benchmark. Is there any reason not to switch? I know this means 2X the stack required, but since this is log_2 space that seems fine?</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7098</id>
      <title>BKDWriter should write ords as ints when possible during offline sort</title>
      <description>Today we write all ords as longs, since we support more than 2.1B values in one segment, but the vast majority of the time an int would suffice. We could look into vLong, but this quickly gets tricky because BKDWriter needs random access to the file and we rely on fixed-width entries to do this now.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7103</id>
      <title>further optimize LatLonPoint.newDistanceSort</title>
      <description>This comparator creates bounding boxes to avoid calling haversin(), so its no longer a hotspot for most use cases. But in the worst case, it could still get called many times. This could be because the user had a massive top N value, or because the incoming data is sorted or mostly sorted by decreasing distance, etc. We can optimize the worst case by not invoking a full haversin, just using something that is rank-equivalent.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7105</id>
      <title>squeeze a little more perf out of LatLonPoint.newDistanceQuery</title>
      <description>This query can make use of some of the same optimizations we applied to the distance sort.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7106</id>
      <title>Make it easy to have cumulated point stats across segments</title>
      <description>For other parts of the index, it is easy thanks to the Multi{Terms,DocValues,...} classes. However, we don't have such a thing for points but it would still be nice to have a convenient way to compute eg. the max value of a field on a whole index.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7113</id>
      <title>OfflineSorter and BKD should verify checksums in their temp files</title>
      <description>I am trying to index all 3.2 B points from the latest OpenStreetMaps export. My SSDs were not up to this, so I added a spinning magnets disk to beast2. But then I was hitting scary bug-like exceptions (ArrayIndexOutOfBoundsException) when indexing the first 2B points, and I finally checked dmesg and saw that my hard drive is dying. I think it's important that our temp file usages also validate checksums (like we do for all our index files, either at reader open or at merge or CheckIndex), so we can hopefully easily differentiate a bit-flipping IO system from a possible Lucene bug, in the future.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7115</id>
      <title>Speed up FieldCache.CacheEntry toString by setting initial StringBuilder capacity</title>
      <description>Solr can end up printing a lot of these objects via the JmxMonitoriedMap, see SOLR-8869 and SOLR-6747 as examples. From looking at some profiles, a lot of time and memory are spent resizing the StringBuilder, which doesn't set the initial capacity. On my cluster, the strings are a bit over 200 chars; I set the initial capacity to 250 and ran tests calling toString 1000 times. Tests consistently show 10-15% improvement when setting the initial capacity.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7116</id>
      <title>Document Classifiers to handle numeric fields</title>
      <description>DocumentClassifiers should handle numeric fields. Knn Document Classifier should be able to handle function queries( like the distance from the numeric value in input) as boost factors on top of the MLT boolean query. Investigations will follow with tentative design and patch e.g. input document has price:5 We can classify the document based on all the ones in the index, in proximity to 5 as the price value.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7119</id>
      <title>enable bypassing docValues check in DocTermOrds</title>
      <description>Currently, DocTermOrds refuses to build if doc values have been enabled for a field. While good for catching bugs, this disabled what can be legitimate use cases (such as just trying out an alternate method w/o having to re-configure and re-index, or even using consistently in conjunction with UninvertingReader). We should restore the ability to use this class in other scenarios via adding a flag to bypass the check.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7120</id>
      <title>Improve BKDWriter's checksum verification</title>
      <description>The checksum verification only works when BKDWriter fully reads one of its temp files, but today it opens a reader, seeks to one slice, reads that, and closes. But it turns out, on the full recursion from any given node in the tree, a given file is read once, fully, so if we just share the readers, then we can get checksum verification for these files as well. This is a non-trivial change ... I don't plan on pushing it for 6.0.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7121</id>
      <title>BKDWriter should not store ords when documents are single valued</title>
      <description>Since we now have stats for points fields, it's easy to know up front whether the field you are about to build a BKD tree for is single valued or not. If it is single valued, we can optimize space by not storing the ordinal to identify a point, since its docID also uniquely identifies it. This saves 4 bytes per point, which for the 1D case is non-trivial (12 bytes down to 8 bytes per doc), and even for the 2D case is good reduction (16 bytes down to 12 bytes per doc). This is an optimization ... I won't push it into 6.0.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7122</id>
      <title>BytesRefArray can be more efficient for fixed width values</title>
      <description>Today BytesRefArray uses one int (int[], overallocated) per value to hold the length, but for dimensional points these values are always the same length. This can save another 4 bytes of heap per indexed dimensional point, which is a big improvement (more points can fit in heap at once) for 1D and 2D lat/lon points.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>7124</id>
      <title>SloppyMath is too accurate</title>
      <description>Followup from LUCENE-7123: Latitude/longitude values are encoded by GeoPointField/LatLonPoint with 1E-6 or 1E-7 error. However, the sloppy distance formula is far too accurate to the actual (10nm), making it slower and relying on large tables. We should only do as good as we need, and especially try to reduce the huge tables.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7130</id>
      <title>fold optimizations from LatLonPoint to GeoPointField</title>
      <description>Followup from LUCENE-7127: I had to remove some distance query optimizations for correctness. According to benchmarks it hurt performance. We can win back half of it by just syncing up with LatLonPoint's distance optimizations. Then separately from this, we can investigate trying to safely implement some of what it was trying to do before (and add to both impls).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7140</id>
      <title>Compute a geo3d point that is halfway between two endpoints, measured as an arc distance</title>
      <description>The need was expressed for a method that finds a midpoint between two other points on the surface. The midpoint would be at, distance-wise, half the arc distance between the two endpoints.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7141</id>
      <title>OfflineSorter shouldn't always forceMerge in the end</title>
      <description>Today it always does a final merge, to collapse all segments into a single segment. But typically the caller is going to re-iterate all values anyway, to go off and build an FST or a BKD tree or something, and so that final forceMerge is often not necessary and the merging can be done on the fly when the caller consumes the result. This is somewhat tricky to do ... I'd like to break it into steps, starting with fixing the ByteSequencesReader API to implement BytesRefIterator instead of its own read(BytesRefBuilder) method as a first step.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7147</id>
      <title>Improve disjoint check for geo distance query traversal</title>
      <description>When doing geo distance queries, it is important to avoid traversing subtrees which do not contain any relevant points. We currently have checks which compare the bbox of the query to the bounds of the subtree. However, it is possible for a subtree to overlap the bbox, but still not intersect the query. This issue is to improve that check to avoid unnecessary traversals.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>7148</id>
      <title>Support boolean subset matching</title>
      <description>In Lucene, I know of the possibility of Occur.SHOULD, Occur.MUST and the “minimum should match” setting on the boolean query. Now, when querying, I want to (1) match the documents which either contain all the terms of the query (Occur.MUST for all terms would do that) or, (2) if all terms for a given field of a document are a subset of the query terms, that document should match as well. Example: Document d hast field f with terms A, B, C Query with the following terms should match that document: A B A B A B C A B C D Query with the following terms should not match: D A B D</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7150</id>
      <title>geo3d public APIs should match the 2D apis?</title>
      <description>I'm struggling to benchmark the equivalent to LatLonPoint.newDistanceQuery in the geo3d world. Ideally, I think we'd have a Geo3DPoint.newDistanceQuery? And it would take degrees, not radians, and radiusMeters, not an angle? And if I index and search using PlanetModel.SPHERE I think it should ideally give the same results as LatLonPoint.newDistanceQuery, which uses haversin.</description>
      <attachments/>
      <comments>49</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7152</id>
      <title>Refactor lucene-spatial GeoUtils to core</title>
      <description>GeoUtils contains a lot of common spatial mathematics that can be reused across multiple packages. As discussed in LUCENE-7150 this issue will refactor GeoUtils to a new o.a.l.util.geo package in core that can be the home for other reusable spatial utility classes required by field and query implementations.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7153</id>
      <title>give GeoPoint and LatLonPoint full polygon support</title>
      <description>These two geo impls have a very limited polygon support that does not support inner rings (holes) or multiple outer rings efficiently. Basically if you want to do this, you are left building crazy logic with booleanquery which will send memory into the gigabytes for a single query, needlessly. For example Russia polygon from geonames is 250KB of geojson and over a thousand outer rings. We should instead support this stuff with the queries themselves, especially it will allow us to implement things more efficiently in the future. I think instead of newPolygonQuery(double[], double[]) it should look like newPolygonQuery(Polygon...). A polygon can be a single outer ring (shape) with 0 or more inner rings (holes). No nesting, you just use multiply polygons if you e.g. have an island. See http://esri.github.io/geometry-api-java/doc/Polygon.html for visuals and examples. I indented their GeoJSON example: { "type":"MultiPolygon", "coordinates": [ // first polygon (order does not matter could have been last instead) [ // clockwise =&gt; outer ring [[0.0,0.0],[-0.5,0.5],[0.0,1.0],[0.5,1.0],[1.0,0.5],[0.5,0.0],[0.0,0.0]], // hole [[0.5,0.2],[0.6,0.5],[0.2,0.9],[-0.2,0.5],[0.1,0.2],[0.2,0.3],[0.5,0.2]] ], // second polygon (order does not matter, could have been first instead) [ // island [[0.1,0.7],[0.3,0.7],[0.3,0.4],[0.1,0.4],[0.1,0.7]] ] ], }</description>
      <attachments/>
      <comments>17</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7154</id>
      <title>Add support for polygon holes to Geo3D</title>
      <description>Real-world polygons (e.g. from ESRI) have holes in them. We need polygon support in geo3D that works in the same way. The proposal would be to change the GeoConvexPolygon constructor to include a number of GeoPolygon inputs, each of which would specify a hole. Then, the GeoPolygonFactory.makeGeoPolygon() method would also need to accept a similar list of GeoPolygon hole descriptions. This change is likely to be fairly complex because of the already tricky algorithm used to create convex polygons from non-convex ones, implemented in GeoPolygonFactory.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7158</id>
      <title>Haversin should use the earth's mean radius, not its max (equitorial)?</title>
      <description>Across our spatial modules we seem to disagree about the earth's radius when we model it as a sphere. I think in our haversin implementation we use equitorial (maximum) radius, but maybe in spatial3d we use the earth's mean radius. I think mean makes more sense: the earth is actually a squashed sphere, so it's polar radius is shorter than its equitorial radius. I think it's important, when we model the earth as a sphere, that we pick one radius and try to use that one consistently?</description>
      <attachments/>
      <comments>14</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>7163</id>
      <title>Refactor GeoRect and Polygon to core</title>
      <description>o.a.l.spatial.util.GeoRect and o.a.l.spatial.util.Polygon are reusable classes across multiple lucene modules. It makes sense for them to be moved to the o.a.l.geo package in the core module so they're exposed across multiple modules. GeoRect should also be refactored to something more straightforward, like Rectangle</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7164</id>
      <title>Refactor SloppyMath.haversin from SloppyMath to GeoUtils</title>
      <description>The haversin method is currently in SloppyMath but is used specifically for geo. With the new o.a.l.geo package in the core module (LUCENE-7152) it makes more sense to refactor this to GeoUtils</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7165</id>
      <title>Return GeoPointField Encoding to use full 64 bit space</title>
      <description>This was originally done in LUCENE-6710 but then reverted due to issues with GeoHash encoding (which has since been removed). This issue will revert the GeoPoint encoding back to full 64 bit space which will synchronize quantization TOLERANCE between LatLonPoint and GeoPointField.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7169</id>
      <title>Add Geo3DPoint.newSolidQuery</title>
      <description>We should include the ability to search for solids as well in the general API. This capability is unique to Geo3D.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7173</id>
      <title>Add Polygon... variant of newPolygonQuery() to Geo3DPoint</title>
      <description>We need to add the Polygon... variant of newPolygonQuery() to Geo3DPoint to support holes and also bring the Geo3DPoint API into agreement with the 2D API's.</description>
      <attachments/>
      <comments>16</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7176</id>
      <title>Geo3d GeoPath should have a factory and implementation should be package private</title>
      <description>GeoPath shapes are the only one that don't at the moment have a factory. This should change because it allows us to limit API access to implementation internals.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7177</id>
      <title>adapt LUCENE-7159 to geopointfield too</title>
      <description>LUCENE-7159 adds a precomputation to speed up complex polygon queries. We should adapt this to GeoPoint too.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7180</id>
      <title>Add distance sorting to GeoPointField</title>
      <description>This adds distance sort capability to GeoPointField similar to LatLonPointSortField.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7181</id>
      <title>JapaneseTokenizer: Validate segmentation of User Dictionary entries on creation</title>
      <description>From the conversation on the dev list The user dictionary in the JapaneseTokenizer allows users to customize how a stream is broken into tokens using a specific set of rules provided like: AABBBCC -&gt; AA BBB CC It does not allow users to change any of the token characters like: (1) AABBBCC -&gt; DD BBB CC (this will just tokenize to "AA", "BBB", "CC", seems to only care about positions) It also doesn't let a character be part of more than one token, like: (2) AABBBCC -&gt; AAB BBB BCC (this will throw an AIOOBE) ..or make the output token bigger than the input text: (3) AA -&gt; AAA (Also AIOOBE) Currently there is no validation for those cases, case 1 doesn't fail but provide unexpected tokens. Cases 2 and 3 fail when the input text is analyzed. We should add validation to the UserDictionary creation.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7184</id>
      <title>Add GeoEncodingUtils to core</title>
      <description>This is part 1 for LUCENE-7165. This task will add a GeoEncodingUtils helper class to o.a.l.geo in the core module for reusing lat/lon encoding methods. Existing encoding methods in LatLonPoint will be refactored to the new helper class so a new numerically stable morton encoding can be added that reuses the same encoding methods.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7186</id>
      <title>Add numerically stable morton encoding to GeoEncodingUtils</title>
      <description>This is the follow on to LUCENE-7184. It adds a numerically stable morton encoding method to o.a.l.geo.GeoEncodingUtils that can be reused by GeoPointField and a new GeoPointField based on Point encoding.</description>
      <attachments/>
      <comments>17</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7190</id>
      <title>Geo3d public classes have some unnecessary protected member methods and classes; this should be cleaned up</title>
      <description>The classes that were allowed to remain public in geo3d need to be reviewed for protected fields, classes, and methods. Some of these can be made private without reducing the flexibility of the package much.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7192</id>
      <title>Geo3d polygon creation should not get upset about co-linear points</title>
      <description>Currently, if you create a polygon with co-linear adjacent points, the polygon fails to create (you get IllegalArgumentException). We should make this more robust.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7193</id>
      <title>Add generic (non class specific) f1-measure</title>
      <description>Add ConfusionMatrix#getF1Measure method that is not tight to a specific class (uses general precision and recall measures).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7196</id>
      <title>DataSplitter should be providing class centric doc sets in all generated indexes</title>
      <description>DataSplitter currently creates 3 indexes (train/test/cv) out of an original index for evaluation of Classifiers however "class coverage" in such generated indexes is not guaranteed; that means e.g. in training index only documents belonging to 50% of the class set could be indexed and hence classifiers may not be very effective. In order to provide more consistent evaluation the generated index should contain _ split-ratio * | docs in c |_ documents for each class c.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7199</id>
      <title>Improve algorithm for randomly finding an interior point for Geo3D polygons</title>
      <description>Michael McCandless's benchmark for polygon search shows that constructing Geo3D polygon queries is quite expensive compared to other technologies. My belief is that this is due largely to how an interior point for determination of clockwise/counterclockwise are found: this is currently searched for randomly across the entire globe. I suspect that we could replace this algorithm with a random algorithm that uses one of the polygon's edge points and looks randomly within a small distance of that point. This would greatly reduce the number of failed attempts to find a "pole" (as I call it).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7203</id>
      <title>Improve O(N^2) GeoPolygon intersection computation performance</title>
      <description>Computing shape intersections with a polygon has an O(N^2) loop in it. This can be removed if two bounds are discovered ahead of time to use instead.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7206</id>
      <title>nest child query explain into ToParentBlockJoinQuery.BlockJoinScorer.explain(int)</title>
      <description>Now to parent query match is explained with {{Score based on child doc range from .. to .. }} that's quite useless. It's proposed to nest child query match explanation from the first matching child document into parent explain. WDYT?</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7208</id>
      <title>Expose Automaton getter on RunAutomaton</title>
      <description>RunAutomaton is (of course) built from an Automaton and it stores it on a field, package-private. It would be convenient to expose it via a getter. In fact CompiledAutomaton wants it. While we're at it, lets mark all the fields private and ensure that the only classes using its state (currently its 2 subclasses) use the getters that already exist for what they need. Those getters are already final so I don't expect a performance impact.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7211</id>
      <title>Spatial RPT Intersects should use DocIdSetBuilder to save memory/GC</title>
      <description>I’ve been continuing some analysis into JVM garbage sources in my Solr index. (5.4, 86M docs/core, 56k 99.9th percentile hit count with my query corpus) After applying SOLR-8922, I find my biggest source of garbage by a literal order of magnitude (by size) is the long[] allocated by FixedBitSet. From the backtraces, it appears the biggest source of FixBitSet creation in my case (by two orders of magnitude) is my use of queries that involve geospatial filtering. Specifically, IntersectsPrefixTreeQuery.getDocIdSet, here: https://github.com/apache/lucene-solr/blob/569b6ca9ca439ee82734622f35f6b6342c0e9228/lucene/spatial-extras/src/java/org/apache/lucene/spatial/prefix/IntersectsPrefixTreeQuery.java#L60 Has this been considered for optimization? I can think of a few paths: 1. Persistent Object pools - FixedBitSet size is allocated based on maxDoc, which presumably changes less frequently than queries are issued. If an existing FixedBitSet were not available from a pool, the worst case (create a new one) would be no worse than the current behavior. The complication would be enforcement around when to return the object to the pool, but it looks like this has some lifecycle hooks already. 2. I note that a thing called a SparseFixedBitSet already exists, and puts considerable effort into allocating smaller chunks only as necessary. Is this not usable for this purpose? How significant is the performance difference? I'd be happy to spend some time on a patch, but I was hoping for a little more data around the current choices before choosing an approach.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7212</id>
      <title>Add Geo3DPoint equivalents of LatLonPointDistanceComparator and LatLonPointSortField</title>
      <description>Geo3D has a number of distance measurements and a generic way of computing interior distance. It would be great to take advantage of that for queries that return results ordered by interior distance.</description>
      <attachments/>
      <comments>49</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7215</id>
      <title>don't invoke full haversin for LatLonPoint.newDistanceQuery</title>
      <description>For tree traversals and edge cases we still sometimes invoke full haversin (with asin() call and everything). this is not necessary: we just need to compute the exact sort key needed for comparisons. While not a huge optimization, its obviously less work and keeps the overhead of the BKD traversal as low as possible. And it removes the slow asin call from any hot path (its already done for sorting too), with its large tables and so on.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7216</id>
      <title>Find an O(N) way to tile arbitrary geo3d polygons</title>
      <description>Construction of GeoPolygons through GeoPolygonFactory.makeGeoPolygon() takes O(N^2) time, where N is the number of points. This is due to the non-transitive nature of edge/point comparisons in Geo3D; just knowing that the next point is on the right side of an edge doesn't tell you anything about previous edges. Still, maybe there is a 2D algorithm somewhere that can be adapted to run in O(N) time.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7217</id>
      <title>I want to have a system of adding predefined Keywords and weights for documents, when new index are being made.</title>
      <description>JATE toolkit gives an easy way to find all the keywords in corpus along with a weight of each keyword. I want to develop a mechanism of incorporating that list into Lucene.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7221</id>
      <title>Improve Geo3d BKD search performance for large polygons</title>
      <description>Geo3d polygon BKD search performance is very slow for large polygons. We need to improve it.</description>
      <attachments/>
      <comments>18</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7226</id>
      <title>Geo3d polygon data cleaning would be helpful</title>
      <description>Some of the OpenStreetMap data has duplicate points, and edges that backtrack upon themselves. For most means of constructing polygons, it would be good to "correct" the data and prune problematic points.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7229</id>
      <title>Improve Polygon.relate</title>
      <description>This method is currently quite slow and in many cases does more work than is required. The speed actually directly impacts queries (tree traversal) and bounds grid size to something tiny making it less effective. I think we should replace it line intersections based on orientation methods described here http://www.cs.berkeley.edu/~jrs/meshpapers/robnotes.pdf and https://www.cs.cmu.edu/~quake/robust.html For one, a naive implementation is considerably faster than the method today: both because it reduces the cost of BKD tree traversals and also because it makes grid construction cheaper. This means we can increase its level of detail with similar or lower startup cost. Now its more like a Mario Brothers 2 picture of your polygon instead of Space Invaders. Synthetic polygons from luceneUtil vertices old QPS new QPS old startup cost new startup cost 50 20.4 21.7 1ms 1ms 500 11.2 14.4 5ms 4ms 1000 7.4 10.0 9ms 8ms Real polygons (33 london districts: http://data.london.gov.uk/2011-boundary-files) vertices old QPS new QPS old startup cost new startup cost avg 5.6k 4.9 8.6 94ms 85ms But I also like using this method because its possible to extend it to remove floating point error completely in the future with techniques described in those links. This may be necessary if we want to do smarter things (e.g. not linear time).</description>
      <attachments/>
      <comments>20</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7230</id>
      <title>IntPoint, LongPoint etc. need a ctor with FieldType</title>
      <description>Previously IntField, LongField etc. had constructors that also take a FieldType. Such a constructor would also be required for IntPoint, LongPoint etc. so that a 'stored' Point can easily be created.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7234</id>
      <title>Add InetAddressPoint.nextUp/nextDown</title>
      <description>This can be useful for dealing with exclusive bounds.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7235</id>
      <title>Avoid taking the lock in LRUQueryCache when not necessary</title>
      <description>LRUQueryCache's CachingWeightWrapper works this way: first it looks up the cache to see if there is an entry for the query in the current leaf if yes, it returns it otherwise it checks whether the query should be cached on this leaf if yes, it builds a cache entry and returns it otherwise it returns a scorer built from the wrapped weight The potential issue is that this first step always takes the lock, and I have seen a couple cases where indices were small and/or queries were very cheap and this showed up as a bottleneck. On the other hand, we have checks in step 3 that tell the cache to not cache on a particular segment regardless of the query. So I would like to move that part before 1 so that we do not even take the lock in that case. For instance right now we require that segments have at least 10k documents and 3% of all docs in the index to be cached. I just looked at a random index that contains 1.7m documents, and only 4 segments out of 29 met this criterion (yet they contain 1.1m documents: 65% of the total index size). So in the case of that index, we would take the lock 7x less often.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7237</id>
      <title>LRUQueryCache should rather not cache than wait on a lock</title>
      <description>This is an idea Robert just mentioned to me: currently the cache is using a lock to keep various data-structures in sync. It is a pity that you might have contention because of caching. So something we could do would be to not cache when the lock is already taken.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7238</id>
      <title>MemoryIndex.createSearcher should disable caching explicitly</title>
      <description>Follow-up of LUCENE-7235: In practice, nothing will be cached with a reasonable cache implementation given the size of the index (a single document). But it would still be better to explicitly disable caching so that we don't eg. take unnecessary locks.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7239</id>
      <title>Speed up LatLonPoint's polygon queries when there are many vertices</title>
      <description>This is inspired by the "reliability and numerical stability" recommendations at the end of http://www-ma2.upc.es/geoc/Schirra-pointPolygon.pdf. Basically our polys need to answer two questions that are slow today: contains(point) crosses(rectangle) Both of these ops only care about a subset of edges: the ones overlapping a y interval range. We can organize these edges in an interval tree to be practical and speed things up a lot. Worst case is still O but those solutions are more complex to do.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7240</id>
      <title>Remove DocValues from LatLonPoint, add DocValuesField for that</title>
      <description>LatLonPoint needed two-phase intersection initially because of big inefficiencies, but as of LUCENE-7239 all of its query operations: newBoxQuery(), newDistanceQuery(), newPolygonQuery() and nearest() only need the points datastructure (BKD). If you want to do newDistanceSort() then you need docvalues for that, but I think it should be moved to a separate field: e.g. docvalues is optional just like any other field in lucene. We can add other methods that make sense to that new docvalues field (e.g. facet by distance/region, expressions support, whatever). It is really disjoint from the core query support: and also currently has a heavyish cost of ~64-bits per value in space.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7241</id>
      <title>Improve performance of geo3d for polygons with very large numbers of points</title>
      <description>This ticket corresponds to LUCENE-7239, except it's for geo3d polygons. The trick here is to organize edges by some criteria, e.g. z value range, and use that to avoid needing to go through all edges and/or tile large irregular polygons. Then we use the ability to quickly determine intersections to figure out whether a point is within the polygon, or not. The current way geo3d polygons are constructed involves finding a single point, or "pole", which all polygon points circle. This point is known to be either "in" or "out" based on the direction of the points. So we have one place of "truth" on the globe that is known at polygon setup time. If edges are organized by z value, where the z values for an edge are computed by the standard way of computing bounds for a plane, then we can readily organize edges into a tree structure such that it is easy to find all edges we need to check for a given z value. Then, we merely need to compute how many intersections to consider as we navigate from the "truth" point to the point being tested. In practice, this means both having a tree that is organized by z, and a tree organized by (x,y), since we need to navigate in both directions. But then we can cheaply count the number of intersections, and once we do that, we know whether our point is "in" or "out". The other performance improvement we need is whether a given plane intersects the polygon within provided bounds. This can be done using the same two trees (z and (x,y)), by virtue of picking which tree to use based on the plane's minimum bounds in z or (x,y). And, in practice, we might well use three trees: one in x, one in y, and one in z, which would mean we didn't have to compute longitudes ever. An implementation like this trades off the cost of finding point membership in near O(log(n)) time vs. the extra expense per step of finding that membership. Setup of the query is O(n) in this scheme, rather than O(n^2) in the current implementation, but once again each individual step is more expensive. Therefore I would expect we'd want to use the current implementation for simpler polygons and this sort of implementation for tougher polygons. Choosing which to use is a topic for another ticket.</description>
      <attachments/>
      <comments>46</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7243</id>
      <title>Remove LeafReaderContext from QueryCachingPolicy.shouldCache</title>
      <description>Now that the heuristic to not cache on small segments has been moved to the cache, we don't need the LeafReaderContext in QueryCachingPolicy.shouldCache.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7245</id>
      <title>Automatic warm-up of the query cache on new segments</title>
      <description>Thanks to the fact that we track recently-used queries, we know which ones are likely to be reused and we could use this information in order to automatically warm up the query cache on new segments (typically after a refresh after a merge).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7246</id>
      <title>Can LRUQueryCache reuse DocIdSets that are created by some queries anyway?</title>
      <description>Some queries need to create a DocIdSet to work. This is for instance the case with TermsQuery, multi-term queries, point-in-set queries and point range queries. We cache them more aggressively because these queries need to evaluate all matches on a segment before they can return a Scorer. But this can also be dangerous: if there is little reuse, then we keep converting the doc id sets that these queries create to another DocIdSet. This worries me a bit eg. for point range queries: they made numeric ranges faster in practice so I would not like caching to make them appear slower than they are when caching is disabled. So I would like to somehow bring back the optimization that we had in 1.x with DocIdSet.isCacheable so that we do not need to convert DocIdSet instances when we could just reuse existing instances.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7253</id>
      <title>Make sparse doc values and segments merging more efficient</title>
      <description>Doc Values were optimized recently to efficiently store sparse data. Unfortunately there is still big problem with Doc Values merges for sparse fields. When we imagine 1 billion documents index it seems it doesn't matter if all documents have value for this field or there is only 1 document with value. Segment merge time is the same for both cases. In most cases this is not a problem but there are several cases in which one can expect having many fields with sparse doc values. I can describe an example. During performance tests of a system with large number of sparse fields I realized that Doc Values merges are a bottleneck. I had hundreds of different numeric fields. Each document contained only small subset of all fields. Average document contains 5-7 different numeric values. As you can see data was very sparse in these fields. It turned out that ingestion process was CPU-bound. Most of CPU time was spent in DocValues related methods (SingletonSortedNumericDocValues#setDocument, DocValuesConsumer$10$1#next, DocValuesConsumer#isSingleValued, DocValuesConsumer$4$1#setNext, ...) - mostly during merging segments. Adrien Grand suggested to reduce the number of sparse fields and replace them with smaller number of denser fields. This helped a lot but complicated fields naming. I am not much familiar with Doc Values source code but I have small suggestion how to improve Doc Values merges for sparse fields. I realized that Doc Values producers and consumers use Iterators. Let's take an example of numeric Doc Values. Would it be possible to replace Iterator which "travels" through all documents with Iterator over collection of non empty values? Of course this would require storing object (instead of numeric) which contains value and document ID. Such an iterator could significantly improve merge time of sparse Doc Values fields. IMHO this won't cause big overhead for dense structures but it can be game changer for sparse structures. This is what happens in NumericDocValuesWriter on flush dvConsumer.addNumericField(fieldInfo, new Iterable&lt;Number&gt;() { @Override public Iterator&lt;Number&gt; iterator() { return new NumericIterator(maxDoc, values, docsWithField); } }); Before this happens during addValue, this loop is executed to fill holes. // Fill in any holes: for (int i = (int)pending.size(); i &lt; docID; ++i) { pending.add(MISSING); } It turns out that variable called pending is used only internally in NumericDocValuesWriter. I know pending is PackedLongValues and it wouldn't be good to change it with different class (some kind of list) because this may break DV performance for dense fields. I hope someone can suggest interesting solutions for this problem . It would be great if discussion about sparse Doc Values merge performance can start here.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>7258</id>
      <title>Tune DocIdSetBuilder allocation rate</title>
      <description>LUCENE-7211 converted IntersectsPrefixTreeQuery to use DocIdSetBuilder, but didn't actually reduce garbage generation for my Solr index. Since something like 40% of my garbage (by space) is now attributed to DocIdSetBuilder.growBuffer, I charted a few different allocation strategies to see if I could tune things more. See here: http://i.imgur.com/7sXLAYv.jpg The jump-then-flatline at the right would be where DocIdSetBuilder gives up and allocates a FixedBitSet for a 100M-doc index. (The 1M-doc index curve/cutoff looked similar) Perhaps unsurprisingly, the 1/8th growth factor in ArrayUtil.oversize is terrible from an allocation standpoint if you're doing a lot of expansions, and is especially terrible when used to build a short-lived data structure like this one. By the time it goes with the FBS, it's allocated around twice as much memory for the buffer as it would have needed for just the FBS.</description>
      <attachments/>
      <comments>27</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7259</id>
      <title>speed up MatchingPoints cost() estimation</title>
      <description>MatchingPoints currently tracks a counter in the super-hot add() loop. While not a big deal, we can easily just use the grow() api for this instead (which is only currently called e.g. every 1k docs).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7260</id>
      <title>StandardQueryParser is over 100 times slower in v5 compared to v3</title>
      <description>The following test code times parsing a large query. import org.apache.lucene.analysis.KeywordAnalyzer; //import org.apache.lucene.analysis.core.KeywordAnalyzer; import org.apache.lucene.queryParser.standard.StandardQueryParser; //import org.apache.lucene.queryparser.flexible.standard.StandardQueryParser; import org.apache.lucene.search.BooleanQuery; public class LargeQueryTest { public static void main(String[] args) throws Exception { BooleanQuery.setMaxClauseCount(50_000); StringBuilder builder = new StringBuilder(50_000*10); builder.append("id:( "); boolean first = true; for (int i = 0; i &lt; 50_000; i++) { if (first) { first = false; } else { builder.append(" OR "); } builder.append(String.valueOf(i)); } builder.append(" )"); String queryString = builder.toString(); StandardQueryParser parser2 = new StandardQueryParser(new KeywordAnalyzer()); for (int i = 0; i &lt; 10; i++) { long t0 = System.currentTimeMillis(); parser2.parse(queryString, "nope"); long t1 = System.currentTimeMillis(); System.out.println(t1-t0); } } } For Lucene 3.6.2, the timings settle down to 200~300 with the fastest being 207. For Lucene 5.4.1, the timings settle down to 20000~30000 with the fastest being 22444. So at some point, some change made the query parser 100 times slower. I would suspect that it has something to do with how the list of children is now handled. Every time someone gets the children, it copies the list. Every time someone sets the children, it walks through to detach parent references and then reattaches them all again. If it were me, I would probably make these collections immutable so that I didn't have to defensively copy them.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7261</id>
      <title>Speed up LSBRadixSorter</title>
      <description>Currently it always does 4 passes over the data (one per byte, since ints have 4 bytes). However, most of the time, we know maxDoc, so we can use this information to do fewer passes when they are not necessary. For instance, if maxDoc is less than or equal to 2^24, we only need 3 passes, and if maxDoc is less than or equals to 2^16, we only need two passes.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7262</id>
      <title>Add back the "estimate match count" optimization</title>
      <description>Follow-up to my last message on LUCENE-7051: I removed this optimization a while ago because it made things a bit more complicated but did not seem to help with point queries. However the reason why it did not seem to help was that the benchmark only runs queries that match 25% of the dataset. This makes the run time completely dominated by calls to FixedBitSet.set so the call to FixedBitSet.cardinality() looks free. However with slightly sparser queries like the geo benchmark generates (dense enough to trigger the creation of a FixedBitSet but sparse enough so that FixedBitSet.set does not dominate the run time), one can notice speed-ups when this call is skipped.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7263</id>
      <title>xmlparser: Allow SpanQueryBuilder to be used by derived classes</title>
      <description>Following on from LUCENE-7210 (and others), the xml queryparser has different factories, one for creating normal queries and one for creating span queries. The former is a protected variable so can be used by derived classes, the latter isn't. This makes the spanFactory a variable that can be used more easily. No functional changes.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7264</id>
      <title>Fewer conditionals in DocIdSetBuilder.add</title>
      <description>As reported in LUCENE-7254, DocIdSetBuilder.add has several conditionals that slow down the construction of the DocIdSet.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7268</id>
      <title>Remove ArrayUtil.timSort?</title>
      <description>Is there some workload where our timSort is better than the JDK one? Should we just remove ours if its slower? Not that its a great test, but i switched Polygon2D edge sorting (just the one where it says "sort the edges then build a balanced tree from them") from Arrays.sort to ArrayUtil.timSort and was surprised when performance was much slower for an enormous polygon (http://people.apache.org/~mikemccand/geobench/cleveland.poly.txt.gz)</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7270</id>
      <title>Use better balanced trees for Geo3d complex polygons</title>
      <description>The current tree data structure in GeoComplexPolygon has a lot of weaknesses. A better algorithm maybe can be taken from Polygon2D, which basically does the following: Each node has: low value (which is for that edge alone) max value (which is for that edge and all children) Balanced tree building: sort by low value (which is for the individual edge), and use max value as tie breaker (which is max for edge and all children) build tree after sorting, picking midpoint and recursively building lesser and greater children that way Balanced tree traversal (looking for range minValue -&gt; maxValue): Descend the entire tree until the node fails this test: if (minValue &lt;= max) { ... } So if the minimum value being sought is greater than the max for this edge and all children, we stop and don't look at children. (Q: does this represent a good split and a targeted range? Maybe... We can certainly try it.)</description>
      <attachments/>
      <comments>12</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7272</id>
      <title>See if there's a way to cheapen geo3d's relationship calculations to make BKD Trees faster</title>
      <description>BKD Tree code does not make use of most of the fine relationship detail returned by getRelationship(). This means a lot of computation is going into figuring out fine details that gets simply wasted. We should consider having a much simpler related relationship method that returns only what BKD trees need to proceed. Here's the current code: // First, check bounds. If the shape is entirely contained, return CELL_CROSSES_QUERY. if (shapeBounds.getMinimumX() &gt;= xMin &amp;&amp; shapeBounds.getMaximumX() &lt;= xMax &amp;&amp; shapeBounds.getMinimumY() &gt;= yMin &amp;&amp; shapeBounds.getMaximumY() &lt;= yMax &amp;&amp; shapeBounds.getMinimumZ() &gt;= zMin &amp;&amp; shapeBounds.getMaximumZ() &lt;= zMax) { return Relation.CELL_CROSSES_QUERY; } // Quick test failed so do slower one... GeoArea xyzSolid = GeoAreaFactory.makeGeoArea(PlanetModel.WGS84, xMin, xMax, yMin, yMax, zMin, zMax); switch(xyzSolid.getRelationship(shape)) { case GeoArea.CONTAINS: // Shape fully contains the cell //System.out.println(" inside"); return Relation.CELL_INSIDE_QUERY; case GeoArea.OVERLAPS: // They do overlap but neither contains the other: //System.out.println(" crosses1"); return Relation.CELL_CROSSES_QUERY; case GeoArea.WITHIN: // Cell fully contains the shape: //System.out.println(" crosses2"); // return Relation.SHAPE_INSIDE_CELL; return Relation.CELL_CROSSES_QUERY; case GeoArea.DISJOINT: // They do not overlap at all //System.out.println(" outside"); return Relation.CELL_OUTSIDE_QUERY; default: assert false; return Relation.CELL_CROSSES_QUERY; } It looks like only CELL_CROSSES_QUERY, CELL_OUTSIDE_QUERY, and CELL_INSIDE_QUERY are ever returned. This means we could (if computationally helpful) have a getRelationship() variant that only distinguishes between: GeoArea.DISJOINT GeoArea.CONTAINS GeoArea.OVERLAPS ... with no GeoArea.WITHIN detection. The question is, would this save significant computation?</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7274</id>
      <title>Add LogisticRegressionDocumentClassifier</title>
      <description>Add LogisticRegressionDocumentClassifier for Lucene.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7276</id>
      <title>Add an optional reason to the MatchNoDocsQuery</title>
      <description>It's sometimes difficult to debug a query that results in a MatchNoDocsQuery. The MatchNoDocsQuery is always rewritten in an empty boolean query. This patch adds an optional reason and implements a weight in order to keep track of the reason why the query did not match any document. The reason is printed on toString and when an explanation for noMatch is asked. For instance the query: new MatchNoDocsQuery("Field not found").toString() =&gt; 'MatchNoDocsQuery["field 'title' not found"]'</description>
      <attachments/>
      <comments>23</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>7277</id>
      <title>Make Query.hashCode and Query.equals abstract</title>
      <description>Custom subclasses of the Query class have the default implementation of hashCode/equals that make all instances of the subclass equal. If somebody doesn't know this it can be pretty tricky to debug with IndexSearcher's query cache on. Is there any rationale for declaring it this way instead of making those methods abstract (and enforcing their proper implementation in a subclass)? public int hashCode() { return getClass().hashCode(); } public boolean equals(Object obj) { if (obj == null) return false; return getClass() == obj.getClass(); }</description>
      <attachments/>
      <comments>28</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7278</id>
      <title>Make template Calendar configurable in DateRangePrefixTree</title>
      <description>DateRangePrefixTree (a SpatialPrefixTree designed for dates and date ranges) currently uses a hard-coded Calendar template for making new instances. This ought to be configurable so that, for example, the Gregorian change date can be configured. This is particularly important for compatibility with Java 8's java.time API which uses the Gregorian calendar for all time (there is no use of Julian prior to 1582).</description>
      <attachments/>
      <comments>19</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7280</id>
      <title>SpatialStrategy impl for GeoPointField</title>
      <description>It would be cool if were a SpatialStrategy (an abstraction in spatial-extras module) implementation for GeoPointField (in "spatial" module). In this way code that uses the SpatialStrategy abstraction could leverage this implementation with no code changes, aside from the construction part. Perhaps the biggest part of doing this is creating implementations of the Spatial4j abstractions to, ultimately, create Shape impls that are basic POJOs to hold the basic data but otherwise do no calculations (throw exceptions from relate(), etc.). Then, SpatialStrategy.createQuery can cast the Shape to see what sort of shape it is before calling, e.g. GeoPointInPolygonQuery. Those Spatial4j abstractions might be used for other future SpatialStrategy wrappers, like for Geo3DPoint, or LatLonPoint.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7283</id>
      <title>Move SlowCompositeReaderWrapper and uninverting package to solr sources</title>
      <description>Spinoff from LUCENE-6766, where we fixed index-time sorting to have first class support in Lucene's ore, and no longer use SlowCompositeReaderWrapper. This is a dangerous, long living class, that tries to pretend a set of N segments is actually just a single segment. It's a leaky abstraction, has poor performance, and puts undue pressure on the APIs of new Lucene features to try to keep up this illusion. With LUCENE-6766, finally all usage of this class (except for UninvertedReader tests, which should maybe also move out?) has been removed from Lucene, so I think we should move it to Solr. This may also lead to a solution for LUCENE-7086 since e.g. the class could tap into solr's schema to "know" how to handle points fields properly.</description>
      <attachments/>
      <comments>15</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7287</id>
      <title>New lemma-tizer plugin for ukrainian language.</title>
      <description>Hi all, I wonder whether you are interested in supporting a plugin which provides a mapping between ukrainian word forms and their lemmas. Some tests and docs go out-of-the-box =) . https://github.com/mrgambal/elasticsearch-ukrainian-lemmatizer It's really simple but still works and generates some value for its users. More: https://github.com/elastic/elasticsearch/issues/18303</description>
      <attachments/>
      <comments>62</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>7288</id>
      <title>Geo3d distance measure for distances outside of shape uses Double.MAX_VALUE; should use Double.POSITIVE_INFINITY</title>
      <description>Geo3d distance measure for distances outside of shape uses Double.MAX_VALUE; should use Double.POSITIVE_INFINITY.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7289</id>
      <title>Half floats</title>
      <description>When it comes to indexing floating-point data, doubles and floats are quite space-intensive even though full precision is rarely needed. So maybe we should consider exposing half floats? Half floats have 5 bits for the exponent and 11 bits (including the implicit bit) for the mantissa so the minimum value is ~6x10−8, the maximum is 65504 and they can represent all integers between -2048 and 2048 accurately.</description>
      <attachments/>
      <comments>11</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7292</id>
      <title>Change build system to use "--release 8" instead of "-source/-target" when invoking javac</title>
      <description>Currently we pass -source 1.8 -target 1.8 to javac and javadoc when compiling our source code. We all know that this brings problems, because cross-compiling does not really work. We create class files that are able to run on Java 8, but when it is compiled with java 9, it is not sure that some code may use Java 9 APIs that are not available in Java 8. Javac prints a warning about this (it complains about the bootclasspath not pointing to JDK 8 when used with source/target 1.8). Java 8 is the last version of Java that has this trap. From Java 9 on, instead of passing source and target, the recommended way is to pass a single -release 8 parameter to javac (see http://openjdk.java.net/jeps/247). This solves the bootsclasspath problem, because it has all the previous java versions as "signatures" (like forbiddenapis), including deprecated APIs,... everything included. You can find this in the $JAVA_HOME/lib/ct.sym file (which is a ZIP file, so you can open it with a ZIP tool of your choice). In Java 9+, this file also contains all old APIs from Java 6+. When invoking the compiler with -release 8, there is no risk of accidentally using API from newer versions. The migration here is quite simple: As we require Java 8 already, there is (theoretically) no need to pass source and target anymore. It is enough to just pass -release 8 if we detect Java 9 as compiling JVM. Nevertheless I plan to do the following: remove properties javac.source and javac.target from Ant build add javac.release property and define it to be "8" (not "1.8", this is new version styling that also works with Java 8+ already) remove attributes in the &lt;javac source="..." target="..."/&gt; calls add a new Ant property javac.release.args that is dynamically evaluated inside our compile macro: On Java 9 it evaluates to -release ${javac.release}, for java 8 it uses -source ${javac.release} -target ${javac.release} for backwards compatibility pass this new arg to javac as &lt;arg line="..."/&gt; By this we could theoretically remove the check from smoketester about the compiling JDK (the MANIFEST check), because although compiled with Java 9, the class files were actually compiled against the old Java API from ct.sym file. I will also align the warnings to reenable -Xlint:options.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7295</id>
      <title>TermAutomatonQuery.hashCode calculates Automaton.toDot().hash</title>
      <description>This is going to be excruciatingly slow? We could at least cache the hash code once computed...</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7296</id>
      <title>Update forbiddenapis to version 2.1</title>
      <description>Forbiddenapis v2.1 was released a few minutes ago. The new version supports: Java 9 b119 support (updated ASM - support for new bytecode version; no dependency on bytecode version of JDK internals anymore) internalRuntimeForbidden deprecated, new bundled sigantures "jdk-non-portable" new bundled signatures: jdk-reflection (forbids setAccessible()) Bugfixes, the most important one is about not detecting violations if a superclass was forbidden, but method was overriden in subclass. This is the reason for a fix together with the commit</description>
      <attachments/>
      <comments>6</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7299</id>
      <title>BytesRefHash.sort() should use radix sort?</title>
      <description>Switching DocIdSetBuilder to radix sort helped make things significantly faster. We should be able to do the same with BytesRefHash.sort()?</description>
      <attachments/>
      <comments>15</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7300</id>
      <title>Add directory wrapper that optionally uses hardlinks in copyFrom</title>
      <description>Today we always do byte-by-byte copy in Directory#copyFrom. While this is reliable and should be the default, certain situations can be improved by using hardlinks if possible to get constant time copy on OS / FS that support such an operation. Something like this could reside in misc if it's contained enough since it requires LinkPermissions to be set and needs to detect if both directories are subclasses of FSDirectory etc.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7302</id>
      <title>IndexWriter should tell you the order of indexing operations</title>
      <description>Today, when you use multiple threads to concurrently index, Lucene knows the effective order that those operations were applied to the index, but doesn't return that information back to you. But this is important to know, if you want to build a reliable search API on top of Lucene. Combined with the recently added NRT replication (LUCENE-5438) it can be a strong basis for an efficient distributed search API. I think we should return this information, since we already have it, and since it could simplify servers (ES/Solr) on top of Lucene: They would not require locking preventing the same id from being indexed concurrently since they could instead check the returned sequence number to know which update "won", for features like "realtime get". (Locking is probably still needed for features like optimistic concurrency). When re-applying operations from a prior commit point, e.g. on recovering after a crash from a transaction log, they can know exactly which operations made it into the commit and which did not, and replay only the truly missing operations. Not returning this just hurts people who try to build servers on top with clear semantics on crashing/recovering ... I also struggled with this when building a simple "server wrapper" on top of Lucene (LUCENE-5376).</description>
      <attachments/>
      <comments>14</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7304</id>
      <title>Doc values based block join implementation</title>
      <description>At query time the block join relies on a bitset for finding the previous parent doc during advancing the doc id iterator. On large indices these bitsets can consume large amounts of jvm heap space. Also typically due the nature how these bitsets are set, the 'FixedBitSet' implementation is used. The idea I had was to replace the bitset usage by a numeric doc values field that stores offsets. Each child doc stores how many docids it is from its parent doc and each parent stores how many docids it is apart from its first child. At query time this information can be used to perform the block join. I think another benefit of this approach is that external tools can now easily determine if a doc is part of a block of documents and perhaps this also helps index time sorting?</description>
      <attachments/>
      <comments>27</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7305</id>
      <title>Use macro average in confusion matrix metrics to normalize imbalanced classes</title>
      <description>ConfusionMatrix multi class measures should be based on macro average to avoid bias (for the good or the bad) from imbalanced classes.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7306</id>
      <title>Use radix sort for points too</title>
      <description>Like postings, points make heavy use of sorting at indexing time, so we should try to leverage radix sort too?</description>
      <attachments/>
      <comments>11</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7309</id>
      <title>Speed up Polygon2D construction</title>
      <description>Polygon2D.create recursively sorts on each dimension to build a tree. However, it does not really need to sort the data, it just needs to compute the median and partition other values around it, which can be performed in O. If I am not mistaken, this would make Polygon2D.create run in O(n log) rather than O(n log^2) today.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7311</id>
      <title>TermWeight shoud seek terms lazily</title>
      <description>Currently the terms are seeked eagerly in TermQuery.createWeight when creating the TermContext. This might be wasteful when scores are not needed since the query might be cached on some segments, thus seeking the term on these segments is not needed. We could change TermWeight to only seek terms in Weight.scorer when scores are not needed.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7313</id>
      <title>Remove DocValuesDocIdSet</title>
      <description>This class is unused since we migrated the doc values filters to queries with two-phase iteration.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7314</id>
      <title>Graduate InetAddressPoint and LatLonPoint to core</title>
      <description>Maybe we should graduate these fields (and related queries) to core for Lucene 6.1?</description>
      <attachments/>
      <comments>16</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7318</id>
      <title>Graduate StandardAnalyzer out of analyzers module into core</title>
      <description>Spinoff from LUCENE-7314: StandardAnalyzer has progressed substantially since we broke out the analyzers module ... it now follows a real Unicode standard (UAX #29 Unicode Text Segmentation). It's also much faster than it used to be, since it switched to JFlex a while back. Many bug fixes, etc. I think it would make a good default for most Lucene users, and we should graduate it from the analyzers module into core, and make it the default for IndexWriter. It's really quite crazy that users must go digging in the analyzers module to get started with Lucene ... we don't make them dig through the codecs module to find a good default codec ...</description>
      <attachments/>
      <comments>41</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>7321</id>
      <title>Character Mapping</title>
      <description>One of the challenges in search is recall of an item with a common typing variant. These cases can be as simple as lower/upper case in most languages, accented characters, or more complex morphological phenomena like prefix omitting, or constructing a character with some combining mark. This component addresses the cases, which are not covered by ASCII folding component, or more complex to design with other tools. The idea is that a linguist could provide the mappings in a tab-delimited file, which then can be directly used by Solr. The mappings are maintained in the tab-delimited file, which could be just a copy paste from Excel spreadsheet. This gives the linguists the opportunity to create the mappings, then for the developer to include them in Solr configuration. There are a few cases, when the mappings grow complex, where some additional debugging may be required. The mappings can contain any sequence of characters to any other sequence of characters. Some of the cases I discuss in detail document are handling the voiced vowels for Japanese; common typing substitutions for Korean, Russian, Polish; transliteration for Polish, Arabic; prefix removal for Arabic; suffix folding for Japanese. In the appendix, I give an example of implementing a Russian light weight stemmer using this component.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7323</id>
      <title>Compound file writing should verify checksum of its sub-files</title>
      <description>For larger segments, there is a non-trivial window, from when IW writes sub-files, to when it then builds the CFS, during which the files can become corrupted (from external process, bad filesystem, hardware, etc.) Today we quietly build the CFS even if the sub-files are corrupted, but we can easily detect it, letting users catch corruption earlier (write time instead of read time).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7324</id>
      <title>ExitableDirectoryReader should not check on every term</title>
      <description>I looked at ExitableDirectoryReader and to me checking the timeout on every term is a pretty heavy operation. I wonder if we can relax that a bit and begin with checking when we pull Terms and maybe only every Nth term by default? I wonder if we even can make it a function of the number of therms ie log(numTerms) I think it's pretty trappy to have something that won't perform well or has the risk to not scale well in lucene core?</description>
      <attachments/>
      <comments>5</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7328</id>
      <title>Remove LegacyNumericEncoding from GeoPointField</title>
      <description>6.0 deprecated legacy NumericEncoding in GeoPointField this issue completely removes the encoding for 7.0.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7330</id>
      <title>Speed up conjunctions</title>
      <description>I am digging into some performance regressions between 4.x and 5.x which seem to be due to how we always run conjunctions with ConjunctionDISI now while 4.x had FilteredQuery, which was optimized for the case that there are only two clauses or that one of the clause supports random access. I'd like to explore the former in this issue.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7331</id>
      <title>GeoPointTestQuery should use GeoTestUtil instead of GeoPointTestUtil</title>
      <description>Initially discussed in LUCENE-7166, and revisited in LUCENE-7325, TestGeoPointQuery should use the RNG provided by GeoTestUtil instead of the GeoPointTestUtil hack.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7336</id>
      <title>Move TermRangeQuery to sandbox</title>
      <description>I think, long ago, this class was abused for numeric range searching, if you converted your numeric terms into text terms "carefully", but we now have dimensional points for that, and I think otherwise this query class is quite dangerous: you can easily accidentally make a very costly query. Furthermore, the common use cases for multi-term queries are already covered by other classes (PrefixQuery, WildcardQuery, FuzzyQuery).</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7339</id>
      <title>Bring back RandomAccessFilterStrategy</title>
      <description>FiteredQuery had 3 ways of running conjunctions: leap-frog, query first and random-access filter. We still use leap-frog for conjunctions and we now have a better "query-first" strategy through two-phase iteration. However, we don't have any equivalent for the random-access filter strategy.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7341</id>
      <title>EarlyTerminatingSortingCollector support for grouped searches</title>
      <description>Currently grouped searches must not use the early terminating sorting collector because the wrong results would be returned. This ticket proposes to change the EarlyTerminatingSortingCollector class and probably the LeafCollector interface to support early termination for grouped searches. Illustration (aaa is an as yet unnamed boolean flag): # fictitious (sorted by X) segment | doc key | D0 D1 D2 D3 D4 D5 ... D10 D11 D12 D13 D14 D15 ... D20 D21 D22 D23 D24 D25 ... | doc grp | G0 G0 G0 G0 G0 G0 ... D10 G10 G10 G10 G10 G10 ... G20 G20 G20 G20 G20 G20 ... # query with rows=3 sort=X group=false | query result | D0 D1 D2 # existing code: # use a EarlyTerminatingSortingCollector with numDocsToCollect=3 # EarlyTerminatingSortingCollector.getLeafCollector returns a LeafCollector # whose collect method uses (++numCollected &gt;= numDocsToCollect) as the terminating condition # query with rows=3 sort=X group=true group.field=grp group.sort=X group.limit=1 | query result | G0(D0) G10(D10) G20(D20) # existing code: # cannot use EarlyTerminatingSortingCollector (query result would wrongly be just 'G0(D0)') # proposed new code: # use a EarlyTerminatingSortingCollector(... numDocsToCollect=3 aaa=true) # query with rows=3 sort=X group=true group.field=grp group.sort=X group.limit=5 | query result | G0(D0,D1,D2,D3,D4) G10(D10,D11,D12,D13,D14) G20(D20,D21,D22,D23,D24) # existing code: # cannot use EarlyTerminatingSortingCollector (query result would wrongly be just 'G0(D0,D1,D2)') # proposed new code: # use a EarlyTerminatingSortingCollector(... numDocsToCollect=3 aaa=true)</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7342</id>
      <title>WordDelimiterFilter should observe KeywordAttribute to pass these tokens through</title>
      <description>I have a text analysis requirement in which I want certain tokens to not be processed by WordDelimiterFilter – i.e. they should pass through that filter. WDF, like several other TokenFilters, has a configurable word list but this list is static producing a concrete CharArraySet. Thus, for example, I can't filter by a regexp nor can I filter based on other attributes. A simple solution that makes sense to me is to have WDF use KeywordAttribute to know if it should skip the token. KeywordAttribute seems fairly generic as to how it can be used, although granted today it's only used by the stemmers. That attribute isn't named "StemmerIgnoreAttribute" or some-such; it's generic so I think it's fine for WDF to use it in a similar way.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7343</id>
      <title>Cleanup GeoPoint Query implementation</title>
      <description>This is a cleanup task to simplify and trim dead code from GeoPointField's query classes. Much of the relation logic in LatLonPoint can also be applied to GeoPointField's CellComparator class eliminating the need to carry its own separate relation methods.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7347</id>
      <title>Remove queryNorm and coords</title>
      <description>These two features are specific to TF-IDF and introduce some complexity (see eg. handling of coords in BooleanWeight) and bugs/corner-cases (see eg. how taking the query norm into account causes scoring challenges on LUCENE-7337). Since we made BM25 the default in 6.0, I propose that we remove these TF-IDF-specific features in 7.0.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7348</id>
      <title>Add dynamic stemmer for Ukrainian</title>
      <description>We're adding a dictionary based lemmatizing analyzer for Ukrainian in https://issues.apache.org/jira/browse/LUCENE-7287. It would be nice to have a dynamic stemmer that can handle words that are not in the dictionary.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7350</id>
      <title>Let classifiers be constructed from IndexReaders</title>
      <description>Current Classifier implementations are built from LeafReaders, this is an heritage of using certain Lucene 4.x AtomicReader's specific APIs; this is no longer required as what is used by current implementations is based on IndexReader APIs and therefore it makes more sense to use that as constructor parameter as it doesn't give any additional benefit whereas it requires client code to deal with classifiers that are tight to segments (which doesn't make much sense).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7351</id>
      <title>BKDWriter should compress doc ids when all values in a block are the same</title>
      <description>BKDWriter writes doc ids using 4 bytes per document. I think it should compress similarly to postings when all docs in a block have the same packed value. This can happen either when a field has a default value which is common across documents or when quantization makes the number of unique values so small that a large index will necessarily have blocks that all contain the same value (eg. there are only 63490 unique half-float values).</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7353</id>
      <title>ScandinavianFoldingFilterFactory and ScandinavianNormalizationFilterFactory should implement MultiTermAwareComponent</title>
      <description>These token filters are safe to apply for multi-term queries.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7355</id>
      <title>Leverage MultiTermAwareComponent in query parsers</title>
      <description>MultiTermAwareComponent is designed to make it possible to do the right thing in query parsers when in comes to analysis of multi-term queries. However, since query parsers just take an analyzer and since analyzers do not propagate the information about what to do for multi-term analysis, query parsers cannot do the right thing out of the box.</description>
      <attachments/>
      <comments>33</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7359</id>
      <title>Add equals() and hashcode() to Explanation</title>
      <description>I don't think there's any reason not to add these?</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7360</id>
      <title>Remove Explanation.toHtml()</title>
      <description>This seems to be something of a relic. It's still used in Solr, but I think it makes more sense to move it directly into the ExplainAugmenter there rather than having it in Lucene itself.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7361</id>
      <title>Terms.toStringDebug</title>
      <description>While fixing LUCENE-7340, MemoryIndex.toString(), I thought MemoryIndex shouldn't need it's own debug toString() impl for its Terms when there could be a generic one. So here I propose that we create a Terms.toStringDebug(Appendable result, int charLimit, String indent) or some-such but probably not override toString() for obvious reasons. Maybe also have this on Fields() that simply loops and calls out to the one on Terms. The format is debatable.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7362</id>
      <title>Implement FieldInfos and FieldInfo toString()</title>
      <description>FieldInfos and FieldInfo ought to override toString(). Perhaps FieldInfo.toString() can look like the pattern popularized by Luke, also seen in Solr?</description>
      <attachments/>
      <comments>2</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7364</id>
      <title>Don't use BooleanScorer for small segments</title>
      <description>If a BooleanQuery meets certain criteria (only contains disjunctions, is likely to match large numbers of docs) then we use a BooleanScorer to score groups of 1024 docs at a time. This allocates arrays of 1024 Bucket objects up-front. On very small segments (for example, a MemoryIndex) this is very wasteful of memory, particularly if the query is large or deeply-nested. We should avoid using a bulk scorer on these segments.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7365</id>
      <title>Don't use BooleanScorer for small segments</title>
      <description>If a BooleanQuery meets certain criteria (only contains disjunctions, is likely to match large numbers of docs) then we use a BooleanScorer to score groups of 1024 docs at a time. This allocates arrays of 1024 Bucket objects up-front. On very small segments (for example, a MemoryIndex) this is very wasteful of memory, particularly if the query is large or deeply-nested. We should avoid using a bulk scorer on these segments.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7366</id>
      <title>Allow RAMDirectory to copy any Directory</title>
      <description>Uwe: "The FSDirectory passed to RAMDirectory in the ctor could be changed to Directory easily. The additional check for "not is a directory inode" is in my opinion lo longer needed, because listFiles should only return files." Use case: For increasing the speed of some of my application tests, I want to re-use/copy a pre-populated RAMDirectory over and over.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7370</id>
      <title>Refactoring: Rename Levenstein to Levenshtein</title>
      <description>In suggest module, Levenshtein edit distance is named as Levenstein. To protect conventions it would be good to rename it as suggested in this issue.</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7373</id>
      <title>Break out Directory.syncMetaData from FSDirectory.renameFile</title>
      <description>Today, when you call FSDirectory.renameFile it also calls fsync on the directory. This is OK for Lucene's current usage of this method, to rename just the one segments_N file on commit. But in playing with adding NRT replication (LUCENE-5438) to the simple demo Lucene server (LUCENE-5376) I found that, on spinning disks, that fsync is very costly, because when copying over an NRT point, we write to N .tmp files and then rename many files (taking seconds) in the end. I think we should just deprecate/remove the existing method, and make a new rename method that does only renaming, and a separate syncMetaData to call fsync on the directory?</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7375</id>
      <title>Can we allow FSDirectory subclasses to customize whether the ctor does a mkdir?</title>
      <description>Today, just instantiating an FSDirectory always brings the directory into existence, even if you will do no writes (createOutput). We have gone back and forth on this, over the ages. E.g. see LUCENE-16 (only 2 digits there!!), LUCENE-773 (only 3 digits there!!), LUCENE-1464. At one point we created the directory lazily, on the first write (createOutput) attempt, but now we always create it when you instantiate FSDirectory. This causes some hassle for consumers, e.g. in https://github.com/elastic/elasticsearch/pull/19338 ES is forking SimpleFSDirectory in order to have a read-only directory impl. Maybe we can do the Files.createDirectories in protected method that a subclass could override?</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7380</id>
      <title>Add Polygon.fromGeoJSON</title>
      <description>Working with Polygon is a bit tricky today because you typically must use an external dependency to convert e.g. a GeoJSON string into Lucene's Polygon class ... I think this is a weakness in our API, and it clearly confuses users: http://markmail.org/thread/mpge4wqo7cfqm4i5 So I created a simplistic GeoJSON parser to extract a single Polygon or MultiPolygon from a GeoJSON string, without any dependencies. The parser only handles the various ways that a single Polygon or MultiPolygon can appear in a GeoJSON string, and throws an exception otherwise.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7381</id>
      <title>Add new RangeField</title>
      <description>I've been tinkering with a new Point-based RangeField for indexing numeric ranges that could be useful for a number of applications. For example, a single dimension represents a span along a single axis such as indexing calendar entries start and end time, 2d range could represent bounding boxes for geometric applications (e.g., supporting Point based geo shapes), 3d ranges bounding cubes for 3d geometric applications (collision detection, 3d geospatial), and 4d ranges for space time applications. I'm sure there's applicability for 5d+ ranges but a first incarnation should likely limit for performance.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7384</id>
      <title>Remove ScoringWrapperSpans</title>
      <description>In LUCENE-6919 (Lucene 5.5), ScoringWrapperSpans was modified in such a way that made the existence of this class pointless, and possibly broke anyone who was using it as it's SimScorer argument isn't used anymore. We should now delete it. SpanWeight has getSimScorer() so people can customize the SimScorer that way. Another small change I observe to improve is have SpanWeight.buildSimWeight's last line use the existing Similarity that has already been populated on the field?</description>
      <attachments/>
      <comments>14</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7385</id>
      <title>SpanScorer's assert message strings should reference spans.toString()</title>
      <description>SpanScorer.setFreqCurrentDoc has a bunch of assert statements, and they refer to this.toString(). I'm pretty confident the intention was for this to actually be spans.toString(), not "this" which is a SpanScorer that doesn't even have a custom toString. It was probably correct once but after some refactoring of Spans got messed up, probably in LUCENE-6919 (Lucene 5.5).</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7386</id>
      <title>Flatten nested disjunctions</title>
      <description>Now that coords are gone it became easier to flatten nested disjunctions. It might sound weird to write nested disjunctions in the first place, but disjunctions can be created implicitly by other queries such as more-like-this, LatLonPoint.newBoxQuery, non-scoring synonym queries, etc.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7388</id>
      <title>Add IntRangeField, FloatRangeField, LongRangeField</title>
      <description>This is the follow on to LUCENE-7381 for adding support for indexing and querying on int, float, and long ranges.</description>
      <attachments/>
      <comments>5</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7390</id>
      <title>Let BKDWriter use temp heap for sorting points in proportion to IndexWriter's indexing buffer</title>
      <description>With Lucene's default codec, when writing dimensional points, we only give BKDWriter 16 MB heap to use for sorting, regardless of how large IW's indexing buffer is. A custom codec can change this but that's a little steep. I've been testing indexing performance on a points-heavy dataset, 1.2 billion taxi rides from http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml , indexing with a 1 GB IW buffer, and the small 16 MB heap limit causes clear performance problems because flushing the large segments forces BKDwriter to switch to offline sorting which causes the DWPTs take too long to flush. They then fall behind, and Lucene does a hard stall on incoming indexing threads until they catch up. Robert Muir had a simple idea to let IW pass the allowed temp heap usage to PointsWriter.writeField.</description>
      <attachments/>
      <comments>14</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7392</id>
      <title>Add point based GeoBoundingBoxField as a new RangeField type</title>
      <description>This issue will add a new point based GeoBoundingBoxField type for indexing and querying 2D or 3D Geo bounding boxes. The intent is to construct this as a RangeField type and limit the first two dimensions to the lat/lon geospatial bounds (at 4 bytes each like LatLonPoint, while allowing an optional 8 byte (double) third dimension to serve as an altitude component for indexing 3D geospatial bounding boxes.</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7394</id>
      <title>Make MemoryIndex immutable</title>
      <description>The MemoryIndex itself should just be a builder that constructs an IndexReader instance. The whole notion of freezing a memory index should be removed. While we change this we should also clean this class up. There are many methods to add a field, we should just have a single method that accepts a `IndexableField`. The `keywordTokenStream(...)` method is unused and untested and should be removed and it doesn't belong with the memory index. The `setSimilarity(...)`, `createSearcher(...)` and `search(...)` methods should be removed, because the MemoryIndex should just be responsible for creating an IndexReader instance.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7396</id>
      <title>Speed up flush of points</title>
      <description>1D points already have an optimized merge implementation which works when points come in order. So maybe we could make IndexWriter's PointValuesWriter sort before feeding the PointsFormat and somehow propagate the information to the PointsFormat? The benefit is that flushing could directly stream points to disk with little memory usage.</description>
      <attachments/>
      <comments>21</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7397</id>
      <title>Inefficient FVhighlighting when set many HighlightedField.</title>
      <description>when highlighting, search result org.apache.lucene.search.vectorhighlight.FastVectorHighlighter.java getBestFragment method ~ FieldTermStack.java read whole doc's termvector every highlighted field. It causes slow query when many highlight field</description>
      <attachments/>
      <comments>4</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7399</id>
      <title>Speed up flush of points v2</title>
      <description>There are improvements we can make on top of LUCENE-7396 to get ever better flush performance.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7400</id>
      <title>DatasetSplitter should work with SortedSetDV on class field too</title>
      <description>DatasetSplitter should be able to perform prior grouping for balanced split either on sorted DV or sorted set DV.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7404</id>
      <title>prepared query for introducing evaluation expression in query (not fixed second comparation member)</title>
      <description>Lucene is very powerfull but has a limitation very important. It is impossible to use when you want create a query where is a expression inside. field1: &gt;= field2 where field1 and field2 is not possible. field1:= field2^2 -4 *field3 it is pratically impossible to index statically document for this query but it is possible to do 2 things: 1) create a third field hidden here is saved the result. 2) when you use a prepared query , it is saved automatically in a different writer this info that you can save. Creating a prepared query lucene do all automatically for making this query using indexes. PS. Pay attention that fields in expression could be not only inside the current document but also in different documents/readers. It could be used also for join different document types(it is a subcase of the above problem).</description>
      <attachments/>
      <comments>1</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7407</id>
      <title>Explore switching doc values to an iterator API</title>
      <description>I think it could be compelling if we restricted doc values to use an iterator API at read time, instead of the more general random access API we have today: It would make doc values disk usage more of a "you pay for what what you actually use", like postings, which is a compelling reduction for sparse usage. I think codecs could compress better and maybe speed up decoding of doc values, even in the non-sparse case, since the read-time API is more restrictive "forward only" instead of random access. We could remove getDocsWithField entirely, since that's implicit in the iteration, and the awkward "return 0 if the document didn't have this field" would go away. We can remove the annoying thread locals we must make today in CodecReader, and close the trappy "I accidentally shared a single XXXDocValues instance across threads", since an iterator is inherently "use once". We could maybe leverage the numerous optimizations we've done for postings over time, since the two problems ("iterate over doc ids and store something interesting for each") are very similar. This idea has come up many in the past, e.g. LUCENE-7253 is a recent example, and very early iterations of doc values started with exactly this However, it's a truly enormous change, likely 7.0 only. Or maybe we could have the new iterator APIs also ported to 6.x side by side with the deprecate existing random-access APIs.</description>
      <attachments/>
      <comments>41</comments>
      <commenters>9</commenters>
    </issue>
    <issue>
      <id>7409</id>
      <title>Look into making mmapdirectory's unmap safer</title>
      <description>I have seen a few bugs around this recently: of course its a bug in application code but a JVM crash is not good. I think we should see if we can prevent the crashes better than the current weak map, e.g. make it a safer option. I made an ugly prototype here: https://github.com/apache/lucene-solr/compare/master...rmuir:ace?expand=1 It has a test that crashes the JVM without the patch but passes with. Hacky patch only implements readBytes() but has no problems with the luceneutil benchmark (1M): Report after iter 19: Task QPS base StdDev QPS patch StdDev Pct diff IntNRQ 105.23 (17.6%) 100.42 (10.1%) -4.6% ( -27% - 28%) Respell 128.35 (13.2%) 125.88 (7.4%) -1.9% ( -19% - 21%) Fuzzy1 110.14 (17.2%) 108.28 (13.2%) -1.7% ( -27% - 34%) LowPhrase 337.02 (13.0%) 333.72 (9.3%) -1.0% ( -20% - 24%) MedPhrase 146.44 (12.9%) 145.55 (8.0%) -0.6% ( -19% - 23%) MedSpanNear 96.85 (13.1%) 96.57 (7.8%) -0.3% ( -18% - 23%) HighSpanNear 95.85 (13.9%) 96.33 (8.2%) 0.5% ( -18% - 26%) HighPhrase 146.84 (13.6%) 148.40 (8.4%) 1.1% ( -18% - 26%) HighTerm 295.15 (15.8%) 298.77 (9.5%) 1.2% ( -20% - 31%) LowSpanNear 268.80 (12.4%) 272.16 (7.9%) 1.2% ( -16% - 24%) Wildcard 284.09 (11.7%) 290.91 (8.9%) 2.4% ( -16% - 25%) Prefix3 212.50 (15.4%) 217.76 (10.0%) 2.5% ( -19% - 32%) OrHighLow 358.65 (15.0%) 368.93 (10.7%) 2.9% ( -19% - 33%) AndHighMed 799.65 (13.2%) 834.74 (7.8%) 4.4% ( -14% - 29%) MedSloppyPhrase 229.36 (15.9%) 239.95 (9.8%) 4.6% ( -18% - 36%) Fuzzy2 69.58 (14.6%) 72.82 (14.5%) 4.7% ( -21% - 39%) AndHighHigh 426.98 (12.8%) 451.77 (7.3%) 5.8% ( -12% - 29%) MedTerm 1361.11 (14.5%) 1450.90 (9.2%) 6.6% ( -14% - 35%) PKLookup 266.61 (13.4%) 284.28 (8.4%) 6.6% ( -13% - 32%) HighSloppyPhrase 251.22 (16.9%) 268.32 (10.7%) 6.8% ( -17% - 41%) OrHighMed 235.92 (17.2%) 253.12 (12.8%) 7.3% ( -19% - 45%) OrHighHigh 186.79 (13.5%) 201.15 (9.7%) 7.7% ( -13% - 35%) LowSloppyPhrase 395.23 (15.9%) 425.93 (9.3%) 7.8% ( -15% - 39%) AndHighLow 1128.28 (14.9%) 1242.11 (8.2%) 10.1% ( -11% - 38%) LowTerm 3024.62 (12.9%) 3367.65 (9.7%) 11.3% ( -9% - 39%) We should do more testing. Maybe its totally the wrong tradeoff, maybe we only need handles for getters and everything inlines correctly, rather than needing a ton for every getXYZ() method...</description>
      <attachments/>
      <comments>34</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7411</id>
      <title>Regex Query with Backreferences</title>
      <description>Hi there, I am currently working on a Regex Engine that supports Backreferences while not losing determinism. It uses Memory Occurence Automata (MOAs) in the engine which are more powerful than normal DFA/NFAs. The engine does no backtracking and recognizes Regexes that cannot be evaluated deterministically as malformed. It has become more and more mature in the last few weeks and I also implemented a Lucene Query that uses these Patterns in the background. Now my question is: Is there any interest for this work to be merged (or adapted) into Lucene core? EDIT: The current state is only a mere proof of concept. The performance can probably be improved by a lot by adapting concepts of the Lucene Regexp Query. As Uwe Schindler correctly stated, the Query currently is quite "dumb" as in it doesn't predict what terms to match next. https://github.com/s4ke/moar Usage example for the Lucene Query: https://github.com/s4ke/moar/blob/master/lucene/src/test/java/com/github/s4ke/moar/lucene/query/test/MoarQueryTest.java#L126 Cheers, Martin</description>
      <attachments/>
      <comments>10</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7416</id>
      <title>BooleanQuery rewrite optimizations</title>
      <description>A couple of BooleanQuery rewrites / optimizations. First, as discussed on the user group, a BooleanQuery with a query that is both a SHOULD and a FILTER can be rewritten as a single MUST query, but care must be taken to decrement minShouldMatch by 1. Another case is if a query is both required (MUST or FILTER) and MUST_NOT at the same time, it can be converted to a MatchNoDocsQuery (although I haven't discussed this yet so hopefully I'm not missing something!).</description>
      <attachments/>
      <comments>27</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7421</id>
      <title>Speed up BS2 by caching the 2nd lowest doc id in the priority queue</title>
      <description>BS2 uses a priority queue in order to merge several sorted iterators into a new sorted iterator. We call updateTop every time that we move the 'top' iterator forward, which requires to check the size of the priority queue at least twice and perform at least two comparisons of doc ids. Instead, DisjunctionSumScorer could cache the 2nd lowest doc id of the priority queue and only call updateTop when the doc id of the entry at the top of the priority queue goes beyong the 2nd lowest doc id. While this would involve slightly more work in the case that the PQ has two high-cardinality clauses whose docs are interleaved, this would help when one clause has a much higher cardinality than the other ones or when the doc ids of the various clauses are clustered.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7423</id>
      <title>AutoPrefixPostingsFormat: a PostingsFormat optimized for prefix queries on text fields.</title>
      <description>The autoprefix terms dict added in https://issues.apache.org/jira/browse/LUCENE-5879 has been removed with https://issues.apache.org/jira/browse/LUCENE-7317. The new points API is now used to do efficient range queries but the replacement for prefix string queries is unclear. The edge ngrams could be used instead but they have a lot of drawbacks and are hard to configure correctly. The completion postings format is also a good replacement but it requires to have a big FST in RAM and it cannot be intersected with other fields. This patch is a proposal for a new PostingsFormat optimized for prefix query on string fields. It detects prefixes that match "enough" terms and writes auto-prefix terms into their own virtual field. At search time the virtual field is used to speed up prefix queries that match "enough" terms. The auto-prefix terms are built in two pass: The first pass builds a compact prefix tree. Since the terms enum is sorted the prefixes are flushed on the fly depending on the input. For each prefix we build its corresponding inverted lists using a DocIdSetBuilder. The first pass visits each term of the field TermsEnum only once. When a prefix is flushed from the prefix tree its inverted lists is dumped into a temporary file for further use. This is necessary since the prefixes are not sorted when they are removed from the tree. The selected auto prefixes are sorted at the end of the first pass. The second pass is a sorted scan of the prefixes and the temporary file is used to read the corresponding inverted lists. The patch is just a POC and there are rooms for optimizations but the first results are promising: I tested the patch with the geonames dataset. I indexed all the titles with the KeywordAnalyzer and compared the index/merge time and the size of the indices. The edge ngram index (with a min edge ngram size of 2 and a max of 20) takes 572M on disk and it took 130s to index and optimize the 11M titles. The auto prefix index takes 287M on disk and took 70s to index and optimize the same 11M titles. Among the 287M, only 170M are used for the auto prefix fields and the rest is for the regular keyword field. All the auto prefixes were generated for this test (at least 2 terms per auto-prefix). The queries have similar performance since we are sure on both sides that one inverted list can answer any prefix query.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7431</id>
      <title>Allow negative pre/post values in SpanNotQuery</title>
      <description>I need to be able to specify a certain range of allowed overlap between the include and exclude parameters of SpanNotQuery. Since this behaviour is the inverse of the behaviour implemented by the pre and post constructor arguments, I suggest that this be implemented with negative pre and post values. Patch incoming.</description>
      <attachments/>
      <comments>6</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7434</id>
      <title>Add minNumberShouldMatch parameter to SpanNearQuery</title>
      <description>On the user list, Saar Carmi asked about a new type of SpanQuery that would allow for something like BooleanQuery's minimumNumberShouldMatch Given a set of search terms (t1, t2, t3, ti), return all documents where in a sequence of x=10 tokens at least c=3 of the search terms appear within the sequence. I think we can modify SpanNearQuery fairly easily to accommodate this. I'll submit a PR in the next few days.</description>
      <attachments/>
      <comments>24</comments>
      <commenters>7</commenters>
    </issue>
    <issue>
      <id>7435</id>
      <title>MiniSolrCloudCluster should not reuse ports by default</title>
      <description>Building on SOLR-9469, this just changes MSCC.startJettySolrRunner(JettySolrRunner) to use a different port when it restarts the passed-in Jetty. This should stop the semi-frequent test failures in CollectionStateWatchersTest.</description>
      <attachments/>
      <comments>8</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7438</id>
      <title>UnifiedHighlighter</title>
      <description>The UnifiedHighlighter is an evolution of the PostingsHighlighter that is able to highlight using offsets in either postings, term vectors, or from analysis (a TokenStream). Lucene’s existing highlighters are mostly demarcated along offset source lines, whereas here it is unified – hence this proposed name. In this highlighter, the offset source strategy is separated from the core highlighting functionalty. The UnifiedHighlighter further improves on the PostingsHighlighter’s design by supporting accurate phrase highlighting using an approach similar to the standard highlighter’s WeightedSpanTermExtractor. The next major improvement is a hybrid offset source strategythat utilizes postings and “light” term vectors (i.e. just the terms) for highlighting multi-term queries (wildcards) without resorting to analysis. Phrase highlighting and wildcard highlighting can both be disabled if you’d rather highlight a little faster albeit not as accurately reflecting the query. We’ve benchmarked an earlier version of this highlighter comparing it to the other highlighters and the results were exciting! It’s tempting to share those results but it’s definitely due for another benchmark, so we’ll work on that. Performance was the main motivator for creating the UnifiedHighlighter, as the standard Highlighter (the only one meeting Bloomberg Law’s accuracy requirements) wasn’t fast enough, even with term vectors along with several improvements we contributed back, and even after we forked it to highlight in multiple threads.</description>
      <attachments/>
      <comments>28</comments>
      <commenters>8</commenters>
    </issue>
    <issue>
      <id>7439</id>
      <title>Should FuzzyQuery match short terms too?</title>
      <description>Today, if you ask FuzzyQuery to match abcd with edit distance 2, it will fail to match the term ab even though it's 2 edits away. Its javadocs explain this: * &lt;p&gt;NOTE: terms of length 1 or 2 will sometimes not match because of how the scaled * distance between two terms is computed. For a term to match, the edit distance between * the terms must be less than the minimum length term (either the input term, or * the candidate term). For example, FuzzyQuery on term "abcd" with maxEdits=2 will * not match an indexed term "ab", and FuzzyQuery on term "a" with maxEdits=2 will not * match an indexed term "abc". On the one hand, I can see that this behavior is sort of justified in that 50% of the characters are different and so this is a very "weak" match, but on the other hand, it's quite unexpected since edit distance is such an exact measure so the terms should have matched. It seems like the behavior is caused by internal implementation details about how the relative (floating point) score is computed. I think we should fix it, so that edit distance 2 does in fact match all terms with edit distance &lt;= 2.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7449</id>
      <title>Add CROSSES query support to RangeField</title>
      <description>RangeField currently supports INTERSECTS, WITHIN, and CONTAINS query behavior. This feature adds support for an explicit CROSSES query. Unlike INTERSECT and OVERLAP queries the CROSSES query finds any indexed ranges whose interior (within range) intersect the interior AND exterior (outside range) of the query range.</description>
      <attachments/>
      <comments>20</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7452</id>
      <title>improve exception message: child query must only match non-parent docs, but parent docID=180314...</title>
      <description>when parent filter intersects with child query the exception exposes internal details: docnum and scorer class. I propose an exception message to suggest to execute a query intersecting them both. There is an opinion to add this suggestion in addition to existing details. My main concern against is, when index is constantly updated even SOLR-9582 allows to search for docnum it would be like catching the wind, also think about cloud case. But, user advised with executing query intersection can catch problem documents even if they occurs sporadically.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7453</id>
      <title>Change naming of variables/apis from docid to docnum</title>
      <description>In SOLR-9528 a suggestion was made to change docid to docnum. The reasoning for this is most notably that docid has a connotation about a persistent unique identifier (eg like _id in elasticsearch or id in solr), while docid in lucene is currently some local to a segment, and not comparable directly across segments. When I first started working on Lucene, I had this same confusion. docnum is a much better name for this transient, segment local identifier for a doc. Regardless of what solr wants to do in their api (eg keeping docid), I think we should switch the lucene apis and variable names to use docnum.</description>
      <attachments/>
      <comments>30</comments>
      <commenters>11</commenters>
    </issue>
    <issue>
      <id>7457</id>
      <title>Default doc values format should optimize for iterator access</title>
      <description>In LUCENE-7407 we switched doc values consumption from random access API to an iterator API, but nothing was done there to improve the codec. We should do that here. At a bare minimum we should fix the existing very-sparse case to be a true iterator, and not wrapped with the silly legacy wrappers. I think we should also increase the threshold (currently 1%?) when we switch from dense to sparse encoding. This should fix LUCENE-7253, making merging of sparse doc values efficient ("pay for what you use"). I'm sure there are many other things to explore to let codecs "take advantage" of the fact that they no longer need to offer random access to doc values.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7459</id>
      <title>LegacyNumericDocValuesWrapper should only check bits when the value is != 0</title>
      <description>This is what we did with the random-access API in order to save lookups in the bit set that stores documents that have a value for the field. See for instance FieldComparator.LongComparator.compareBottom in 6.x.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7461</id>
      <title>Refactor doc values queries to better use the new doc values APIs</title>
      <description>The new doc values APIs make it easy to implement a TwoPhaseIterator, and things are going to be faster in the sparse case since we can use the doc values object as an approximation.</description>
      <attachments/>
      <comments>4</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7462</id>
      <title>Faster search APIs for doc values</title>
      <description>While the iterator API helps deal with sparse doc values more efficiently, it also makes search-time operations more costly. For instance, the old random-access API allowed to compute facets on a given segment without any conditionals, by just incrementing the counter at index ordinal+1 while the new API requires to advance the iterator if necessary and then check whether it is exactly on the right document or not. Since it is very common for fields to exist across most documents, I suspect codecs will keep an internal structure that is similar to the current codec in the dense case, by having a dense representation of the data and just making the iterator skip over the minority of documents that do not have a value. I suggest that we add APIs that make things cheaper at search time. For instance in the case of SORTED doc values, it could look like LegacySortedDocValues with the additional restriction that documents can only be consumed in order. Codecs that can implement this API efficiently would hide it behind a SortedDocValues adapter, and then at search time facets and comparators (which liked the LegacySortedDocValues API better) would either unwrap or hide the SortedDocValues they got behind a more random-access API (which would only happen in the truly sparse case if the codec optimizes the dense case). One challenge is that we already use the same idea for hiding single-valued impls behind multi-valued impls, so we would need to enforce the order in which the wrapping needs to happen. At first sight, it seems that it would be best to do the single-value-behind-multi-value-API wrapping above the random-access-behind-iterator-API wrapping. The complexity of wrapping/unwrapping in the right order could be contained in the DocValues helper class. I think this change would also simplify search-time consumption of doc values, which currently needs to spend several lines of code positioning the iterator everytime it needs to do something interesting with doc values.</description>
      <attachments/>
      <comments>12</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7463</id>
      <title>Create a Lucene70DocValuesFormat</title>
      <description>Even if it is not optimal, I think it would help to create a Lucene70DocValuesFormat now by copying the current Lucene54DocValuesFormat and including some minor changes like making the sparse case use a true iterator API as described in LUCENE-7457 (which should make it to Lucene54DocValuesFormat a well so that merging from an old codec would be efficient) as well as raising the threshold to enable sparse encoding and using nextSetBit operations when iterating bit sets, which cannot be done easily in Lucene54DocValuesFormat because we'd need to add a couple trailing bytes to make sure we can read a long at any valid index.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7465</id>
      <title>Add a PatternTokenizer that uses Lucene's RegExp implementation</title>
      <description>I think there are some nice benefits to a version of PatternTokenizer that uses Lucene's RegExp impl instead of the JDK's: Lucene's RegExp is compiled to a DFA up front, so if a "too hard" RegExp is attempted the user discovers it up front instead of later on when a "lucky" document arrives It processes the incoming characters as a stream, only pulling 128 characters at a time, vs the existing PatternTokenizer which currently reads the entire string up front (this has caused heap problems in the past) It should be fast. I named it SimplePatternTokenizer, and it still needs a factory and improved tests, but I think it's otherwise close. It currently does not take a group parameter because Lucene's RegExps don't yet implement sub group capture. I think we could add that at some point, but it's a bit tricky. This doesn't even have group=-1 support (like String.split) ... I think if we did that we should maybe name it differently (SimplePatternSplitTokenizer?).</description>
      <attachments/>
      <comments>31</comments>
      <commenters>6</commenters>
    </issue>
    <issue>
      <id>7466</id>
      <title>add axiomatic similarity</title>
      <description>Add axiomatic similarity approaches to the similarity family. More details can be found at http://dl.acm.org/citation.cfm?id=1076116 and https://www.eecis.udel.edu/~hfang/pubs/sigir05-axiom.pdf There are in total six similarity models. All of them are based on BM25, Pivoted Document Length Normalization or Language Model with Dirichlet prior. We think it is worthy to add the models as part of Lucene.</description>
      <attachments/>
      <comments>19</comments>
      <commenters>5</commenters>
    </issue>
    <issue>
      <id>7471</id>
      <title>Simplify NearSpansOrdered</title>
      <description>Extend the span positions priority queue, remove SpansCell.</description>
      <attachments/>
      <comments>2</comments>
      <commenters>1</commenters>
    </issue>
    <issue>
      <id>7474</id>
      <title>Improve doc values writers</title>
      <description>One of the goals of the new iterator-based API is to better handle sparse data. However, the current doc values writers still use a dense representation, and some of them perform naive linear scans in the nextDoc implementation.</description>
      <attachments/>
      <comments>7</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7475</id>
      <title>Sparse norms</title>
      <description>Even though norms now have an iterator API, they are still always dense in practice since documents that do not have a value get assigned 0 as a norm value.</description>
      <attachments/>
      <comments>9</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7477</id>
      <title>ExternalRefSorter should use OfflineSorter's actual writer for writing the input file</title>
      <description>Consider this constructor in ExternalRefSorter: public ExternalRefSorter(OfflineSorter sorter) throws IOException { this.sorter = sorter; this.input = sorter.getDirectory().createTempOutput(sorter.getTempFileNamePrefix(), "RefSorterRaw", IOContext.DEFAULT); this.writer = new OfflineSorter.ByteSequencesWriter(this.input); } The problem with it is that the writer for the initial input file is written with the default OfflineSorter.ByteSequencesWriter, but the instance of OfflineSorter may be unable to read it if it overrides getReader to use something else than the default. While this works now, it should be cleaned up (I think). It'd be probably ideal to allow OfflineSorter to generate its own temporary file and just return the ByteSequencesWriter it chooses to use, so the above snippet would read: public ExternalRefSorter(OfflineSorter sorter) throws IOException { this.sorter = sorter; this.writer = sorter.newUnsortedPartition(); } This could be also extended so that OfflineSorter is in charge of managing its own (sorted and unsorted) partitions. Then sort(String file) would simply become ByteSequenceIterator sort() (or even Stream&lt;BytesRef&gt; sort() as Stream is conveniently AutoCloseable). If we made OfflineSorter implement Closeable it could also take care of cleaning up any resources it opens in the directory we pass to it. An additional bonus would be the ability to dodge the final internal merge(1) – if we manage sorted and unsorted partitions then there are open possibilities of returning an iterator that dynamically merges from multiple partitions.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>2</commenters>
    </issue>
    <issue>
      <id>7482</id>
      <title>Faster sorted index search for reverse order search</title>
      <description>We are currently using Lucene here in my company for our main product. Our search functionnality is quite basic and the results are always sorted given a predefined field. The user is only able to choose the sort order (Asc/Desc). I am currently investigating using the index sort feature with EarlyTerminationSortingCollector. This is quite a shame searching on a sorted index in reverse order do not have any optimization and was wondering if it would be possible to make it faster by creating a special "ReverseSortingCollector" for this purpose. I am aware the posting list is designed to be always iterated in the same order, so it is not about early-terminating the search but more about filtering-out unneeded documents more efficiently. If a segment is sorted in reverse order, we just have to delegate collection of the last matched documents. Here is a sample quick code: ReverseSortingCollector.java public class ReverseSortingCollector extends FilterCollector { /** Sort used to sort the search results */ protected final Sort sort; /** Number of documents to collect in each segment */ protected final int numDocsToCollect; [...] private List&lt;FlushData&gt; flushList = new ArrayList&lt;&gt;(); private static final class FlushData { // ring buffer int[] buffer; // index of the first element in the buffer int index; LeafCollector leafCollector; FlushData(int[] buffer, LeafCollector leafCollector) { super(); this.buffer = buffer; this.leafCollector = leafCollector; } } @Override public LeafCollector getLeafCollector(LeafReaderContext context) throws IOException { //flush previous data if any flush(); LeafReader reader = context.reader(); Sort segmentSort = reader.getIndexSort(); if (isReverseOrder(sort, segmentSort)) {//segment is sorted in reverse order than the search sort int[] buffer = new int[numDocsToCollect]; Arrays.fill(buffer, -1); FlushData flushData = new FlushData(buffer, in.getLeafCollector(context)); flushList.add(flushData); return new LeafCollector() { @Override public void setScorer(Scorer scorer) throws IOException { } @Override public void collect(int doc) throws IOException { //we remember the last `numDocsToCollect` documents that matched buffer[flushData.index % buffer.length] = doc; flushData.index++; } }; } else { return in.getLeafCollector(context); } } //flush the last `numDocsToCollect` collected documents do the delegated Collector public void flush() throws IOException { for (FlushData flushData : flushList) { for (int i = 0; i &lt; flushData.buffer.length; i++) { int doc = flushData.buffer[(flushData.index + i) % flushData.buffer.length]; if (doc != -1) { flushData.leafCollector.collect(doc); } } } flushList.clear(); } } This is specially efficient when used along with TopFieldCollector as a lot of docValue lookup would not take place. In my experiment it reduced search time up to 90%. Note 1: Does not support paging. Note 2: Current implementation probably not thread safe</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7485</id>
      <title>Better storage for `docsWithField` in Lucene70NormsFormat</title>
      <description>Currently Lucene70NormsFormat uses a bit set to store documents that have a norm, and counts one bits using Long.bitCount in order to know the index of the current document in the set of docs that have a norm value. I think this is fairly good if a field is moderately sparse (somewhere between 5% and 99%) but it still has some issues like slow advance by large deltas (it still needs to visit all words in order to accumulate the number of ones to know the index of a document) or when very few bits are set. I have been working on a disk-based adaptation of RoaringDocIdSet that would still give the ability to know the index of the current document. It seems to be only a bit slower than the current implementation on moderately sparse fields. However, it also comes with benefits: it is faster in the sparse case when it uses the sparse encoding that uses shorts to store doc IDs (when the density is 6% or less) it has faster advance() by large deltas (still linear, but by a factor of 65536 so that should always be fine in practice since doc IDs are bound to 2B) it uses O(numDocsWithField) storage rather than O(maxDoc), the worst case in 6 bytes per field, which occurs when each range of 65k docs contains exactly one document. it is faster if some ranges of documents that share the same 16 upper bits are full, this is useful eg. if there is a single document that misses a field in the whole index or for use-cases that would store multiple types of documents (with different fields) within a single index and would use index sorting to put documents of the same type together</description>
      <attachments/>
      <comments>4</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7487</id>
      <title>Remove unnecessary synchronization from Lucene70NormsProducer</title>
      <description>Slice creation is thread-safe so synchronization is not necessary.</description>
      <attachments/>
      <comments>3</comments>
      <commenters>3</commenters>
    </issue>
    <issue>
      <id>7488</id>
      <title>Consider tracking modification time of external file fields for faster reloading</title>
      <description>I have an index of about 4M legal documents that has pagerank boosting configured as an external file field. The external file is about 100MB in size and has one row per document in the index. Each row indicates the pagerank score of a document. When we open new searchers, this file has to get reloaded, and it creates a noticeable delay for our users – takes several seconds to reload. An idea to fix this came up in a recent discussion in the Solr mailing list: Could the file only be reloaded if it has changed on disk? In other words, when new searchers are opened, could they check the modtime of the file, and avoid reloading it if the file hasn't changed? In our configuration, this would be a big improvement. We only change the pagerank file once/week because computing it is intensive and new documents don't tend to have a big impact. At the same time, because we're regularly adding new documents, we do hundreds of commits per day, all of which have a delay as the (largish) external file field is reloaded. Is this a reasonable improvement to request?</description>
      <attachments/>
      <comments>9</comments>
      <commenters>4</commenters>
    </issue>
    <issue>
      <id>7489</id>
      <title>Improve sparsity support of Lucene70DocValuesFormat</title>
      <description>Like Lucene70NormsFormat, it should be able to only encode actual values.</description>
      <attachments/>
      <comments>13</comments>
      <commenters>3</commenters>
    </issue>
  </issues>
</root>
