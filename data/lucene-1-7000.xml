<?xml version='1.0' encoding='UTF-8'?>
<root>
  <system>lucene</system>
  <issues>
    <issue>
      <id>95</id>
      <title>[PATCH] Term Vector support</title>
      <description>Moved from todo.xml: http://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene- dev@jakarta.apache.org&amp;msgNo=273 http://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene- dev@jakarta.apache.org&amp;msgNo=272 I don't know enough about the lucene internals to know if this was implemented in 1.3 so I'm adding it here as an open enhancement.</description>
      <attachments/>
    </issue>
    <issue>
      <id>152</id>
      <title>[PATCH] KStem for Lucene</title>
      <description>September 10th 2003 contributionn from "Sergio Guzman-Lara" &lt;guzman@cs.umass.edu&gt; Original email: Hi all, I have ported the kstem stemmer to Java and incorporated it to Lucene. You can get the source code (Kstem.jar) from the following website: http://ciir.cs.umass.edu/downloads/ Just click on "KStem Java Implementation" (you will need to register your e-mail, for free of course, with the CIIR --Center for Intelligent Information Retrieval, UMass – and get an access code). Content of Kstem.jar: java/org/apache/lucene/analysis/KStemData1.java java/org/apache/lucene/analysis/KStemData2.java java/org/apache/lucene/analysis/KStemData3.java java/org/apache/lucene/analysis/KStemData4.java java/org/apache/lucene/analysis/KStemData5.java java/org/apache/lucene/analysis/KStemData6.java java/org/apache/lucene/analysis/KStemData7.java java/org/apache/lucene/analysis/KStemData8.java java/org/apache/lucene/analysis/KStemFilter.java java/org/apache/lucene/analysis/KStemmer.java KStemData1.java, ..., KStemData8.java Contain several lists of words used by Kstem KStemmer.java Implements the Kstem algorithm KStemFilter.java Extends TokenFilter applying Kstem To compile unjar the file Kstem.jar to Lucene's "src" directory, and compile it there. What is Kstem? A stemmer designed by Bob Krovetz (for more information see http://ciir.cs.umass.edu/pubfiles/ir-35.pdf). Copyright issues This is open source. The actual license agreement is included at the top of every source file. Any comments/questions/suggestions are welcome, Sergio Guzman-Lara Senior Research Fellow CIIR UMass</description>
      <attachments/>
    </issue>
    <issue>
      <id>178</id>
      <title>[PATCH] arbitrary sorting</title>
      <description>new code to allow search results to be sorted by field contents</description>
      <attachments/>
    </issue>
    <issue>
      <id>196</id>
      <title>[PATCH] Added support for segmented field data files and cached directories</title>
      <description>Hello, I would like to contribute the following enhancement, hoping that it would be as useful for you as it is for me. For one of my applications, it was necessary to reprocess the Documents returned by a search in a Lucene index according to some Field values (for applying an "edit distance" function on unindexed fields, in my case). Because Lucene has to load every possibly relevant document (all fields, including the ones which are irrelevant for the algorithm) from disk into memory for this operation - doing so is extensively time-consuming. As far as I can see, currently, there is no satisfying solution to improve this situation except buffering all data in RAM using a RAMDirectory. But what if the field data is just too big to fit in RAM? My patch will handle this by splitting the monolithic "*.fdt"-Field data file into several "data store" files .fdt, .fd1, .fd2 and so on. These "data store" files are connected as a linked-list which permits you to load only the part of the field data that is relevant for the current operation. So, you can load all field data (as in the current implementation), or the fields from a specific interval [0;n] of data stores. Store 0 represents the data in the ".fdt" file, all data stores with ids &gt; 0 are represented by files ".fd1", ".fd2", and so on. In my case, I would then simply cache the ".fdt" (data store 0) file in RAM (using a symbolic link to shm-/tmp), but leave all other .fd* files on harddisk. The .fdt file only contains the relevant field for my algorithm (which therefore remains quite small); all the other fields are stored in the rather big ".fd0" file. So, accessing Fields in .fdt requires no disk I/O, which speeds up things remarkably. You can compare this feature with having multiple tables in a relational database that are linked with 1..1 cardinality instead of having one big table. My proposed enhancement requires some API additions, which I try to explain now. To specify the desired data store for a Field, simply call the new method "Field setDataStore(int)" (docstore 0 is the default): doc.add(Field.Keyword("fieldA", "this is in docstore 0")); doc.add(Field.Keyword("fieldB", "this is in docstore 1").setDataStore(1)); In this example, fieldA would be stored in ".fdt"; fieldB in ".fd1". When you retrieve the Document object (example docId = 123) using an IndexReader, you have the following options: "indexReader.document(123)" would load all fields from all data stores. "indexReader.document(123, 0)" would load only the fields from data store 0. "indexReader.document(123, 1)" would explictly load only the fields from data stores 0 and 1. The method "IndexReader.document(int n, int k)" is defined to fetch all fields from all data stores at least up to ID k. That way, existing IndexReader subclasses do not have to be modified, as I provide an overridable method in IndexReader which simply calls document(int n). A more concrete example is attached to this feature request as a JUnit-Testcase, as well as the patch itself. Have fun with it! Best regards, Christian Kohlschuetter</description>
      <attachments/>
    </issue>
    <issue>
      <id>294</id>
      <title>DisjunctionScorer</title>
      <description>This disjunction scorer can match a minimum nr. of docs, it provides skipTo() and it uses skipTo() on the subscorers. The score() method is abstract in DisjunctionScorer and implemented in DisjunctionSumScorer as an example.</description>
      <attachments/>
    </issue>
    <issue>
      <id>328</id>
      <title>Some utilities for a compact sparse filter</title>
      <description>Two files are attached that might form the basis for an alternative filter implementation that is more memory efficient than one bit per doc when less than about 1/8 of the docs pass through the filter. The document numbers are stored in RAM as VInt's from the Lucene index format. These VInt's encode the difference between two successive document numbers, much like a PositionDelta in the Positions: http://jakarta.apache.org/lucene/docs/fileformats.html The getByteSize() method can be used to verify the compression once a SortedVIntList is constructed. The precise conditions under which this is more memory efficient than one bit per document are not easy to specify in advance.</description>
      <attachments/>
    </issue>
    <issue>
      <id>330</id>
      <title>[PATCH] Use filter bits for next() and skipTo() in FilteredQuery</title>
      <description>This improves performance of FilteredQuery by not calling score() on documents that do not pass the filter. This passes the current tests for FilteredQuery, but these tests have not been adapted/extended.</description>
      <attachments/>
    </issue>
    <issue>
      <id>395</id>
      <title>CoordConstrainedBooleanQuery + QueryParser support</title>
      <description>Attached 2 new classes: 1) CoordConstrainedBooleanQuery A boolean query that only matches if a specified number of the contained clauses match. An example use might be a query that returns a list of books where ANY 2 people from a list of people were co-authors, eg: "Lucene In Action" would match ("Erik Hatcher" "Otis Gospodnetić" "Mark Harwood" "Doug Cutting") with a minRequiredOverlap of 2 because Otis and Erik wrote that. The book "Java Development with Ant" would not match because only 1 element in the list (Erik) was selected. 2) CustomQueryParserExample A customised QueryParser that allows definition of CoordConstrainedBooleanQueries. The solution (mis)uses fieldnames to pass parameters to the custom query.</description>
      <attachments/>
    </issue>
    <issue>
      <id>400</id>
      <title>NGramFilter -- construct n-grams from a TokenStream</title>
      <description>This filter constructs n-grams (token combinations up to a fixed size, sometimes called "shingles") from a token stream. The filter sets start offsets, end offsets and position increments, so highlighting and phrase queries should work. Position increments &gt; 1 in the input stream are replaced by filler tokens (tokens with termText "_" and endOffset - startOffset = 0) in the output n-grams. (Position increments &gt; 1 in the input stream are usually caused by removing some tokens, eg. stopwords, from a stream.) The filter uses CircularFifoBuffer and UnboundedFifoBuffer from Apache Commons-Collections. Filter, test case and an analyzer are attached.</description>
      <attachments/>
    </issue>
    <issue>
      <id>403</id>
      <title>Alternate Lucene Query Highlighter</title>
      <description>I created a lucene query highlighter (borrowing some code from the one in the sandbox) that my company is using. It better handles phrase queries, doesn't break HTML entities, and has the ability to either highlight terms in an entire document or to highlight fragments from the document. I would like to make it available to anyone who wants it.</description>
      <attachments/>
    </issue>
    <issue>
      <id>421</id>
      <title>Numeric range searching with large value sets</title>
      <description>I have a set of enhancements that build on the numeric sorting cache introduced by Tim Jones and that provide integer and floating point range searches over numeric ranges that are far too large to be implemented via the current term range rewrite mechanism. I'm new to Apache and trying to find out how to attach the source files for the changes for your consideration.</description>
      <attachments/>
    </issue>
    <issue>
      <id>436</id>
      <title>[PATCH] TermInfosReader, SegmentTermEnum Out Of Memory Exception</title>
      <description>We've been experiencing terrible memory problems on our production search server, running lucene (1.4.3). Our live app regularly opens new indexes and, in doing so, releases old IndexReaders for garbage collection. But...there appears to be a memory leak in org.apache.lucene.index.TermInfosReader.java. Under certain conditions (possibly related to JVM version, although I've personally observed it under both linux JVM 1.4.2_06, and 1.5.0_03, and SUNOS JVM 1.4.1) the ThreadLocal member variable, "enumerators" doesn't get garbage-collected when the TermInfosReader object is gc-ed. Looking at the code in TermInfosReader.java, there's no reason why it shouldn't be gc-ed, so I can only presume (and I've seen this suggested elsewhere) that there could be a bug in the garbage collector of some JVMs. I've seen this problem briefly discussed; in particular at the following URL: http://java2.5341.com/msg/85821.html The patch that Doug recommended, which is included in lucene-1.4.3 doesn't work in our particular circumstances. Doug's patch only clears the ThreadLocal variable for the thread running the finalizer (my knowledge of java breaks down here - I'm not sure which thread actually runs the finalizer). In our situation, the TermInfosReader is (potentially) used by more than one thread, meaning that Doug's patch doesn't allow the affected JVMs to correctly collect garbage. So...I've devised a simple patch which, from my observations on linux JVMs 1.4.2_06, and 1.5.0_03, fixes this problem. Kieran PS Thanks to daniel naber for pointing me to jira/lucene @@ -19,6 +19,7 @@ import java.io.IOException; import org.apache.lucene.store.Directory; +import java.util.Hashtable; /** This stores a monotonically increasing set of &lt;Term, TermInfo&gt; pairs in a Directory. Pairs are accessed either by Term or by ordinal position the @@ -29,7 +30,7 @@ private String segment; private FieldInfos fieldInfos; private ThreadLocal enumerators = new ThreadLocal(); + private final Hashtable enumeratorsByThread = new Hashtable(); private SegmentTermEnum origEnum; private long size; @@ -60,10 +61,10 @@ } private SegmentTermEnum getEnum() { SegmentTermEnum termEnum = (SegmentTermEnum)enumerators.get(); + SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread()); if (termEnum == null) { termEnum = terms(); - enumerators.set(termEnum); + enumeratorsByThread.put(Thread.currentThread(), termEnum); } return termEnum; } @@ -195,5 +196,15 @@ public SegmentTermEnum terms(Term term) throws IOException { get(term); return (SegmentTermEnum)getEnum().clone(); + } + + /* some jvms might have trouble gc-ing enumeratorsByThread */ + protected void finalize() throws Throwable Unknown macro: {+ try { + // make sure gc can clear up. + enumeratorsByThread.clear(); + } finally { + super.finalize(); + } } } TermInfosReader.java, full source: ====================================== package org.apache.lucene.index; /** Copyright 2004 The Apache Software Foundation * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at * http://www.apache.org/licenses/LICENSE-2.0 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ import java.io.IOException; import org.apache.lucene.store.Directory; import java.util.Hashtable; /** This stores a monotonically increasing set of &lt;Term, TermInfo&gt; pairs in a Directory. Pairs are accessed either by Term or by ordinal position the set. */ final class TermInfosReader { private Directory directory; private String segment; private FieldInfos fieldInfos; private final Hashtable enumeratorsByThread = new Hashtable(); private SegmentTermEnum origEnum; private long size; TermInfosReader(Directory dir, String seg, FieldInfos fis) throws IOException { directory = dir; segment = seg; fieldInfos = fis; origEnum = new SegmentTermEnum(directory.openFile(segment + ".tis"), fieldInfos, false); size = origEnum.size; readIndex(); } public int getSkipInterval() { return origEnum.skipInterval; } final void close() throws IOException { if (origEnum != null) origEnum.close(); } /** Returns the number of term/value pairs in the set. */ final long size() { return size; } private SegmentTermEnum getEnum() { SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread()); if (termEnum == null) { termEnum = terms(); enumeratorsByThread.put(Thread.currentThread(), termEnum); } return termEnum; } Term[] indexTerms = null; TermInfo[] indexInfos; long[] indexPointers; private final void readIndex() throws IOException { SegmentTermEnum indexEnum = new SegmentTermEnum(directory.openFile(segment + ".tii"), fieldInfos, true); try { int indexSize = (int)indexEnum.size; indexTerms = new Term[indexSize]; indexInfos = new TermInfo[indexSize]; indexPointers = new long[indexSize]; for (int i = 0; indexEnum.next(); i++) { indexTerms[i] = indexEnum.term(); indexInfos[i] = indexEnum.termInfo(); indexPointers[i] = indexEnum.indexPointer; } } finally { indexEnum.close(); } } /** Returns the offset of the greatest index entry which is less than or equal to term.*/ private final int getIndexOffset(Term term) throws IOException { int lo = 0; // binary search indexTerms[] int hi = indexTerms.length - 1; while (hi &gt;= lo) { int mid = (lo + hi) &gt;&gt; 1; int delta = term.compareTo(indexTerms[mid]); if (delta &lt; 0) hi = mid - 1; else if (delta &gt; 0) lo = mid + 1; else return mid; } return hi; } private final void seekEnum(int indexOffset) throws IOException { getEnum().seek(indexPointers[indexOffset], (indexOffset * getEnum().indexInterval) - 1, indexTerms[indexOffset], indexInfos[indexOffset]); } /** Returns the TermInfo for a Term in the set, or null. */ TermInfo get(Term term) throws IOException { if (size == 0) return null; // optimize sequential access: first try scanning cached enum w/o seeking SegmentTermEnum enumerator = getEnum(); if (enumerator.term() != null // term is at or past current &amp;&amp; ((enumerator.prev != null &amp;&amp; term.compareTo(enumerator.prev) &gt; 0) term.compareTo(enumerator.term()) &gt;= 0)) { int enumOffset = (int)(enumerator.position/enumerator.indexInterval)+1; if (indexTerms.length == enumOffset // but before end of block || term.compareTo(indexTerms[enumOffset]) &lt; 0) return scanEnum(term); // no need to seek } // random-access: must seek seekEnum(getIndexOffset(term)); return scanEnum(term); } /** Scans within block for matching term. */ private final TermInfo scanEnum(Term term) throws IOException { SegmentTermEnum enumerator = getEnum(); while (term.compareTo(enumerator.term()) &gt; 0 &amp;&amp; enumerator.next()) {} if (enumerator.term() != null &amp;&amp; term.compareTo(enumerator.term()) == 0) return enumerator.termInfo(); else return null; } /** Returns the nth term in the set. */ final Term get(int position) throws IOException { if (size == 0) return null; SegmentTermEnum enumerator = getEnum(); if (enumerator != null &amp;&amp; enumerator.term() != null &amp;&amp; position &gt;= enumerator.position &amp;&amp; position &lt; (enumerator.position + enumerator.indexInterval)) return scanEnum(position); // can avoid seek seekEnum(position / enumerator.indexInterval); // must seek return scanEnum(position); } private final Term scanEnum(int position) throws IOException { SegmentTermEnum enumerator = getEnum(); while(enumerator.position &lt; position) if (!enumerator.next()) return null; return enumerator.term(); } /** Returns the position of a Term in the set or -1. */ final long getPosition(Term term) throws IOException { if (size == 0) return -1; int indexOffset = getIndexOffset(term); seekEnum(indexOffset); SegmentTermEnum enumerator = getEnum(); while(term.compareTo(enumerator.term()) &gt; 0 &amp;&amp; enumerator.next()) {} if (term.compareTo(enumerator.term()) == 0) return enumerator.position; else return -1; } /** Returns an enumeration of all the Terms and TermInfos in the set. */ public SegmentTermEnum terms() { return (SegmentTermEnum)origEnum.clone(); } /** Returns an enumeration of terms starting at or after the named term. */ public SegmentTermEnum terms(Term term) throws IOException { get(term); return (SegmentTermEnum)getEnum().clone(); } /* some jvms might have trouble gc-ing enumeratorsByThread */ protected void finalize() throws Throwable { try { // make sure gc can clear up. enumeratorsByThread.clear(); } finally { super.finalize(); } } }</description>
      <attachments/>
    </issue>
    <issue>
      <id>446</id>
      <title>search.function - (1) score based on field value, (2) simple score customizability</title>
      <description>FunctionQuery can return a score based on a field's value or on it's ordinal value. FunctionFactory subclasses define the details of the function. There is currently a LinearFloatFunction (a line specified by slope and intercept). Field values are typically obtained from FieldValueSourceFactory. Implementations include FloatFieldSource, IntFieldSource, and OrdFieldSource.</description>
      <attachments/>
    </issue>
    <issue>
      <id>505</id>
      <title>MultiReader.norm() takes up too much memory: norms byte[] should be made into an Object</title>
      <description>MultiReader.norms() is very inefficient: it has to construct a byte array that's as long as all the documents in every segment. This doubles the memory requirement for scoring MultiReaders vs. Segment Readers. Although this is cached, it's still a baseline of memory that is unnecessary. The problem is that the Normalization Factors are passed around as a byte[]. If it were instead replaced with an Object, you could perform a whole host of optimizations a. When reading, you wouldn't have to construct a "fakeNorms" array of all 1.0fs. You could instead return a singleton object that would just return 1.0f. b. MultiReader could use an object that could delegate to NormFactors of the subreaders c. You could write an implementation that could use mmap to access the norm factors. Or if the index isn't long lived, you could use an implementation that reads directly from the disk. The patch provided here replaces the use of byte[] with a new abstract class called NormFactors. NormFactors has two methods on it public abstract byte getByte(int doc) throws IOException; // Returns the byte[doc] public float getFactor(int doc) throws IOException; // Calls Similarity.decodeNorm(getByte(doc)) There are four implementations of this abstract class 1. NormFactors.EmptyNormFactors - This replaces the fakeNorms with a singleton that only returns 1.0 2. NormFactors.ByteNormFactors - Converts a byte[] to a NormFactors for backwards compatibility in constructors. 3. MultiNormFactors - Multiplexes the NormFactors in MultiReader to prevent the need to construct the gigantic norms array. 4. SegmentReader.Norm - Same class, but now extends NormFactors to provide the same access. In addition, Many of the Query and Scorer classes were changes to pass around NormFactors instead of byte[], and to call getFactor() instead of using the byte[]. I have kept around IndexReader.norms(String) for backwards compatibiltiy, but marked it as deprecated. I believe that the use of ByteNormFactors in IndexReader.getNormFactors() will keep backward compatibility with other IndexReader implementations, but I don't know how to test that.</description>
      <attachments/>
    </issue>
    <issue>
      <id>509</id>
      <title>Performance optimization when retrieving a single field from a document</title>
      <description>If you just want to retrieve a single field from a Document, the only way to do it is to retrieve all the fields from the Document and then search it. This patch is an optimization that allows you retrieve a specific field from a document without instantiating a lot of field and string objects. This reduces our memory consumption on a per query basis by around around 20% when a lot of documents are returned. I've added a lot of comments saying you should only call it if you only ever need one field. There's also a unit test.</description>
      <attachments/>
    </issue>
    <issue>
      <id>510</id>
      <title>IndexOutput.writeString() should write length in bytes</title>
      <description>We should change the format of strings written to indexes so that the length of the string is in bytes, not Java characters. This issue has been discussed at: http://www.mail-archive.com/java-dev@lucene.apache.org/msg01970.html We must increment the file format number to indicate this change. At least the format number in the segments file should change. I'm targetting this for 2.1, i.e., we shouldn't commit it to trunk until after 2.0 is released, to minimize incompatible changes between 1.9 and 2.0 (other than removal of deprecated features).</description>
      <attachments/>
    </issue>
    <issue>
      <id>532</id>
      <title>[PATCH] Indexing on Hadoop distributed file system</title>
      <description>In my current project we needed a way to create very large Lucene indexes on Hadoop distributed file system. When we tried to do it directly on DFS using Nutch FsDirectory class - we immediately found that indexing fails because DfsIndexOutput.seek() method throws UnsupportedOperationException. The reason for this behavior is clear - DFS does not support random updates and so seek() method can't be supported (at least not easily). Well, if we can't support random updates - the question is: do we really need them? Search in the Lucene code revealed 2 places which call IndexOutput.seek() method: one is in TermInfosWriter and another one in CompoundFileWriter. As we weren't planning to use CompoundFileWriter - the only place that concerned us was in TermInfosWriter. TermInfosWriter uses IndexOutput.seek() in its close() method to write total number of terms in the file back into the beginning of the file. It was very simple to change file format a little bit and write number of terms into last 8 bytes of the file instead of writing them into beginning of file. The only other place that should be fixed in order for this to work is in SegmentTermEnum constructor - to read this piece of information at position = file length - 8. With this format hack - we were able to use FsDirectory to write index directly to DFS without any problems. Well - we still don't index directly to DFS for performance reasons, but at least we can build small local indexes and merge them into the main index on DFS without copying big main index back and forth.</description>
      <attachments/>
    </issue>
    <issue>
      <id>550</id>
      <title>InstantiatedIndex - faster but memory consuming index</title>
      <description>Represented as a coupled graph of class instances, this all-in-memory index store implementation delivers search results up to a 100 times faster than the file-centric RAMDirectory at the cost of greater RAM consumption. Performance seems to be a little bit better than log2n (binary search). No real data on that, just my eyes. Populated with a single document InstantiatedIndex is almost, but not quite, as fast as MemoryIndex. At 20,000 document 10-50 characters long InstantiatedIndex outperforms RAMDirectory some 30x, 15x at 100 documents of 2000 charachters length, and is linear to RAMDirectory at 10,000 documents of 2000 characters length. Mileage may vary depending on term saturation.</description>
      <attachments/>
    </issue>
    <issue>
      <id>580</id>
      <title>Pre-analyzed fields</title>
      <description>Adds the possibility to set a TokenStream at Field constrution time, available as tokenStreamValue in addition to stringValue, readerValue and binaryValue. There might be some problems with mixing stored fields with the same name as a field with tokenStreamValue.</description>
      <attachments/>
    </issue>
    <issue>
      <id>584</id>
      <title>Decouple Filter from BitSet</title>
      <description>package org.apache.lucene.search; public abstract class Filter implements java.io.Serializable { public abstract AbstractBitSet bits(IndexReader reader) throws IOException; } public interface AbstractBitSet { public boolean get(int index); } It would be useful if the method =Filter.bits()= returned an abstract interface, instead of =java.util.BitSet=. Use case: there is a very large index, and, depending on the user's privileges, only a small portion of the index is actually visible. Sparsely populated =java.util.BitSet=s are not efficient and waste lots of memory. It would be desirable to have an alternative BitSet implementation with smaller memory footprint. Though it is possibly to derive classes from =java.util.BitSet=, it was obviously not designed for that purpose. That's why I propose to use an interface instead. The default implementation could still delegate to =java.util.BitSet=.</description>
      <attachments/>
    </issue>
    <issue>
      <id>626</id>
      <title>Extended spell checker with phrase support and adaptive user session analysis.</title>
      <description>Extensive javadocs available in patch, but I also try to keep it compiled here: http://ginandtonique.org/~kalle/javadocs/didyoumean/org/apache/lucene/search/didyoumean/package-summary.html#package_description A semi-retarded reinforcement learning thingy backed by algorithmic second level suggestion schemes that learns from and adapts to user behavior as queries change, suggestions are accepted or declined, etc. Except for detecting spelling errors it considers context, composition/decomposition and a few other things. heroes of light and magik -&gt; heroes of might and magic vinci da code -&gt; da vinci code java docs -&gt; javadocs blacksabbath -&gt; black sabbath Depends on LUCENE-550</description>
      <attachments/>
    </issue>
    <issue>
      <id>635</id>
      <title>[PATCH] Decouple locking implementation from Directory implementation</title>
      <description>This is a spinoff of http://issues.apache.org/jira/browse/LUCENE-305. I've opened this new issue to capture that it's wider scope than LUCENE-305. This is a patch originally created by Jeff Patterson (see above link) and then modified as described here: http://issues.apache.org/jira/browse/LUCENE-305#action_12418493 with some small additional changes: For each FSDirectory.getDirectory(), I made a corresponding version that also accepts a LockFactory instance. So, you can construct an FSDirectory with your own LockFactory. Cascaded defaulting for FSDirectory's LockFactory implementation: if you pass in a LockFactory instance, it's used; else if setDisableLocks was called, we use NoLockFactory; else, if the system property "org.apache.lucene.store.FSDirectoryLockFactoryClass" is defined, we use that; finally, we'll use the original locking implementation (SimpleFSLockFactory). The gist is that all locking code has been moved out of *Directory and into subclasses of a new abstract LockFactory class. You can now set the LockFactory of a Directory to change how it does locking. For example, you can create an FSDirectory but set its locking to SingleInstanceLockFactory (if you know all writing/reading will take place a single JVM). The changes pass all unit tests (on Ubuntu Linux Sun Java 1.5 and Windows XP Sun Java 1.4), and I added another TestCase to test the LockFactory code. Note that LockFactory defaults are not changed: FSDirectory defaults to SimpleFSLockFactory and RAMDirectory defaults to SingleInstanceLockFactory. Next step (separate issue) is to create a LockFactory that uses the OS native locks (through java.nio).</description>
      <attachments/>
    </issue>
    <issue>
      <id>639</id>
      <title>[PATCH] Slight performance improvement for readVInt() of IndexInput</title>
      <description>By unrolling the loop in readVInt() I was able to get a slight, about 1.8 %, performance improvement for this method. The test program invoked the method over 17 million times on each run. I ran the performance tests on: Windows XP Pro SP2 Sun JDK 1.5.0_07 YourKit 5.5.4 Lucene trunk</description>
      <attachments/>
    </issue>
    <issue>
      <id>662</id>
      <title>Extendable writer and reader of field data</title>
      <description>As discussed on the dev mailing list, I have modified Lucene to allow to define how the data of a field is writen and read in the index. Basically, I have introduced the notion of IndexFormat. It is in fact a factory of FieldsWriter and FieldsReader. So the IndexReader, the indexWriter and the SegmentMerger are using this factory and not doing a "new FieldsReader/Writer()". I have also introduced the notion of FieldData. It handles every data of a field, and also the writing and the reading in a stream. I have done this way because in the current design of Lucene, Fiedable is an interface, so methods with a protected or package visibility cannot be defined. A FieldsWriter just writes data into a stream via the FieldData of the field. A FieldsReader instanciates a FieldData depending on the field name. Then it use the field data to read the stream. And finnaly it instanciates a Field with the field data. About compatibility, I think it is kept, as I have writen a DefaultIndexFormat that provides some DefaultFieldsWriter and DefaultFieldsReader. These implementations do the exact job that is done today. To acheive this modification, some classes and methods had to be moved from private and/or final to public or protected. About the lazy fields, I have implemented them in a more general way in the implementation of the abstract class FieldData, so it will be totally transparent for the Lucene user that will extends FieldData. The stream is kept in the fieldData and used as soon as the stringValue (or something else) is called. Implementing this way allowed me to handle the recently introduced LOAD_FOR_MERGE; it is just a lazy field data, and when read() is called on this lazy field data, the saved input stream is directly copied in the output stream. I have a last issue with this patch. The current design allow to read an index in an old format, and just do a writer.addIndexes() into a new format. With the new design, you cannot, because the writer will use the FieldData.write provided by the reader. enjoy !</description>
      <attachments/>
    </issue>
    <issue>
      <id>664</id>
      <title>[PATCH] small fixes to the new scoring.html doc</title>
      <description>This is an awesome initiative. We need more docs that cleanly explain the inner workings of Lucene in general... thanks Grant &amp; Steve &amp; others! I have a few small initial proposed fixes, largely just adding some more description around the components of the formula. But also a couple typos, another link out to Wikipedia, a missing closing ), etc. I've only made it through the "Understanding the Scoring Formula" section so far.</description>
      <attachments/>
    </issue>
    <issue>
      <id>675</id>
      <title>Lucene benchmark: objective performance test for Lucene</title>
      <description>We need an objective way to measure the performance of Lucene, both indexing and querying, on a known corpus. This issue is intended to collect comments and patches implementing a suite of such benchmarking tests. Regarding the corpus: one of the widely used and freely available corpora is the original Reuters collection, available from http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz or http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz. I propose to use this corpus as a base for benchmarks. The benchmarking suite could automatically retrieve it from known locations, and cache it locally.</description>
      <attachments/>
    </issue>
    <issue>
      <id>701</id>
      <title>Lock-less commits</title>
      <description>This is a patch based on discussion a while back on lucene-dev: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3c44E5B16D.4010805@mikemccandless.com%3e The approach is a small modification over the original discussion (see Retry Logic below). It works correctly in all my cross-machine test case, but I want to open it up for feedback, testing by users/developers in more diverse environments, etc. This is a small change to how lucene stores its index that enables elimination of the commit lock entirely. The write lock still remains. Of the two, the commit lock has been more troublesome for users since it typically serves an active role in production. Whereas the write lock is usually more of a design check to make sure you only have one writer against the index at a time. The basic idea is that filenames are never reused ("write once"), meaning, a writer never writes to a file that a reader may be reading (there is one exception: the segments.gen file; see "RETRY LOGIC" below). Instead it writes to generational files, ie, segments_1, then segments_2, etc. Besides the segments file, the .del files and norm files (.sX suffix) are also now generational. A generation is stored as an "_N" suffix before the file extension (eg, _p_4.s0 is the separate norms file for segment "p", generation 4). One important benefit of this is it avoids files contents caching entirely (the likely cause of errors when readers open an index mounted on NFS) since the file is always a new file. With this patch I can reliably instantiate readers over NFS when a writer is writing to the index. However, with NFS, you are still forced to refresh your reader once a writer has committed because "point in time" searching doesn't work over NFS (see LUCENE-673 ). The changes are fully backwards compatible: you can open an old index for searching, or to add/delete docs, etc. I've added a new unit test to test these cases. All units test pass, and I've added a number of additional unit tests, some of which fail on WIN32 in the current lucene but pass with this patch. The "fileformats.xml" has been updated to describe the changes to the files (but XXX references need to be fixed before committing). There are some other important benefits: Readers are now entirely read-only. Readers no longer block one another (false contention) on initialization. On hitting contention, we immediately retry instead of a fixed (default 1.0 second now) pause. No file renaming is ever done. File renaming has caused sneaky access denied errors on WIN32 (see LUCENE-665 ). (Yonik, I used your approach here to not rename the segments_N file(try segments_(N-1) on hitting IOException on segments_N): the separate ".done" file did not work reliably under very high stress testing when a directory listing was not "point in time"). On WIN32, you can now call IndexReader.setNorm() even if other readers have the index open (fixes a pre-existing minor bug in Lucene). On WIN32, You can now create an IndexWriter with create=true even if readers have the index open (eg see www.gossamer-threads.com/lists/lucene/java-user/39265) . Here's an overview of the changes: Every commit writes to the next segments_(N+1). Loading the segments_N file (&amp; opening the segments) now requires retry logic. I've captured this logic into a new static class: SegmentInfos.FindSegmentsFile. All places that need to do something on the current segments file now use this class. No more deletable file. Instead, the writer computes what's deletable on instantiation and updates this in memory whenever files can be deleted (ie, when it commits). Created a common class index.IndexFileDeleter shared by reader &amp; writer, to manage deletes. Storing more information into segments info file: whether it has separate deletes (and which generation), whether it has separate norms, per field (and which generation), whether it's compound or not. This is instead of relying on IO operations (file exists calls). Note that this fixes the current misleading FileNotFoundException users now see when an _X.cfs file is missing (eg http://www.nabble.com/FileNotFound-Exception-t6987.html). Fixed some small things about RAMDirectory that were not filesystem-like (eg opening a non-existent IndexInput failed to raise IOException; renames were not atomic). I added a stress test against a RAMDirectory (1 writer thread &amp; 2 reader threads) that uncovered these. Added option to not remove old files when create=true on creating FSDirectory; this is so the writer can do its own [more sophisticated because it retries on errors] removal. Removed all references to commit lock, COMMIT_LOCK_TIMEOUT, etc. (This is an API change). Extended index/IndexFileNames.java and index/IndexFileNameFilter.java with logic for computing generational file names. Changed index/IndexFileNameFilter.java to use a HashSet to check file extentsions for better performance. Fixed the test case TestIndexReader.testLastModified: it was incorrectly (I think?) comparing lastModified to version, of the index. I fixed that and then added a new test case for version. Retry Logic (in index/SegmentInfos.java) If a reader tries to load the segments just as a writer is committing, it may hit an IOException. This is just normal contention. In current Lucene contention causes a [default] 1.0 second pause then retry. With lock-less the contention causes no added delay beyond the time to retry. When this happens, we first try segments_(N-1) if present, because it could be segments_N is still being written. If that fails, we re-check to see if there is now a newer segments_M where M &gt; N and advance if so. Else we retry segments_N once more (since it could be it was in process previously but must now be complete since segments_(N-1) did not load). In order to find the current segments_N file, I list the directory and take the biggest segments_N that exists. However, under extreme stress testing (5 threads just opening &amp; closing readers over and over), on one platform (OS X) I found that the directory listing can be incorrect (stale) by up to 1.0 seconds. This means the listing will show a segments_N file but that file does not exist (fileExists() returns false). In order to handle this (and other such platforms), I switched to a hybrid approach (originally proposed by Doron Cohen in the original thread): on committing, the writer writes to a file "segments.gen" the generation it just committed. It writes 2 identical longs into this file. The retry logic, on detecting that the directory listing is stale falls back to the contents of this file. If that file is consistent (the two longs are identical), and, the generation is indeed newer than the dir listing, it will use that. Finally, if this approach is also stale, we fallback to stepping through sequential generations (up to a maximum # tries). If all 3 methods fail, we throw the original exception we hit. I added a static method SegmentInfos.setInfoStream() which will print details of retry attempts. In the patch it's set to System.out right now (we should turn off before a real commit) so if there are problems we can see what retry logic had done.</description>
      <attachments/>
    </issue>
    <issue>
      <id>707</id>
      <title>Lucene Java Site docs</title>
      <description>It would be really nice if the Java site docs where consistent with the rest of the Lucene family (namely, with navigation tabs, etc.) so that one can easily go between Nutch, Hadoop, etc.</description>
      <attachments/>
    </issue>
    <issue>
      <id>709</id>
      <title>[PATCH] Enable application-level management of IndexWriter.ramDirectory size</title>
      <description>IndexWriter currently only supports bounding of in the in-memory index cache using maxBufferedDocs, which limits it to a fixed number of documents. When document sizes vary substantially, especially when documents cannot be truncated, this leads either to inefficiencies from a too-small value or OutOfMemoryErrors from a too large value. This simple patch exposes IndexWriter.flushRamSegments(), and provides access to size information about IndexWriter.ramDirectory so that an application can manage this based on total number of bytes consumed by the in-memory cache, thereby allow a larger number of smaller documents or a smaller number of larger documents. This can lead to much better performance while elimianting the possibility of OutOfMemoryErrors. The actual job of managing to a size constraint, or any other constraint, is left up the applicatation. The addition of synchronized to flushRamSegments() is only for safety of an external call. It has no significant effect on internal calls since they all come from a sychronized caller.</description>
      <attachments/>
    </issue>
    <issue>
      <id>710</id>
      <title>Implement "point in time" searching without relying on filesystem semantics</title>
      <description>This was touched on in recent discussion on dev list: http://www.gossamer-threads.com/lists/lucene/java-dev/41700#41700 and then more recently on the user list: http://www.gossamer-threads.com/lists/lucene/java-user/42088 Lucene's "point in time" searching currently relies on how the underlying storage handles deletion files that are held open for reading. This is highly variable across filesystems. For example, UNIX-like filesystems usually do "close on last delete", and Windows filesystem typically refuses to delete a file open for reading (so Lucene retries later). But NFS just removes the file out from under the reader, and for that reason "point in time" searching doesn't work on NFS (see LUCENE-673 ). With the lockless commits changes (LUCENE-701 ), it's quite simple to re-implement "point in time searching" so as to not rely on filesystem semantics: we can just keep more than the last segments_N file (as well as all files they reference). This is also in keeping with the design goal of "rely on as little as possible from the filesystem". EG with lockless we no longer re-use filenames (don't rely on filesystem cache being coherent) and we no longer use file renaming (because on Windows it can fails). This would be another step of not relying on semantics of "deleting open files". The less we require from filesystem the more portable Lucene will be! Where it gets interesting is what "policy" we would then use for removing segments_N files. The policy now is "remove all but the last one". I think we would keep this policy as the default. Then you could imagine other policies: Keep past N day's worth Keep the last N Keep only those in active use by a reader somewhere (note: tricky how to reliably figure this out when readers have crashed, etc.) Keep those "marked" as rollback points by some transaction, or marked explicitly as a "snaphshot". Or, roll your own: the "policy" would be an interface or abstract class and you could make your own implementation. I think for this issue we could just create the framework (interface/abstract class for "policy" and invoke it from IndexFileDeleter) and then implement the current policy (delete all but most recent segments_N) as the default policy. In separate issue(s) we could then create the above more interesting policies. I think there are some important advantages to doing this: "Point in time" searching would work on NFS (it doesn't now because NFS doesn't do "delete on last close"; see LUCENE-673 ) and any other Directory implementations that don't work currently. Transactional semantics become a possibility: you can set a snapshot, do a bunch of stuff to your index, and then rollback to the snapshot at a later time. If a reader crashes or machine gets rebooted, etc, it could choose to re-open the snapshot it had previously been using, whereas now the reader must always switch to the last commit point. Searchers could search the same snapshot for follow-on actions. Meaning, user does search, then next page, drill down (Solr), drill up, etc. These are each separate trips to the server and if searcher has been re-opened, user can get inconsistent results (= lost trust). But with, one series of search interactions could explicitly stay on the snapshot it had started with.</description>
      <attachments/>
    </issue>
    <issue>
      <id>730</id>
      <title>Restore top level disjunction performance</title>
      <description>This patch restores the performance of top level disjunctions. The introduction of BooleanScorer2 had impacted this as reported on java-user on 21 Nov 2006 by Stanislav Jordanov.</description>
      <attachments/>
    </issue>
    <issue>
      <id>743</id>
      <title>IndexReader.reopen()</title>
      <description>This is Robert Engels' implementation of IndexReader.reopen() functionality, as a set of 3 new classes (this was easier for him to implement, but should probably be folded into the core, if this looks good).</description>
      <attachments/>
    </issue>
    <issue>
      <id>753</id>
      <title>Use NIO positional read to avoid synchronization in FSIndexInput</title>
      <description>As suggested by Doug, we could use NIO pread to avoid synchronization on the underlying file. This could mitigate any MT performance drop caused by reducing the number of files in the index format.</description>
      <attachments/>
    </issue>
    <issue>
      <id>756</id>
      <title>Maintain norms in a single file .nrm</title>
      <description>Non-compound indexes are ~10% faster at indexing, and perform 50% IO activity comparing to compound indexes. But their file descriptors foot print is much higher. By maintaining all field norms in a single .nrm file, we can bound the number of files used by non compound indexes, and possibly allow more applications to use this format. More details on the motivation for this in: http://www.nabble.com/potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-tf2826909.html (in particular http://www.nabble.com/Re%3A-potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-p7910403.html).</description>
      <attachments/>
    </issue>
    <issue>
      <id>759</id>
      <title>Add n-gram tokenizers to contrib/analyzers</title>
      <description>It would be nice to have some n-gram-capable tokenizers in contrib/analyzers. Patch coming shortly.</description>
      <attachments/>
    </issue>
    <issue>
      <id>769</id>
      <title>[PATCH] Performance improvement for some cases of sorted search</title>
      <description>It's a small addition to Lucene that significantly lowers memory consumption and improves performance for sorted searches with frequent index updates and relatively big indexes (&gt;1mln docs) scenario. This solution supports only single-field sorting currently (which seem to be quite popular use case). Multiple fields support can be added without much trouble. The solution is this: documents from the sorting set (instead of given field's values from the whole index - current FieldCache approach) are cached in a WeakHashMap so the cached items are candidates for GC. Their fields values are then fetched from the cache and compared while sorting.</description>
      <attachments/>
    </issue>
    <issue>
      <id>778</id>
      <title>Allow overriding a Document</title>
      <description>In our application, we have some kind of generic API that is handling how we are using Lucene. The different other applications are using this API with different semantics, and are using the Lucene fields quite differently. We wrote some usefull functions to do this mapping. Today, as the Document class cannot be overriden, we are obliged to make a document wrapper by application, ie some MyAppDocument and MyOtherAppDocument which have a property holding a real Lucene Document. Then, when MyApp or MyOtherApp want to use our generic lucene API, we have to "get out" the Lucene document, ie do some genericLuceneAPI.writeDoc(myAppDoc.getLuceneDocument()). This work fine, but it becomes quite tricky to use the other function of our generic API which is genericLuceneAPI.writeDocs(Collection&lt;Document&gt; docs). I don't know the rational behind making final Document, but removing it will allow more object-oriented code.</description>
      <attachments/>
    </issue>
    <issue>
      <id>794</id>
      <title>Extend contrib Highlighter to properly support PhraseQuery, SpanQuery, ConstantScoreRangeQuery</title>
      <description>This patch adds a new Scorer class (SpanQueryScorer) to the Highlighter package that scores just like QueryScorer, but scores a 0 for Terms that did not cause the Query hit. This gives 'actual' hit highlighting for the range of SpanQuerys, PhraseQuery, and ConstantScoreRangeQuery. New Query types are easy to add. There is also a new Fragmenter that attempts to fragment without breaking up Spans. See http://issues.apache.org/jira/browse/LUCENE-403 for some background. There is a dependency on MemoryIndex.</description>
      <attachments/>
    </issue>
    <issue>
      <id>831</id>
      <title>Complete overhaul of FieldCache API/Implementation</title>
      <description>Motivation: 1) Complete overhaul the API/implementation of "FieldCache" type things... a) eliminate global static map keyed on IndexReader (thus eliminating synch block between completley independent IndexReaders) b) allow more customization of cache management (ie: use expiration/replacement strategies, disk backed caches, etc) c) allow people to define custom cache data logic (ie: custom parsers, complex datatypes, etc... anything tied to a reader) d) allow people to inspect what's in a cache (list of CacheKeys) for an IndexReader so a new IndexReader can be likewise warmed. e) Lend support for smarter cache management if/when IndexReader.reopen is added (merging of cached data from subReaders). 2) Provide backwards compatibility to support existing FieldCache API with the new implementation, so there is no redundent caching as client code migrades to new API.</description>
      <attachments/>
    </issue>
    <issue>
      <id>843</id>
      <title>improve how IndexWriter uses RAM to buffer added documents</title>
      <description>I'm working on a new class (MultiDocumentWriter) that writes more than one document directly into a single Lucene segment, more efficiently than the current approach. This only affects the creation of an initial segment from added documents. I haven't changed anything after that, eg how segments are merged. The basic ideas are: Write stored fields and term vectors directly to disk (don't use up RAM for these). Gather posting lists &amp; term infos in RAM, but periodically do in-RAM merges. Once RAM is full, flush buffers to disk (and merge them later when it's time to make a real segment). Recycle objects/buffers to reduce time/stress in GC. Other various optimizations. Some of these changes are similar to how KinoSearch builds a segment. But, I haven't made any changes to Lucene's file format nor added requirements for a global fields schema. So far the only externally visible change is a new method "setRAMBufferSize" in IndexWriter (and setMaxBufferedDocs is deprecated) so that it flushes according to RAM usage and not a fixed number documents added.</description>
      <attachments/>
    </issue>
    <issue>
      <id>847</id>
      <title>Factor merge policy out of IndexWriter</title>
      <description>If we factor the merge policy out of IndexWriter, we can make it pluggable, making it possible for apps to choose a custom merge policy and for easier experimenting with merge policy variants.</description>
      <attachments/>
    </issue>
    <issue>
      <id>848</id>
      <title>Add supported for Wikipedia English as a corpus in the benchmarker stuff</title>
      <description>Add support for using Wikipedia for benchmarking.</description>
      <attachments/>
    </issue>
    <issue>
      <id>855</id>
      <title>MemoryCachedRangeFilter to boost performance of Range queries</title>
      <description>Currently RangeFilter uses TermEnum and TermDocs to find documents that fall within the specified range. This requires iterating through every single term in the index and can get rather slow for large document sets. MemoryCachedRangeFilter reads all &lt;docId, value&gt; pairs of a given field, sorts by value, and stores in a SortedFieldCache. During bits(), binary searches are used to find the start and end indices of the lower and upper bound values. The BitSet is populated by all the docId values that fall in between the start and end indices. TestMemoryCachedRangeFilterPerformance creates a 100K RAMDirectory-backed index with random date values within a 5 year range. Executing bits() 1000 times on standard RangeQuery using random date intervals took 63904ms. Using MemoryCachedRangeFilter, it took 876ms. Performance increase is less dramatic when you have less unique terms in a field or using less number of documents. Currently MemoryCachedRangeFilter only works with numeric values (values are stored in a long[] array) but it can be easily changed to support Strings. A side "benefit" of storing the values are stored as longs, is that there's no longer the need to make the values lexographically comparable, i.e. padding numeric values with zeros. The downside of using MemoryCachedRangeFilter is there's a fairly significant memory requirement. So it's designed to be used in situations where range filter performance is critical and memory consumption is not an issue. The memory requirements are: (sizeof(int) + sizeof(long)) * numDocs. MemoryCachedRangeFilter also requires a warmup step which can take a while to run in large datasets (it took 40s to run on a 3M document corpus). Warmup can be called explicitly or is automatically called the first time MemoryCachedRangeFilter is applied using a given field. So in summery, MemoryCachedRangeFilter can be useful when: Performance is critical Memory is not an issue Field contains many unique numeric values Index contains large amount of documents</description>
      <attachments/>
    </issue>
    <issue>
      <id>866</id>
      <title>Multi-level skipping on posting lists</title>
      <description>To accelerate posting list skips (TermDocs.skipTo(int)) Lucene uses skip lists. The default skip interval is set to 16. If we want to skip e. g. 100 documents, then it is not necessary to read 100 entries from the posting list, but only 100/16 = 6 skip list entries plus 100%16 = 4 entries from the posting list. This speeds up conjunction (AND) and phrase queries significantly. However, the skip interval is always a compromise. If you have a very big index with huge posting lists and you want to skip over lets say 100k documents, then it is still necessary to read 100k/16 = 6250 entries from the skip list. For big indexes the skip interval could be set to a higher value, but then after a big skip a long scan to the target doc might be necessary. A solution for this compromise is to have multi-level skip lists that guarantee a logarithmic amount of skips to any target in the posting list. This patch implements such an approach in the following way: Example for skipInterval = 3: c (skip level 2) c c c (skip level 1) x x x x x x x x x x (skip level 0) d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d (posting list) 3 6 9 12 15 18 21 24 27 30 (df) d - document x - skip data c - skip data with child pointer Skip level i contains every skipInterval-th entry from skip level i-1. Therefore the number of entries on level i is: floor(df / ((skipInterval ^ (i + 1))). Each skip entry on a level i&gt;0 contains a pointer to the corresponding skip entry in list i-1. This guarantees a logarithmic amount of skips to find the target document. Implementations details: I factored the skipping code out of SegmentMerger and SegmentTermDocs to simplify those classes. The two new classes AbstractSkipListReader and AbstractSkipListWriter implement the skipping functionality. While AbstractSkipListReader and Writer take care of writing and reading the multiple skip levels, they do not implement an actual skip data format. The two new subclasses DefaultSkipListReader and Writer implement the skip data format that is currently used in Lucene (with two file pointers for the freq and prox file and with payload length information). I added this extra layer to be prepared for flexible indexing and different posting list formats. File format changes: I added the new parameter 'maxSkipLevels' to the term dictionary and increased the version of this file. If maxSkipLevels is set to one, then the format of the freq file does not change at all, because we only have one skip level as before. For backwards compatibility maxSkipLevels is set to one automatically if an index without the new parameter is read. In case maxSkipLevels &gt; 1, then the frq file changes as follows: FreqFile (.frq) --&gt; &lt;TermFreqs, SkipData&gt;^TermCount SkipData --&gt; &lt;&lt;SkipLevelLength, SkipLevel&gt;^(Min(maxSkipLevels, floor(log(DocFreq/log(skipInterval))) - 1)&gt;, SkipLevel&gt; SkipLevel --&gt; &lt;SkipDatum&gt;DocFreq/(SkipInterval(Level + 1)) Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not needed, and 2) the format of this file does not change for maxSkipLevels=1 then. All unit tests pass with this patch.</description>
      <attachments/>
    </issue>
    <issue>
      <id>868</id>
      <title>Making Term Vectors more accessible</title>
      <description>One of the big issues with term vector usage is that the information is loaded into parallel arrays as it is loaded, which are then often times manipulated again to use in the application (for instance, they are sorted by frequency). Adding a callback mechanism that allows the vector loading to be handled by the application would make this a lot more efficient. I propose to add to IndexReader: abstract public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException; and a similar one for the all fields version Where TermVectorMapper is an interface with a single method: void map(String term, int frequency, int offset, int position); The TermVectorReader will be modified to just call the TermVectorMapper. The existing getTermFreqVectors will be reimplemented to use an implementation of TermVectorMapper that creates the parallel arrays. Additionally, some simple implementations that automatically sort vectors will also be created. This is my first draft of this API and is subject to change. I hope to have a patch soon. See http://www.gossamer-threads.com/lists/lucene/java-user/48003?search_string=get%20the%20total%20term%20frequency;#48003 for related information.</description>
      <attachments/>
    </issue>
    <issue>
      <id>885</id>
      <title>clean up build files so contrib tests are run more easily</title>
      <description>Per mailing list discussion... http://www.nabble.com/Tests%2C-Contribs%2C-and-Releases-tf3768924.html#a10655448 Tests for contribs should be run when "ant test" is used, existing "test" target renamed to "test-core"</description>
      <attachments/>
    </issue>
    <issue>
      <id>887</id>
      <title>Interruptible segment merges</title>
      <description>Adds the ability to IndexWriter to interrupt an ongoing merge. This might be necessary when Lucene is e. g. running as a service and has to stop indexing within a certain period of time due to a shutdown request. A solution would be to add a new method shutdown() to IndexWriter which satisfies the following two requirements: if a merge is happening, abort it flush the buffered docs but do not trigger a merge See also discussions about this feature on java-dev: http://www.gossamer-threads.com/lists/lucene/java-dev/49008</description>
      <attachments/>
    </issue>
    <issue>
      <id>888</id>
      <title>Improve indexing performance by increasing internal buffer sizes</title>
      <description>In working on LUCENE-843, I noticed that two buffer sizes have a substantial impact on overall indexing performance. First is BufferedIndexOutput.BUFFER_SIZE (also used by BufferedIndexInput). Second is CompoundFileWriter's buffer used to actually build the compound file. Both are now 1 KB (1024 bytes). I ran the same indexing test I'm using for LUCENE-843. I'm indexing ~5,500 byte plain text docs derived from the Europarl corpus (English). I index 200,000 docs with compound file enabled and term vector positions &amp; offsets stored plus stored fields. I flush documents at 16 MB RAM usage, and I set maxBufferedDocs carefully to not hit LUCENE-845. The resulting index is 1.7 GB. The index is not optimized in the end and I left mergeFactor @ 10. I ran the tests on a quad-core OS X 10 machine with 4-drive RAID 0 IO system. At 1 KB (current Lucene trunk) it takes 622 sec to build the index; if I increase both buffers to 8 KB it takes 554 sec to build the index, which is an 11% overall gain! I will run more tests to see if there is a natural knee in the curve (buffer size above which we don't really gain much more performance). I'm guessing we should leave BufferedIndexInput's default BUFFER_SIZE at 1024, at least for now. During searching there can be quite a few of this class instantiated, and likely a larger buffer size for the freq/prox streams could actually hurt search performance for those searches that use skipping. The CompoundFileWriter buffer is created only briefly, so I think we can use a fairly large (32 KB?) buffer there. And there should not be too many BufferedIndexOutputs alive at once so I think a large-ish buffer (16 KB?) should be OK.</description>
      <attachments/>
    </issue>
    <issue>
      <id>935</id>
      <title>Improve maven artifacts</title>
      <description>There are a couple of things we can improve for the next release: "*pom.xml" files should be renamed to "*pom.xml.template" artifacts "lucene-parent" should extend "apache-parent" add source jars as artifacts update &lt;generate-maven-artifacts&gt; task to work with latest version of maven-ant-tasks.jar metadata filenames should not contain "local"</description>
      <attachments/>
    </issue>
    <issue>
      <id>937</id>
      <title>Make CachingTokenFilter faster</title>
      <description>The LinkedList used by CachingTokenFilter is accessed using the get() method. Direct access on a LinkedList is slow and an Iterator should be used instead. For more than a handful of tokens, the difference in speed grows exponentially.</description>
      <attachments/>
    </issue>
    <issue>
      <id>944</id>
      <title>Remove deprecated methods in BooleanQuery</title>
      <description>Remove deprecated methods setUseScorer14 and getUseScorer14 in BooleanQuery, and adapt javadocs.</description>
      <attachments/>
    </issue>
    <issue>
      <id>954</id>
      <title>Toggle score normalization in Hits</title>
      <description>The current implementation of the "Hits" class sometimes performs score normalization. In particular, whenever the top-ranked score is bigger than 1.0, it is normalized to a maximum of 1.0. In this case, Hits may return different score results than TopDocs-based methods. In my scenario (a federated search system), Hits delievered just plain wrong results. I was merging results from several sources, all having homogeneous statistics (similar to MultiSearcher, but over the Internet using HTTP/XML-based protocols). Sometimes, some of the sources had a top-score greater than 1, so I ended up with garbled results. I suggest to add a switch to enable/disable this score-normalization at runtime. My patch (attached) has an additional peformance benefit, since score normalization now occurs only when Hits#score() is called, not when creating the Hits result list. Whenever scores are not required, you save one multiplication per retrieved hit (i.e., at least 100 multiplications with the current implementation of Hits).</description>
      <attachments/>
    </issue>
    <issue>
      <id>965</id>
      <title>Implement a state-of-the-art retrieval function in Lucene</title>
      <description>We implemented the axiomatic retrieval function, which is a state-of-the-art retrieval function, to replace the default similarity function in Lucene. We compared the performance of these two functions and reported the results at http://sifaka.cs.uiuc.edu/hfang/lucene/Lucene_exp.pdf. The report shows that the performance of the axiomatic retrieval function is much better than the default function. The axiomatic retrieval function is able to find more relevant documents and users can see more relevant documents in the top-ranked documents. Incorporating such a state-of-the-art retrieval function could improve the search performance of all the applications which were built upon Lucene. Most changes related to the implementation are made in AXSimilarity, TermScorer and TermQuery.java. However, many test cases are hand coded to test whether the implementation of the default function is correct. Thus, I also made the modification to many test files to make the new retrieval function pass those cases. In fact, we found that some old test cases are not reasonable. For example, in the testQueries02 of TestBoolean2.java, the query is "+w3 xx", and we have two documents "w1 xx w2 yy w3" and "w1 w3 xx w2 yy w3". The second document should be more relevant than the first one, because it has more occurrences of the query term "w3". But the original test case would require us to rank the first document higher than the second one, which is not reasonable.</description>
      <attachments/>
    </issue>
    <issue>
      <id>966</id>
      <title>A faster JFlex-based replacement for StandardAnalyzer</title>
      <description>JFlex (http://www.jflex.de/) can be used to generate a faster (up to several times) replacement for StandardAnalyzer. Will add a patch and a simple benchmark code in a while.</description>
      <attachments/>
    </issue>
    <issue>
      <id>994</id>
      <title>Change defaults in IndexWriter to maximize "out of the box" performance</title>
      <description>This is follow-through from LUCENE-845, LUCENE-847 and LUCENE-870; I'll commit this once those three are committed. Out of the box performance of IndexWriter is maximized when flushing by RAM instead of a fixed document count (the default today) because documents can vary greatly in size. Likewise, merging performance should be faster when merging by net segment size since, to minimize the net IO cost of merging segments over time, you want to merge segments of equal byte size. Finally, ConcurrentMergeScheduler improves indexing speed substantially (25% in a simple initial test in LUCENE-870) because it runs the merges in the backround and doesn't block add/update/deleteDocument calls. Most machines have concurrency between CPU and IO and so it makes sense to default to this MergeScheduler. Note that these changes will break users of ParallelReader because the parallel indices will no longer have matching docIDs. Such users need to switch IndexWriter back to flushing by doc count, and switch the MergePolicy back to LogDocMergePolicy. It's likely also necessary to switch the MergeScheduler back to SerialMergeScheduler to ensure deterministic docID assignment. I think the combination of these three default changes, plus other performance improvements for indexing (LUCENE-966, LUCENE-843, LUCENE-963, LUCENE-969, LUCENE-871, etc.) should make for some sizable performance gains Lucene 2.3!</description>
      <attachments/>
    </issue>
    <issue>
      <id>995</id>
      <title>Add open ended range query syntax to QueryParser</title>
      <description>The QueryParser fails to generate open ended range queries. Parsing e.g. "date:[1990 TO *]" gives zero results, but ConstantRangeQuery("date","1990",null,true,true) does produce the expected results. "date:[* TO 1990]" gives the same results as ConstantRangeQuery("date",null,"1990",true,true).</description>
      <attachments/>
    </issue>
    <issue>
      <id>997</id>
      <title>Add search timeout support to Lucene</title>
      <description>This patch is based on Nutch-308. This patch adds support for a maximum search time limit. After this time is exceeded, the search thread is stopped, partial results (if any) are returned and the total number of results is estimated. This patch tries to minimize the overhead related to time-keeping by using a version of safe unsynchronized timer. This was also discussed in an e-mail thread. http://www.nabble.com/search-timeout-tf3410206.html#a9501029</description>
      <attachments/>
    </issue>
    <issue>
      <id>1001</id>
      <title>Add Payload retrieval to Spans</title>
      <description>It will be nice to have access to payloads when doing SpanQuerys. See http://www.gossamer-threads.com/lists/lucene/java-dev/52270 and http://www.gossamer-threads.com/lists/lucene/java-dev/51134 Current API, added to Spans.java is below. I will try to post a patch as soon as I can figure out how to make it work for unordered spans (I believe I have all the other cases working). /** * Returns the payload data for the current span. * This is invalid until {@link #next()} is called for * the first time. * This method must not be called more than once after each call * of {@link #next()}. However, payloads are loaded lazily, * so if the payload data for the current position is not needed, * this method may not be called at all for performance reasons.&lt;br&gt; * &lt;br&gt; * &lt;p&gt;&lt;font color="#FF0000"&gt; * WARNING: The status of the &lt;b&gt;Payloads&lt;/b&gt; feature is experimental. * The APIs introduced here might change in the future and will not be * supported anymore in such a case.&lt;/font&gt; * * @return a List of byte arrays containing the data of this payload * @throws IOException */ // TODO: Remove warning after API has been finalized List/*&lt;byte[]&gt;*/ getPayload() throws IOException; /** * Checks if a payload can be loaded at this position. * &lt;p/&gt; * Payloads can only be loaded once per call to * {@link #next()}. * &lt;p/&gt; * &lt;p&gt;&lt;font color="#FF0000"&gt; * WARNING: The status of the &lt;b&gt;Payloads&lt;/b&gt; feature is experimental. * The APIs introduced here might change in the future and will not be * supported anymore in such a case.&lt;/font&gt; * * @return true if there is a payload available at this position that can be loaded */ // TODO: Remove warning after API has been finalized public boolean isPayloadAvailable();</description>
      <attachments/>
    </issue>
    <issue>
      <id>1016</id>
      <title>TermVectorAccessor, transparent vector space access</title>
      <description>This class visits TermVectorMapper and populates it with information transparent by either passing it down to the default terms cache (documents indexed with Field.TermVector) or by resolving the inverted index.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1026</id>
      <title>Provide a simple way to concurrently access a Lucene index from multiple threads</title>
      <description>For building interactive indexes accessed through a network/internet (multiple threads). This builds upon the LuceneIndexAccessor patch. That patch was not very newbie friendly and did not properly handle MultiSearchers (or at the least made it easy to get into trouble). This patch simplifies things and provides out of the box support for sharing the IndexAccessors across threads. There is also a simple test class and example SearchServer to get you started. Future revisions will be zipped. Works pretty solid as is, but could use the ability to warm new Searchers.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1035</id>
      <title>Optional Buffer Pool to Improve Search Performance</title>
      <description>Index in RAMDirectory provides better performance over that in FSDirectory. But many indexes cannot fit in memory or applications cannot afford to spend that much memory on index. On the other hand, because of locality, a reasonably sized buffer pool may provide good improvement over FSDirectory. This issue aims at providing such an optional buffer pool layer. In cases where it fits, i.e. a reasonable hit ratio can be achieved, it should provide a good improvement over FSDirectory.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1052</id>
      <title>Add an "termInfosIndexDivisor" to IndexReader</title>
      <description>The termIndexInterval, set during indexing time, let's you tradeoff how much RAM is used by a reader to load the indexed terms vs cost of seeking to the specific term you want to load. But the downside is you must set it at indexing time. This issue adds an indexDivisor to TermInfosReader so that on opening a reader you could further sub-sample the the termIndexInterval to use less RAM. EG a setting of 2 means every 2 * termIndexInterval is loaded into RAM. This is particularly useful if your index has a great many terms (eg you accidentally indexed binary terms). Spinoff from this thread: http://www.gossamer-threads.com/lists/lucene/java-dev/54371</description>
      <attachments/>
    </issue>
    <issue>
      <id>1058</id>
      <title>New Analyzer for buffering tokens</title>
      <description>In some cases, it would be handy to have Analyzer/Tokenizer/TokenFilters that could siphon off certain tokens and store them in a buffer to be used later in the processing pipeline. For example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field. Patch to follow, but I am still not sure about a couple of things, mostly how it plays with the new reuse API. See http://www.gossamer-threads.com/lists/lucene/java-dev/54397?search_string=BufferingAnalyzer;#54397</description>
      <attachments/>
    </issue>
    <issue>
      <id>1076</id>
      <title>Allow MergePolicy to select non-contiguous merges</title>
      <description>I started work on this but with LUCENE-1044 I won't make much progress on it for a while, so I want to checkpoint my current state/patch. For backwards compatibility we must leave the default MergePolicy as selecting contiguous merges. This is necessary because some applications rely on "temporal monotonicity" of doc IDs, which means even though merges can re-number documents, the renumbering will always reflect the order in which the documents were added to the index. Still, for those apps that do not rely on this, we should offer a MergePolicy that is free to select the best merges regardless of whether they are continuguous. This requires fixing IndexWriter to accept such a merge, and, fixing LogMergePolicy to optionally allow it the freedom to do so.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1084</id>
      <title>increase default maxFieldLength?</title>
      <description>To my understanding, Lucene 2.3 will easily index large documents. So shouldn't we get rid of the 10,000 default limit for the field length? 10,000 isn't that much and as Lucene doesn't have any error logging by default, this is a common problem for users that is difficult to debug if you don't know where to look. A better new default might be Integer.MAX_VALUE.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1126</id>
      <title>Simplify StandardTokenizer JFlex grammar</title>
      <description>Summary of thread entitled "Fullwidth alphanumeric characters, plus a question on Korean ranges" begun by Daniel Noll on java-user, and carried over to java-dev: On 01/07/2008 at 5:06 PM, Daniel Noll wrote: &gt; I wish the tokeniser could just use Character.isLetter and &gt; Character.isDigit instead of having to know all the ranges itself, since &gt; the JRE already has all this information. Character.isLetter does &gt; return true for CJK characters though, so the ranges would still come in &gt; handy for determining what kind of letter they are. I don't support &gt; JFlex has a way to do this... The DIGIT macro could be replaced by JFlex's predefined character class [:digit:], which has the same semantics as java.lang.Character.isDigit(). Although JFlex's predefined character class [:letter:] (same semantics as java.lang.Character.isLetter()) includes CJK characters, there is a way to handle this using JFlex's regex negation syntax !. From the JFlex documentation: [T]he expression that matches everything of a not matched by b is !(!a|b) So to exclude CJ characters from the LETTER macro: LETTER = ! ( ! [:letter:] | {CJ} ) Since [:letter:] includes all of the Korean ranges, there's no reason (AFAICT) to treat them separately; unlike Chinese and Japanese characters, which are individually tokenized, the Korean characters should participate in the same token boundary rules as all of the other letters. I looked at some of the differences between Unicode 3.0.0, which Java 1.4.2 supports, and Unicode 5.0, the latest version, and there are lots of new and modified letter and digit ranges. This stuff gets tweaked all the time, and I don't think Lucene should be in the business of trying to track it, or take a position on which Unicode version users' data should conform to. Switching to using JFlex's [:letter:] and [:digit:] predefined character classes ties (most of) these decisions to the user's choice of JVM version, and this seems much more reasonable to me than the current status quo. I will attach a patch shortly.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1157</id>
      <title>Formatable changes log (CHANGES.txt is easy to edit but not so friendly to read by Lucene users)</title>
      <description>Background in http://www.nabble.com/formatable-changes-log-tt15078749.html</description>
      <attachments/>
    </issue>
    <issue>
      <id>1166</id>
      <title>A tokenfilter to decompose compound words</title>
      <description>A tokenfilter to decompose compound words you find in many germanic languages (like German, Swedish, ...) into single tokens. An example: Donaudampfschiff would be decomposed to Donau, dampf, schiff so that you can find the word even when you only enter "Schiff". I use the hyphenation code from the Apache XML project FOP (http://xmlgraphics.apache.org/fop/) to do the first step of decomposition. Currently I use the FOP jars directly. I only use a handful of classes from the FOP project. My question now: Would it be OK to copy this classes over to the Lucene project (renaming the packages of course) or should I stick with the dependency to the FOP jars? The FOP code uses the ASF V2 license as well. What do you think?</description>
      <attachments/>
    </issue>
    <issue>
      <id>1183</id>
      <title>TRStringDistance uses way too much memory (with patch)</title>
      <description>The implementation of TRStringDistance is based on version 2.1 of org.apache.commons.lang.StringUtils#getLevenshteinDistance(String, String), which uses an un-optimized implementation of the Levenshtein Distance algorithm (it uses way too much memory). Please see Bug 38911 (http://issues.apache.org/bugzilla/show_bug.cgi?id=38911) for more information. The commons-lang implementation has been heavily optimized as of version 2.2 (3x speed-up). I have reported the new implementation to TRStringDistance.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1187</id>
      <title>Things to be done now that Filter is independent from BitSet</title>
      <description>(Aside: where is the documentation on how to mark up text in jira comments?) The following things are left over after LUCENE-584 : For Lucene 3.0 Filter.bits() will have to be removed. There is a CHECKME in IndexSearcher about using ConjunctionScorer to have the boolean behaviour of a Filter. I have not looked into Filter caching yet, but I suppose there will be some room for improvement there. Iirc the current core has moved to use OpenBitSetFilter and that is probably what is being cached. In some cases it might be better to cache a SortedVIntList instead. Boolean logic on DocIdSetIterator is already available for Scorers (that inherit from DocIdSetIterator) in the search package. This is currently implemented by ConjunctionScorer, DisjunctionSumScorer, ReqOptSumScorer and ReqExclScorer. Boolean logic on BitSets is available in contrib/misc and contrib/queries DisjunctionSumScorer calls score() on its subscorers before the score value actually needed. This could be a reason to introduce a DisjunctionDocIdSetIterator, perhaps as a superclass of DisjunctionSumScorer. To fully implement non scoring queries a TermDocIdSetIterator will be needed, perhaps as a superclass of TermScorer. The javadocs in org.apache.lucene.search using matching vs non-zero score: I'll investigate this soon, and provide a patch when necessary. An early version of the patches of LUCENE-584 contained a class Matcher, that differs from the current DocIdSet in that Matcher has an explain() method. It remains to be seen whether such a Matcher could be useful between DocIdSet and Scorer. The semantics of scorer.skipTo(scorer.doc()) was discussed briefly. This was also discussed at another issue recently, so perhaps it is wortwhile to open a separate issue for this. Skipping on a SortedVIntList is done using linear search, this could be improved by adding multilevel skiplist info much like in the Lucene index for documents containing a term. One comment by me of 3 Dec 2008: A few complete (test) classes are deprecated, it might be good to add the target release for removal there.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1195</id>
      <title>Performance improvement for TermInfosReader</title>
      <description>Currently we have a bottleneck for multi-term queries: the dictionary lookup is being done twice for each term. The first time in Similarity.idf(), where searcher.docFreq() is called. The second time when the posting list is opened (TermDocs or TermPositions). The dictionary lookup is not cheap, that's why a significant performance improvement is possible here if we avoid the second lookup. An easy way to do this is to add a small LRU cache to TermInfosReader. I ran some performance experiments with an LRU cache size of 20, and an mid-size index of 500,000 documents from wikipedia. Here are some test results: 50,000 AND queries with 3 terms each: old: 152 secs new (with LRU cache): 112 secs (26% faster) 50,000 OR queries with 3 terms each: old: 175 secs new (with LRU cache): 133 secs (24% faster) For bigger indexes this patch will probably have less impact, for smaller once more. I will attach a patch soon.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1219</id>
      <title>support array/offset/ length setters for Field with binary data</title>
      <description>currently Field/Fieldable interface supports only compact, zero based byte arrays. This forces end users to create and copy content of new objects before passing them to Lucene as such fields are often of variable size. Depending on use case, this can bring far from negligible performance improvement. this approach extends Fieldable interface with 3 new methods getOffset(); gettLenght(); and getBinaryValue() (this only returns reference to the array)</description>
      <attachments/>
    </issue>
    <issue>
      <id>1231</id>
      <title>Column-stride fields (aka per-document Payloads)</title>
      <description>This new feature has been proposed and discussed here: http://markmail.org/search/?q=per-document+payloads#query:per-document%20payloads+page:1+mid:jq4g5myhlvidw3oc+state:results Currently it is possible in Lucene to store data as stored fields or as payloads. Stored fields provide good performance if you want to load all fields for one document, because this is an sequential I/O operation. If you however want to load the data from one field for a large number of documents, then stored fields perform quite badly, because lot's of I/O seeks might have to be performed. A better way to do this is using payloads. By creating a "special" posting list that has one posting with payload for each document you can "simulate" a column- stride field. The performance is significantly better compared to stored fields, however still not optimal. The reason is that for each document the freq value, which is in this particular case always 1, has to be decoded, also one position value, which is always 0, has to be loaded. As a solution we want to add real column-stride fields to Lucene. A possible format for the new data structure could look like this (CSD stands for column- stride data, once we decide for a final name for this feature we can change this): CSDList --&gt; FixedLengthList | &lt;VariableLengthList, SkipList&gt; FixedLengthList --&gt; &lt;Payload&gt;^SegSize VariableLengthList --&gt; &lt;DocDelta, PayloadLength?, Payload&gt; Payload --&gt; Byte^PayloadLength PayloadLength --&gt; VInt SkipList --&gt; see frq.file We distinguish here between the fixed length and the variable length cases. To allow flexibility, Lucene could automatically pick the "right" data structure. This could work like this: When the DocumentsWriter writes a segment it checks whether all values of a field have the same length. If yes, it stores them as FixedLengthList, if not, then as VariableLengthList. When the SegmentMerger merges two or more segments it checks if all segments have a FixedLengthList with the same length for a column-stride field. If not, it writes a VariableLengthList to the new segment. Once this feature is implemented, we should think about making the column- stride fields updateable, similar to the norms. This will be a very powerful feature that can for example be used for low-latency tagging of documents. Other use cases: replace norms allow to store boost values separately from norms as input for the FieldCache, thus providing significantly improved loading performance (see LUCENE-831) Things that need to be done here: decide for a name for this feature - I think "column-stride fields" was liked better than "per-document payloads" Design an API for this feature. We should keep in mind here that these fields are supposed to be updateable. Define datastructures. I would like to get this feature into 2.4. Feedback about the open questions is very welcome so that we can finalize the design soon and start implementing.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1256</id>
      <title>Changes.html formatting improvements</title>
      <description>Some improvements to the Changes.html generated by the changes2html.pl script via the 'changes-to-html' ant task: Simplified the Simple stylesheet (removed monospace font specification) and made it the default. The Fancy stylesheet is really hard for me to look at (yellow text on light blue background may provide high contrast with low eye strain, but IMHO it's ugly). Moved the monospace style from the Simple stylesheet to a new stylesheet named "Fixed Width" Fixed syntax errors in the Fancy stylesheet, so that it displays as intended. Added &lt;span style="attrib"&gt; to change attributions. In the Fancy and Simple stylesheets, change attributions are colored dark green. Now properly handling change attributions in CHANGES.txt that have trailing periods. Clicking on an anchor to expand its children now changes the document location to show the children. Hovering over anchors now causes a tooltip to be displayed - either "Click to expand" or "Click to collapse" - the tooltip changes appropriately after a collapse or expansion.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1257</id>
      <title>Port to Java5</title>
      <description>For my needs I've updated Lucene so that it uses Java 5 constructs. I know Java 5 migration had been planned for 2.1 someday in the past, but don't know when it is planned now. This patch against the trunk includes : most obvious generics usage (there are tons of usages of sets, ... Those which are commonly used have been generified) PriorityQueue generification replacement of indexed for loops with for each constructs removal of unnececessary unboxing The code is to my opinion much more readable with those features (you actually know what is stored in collections reading the code, without the need to lookup for field definitions everytime) and it simplifies many algorithms. Note that this patch also includes an interface for the Query class. This has been done for my company's needs for building custom Query classes which add some behaviour to the base Lucene queries. It prevents multiple unnnecessary casts. I know this introduction is not wanted by the team, but it really makes our developments easier to maintain. If you don't want to use this, replace all /Queriable/ calls with standard /Query/.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1260</id>
      <title>Norm codec strategy in Similarity</title>
      <description>The static span and resolution of the 8 bit norms codec might not fit with all applications. My use case requires that 100f-250f is discretized in 60 bags instead of the default.. 10?</description>
      <attachments/>
    </issue>
    <issue>
      <id>1278</id>
      <title>Add optional storing of document numbers in term dictionary</title>
      <description>Add optional storing of document numbers in term dictionary. String index field cache and range filter creation will be faster. Example read code: TermEnum termEnum = indexReader.terms(TermEnum.LOAD_DOCS); do { Term term = termEnum.term(); if (term == null || term.field() != field) break; int[] docs = termEnum.docs(); } while (termEnum.next()); Example write code: Document document = new Document(); document.add(new Field("tag", "dog", Field.Store.YES, Field.Index.UN_TOKENIZED, Field.Term.STORE_DOCS)); indexWriter.addDocument(document);</description>
      <attachments/>
    </issue>
    <issue>
      <id>1279</id>
      <title>RangeQuery and RangeFilter should use collation to check for range inclusion</title>
      <description>See this java-user discussion of problems caused by Unicode code-point comparison, instead of collation, in RangeQuery. RangeQuery could take in a Locale via a setter, which could be used with a java.text.Collator and/or CollationKey's, to handle ranges for languages which have alphabet orderings different from those in Unicode.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1284</id>
      <title>Set of Java classes that allow the Lucene search engine to use morphological information developed for the Apertium open-source machine translation platform (http://www.apertium.org)</title>
      <description>Set of Java classes that allow the Lucene search engine to use morphological information developed for the Apertium open-source machine translation platform (http://www.apertium.org). Morphological information is used to index new documents and to process smarter queries in which morphological attributes can be used to specify query terms. The tool makes use of morphological analyzers and dictionaries developed for the open-source machine translation platform Apertium (http://apertium.org) and, optionally, the part-of-speech taggers developed for it. Currently there are morphological dictionaries available for Spanish, Catalan, Galician, Portuguese, Aranese, Romanian, French and English. In addition new dictionaries are being developed for Esperanto, Occitan, Basque, Swedish, Danish, Welsh, Polish and Italian, among others; we hope more language pairs to be added to the Apertium machine translation platform in the near future.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1296</id>
      <title>Allow use of compact DocIdSet in CachingWrapperFilter</title>
      <description>Extends CachingWrapperFilter with a protected method to determine the DocIdSet to be cached.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1297</id>
      <title>Allow other string distance measures in spellchecker</title>
      <description>Updated spelling code to allow for other string distance measures to be used. Created StringDistance interface. Modified existing Levenshtein distance measure to implement interface (and renamed class). Verified that change to Levenshtein distance didn't impact runtime performance. Implemented Jaro/Winkler distance metric Modified SpellChecker to take distacne measure as in constructor or in set method and to use interface when calling.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1313</id>
      <title>Near Realtime Search (using a built in RAMDirectory)</title>
      <description>Enable near realtime search in Lucene without external dependencies. When RAM NRT is enabled, the implementation adds a RAMDirectory to IndexWriter. Flushes go to the ramdir unless there is no available space. Merges are completed in the ram dir until there is no more available ram. IW.optimize and IW.commit flush the ramdir to the primary directory, all other operations try to keep segments in ram until there is no more space.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1314</id>
      <title>IndexReader.clone</title>
      <description>Based on discussion http://www.nabble.com/IndexReader.reopen-issue-td18070256.html. The problem is reopen returns the same reader if there are no changes, so if docs are deleted from the new reader, they are also reflected in the previous reader which is not always desired behavior.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1320</id>
      <title>ShingleMatrixFilter, a three dimensional permutating shingle filter</title>
      <description>Backed by a column focused matrix that creates all permutations of shingle tokens in three dimensions. I.e. it handles multi token synonyms. Could for instance in some cases be used to replaces 0-slop phrase queries with something speedier. Token[][][]{ {{hello}, {greetings, and, salutations}}, {{world}, {earth}, {tellus}} } passes the following test with 2-3 grams: assertNext(ts, "hello_world"); assertNext(ts, "greetings_and"); assertNext(ts, "greetings_and_salutations"); assertNext(ts, "and_salutations"); assertNext(ts, "and_salutations_world"); assertNext(ts, "salutations_world"); assertNext(ts, "hello_earth"); assertNext(ts, "and_salutations_earth"); assertNext(ts, "salutations_earth"); assertNext(ts, "hello_tellus"); assertNext(ts, "and_salutations_tellus"); assertNext(ts, "salutations_tellus"); Contains more and less complex tests that demonstrate offsets, posincr, payload boosts calculation and construction of a matrix from a token stream. The matrix attempts to hog as little memory as possible by seeking no more than maximumShingleSize columns forward in the stream and clearing up unused resources (columns and unique token sets). Can still be optimized quite a bit though.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1333</id>
      <title>Token implementation needs improvements</title>
      <description>This was discussed in the thread (not sure which place is best to reference so here are two): http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200805.mbox/%3C21F67CC2-EBB4-48A0-894E-FBA4AECC0D50@gmail.com%3E or to see it all at once: http://www.gossamer-threads.com/lists/lucene/java-dev/62851 Issues: 1. JavaDoc is insufficient, leading one to read the code to figure out how to use the class. 2. Deprecations are incomplete. The constructors that take String as an argument and the methods that take and/or return String should all be deprecated. 3. The allocation policy is too aggressive. With large tokens the resulting buffer can be over-allocated. A less aggressive algorithm would be better. In the thread, the Python example is good as it is computationally simple. 4. The parts of the code that currently use Token's deprecated methods can be upgraded now rather than waiting for 3.0. As it stands, filter chains that alternate between char[] and String are sub-optimal. Currently, it is used in core by Query classes. The rest are in contrib, mostly in analyzers. 5. Some internal optimizations can be done with regard to char[] allocation. 6. TokenStream has next() and next(Token), next() should be deprecated, so that reuse is maximized and descendant classes should be rewritten to over-ride next(Token) 7. Tokens are often stored as a String in a Term. It would be good to add constructors that took a Token. This would simplify the use of the two together.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1340</id>
      <title>Make it posible not to include TF information in index</title>
      <description>Term Frequency is typically not needed for all fields, some CPU (reading one VInt less and one X&gt;&gt;&gt;1...) and IO can be spared by making pure boolen fields possible in Lucene. This topic has already been discussed and accepted as a part of Flexible Indexing... This issue tries to push things a bit faster forward as I have some concrete customer demands. benefits can be expected for fields that are typical candidates for Filters, enumerations, user rights, IDs or very short "texts", phone numbers, zip codes, names... Status: just passed standard test (compatibility), commited for early review, I have not tried new feature, missing some asserts and one two unit tests Complexity: simpler than expected can be used via omitTf() (who used omitNorms() will know where to find it</description>
      <attachments/>
    </issue>
    <issue>
      <id>1343</id>
      <title>A replacement for AsciiFoldingFilter that does a more thorough job of removing diacritical marks or non-spacing modifiers.</title>
      <description>The ISOLatin1AccentFilter takes Unicode characters that have diacritical marks and replaces them with a version of that character with the diacritical mark removed. For example é becomes e. However another equally valid way of representing an accented character in Unicode is to have the unaccented character followed by a non-spacing modifier character (like this: é ) The ISOLatin1AccentFilter doesn't handle the accents in decomposed unicode characters at all. Additionally there are some instances where a word will contain what looks like an accented character, that is actually considered to be a separate unaccented character such as Ł but which to make searching easier you want to fold onto the latin1 lookalike version L . The UnicodeNormalizationFilter can filter out accents and diacritical marks whether they occur as composed characters or decomposed characters, it can also handle cases where as described above characters that look like they have diacritics (but don't) are to be folded onto the letter that they look like ( Ł -&gt; L )</description>
      <attachments/>
    </issue>
    <issue>
      <id>1344</id>
      <title>Make the Lucene jar an OSGi bundle</title>
      <description>In order to use Lucene in an OSGi environment, some additional headers are needed in the manifest of the jar. As Lucene has no dependency, it is pretty straight forward and it ill be easy to maintain I think.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1370</id>
      <title>Add ShingleFilter option to output unigrams if no shingles can be generated</title>
      <description>Currently if ShingleFilter.outputUnigrams==false and the underlying token stream is only one token long, then ShingleFilter.next() won't return any tokens. This patch provides a new option, outputUnigramIfNoNgrams; if this option is set and the underlying stream is only one token long, then ShingleFilter will return that token, regardless of the setting of outputUnigrams. My use case here is speeding up phrase queries. The technique is as follows: First, doing index-time analysis using ShingleFilter (using outputUnigrams==true), thereby expanding things as follows: "please divide this sentence into shingles" -&gt; "please", "please divide" "divide", "divide this" "this", "this sentence" "sentence", "sentence into" "into", "into shingles" "shingles" Second, do query-time analysis using ShingleFilter (using outputUnigrams==false and outputUnigramIfNoNgrams==true). If the user enters a phrase query, it will get tokenized in the following manner: "please divide this sentence into shingles" -&gt; "please divide" "divide this" "this sentence" "sentence into" "into shingles" By doing phrase queries with bigrams like this, I can gain a very considerable speedup. Without the outputUnigramIfNoNgrams option, then a single word query would tokenize like this: "please" -&gt; [no tokens] But thanks to outputUnigramIfNoNgrams, single words will now tokenize like this: "please" -&gt; "please" **** The patch also adds a little to the pre-outputUnigramIfNoNgrams option tests. **** I'm not sure if the patch in this state is useful to anyone else, but I thought I should throw it up here and try to find out.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1377</id>
      <title>Add HTMLStripReader and WordDelimiterFilter from SOLR</title>
      <description>SOLR has two classes HTMLStripReader and WordDelimiterFilter which are very useful for a wide variety of use cases. It would be good to place them into core Lucene.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1380</id>
      <title>Patch for ShingleFilter.enablePositions (or PositionFilter)</title>
      <description>Make it possible for all words and shingles to be placed at the same position, that is for all shingles (and unigrams if included) to be treated as synonyms of each other. Today the shingles generated are synonyms only to the first term in the shingle. For example the query "abcd efgh ijkl" results in: ("abcd" "abcd efgh" "abcd efgh ijkl") ("efgh" efgh ijkl") ("ijkl") where "abcd efgh" and "abcd efgh ijkl" are synonyms of "abcd", and "efgh ijkl" is a synonym of "efgh". There exists no way today to alter which token a particular shingle is a synonym for. This patch takes the first step in making it possible to make all shingles (and unigrams if included) synonyms of each other. See http://comments.gmane.org/gmane.comp.jakarta.lucene.user/34746 for mailing list thread.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1387</id>
      <title>Add LocalLucene</title>
      <description>Local Lucene (Geo-search) has been donated to the Lucene project, per https://issues.apache.org/jira/browse/INCUBATOR-77. This issue is to handle the Lucene portion of integration. See http://lucene.markmail.org/message/orzro22sqdj3wows?q=LocalLucene</description>
      <attachments/>
    </issue>
    <issue>
      <id>1390</id>
      <title>add ASCIIFoldingFilter and deprecate ISOLatin1AccentFilter</title>
      <description>The ISOLatin1AccentFilter is removing accents from accented characters in the ISO Latin 1 character set. It does what it does and there is no bug with it. It would be nicer, though, if there was a more comprehensive version of this code that included not just ISO-Latin-1 (ISO-8859-1) but the entire Latin 1 and Latin Extended A unicode blocks. See: http://en.wikipedia.org/wiki/Latin-1_Supplement_unicode_block See: http://en.wikipedia.org/wiki/Latin_Extended-A_unicode_block That way, all languages using roman characters are covered. A new class, ISOLatinAccentFilter is attached. It is intended to supercede ISOLatin1AccentFilter which should get deprecated.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1407</id>
      <title>Refactor Searchable to not have RMI Remote dependency</title>
      <description>Per http://lucene.markmail.org/message/fu34tuomnqejchfj?q=RemoteSearchable We should refactor Searchable slightly so that it doesn't extend the java.rmi.Remote marker interface. I believe the same could be achieved by just marking the RemoteSearchable and refactoring the RMI implementation out of core and into a contrib. If we do this, we should deprecate/denote it for 2.9 and then move it for 3.0</description>
      <attachments/>
    </issue>
    <issue>
      <id>1410</id>
      <title>PFOR implementation</title>
      <description>Implementation of Patched Frame of Reference.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1421</id>
      <title>Ability to group search results by field</title>
      <description>It would be awesome to group search results by specified field. Some functionality was provided for Apache Solr but I think it should be done in Core Lucene. There could be some useful information like total hits about collapsed data like total count and so on. Thanks, Artyom</description>
      <attachments/>
    </issue>
    <issue>
      <id>1422</id>
      <title>New TokenStream API</title>
      <description>This is a very early version of the new TokenStream API that we started to discuss here: http://www.gossamer-threads.com/lists/lucene/java-dev/66227 This implementation is a bit different from what I initially proposed in the thread above. I introduced a new class called AttributedToken, which contains the same termBuffer logic from Token. In addition it has a lazily-initialized map of Class&lt;? extends Attribute&gt; -&gt; Attribute. Attribute is also a new class in a new package, plus several implementations like PositionIncrementAttribute, PayloadAttribute, etc. Similar to my initial proposal is the prototypeToken() method which the consumer (e. g. DocumentsWriter) needs to call. The token is created by the tokenizer at the end of the chain and pushed through all filters to the end consumer. The tokenizer and also all filters can add Attributes to the token and can keep references to the actual types of the attributes that they need to read of modify. This way, when boolean nextToken() is called, no casting is necessary. I added a class called TestNewTokenStreamAPI which is not really a test case yet, but has a static demo() method, which demonstrates how to use the new API. The reason to not merge Token and TokenStream into one class is that we might have caching (or tee/sink) filters in the chain that might want to store cloned copies of the tokens in a cache. I added a new class NewCachingTokenStream that shows how such a class could work. I also implemented a deep clone method in AttributedToken and a copyFrom(AttributedToken) method, which is needed for the caching. Both methods have to iterate over the list of attributes. The Attribute subclasses itself also have a copyFrom(Attribute) method, which unfortunately has to down- cast to the actual type. I first thought that might be very inefficient, but it's not so bad. Well, if you add all Attributes to the AttributedToken that our old Token class had (like offsets, payload, posIncr), then the performance of the caching is somewhat slower (~40%). However, if you add less attributes, because not all might be needed, then the performance is even slightly faster than with the old API. Also the new API is flexible enough so that someone could implement a custom caching filter that knows all attributes the token can have, then the caching should be just as fast as with the old API. This patch is not nearly ready, there are lot's of things missing: unit tests change DocumentsWriter to use new API (in backwards-compatible fashion) patch is currently java 1.5; need to change before commiting to 2.9 all TokenStreams and -Filters should be changed to use new API javadocs incorrect or missing hashcode and equals methods missing in Attributes and AttributedToken I wanted to submit it already for brave people to give me early feedback before I spend more time working on this.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1424</id>
      <title>Change all multi-term querys so that they extend MultiTermQuery and allow for a constant score mode</title>
      <description>Cleans up a bunch of code duplication, closer to how things should be - design wise, gives us constant score for all the multi term queries, and allows us at least the option of highlighting the constant score queries without much further work.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1435</id>
      <title>CollationKeyFilter: convert tokens into CollationKeys encoded using IndexableBinaryStringTools</title>
      <description>Converts each token into its CollationKey using the provided collator, and then encodes the CollationKey with IndexableBinaryStringTools, to allow it to be stored as an index term. This will allow for efficient range searches and Sorts over fields that need collation for proper ordering.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1458</id>
      <title>Further steps towards flexible indexing</title>
      <description>I attached a very rough checkpoint of my current patch, to get early feedback. All tests pass, though back compat tests don't pass due to changes to package-private APIs plus certain bugs in tests that happened to work (eg call TermPostions.nextPosition() too many times, which the new API asserts against). [Aside: I think, when we commit changes to package-private APIs such that back-compat tests don't pass, we could go back, make a branch on the back-compat tag, commit changes to the tests to use the new package private APIs on that branch, then fix nightly build to use the tip of that branch?o] There's still plenty to do before this is committable! This is a rather large change: Switches to a new more efficient terms dict format. This still uses tii/tis files, but the tii only stores term &amp; long offset (not a TermInfo). At seek points, tis encodes term &amp; freq/prox offsets absolutely instead of with deltas delta. Also, tis/tii are structured by field, so we don't have to record field number in every term. . On first 1 M docs of Wikipedia, tii file is 36% smaller (0.99 MB -&gt; 0.64 MB) and tis file is 9% smaller (75.5 MB -&gt; 68.5 MB). . RAM usage when loading terms dict index is significantly less since we only load an array of offsets and an array of String (no more TermInfo array). It should be faster to init too. . This part is basically done. Introduces modular reader codec that strongly decouples terms dict from docs/positions readers. EG there is no more TermInfo used when reading the new format. . There's nice symmetry now between reading &amp; writing in the codec chain – the current docs/prox format is captured in: FormatPostingsTermsDictWriter/Reader FormatPostingsDocsWriter/Reader (.frq file) and FormatPostingsPositionsWriter/Reader (.prx file). This part is basically done. Introduces a new "flex" API for iterating through the fields, terms, docs and positions: FieldProducer -&gt; TermsEnum -&gt; DocsEnum -&gt; PostingsEnum This replaces TermEnum/Docs/Positions. SegmentReader emulates the old API on top of the new API to keep back-compat. Next steps: Plug in new codecs (pulsing, pfor) to exercise the modularity / fix any hidden assumptions. Expose new API out of IndexReader, deprecate old API but emulate old API on top of new one, switch all core/contrib users to the new API. Maybe switch to AttributeSources as the base class for TermsEnum, DocsEnum, PostingsEnum – this would give readers API flexibility (not just index-file-format flexibility). EG if someone wanted to store payload at the term-doc level instead of term-doc-position level, you could just add a new attribute. Test performance &amp; iterate.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1461</id>
      <title>Cached filter for a single term field</title>
      <description>These classes implement inexpensive range filtering over a field containing a single term. They do this by building an integer array of term numbers (storing the term-&gt;number mapping in a TreeMap) and then implementing a fast integer comparison based DocSetIdIterator. This code is currently being used to do age range filtering, but could also be used to do other date filtering or in any application where there need to be multiple filters based on the same single term field. I have an untested implementation of single term filtering and have considered but not yet implemented term set filtering (useful for location based searches) as well. The code here is fairly rough; it works but lacks javadocs and toString() and hashCode() methods etc. I'm posting it here to discover if there is other interest in this feature; I don't mind fixing it up but would hate to go to the effort if it's not going to make it into Lucene.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1466</id>
      <title>CharFilter - normalize characters before tokenizer</title>
      <description>This proposes to import CharFilter that has been introduced in Solr 1.4. Please see for the details: SOLR-822 http://www.nabble.com/Proposal-for-introducing-CharFilter-to20327007.html</description>
      <attachments/>
    </issue>
    <issue>
      <id>1470</id>
      <title>Add TrieRangeFilter to contrib</title>
      <description>According to the thread in java-dev (http://www.gossamer-threads.com/lists/lucene/java-dev/67807 and http://www.gossamer-threads.com/lists/lucene/java-dev/67839), I want to include my fast numerical range query implementation into lucene contrib-queries. I implemented (based on RangeFilter) another approach for faster RangeQueries, based on longs stored in index in a special format. The idea behind this is to store the longs in different precision in index and partition the query range in such a way, that the outer boundaries are search using terms from the highest precision, but the center of the search Range with lower precision. The implementation stores the longs in 8 different precisions (using a class called TrieUtils). It also has support for Doubles, using the IEEE 754 floating-point "double format" bit layout with some bit mappings to make them binary sortable. The approach is used in rather big indexes, query times are even on low performance desktop computers &lt;&lt;100 ms for very big ranges on indexes with 500000 docs. I called this RangeQuery variant and format "TrieRangeRange" query because the idea looks like the well-known Trie structures (but it is not identical to real tries, but algorithms are related to it).</description>
      <attachments/>
    </issue>
    <issue>
      <id>1472</id>
      <title>DateTools.stringToDate() can cause lock contention under load</title>
      <description>Load testing our application (the JIRA Issue Tracker) has shown that threads spend a lot of time blocked in DateTools.stringToDate(). The stringToDate() method uses a singleton SimpleDateFormat object to parse the dates. Each call to SimpleDateFormat.parse() is synchronized because SimpleDateFormat is not thread safe.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1476</id>
      <title>BitVector implement DocIdSet, IndexReader returns DocIdSet deleted docs</title>
      <description>Update BitVector to implement DocIdSet. Expose deleted docs DocIdSet from IndexReader.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1478</id>
      <title>Missing possibility to supply custom FieldParser when sorting search results</title>
      <description>When implementing the new TrieRangeQuery for contrib (LUCENE-1470), I was confronted by the problem that the special trie-encoded values (which are longs in a special encoding) cannot be sorted by Searcher.search() and SortField. The problem is: If you use SortField.LONG, you get NumberFormatExceptions. The trie encoded values may be sorted using SortField.String (as the encoding is in such a way, that they are sortable as Strings), but this is very memory ineffective. ExtendedFieldCache gives the possibility to specify a custom LongParser when retrieving the cached values. But you cannot use this during searching, because there is no possibility to supply this custom LongParser to the SortField. I propose a change in the sort classes: Include a pointer to the parser instance to be used in SortField (if not given use the default). My idea is to create a SortField using a new constructor SortField(String field, int type, Object parser, boolean reverse) The parser is "object" because all current parsers have no super-interface. The ideal solution would be to have: SortField(String field, int type, FieldCache.Parser parser, boolean reverse) and FieldCache.Parser is a super-interface (just empty, more like a marker-interface) of all other parsers (like LongParser...). The sort implementation then must be changed to respect the given parser (if not NULL), else use the default FieldCache.getXXXX without parser.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1482</id>
      <title>Replace infoSteram by a logging framework (SLF4J)</title>
      <description>Lucene makes use of infoStream to output messages in its indexing code only. For debugging purposes, when the search application is run on the customer side, getting messages from other code flows, like search, query parsing, analysis etc can be extremely useful. There are two main problems with infoStream today: 1. It is owned by IndexWriter, so if I want to add logging capabilities to other classes I need to either expose an API or propagate infoStream to all classes (see for example DocumentsWriter, which receives its infoStream instance from IndexWriter). 2. I can either turn debugging on or off, for the entire code. Introducing a logging framework can allow each class to control its logging independently, and more importantly, allows the application to turn on logging for only specific areas in the code (i.e., org.apache.lucene.index.*). I've investigated SLF4J (stands for Simple Logging Facade for Java) which is, as it names states, a facade over different logging frameworks. As such, you can include the slf4j.jar in your application, and it recognizes at deploy time what is the actual logging framework you'd like to use. SLF4J comes with several adapters for Java logging, Log4j and others. If you know your application uses Java logging, simply drop slf4j.jar and slf4j-jdk14.jar in your classpath, and your logging statements will use Java logging underneath the covers. This makes the logging code very simple. For a class A the logger will be instantiated like this: public class A { private static final logger = LoggerFactory.getLogger(A.class); } And will later be used like this: public class A { private static final logger = LoggerFactory.getLogger(A.class); public void foo() { if (logger.isDebugEnabled()) { logger.debug("message"); } } } That's all ! Checking for isDebugEnabled is very quick, at least using the JDK14 adapter (but I assume it's fast also over other logging frameworks). The important thing is, every class controls its own logger. Not all classes have to output logging messages, and we can improve Lucene's logging gradually, w/o changing the API, by adding more logging messages to interesting classes. I will submit a patch shortly</description>
      <attachments/>
    </issue>
    <issue>
      <id>1483</id>
      <title>Change IndexSearcher multisegment searches to search each individual segment using a single HitCollector</title>
      <description>This issue changes how an IndexSearcher searches over multiple segments. The current method of searching multiple segments is to use a MultiSegmentReader and treat all of the segments as one. This causes filters and FieldCaches to be keyed to the MultiReader and makes reopen expensive. If only a few segments change, the FieldCache is still loaded for all of them. This patch changes things by searching each individual segment one at a time, but sharing the HitCollector used across each segment. This allows FieldCaches and Filters to be keyed on individual SegmentReaders, making reopen much cheaper. FieldCache loading over multiple segments can be much faster as well - with the old method, all unique terms for every segment is enumerated against each segment - because of the likely logarithmic change in terms per segment, this can be very wasteful. Searching individual segments avoids this cost. The term/document statistics from the multireader are used to score results for each segment. When sorting, its more difficult to use a single HitCollector for each sub searcher. Ordinals are not comparable across segments. To account for this, a new field sort enabled HitCollector is introduced that is able to collect and sort across segments (because of its ability to compare ordinals across segments). This TopFieldCollector class will collect the values/ordinals for a given segment, and upon moving to the next segment, translate any ordinals/values so that they can be compared against the values for the new segment. This is done lazily. All and all, the switch seems to provide numerous performance benefits, in both sorted and non sorted search. We were seeing a good loss on indices with lots of segments (1000?) and certain queue sizes / queries, but the latest results seem to show thats been mostly taken care of (you shouldnt be using such a large queue on such a segmented index anyway). Introduces MultiReaderHitCollector - a HitCollector that can collect across multiple IndexReaders. Old HitCollectors are wrapped to support multiple IndexReaders. TopFieldCollector - a HitCollector that can compare values/ordinals across IndexReaders and sort on fields. FieldValueHitQueue - a Priority queue that is part of the TopFieldCollector implementation. FieldComparator - a new Comparator class that works across IndexReaders. Part of the TopFieldCollector implementation. FieldComparatorSource - new class to allow for custom Comparators. Alters IndexSearcher uses a single HitCollector to collect hits against each individual SegmentReader. All the other changes stem from this Deprecates TopFieldDocCollector FieldSortedHitQueue</description>
      <attachments/>
    </issue>
    <issue>
      <id>1486</id>
      <title>Wildcards, ORs etc inside Phrase queries</title>
      <description>An extension to the default QueryParser that overrides the parsing of PhraseQueries to allow more complex syntax e.g. wildcards in phrase queries. The implementation feels a little hacky - this is arguably better handled in QueryParser itself. This works as a proof of concept for much of the query parser syntax. Examples from the Junit test include: checkMatches("\"j* smyth~\"", "1,2"); //wildcards and fuzzies are OK in phrases checkMatches("\"(jo* -john) smith\"", "2"); // boolean logic works checkMatches("\"jo* smith\"~2", "1,2,3"); // position logic works. checkBadQuery("\"jo* id:1 smith\""); //mixing fields in a phrase is bad checkBadQuery("\"jo* \"smith\" \""); //phrases inside phrases is bad checkBadQuery("\"jo* [sma TO smZ]\" \""); //range queries inside phrases not supported Code plus Junit test to follow...</description>
      <attachments/>
    </issue>
    <issue>
      <id>1487</id>
      <title>FieldCacheTermsFilter</title>
      <description>This is a companion to FieldCacheRangeFilter except it operates on a set of terms rather than a range. It works best when the set is comparatively large or the terms are comparatively common.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1488</id>
      <title>multilingual analyzer based on icu</title>
      <description>The standard analyzer in lucene is not exactly unicode-friendly with regards to breaking text into words, especially with respect to non-alphabetic scripts. This is because it is unaware of unicode bounds properties. I actually couldn't figure out how the Thai analyzer could possibly be working until i looked at the jflex rules and saw that codepoint range for most of the Thai block was added to the alphanum specification. defining the exact codepoint ranges like this for every language could help with the problem but you'd basically be reimplementing the bounds properties already stated in the unicode standard. in general it looks like this kind of behavior is bad in lucene for even latin, for instance, the analyzer will break words around accent marks in decomposed form. While most latin letter + accent combinations have composed forms in unicode, some do not. (this is also an issue for asciifoldingfilter i suppose). I've got a partially tested standardanalyzer that uses icu Rule-based BreakIterator instead of jflex. Using this method you can define word boundaries according to the unicode bounds properties. After getting it into some good shape i'd be happy to contribute it for contrib but I wonder if theres a better solution so that out of box lucene will be more friendly to non-ASCII text. Unfortunately it seems jflex does not support use of these properties such as [\p {Word_Break = Extend} ] so this is probably the major barrier. Thanks, Robert</description>
      <attachments/>
    </issue>
    <issue>
      <id>1504</id>
      <title>Contrib-Spatial should use DocSet API rather then deprecated BitSet API</title>
      <description>Contrib-Spatial should be rewritten to use the new DocIdSet Filter API with OpenBitSets instead of j.u.BitSets. FilteredDocIdSet can be used to replace (I)SerialChainFilter.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1516</id>
      <title>Integrate IndexReader with IndexWriter</title>
      <description>The current problem is an IndexReader and IndexWriter cannot be open at the same time and perform updates as they both require a write lock to the index. While methods such as IW.deleteDocuments enables deleting from IW, methods such as IR.deleteDocument(int doc) and norms updating are not available from IW. This limits the capabilities of performing updates to the index dynamically or in realtime without closing the IW and opening an IR, deleting or updating norms, flushing, then opening the IW again, a process which can be detrimental to realtime updates. This patch will expose an IndexWriter.getReader method that returns the currently flushed state of the index as a class that implements IndexReader. The new IR implementation will differ from existing IR implementations such as MultiSegmentReader in that flushing will synchronize updates with IW in part by sharing the write lock. All methods of IR will be usable including reopen and clone.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1518</id>
      <title>Merge Query and Filter classes</title>
      <description>This issue presents a patch, that merges Queries and Filters in a way, that the new Filter class extends Query. This would make it possible, to use every filter as a query. The new abstract filter class would contain all methods of ConstantScoreQuery, deprecate ConstantScoreQuery. If somebody implements the Filter's getDocIdSet()/bits() methods he has nothing more to do, he could just use the filter as a normal query. I do not want to completely convert Filters to ConstantScoreQueries. The idea is to combine Queries and Filters in such a way, that every Filter can automatically be used at all places where a Query can be used (e.g. also alone a search query without any other constraint). For that, the abstract Query methods must be implemented and return a "default" weight for Filters which is the current ConstantScore Logic. If the filter is used as a real filter (where the API wants a Filter), the getDocIdSet part could be directly used, the weight is useless (as it is currently, too). The constant score default implementation is only used when the Filter is used as a Query (e.g. as direct parameter to Searcher.search()). For the special case of BooleanQueries combining Filters and Queries the idea is, to optimize the BooleanQuery logic in such a way, that it detects if a BooleanClause is a Filter (using instanceof) and then directly uses the Filter API and not take the burden of the ConstantScoreQuery (see LUCENE-1345). Here some ideas how to implement Searcher.search() with Query and Filter: User runs Searcher.search() using a Filter as the only parameter. As every Filter is also a ConstantScoreQuery, the query can be executed and returns score 1.0 for all matching documents. User runs Searcher.search() using a Query as the only parameter: No change, all is the same as before User runs Searcher.search() using a BooleanQuery as parameter: If the BooleanQuery does not contain a Query that is subclass of Filter (the new Filter) everything as usual. If the BooleanQuery only contains exactly one Filter and nothing else the Filter is used as a constant score query. If BooleanQuery contains clauses with Queries and Filters the new algorithm could be used: The queries are executed and the results filtered with the filters. For the user this has the main advantage: That he can construct his query using a simplified API without thinking about Filters oder Queries, you can just combine clauses together. The scorer/weight logic then identifies the cases to use the filter or the query weight API. Just like the query optimizer of a RDB.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1522</id>
      <title>another highlighter</title>
      <description>I've written this highlighter for my project to support bi-gram token stream (general token stream (e.g. WhitespaceTokenizer) also supported. see test code in patch). The idea was inherited from my previous project with my colleague and LUCENE-644. This approach needs highlight fields to be TermVector.WITH_POSITIONS_OFFSETS, but is fast and can support N-grams. This depends on LUCENE-1448 to get refined term offsets. usage: TopDocs docs = searcher.search( query, 10 ); Highlighter h = new Highlighter(); FieldQuery fq = h.getFieldQuery( query ); for( ScoreDoc scoreDoc : docs.scoreDocs ){ // fieldName="content", fragCharSize=100, numFragments=3 String[] fragments = h.getBestFragments( fq, reader, scoreDoc.doc, "content", 100, 3 ); if( fragments != null ){ for( String fragment : fragments ) System.out.println( fragment ); } } features: fast for large docs supports not only whitespace-based token stream, but also "fixed size" N-gram (e.g. (2,2), not (1,3)) (can solve LUCENE-1489) supports PhraseQuery, phrase-unit highlighting with slops q="w1 w2" &lt;b&gt;w1 w2&lt;/b&gt; --------------- q="w1 w2"~1 &lt;b&gt;w1&lt;/b&gt; w3 &lt;b&gt;w2&lt;/b&gt; w3 &lt;b&gt;w1 w2&lt;/b&gt; highlight fields need to be TermVector.WITH_POSITIONS_OFFSETS easy to apply patch due to independent package (contrib/highlighter2) uses Java 1.5 looks query boost to score fragments (currently doesn't see idf, but it should be possible) pluggable FragListBuilder pluggable FragmentsBuilder to do: term positions can be unnecessary when phraseHighlight==false collects performance numbers</description>
      <attachments/>
    </issue>
    <issue>
      <id>1526</id>
      <title>For near real-time search, use paged copy-on-write BitVector impl</title>
      <description>SegmentReader currently uses a BitVector to represent deleted docs. When performing rapid clone (see LUCENE-1314) and delete operations, performing a copy on write of the BitVector can become costly because the entire underlying byte array must be created and copied. A way to make this clone delete process faster is to implement tombstones, a term coined by Marvin Humphrey. Tombstones represent new deletions plus the incremental deletions from previously reopened readers in the current reader. The proposed implementation of tombstones is to accumulate deletions into an int array represented as a DocIdSet. With LUCENE-1476, SegmentTermDocs iterates over deleted docs using a DocIdSet rather than accessing the BitVector by calling get. This allows a BitVector and a set of tombstones to by ANDed together as the current reader's delete docs. A tombstone merge policy needs to be defined to determine when to merge tombstone DocIdSets into a new deleted docs BitVector as too many tombstones would eventually be detrimental to performance. A probable implementation will merge tombstones based on the number of tombstones and the total number of documents in the tombstones. The merge policy may be set in the clone/reopen methods or on the IndexReader.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1532</id>
      <title>File based spellcheck with doc frequencies supplied</title>
      <description>The file-based spellchecker treats all words in the dictionary as equally valid, so it can suggest a very obscure word rather than a more common word which is equally close to the misspelled word that was entered. It would be very useful to have the option of supplying an integer with each word which indicates its commonness. I.e. the integer could be the document frequency in some index or set of indexes. I've implemented a modification to the spellcheck API to support this by defining a DocFrequencyInfo interface for obtaining the doc frequency of a word, and a class which implements the interface by looking up the frequency in an index. So Lucene users can provide alternative implementations of DocFrequencyInfo. I could submit this as a patch if there is interest. Alternatively, it might be better to just extend the spellcheck API to have a way to supply the frequencies when you create a PlainTextDictionary, but that would mean storing the frequencies somewhere when building the spellcheck index, and I'm not sure how best to do that.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1536</id>
      <title>if a filter can support random access API, we should use it</title>
      <description>I ran some performance tests, comparing applying a filter via random-access API instead of current trunk's iterator API. This was inspired by LUCENE-1476, where we realized deletions should really be implemented just like a filter, but then in testing found that switching deletions to iterator was a very sizable performance hit. Some notes on the test: Index is first 2M docs of Wikipedia. Test machine is Mac OS X 10.5.6, quad core Intel CPU, 6 GB RAM, java 1.6.0_07-b06-153. I test across multiple queries. 1-X means an OR query, eg 1-4 means 1 OR 2 OR 3 OR 4, whereas +1-4 is an AND query, ie 1 AND 2 AND 3 AND 4. "u s" means "united states" (phrase search). I test with multiple filter densities (0, 1, 2, 5, 10, 25, 75, 90, 95, 98, 99, 99.99999 (filter is non-null but all bits are set), 100 (filter=null, control)). Method high means I use random-access filter API in IndexSearcher's main loop. Method low means I use random-access filter API down in SegmentTermDocs (just like deleted docs today). Baseline (QPS) is current trunk, where filter is applied as iterator up "high" (ie in IndexSearcher's search loop).</description>
      <attachments/>
    </issue>
    <issue>
      <id>1539</id>
      <title>Improve Benchmark</title>
      <description>Benchmark can be improved by incorporating recent suggestions posted on java-dev. M. McCandless' Python scripts that execute multiple rounds of tests can either be incorporated into the codebase or converted to Java.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1540</id>
      <title>Improvements to contrib.benchmark for TREC collections</title>
      <description>The benchmarking utilities for TREC test collections (http://trec.nist.gov) are quite limited and do not support some of the variations in format of older TREC collections. I have been doing some benchmarking work with Lucene and have had to modify the package to support: Older TREC document formats, which the current parser fails on due to missing document headers. Variations in query format - newlines after &lt;title&gt; tag causing the query parser to get confused. Ability to detect and read in uncompressed text collections Storage of document numbers by default without storing full text. I can submit a patch if there is interest, although I will probably want to write unit tests for the new functionality first.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1567</id>
      <title>New flexible query parser</title>
      <description>From "New flexible query parser" thread by Micheal Busch in my team at IBM we have used a different query parser than Lucene's in our products for quite a while. Recently we spent a significant amount of time in refactoring the code and designing a very generic architecture, so that this query parser can be easily used for different products with varying query syntaxes. This work was originally driven by Andreas Neumann (who, however, left our team); most of the code was written by Luis Alves, who has been a bit active in Lucene in the past, and Adriano Campos, who joined our team at IBM half a year ago. Adriano is Apache committer and PMC member on the Tuscany project and getting familiar with Lucene now too. We think this code is much more flexible and extensible than the current Lucene query parser, and would therefore like to contribute it to Lucene. I'd like to give a very brief architecture overview here, Adriano and Luis can then answer more detailed questions as they're much more familiar with the code than I am. The goal was it to separate syntax and semantics of a query. E.g. 'a AND b', '+a +b', 'AND(a,b)' could be different syntaxes for the same query. We distinguish the semantics of the different query components, e.g. whether and how to tokenize/lemmatize/normalize the different terms or which Query objects to create for the terms. We wanted to be able to write a parser with a new syntax, while reusing the underlying semantics, as quickly as possible. In fact, Adriano is currently working on a 100% Lucene-syntax compatible implementation to make it easy for people who are using Lucene's query parser to switch. The query parser has three layers and its core is what we call the QueryNodeTree. It is a tree that initially represents the syntax of the original query, e.g. for 'a AND b': AND / \ A B The three layers are: 1. QueryParser 2. QueryNodeProcessor 3. QueryBuilder 1. The upper layer is the parsing layer which simply transforms the query text string into a QueryNodeTree. Currently our implementations of this layer use javacc. 2. The query node processors do most of the work. It is in fact a configurable chain of processors. Each processors can walk the tree and modify nodes or even the tree's structure. That makes it possible to e.g. do query optimization before the query is executed or to tokenize terms. 3. The third layer is also a configurable chain of builders, which transform the QueryNodeTree into Lucene Query objects. Furthermore the query parser uses flexible configuration objects, which are based on AttributeSource/Attribute. It also uses message classes that allow to attach resource bundles. This makes it possible to translate messages, which is an important feature of a query parser. This design allows us to develop different query syntaxes very quickly. Adriano wrote the Lucene-compatible syntax in a matter of hours, and the underlying processors and builders in a few days. We now have a 100% compatible Lucene query parser, which means the syntax is identical and all query parser test cases pass on the new one too using a wrapper. Recent posts show that there is demand for query syntax improvements, e.g improved range query syntax or operator precedence. There are already different QP implementations in Lucene+contrib, however I think we did not keep them all up to date and in sync. This is not too surprising, because usually when fixes and changes are made to the main query parser, people don't make the corresponding changes in the contrib parsers. (I'm guilty here too) With this new architecture it will be much easier to maintain different query syntaxes, as the actual code for the first layer is not very much. All syntaxes would benefit from patches and improvements we make to the underlying layers, which will make supporting different syntaxes much more manageable.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1574</id>
      <title>PooledSegmentReader, pools SegmentReader underlying byte arrays</title>
      <description>PooledSegmentReader pools the underlying byte arrays of deleted docs and norms for realtime search. It is designed for use with IndexReader.clone which can create many copies of byte arrays, which are of the same length for a given segment. When pooled they can be reused which could save on memory. Do we want to benchmark the memory usage comparison of PooledSegmentReader vs GC? Many times GC is enough for these smaller objects.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1575</id>
      <title>Refactoring Lucene collectors (HitCollector and extensions)</title>
      <description>This issue is a result of a recent discussion we've had on the mailing list. You can read the thread here. We have agreed to do the following refactoring: Rename MultiReaderHitCollector to Collector, with the purpose that it will be the base class for all Collector implementations. Deprecate HitCollector in favor of the new Collector. Introduce new methods in IndexSearcher that accept Collector, and deprecate those that accept HitCollector. Create a final class HitCollectorWrapper, and use it in the deprecated methods in IndexSearcher, wrapping the given HitCollector. HitCollectorWrapper will be marked deprecated, so we can remove it in 3.0, when we remove HitCollector. It will remove any instanceof checks that currently exist in IndexSearcher code. Create a new (abstract) TopDocsCollector, which will: Leave collect and setNextReader unimplemented. Introduce protected members PriorityQueue and totalHits. Introduce a single protected constructor which accepts a PriorityQueue. Implement topDocs() and getTotalHits() using the PQ and totalHits members. These can be used as-are by extending classes, as well as be overridden. Introduce a new topDocs(start, howMany) method which will be used a convenience method when implementing a search application which allows paging through search results. It will also attempt to improve the memory allocation, by allocating a ScoreDoc[] of the requested size only. Change TopScoreDocCollector to extend TopDocsCollector, use the topDocs() and getTotalHits() implementations as they are from TopDocsCollector. The class will also be made final. Change TopFieldCollector to extend TopDocsCollector, and make the class final. Also implement topDocs(start, howMany). Change TopFieldDocCollector (deprecated) to extend TopDocsCollector, instead of TopScoreDocCollector. Implement topDocs(start, howMany) Review other places where HitCollector is used, such as in Scorer, deprecate those places and use Collector instead. Additionally, the following proposal was made w.r.t. decoupling score from collect(): Change collect to accecpt only a doc Id (unbased). Introduce a setScorer(Scorer) method. If during collect the implementation needs the score, it can call scorer.score(). If we do this, then we need to review all places in the code where collect(doc, score) is called, and assert whether Scorer can be passed. Also this raises few questions: What if during collect() Scorer is null? (i.e., not set) - is it even possible? I noticed that many (if not all) of the collect() implementations discard the document if its score is not greater than 0. Doesn't it mean that score is needed in collect() always? Open issues: The name for Collector TopDocsCollector was mentioned on the thread as TopResultsCollector, but that was when we thought to call Colletor ResultsColletor. Since we decided (so far) on Collector, I think TopDocsCollector makes sense, because of its TopDocs output. Decoupling score from collect(). I will post a patch a bit later, as this is expected to be a very large patch. I will split it into 2: (1) code patch (2) test cases (moving to use Collector instead of HitCollector, as well as testing the new topDocs(start, howMany) method. There might be even a 3rd patch which handles the setScorer thing in Collector (maybe even a different issue?)</description>
      <attachments/>
    </issue>
    <issue>
      <id>1581</id>
      <title>LowerCaseFilter should be able to be configured to use a specific locale.</title>
      <description>//Since I am a .Net programmer, Sample codes will be in c# but I don't think that it would be a problem to understand them. // Assume an input text like "İ" and and analyzer like below public class SomeAnalyzer : Analyzer { public override TokenStream TokenStream(string fieldName, System.IO.TextReader reader) { TokenStream t = new SomeTokenizer(reader); t = new Lucene.Net.Analysis.ASCIIFoldingFilter(t); t = new LowerCaseFilter(t); return t; } } ASCIIFoldingFilter will return "I" and after, LowerCaseFilter will return "i" (if locale is "en-US") or "ı' if(locale is "tr-TR") (that means,this token should be input to another instance of ASCIIFoldingFilter) So, calling LowerCaseFilter before ASCIIFoldingFilter would be a solution, but a better approach can be adding a new constructor to LowerCaseFilter and forcing it to use a specific locale. public sealed class LowerCaseFilter : TokenFilter { /* +++ */System.Globalization.CultureInfo CultureInfo = System.Globalization.CultureInfo.CurrentCulture; public LowerCaseFilter(TokenStream in) : base(in) { } /* +++ */ public LowerCaseFilter(TokenStream in, System.Globalization.CultureInfo CultureInfo) : base(in) /* +++ */ { /* +++ */ this.CultureInfo = CultureInfo; /* +++ */ } public override Token Next(Token result) { result = Input.Next(result); if (result != null) { char[] buffer = result.TermBuffer(); int length = result.termLength; for (int i = 0; i &lt; length; i++) /* +++ */ buffer[i] = System.Char.ToLower(buffer[i],CultureInfo); return result; } else return null; } } DIGY</description>
      <attachments/>
    </issue>
    <issue>
      <id>1584</id>
      <title>Callback for intercepting merging segments in IndexWriter</title>
      <description>For things like merging field caches or bitsets, it's useful to know which segments were merged to create a new segment.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1585</id>
      <title>Allow to control how payloads are merged</title>
      <description>Lucene handles backwards-compatibility of its data structures by converting them from the old into the new formats during segment merging. Payloads are simply byte arrays in which users can store arbitrary data. Applications that use payloads might want to convert the format of their payloads in a similar fashion. Otherwise it's not easily possible to ever change the encoding of a payload without reindexing. So I propose to introduce a PayloadMerger class that the SegmentMerger invokes to merge the payloads from multiple segments. Users can then implement their own PayloadMerger to convert payloads from an old into a new format. In the future we need this kind of flexibility also for column-stride fields (LUCENE-1231) and flexible indexing codecs. In addition to that it would be nice if users could store version information in the segments file. E.g. they could store "in segment _2 the term a:b uses payloads of format x.y".</description>
      <attachments/>
    </issue>
    <issue>
      <id>1591</id>
      <title>Enable bzip compression in benchmark</title>
      <description>bzip compression can aid the benchmark package by not requiring extracting bzip files (such as enwiki) in order to index them. The plan is to add a config parameter bzip.compression=true/false and in the relevant tasks either decompress the input file or compress the output file using the bzip streams. It will add a dependency on ant.jar which contains two classes similar to GZIPOutputStream and GZIPInputStream which compress/decompress files using the bzip algorithm. bzip is known to be superior in its compression performance to the gzip algorithm (~20% better compression), although it does the compression/decompression a bit slower. I wil post a patch which adds this parameter and implement it in LineDocMaker, EnwikiDocMaker and WriteLineDoc task. Maybe even add the capability to DocMaker or some of the super classes, so it can be inherited by all sub-classes.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1593</id>
      <title>Optimizations to TopScoreDocCollector and TopFieldCollector</title>
      <description>This is a spin-off of LUCENE-1575 and proposes to optimize TSDC and TFC code to remove unnecessary checks. The plan is: Ensure that IndexSearcher returns segements in increasing doc Id order, instead of numDocs(). Change TSDC and TFC's code to not use the doc id as a tie breaker. New docs will always have larger ids and therefore cannot compete. Pre-populate HitQueue with sentinel values in TSDC (score = Float.NEG_INF) and remove the check if reusableSD == null. Also move to use "changing top" and then call adjustTop(), in case we update the queue. some methods in Sort explicitly add SortField.FIELD_DOC as a "tie breaker" for the last SortField. But, doing so should not be necessary (since we already break ties by docID), and is in fact less efficient (once the above optimization is in). Investigate PQ - can we deprecate insert() and have only insertWithOverflow()? Add a addDummyObjects method which will populate the queue without "arranging" it, just store the objects in the array (this can be used to pre-populate sentinel values)? I will post a patch as well as some perf measurements as soon as I have them.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1595</id>
      <title>Split DocMaker into ContentSource and DocMaker</title>
      <description>This issue proposes some refactoring to the benchmark package. Today, DocMaker has two roles: collecting documents from a collection and preparing a Document object. These two should actually be split up to ContentSource and DocMaker, which will use a ContentSource instance. ContentSource will implement all the methods of DocMaker, like getNextDocData, raw size in bytes tracking etc. This can actually fit well w/ 1591, by having a basic ContentSource that offers input stream services, and wraps a file (for example) with a bzip or gzip streams etc. DocMaker will implement the makeDocument methods, reusing DocState etc. The idea is that collecting the Enwiki documents, for example, should be the same whether I create documents using DocState, add payloads or index additional metadata. Same goes for Trec and Reuters collections, as well as LineDocMaker. In fact, if one inspects EnwikiDocMaker and LineDocMaker closely, they are 99% the same and 99% different. Most of their differences lie in the way they read the data, while most of the similarity lies in the way they create documents (using DocState). That led to a somehwat bizzare extension of LineDocMaker by EnwikiDocMaker (just the reuse of DocState). Also, other DocMakers do not use that DocState today, something they could have gotten for free with this refactoring proposed. So by having a EnwikiContentSource, ReutersContentSource and others (TREC, Line, Simple), I can write several DocMakers, such as DocStateMaker, ConfigurableDocMaker (one which accpets all kinds of config options) and custom DocMakers (payload, facets, sorting), passing to them a ContentSource instance and reuse the same DocMaking algorithm with many content sources, as well as the same ContentSource algorithm with many DocMaker implementations. This will also give us the opportunity to perf test content sources alone (i.e., compare bzip, gzip and regular input streams), w/o the overhead of creating a Document object. I've already done so in my code environment (I extend the benchmark package for my application's purposes) and I like the flexibility I have. I think this can be a nice contribution to the benchmark package, which can result in some code cleanup as well.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1602</id>
      <title>Rewrite TrieRange to use MultiTermQuery</title>
      <description>Issue for discussion here: http://www.lucidimagination.com/search/document/46a548a79ae9c809/move_trierange_to_core_module_and_integration_issues This patch is a rewrite of TrieRange using MultiTermQuery like all other core queries. This should make TrieRange identical in functionality to core range queries.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1603</id>
      <title>Changes for TrieRange in FilteredTermEnum and MultiTermQuery improvement</title>
      <description>This is a patch, that is needed for the MultiTermQuery-rewrite of TrieRange (LUCENE-1602): Make the private members protected, to have access to them from the very special TrieRangeTermEnum Fix a small inconsistency (docFreq() now only returns a value, if a valid term is existing) Improvement of MultiTermFilter.getDocIdSet to return DocIdSet.EMPTY_DOCIDSET, if the TermEnum is empty (less memory usage) and faster. Add the getLastNumberOfTerms() to MultiTermQuery for statistics on different multi term queries and how may terms they affect, using this new functionality, the improvement of TrieRange can be shown (extract from test case there, 10000 docs index, long values): [junit] Average number of terms during random search on 'field8': [junit] Trie query: 244.2 [junit] Classical query: 3136.94 [junit] Average number of terms during random search on 'field4': [junit] Trie query: 38.3 [junit] Classical query: 3018.68 [junit] Average number of terms during random search on 'field2': [junit] Trie query: 18.04 [junit] Classical query: 3539.42 All core tests pass.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1604</id>
      <title>Stop creating huge arrays to represent the absense of field norms</title>
      <description>Creating and keeping around huge arrays that hold a constant value is very inefficient both from a heap usage standpoint and from a localility of reference standpoint. It would be much more efficient to use null to represent a missing norms table.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1606</id>
      <title>Automaton Query/Filter (scalable regex)</title>
      <description>Attached is a patch for an AutomatonQuery/Filter (name can change if its not suitable). Whereas the out-of-box contrib RegexQuery is nice, I have some very large indexes (100M+ unique tokens) where queries are quite slow, 2 minutes, etc. Additionally all of the existing RegexQuery implementations in Lucene are really slow if there is no constant prefix. This implementation does not depend upon constant prefix, and runs the same query in 640ms. Some use cases I envision: 1. lexicography/etc on large text corpora 2. looking for things such as urls where the prefix is not constant (http:// or ftp://) The Filter uses the BRICS package (http://www.brics.dk/automaton/) to convert regular expressions into a DFA. Then, the filter "enumerates" terms in a special way, by using the underlying state machine. Here is my short description from the comments: The algorithm here is pretty basic. Enumerate terms but instead of a binary accept/reject do: 1. Look at the portion that is OK (did not enter a reject state in the DFA) 2. Generate the next possible String and seek to that. the Query simply wraps the filter with ConstantScoreQuery. I did not include the automaton.jar inside the patch but it can be downloaded from http://www.brics.dk/automaton/ and is BSD-licensed.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1607</id>
      <title>String.intern() faster alternative</title>
      <description>By using our own interned string pool on top of default, String.intern() can be greatly optimized. On my setup (java 6) this alternative runs ~15.8x faster for already interned strings, and ~2.2x faster for 'new String(interned)' For java 5 and 4 speedup is lower, but still considerable.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1609</id>
      <title>Eliminate synchronization contention on initial index reading in TermInfosReader ensureIndexIsRead</title>
      <description>synchronized method ensureIndexIsRead in TermInfosReader causes contention under heavy load Simple to reproduce: e.g. Under Solr, with all caches turned off, do a simple range search e.g. id:[0 TO 999999] on even a small index (in my case 28K docs) and under a load/stress test application, and later, examining the Thread dump (kill -3) , many threads are blocked on 'waiting for monitor entry' to this method. Rather than using Double-Checked Locking which is known to have issues, this implementation uses a state pattern, where only one thread can move the object from IndexNotRead state to IndexRead, and in doing so alters the objects behavior, i.e. once the index is loaded, the index nolonger needs a synchronized method. In my particular test, this uncreased throughput at least 30 times.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1614</id>
      <title>Add next() and skipTo() variants to DocIdSetIterator that return the current doc, instead of boolean</title>
      <description>See http://www.nabble.com/Another-possible-optimization---now-in-DocIdSetIterator-p23223319.html for the full discussion. The basic idea is to add variants to those two methods that return the current doc they are at, to save successive calls to doc(). If there are no more docs, return -1. A summary of what was discussed so far: Deprecate those two methods. Add nextDoc() and skipToDoc(int) that return doc, with default impl in DISI (calls next() and skipTo() respectively, and will be changed to abstract in 3.0). I actually would like to propose an alternative to the names: advance() and advance(int) - the first advances by one, the second advances to target. Wherever these are used, do something like '(doc = advance()) &gt;= 0' instead of comparing to -1 for improved performance. I will post a patch shortly</description>
      <attachments/>
    </issue>
    <issue>
      <id>1616</id>
      <title>add one setter for start and end offset to OffsetAttribute</title>
      <description>add OffsetAttribute. setOffset(startOffset, endOffset); trivial change, no JUnit needed Changed CharTokenizer to use it</description>
      <attachments/>
    </issue>
    <issue>
      <id>1618</id>
      <title>Allow setting the IndexWriter docstore to be a different directory</title>
      <description>Add an IndexWriter.setDocStoreDirectory method that allows doc stores to be placed in a different directory than the IW default dir.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1628</id>
      <title>Persian Analyzer</title>
      <description>A simple persian analyzer. i measured trec scores with the benchmark package below against http://ece.ut.ac.ir/DBRG/Hamshahri/ : SimpleAnalyzer: SUMMARY Search Seconds: 0.012 DocName Seconds: 0.020 Num Points: 981.015 Num Good Points: 33.738 Max Good Points: 36.185 Average Precision: 0.374 MRR: 0.667 Recall: 0.905 Precision At 1: 0.585 Precision At 2: 0.531 Precision At 3: 0.513 Precision At 4: 0.496 Precision At 5: 0.486 Precision At 6: 0.487 Precision At 7: 0.479 Precision At 8: 0.465 Precision At 9: 0.458 Precision At 10: 0.460 Precision At 11: 0.453 Precision At 12: 0.453 Precision At 13: 0.445 Precision At 14: 0.438 Precision At 15: 0.438 Precision At 16: 0.438 Precision At 17: 0.429 Precision At 18: 0.429 Precision At 19: 0.419 Precision At 20: 0.415 PersianAnalyzer: SUMMARY Search Seconds: 0.004 DocName Seconds: 0.011 Num Points: 987.692 Num Good Points: 36.123 Max Good Points: 36.185 Average Precision: 0.481 MRR: 0.833 Recall: 0.998 Precision At 1: 0.754 Precision At 2: 0.715 Precision At 3: 0.646 Precision At 4: 0.646 Precision At 5: 0.631 Precision At 6: 0.621 Precision At 7: 0.593 Precision At 8: 0.577 Precision At 9: 0.573 Precision At 10: 0.566 Precision At 11: 0.572 Precision At 12: 0.562 Precision At 13: 0.554 Precision At 14: 0.549 Precision At 15: 0.542 Precision At 16: 0.538 Precision At 17: 0.533 Precision At 18: 0.527 Precision At 19: 0.525 Precision At 20: 0.518</description>
      <attachments/>
    </issue>
    <issue>
      <id>1629</id>
      <title>contrib intelligent Analyzer for Chinese</title>
      <description>I wrote a Analyzer for apache lucene for analyzing sentences in Chinese language. it's called "imdict-chinese-analyzer", the project on google code is here: http://code.google.com/p/imdict-chinese-analyzer/ In Chinese, "我是中国人"(I am Chinese), should be tokenized as "我"(I) "是"(am) "中国人"(Chinese), not "我" "是中" "国人". So the analyzer must handle each sentence properly, or there will be mis-understandings everywhere in the index constructed by Lucene, and the accuracy of the search engine will be affected seriously! Although there are two analyzer packages in apache repository which can handle Chinese: ChineseAnalyzer and CJKAnalyzer, they take each character or every two adjoining characters as a single word, this is obviously not true in reality, also this strategy will increase the index size and hurt the performance baddly. The algorithm of imdict-chinese-analyzer is based on Hidden Markov Model (HMM), so it can tokenize chinese sentence in a really intelligent way. Tokenizaion accuracy of this model is above 90% according to the paper "HHMM-based Chinese Lexical analyzer ICTCLAL" while other analyzer's is about 60%. As imdict-chinese-analyzer is a really fast and intelligent. I want to contribute it to the apache lucene repository.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1630</id>
      <title>Mating Collector and Scorer on doc Id orderness</title>
      <description>This is a spin off of LUCENE-1593. This issue proposes to expose appropriate API on Scorer and Collector such that one can create an optimized Collector based on a given Scorer's doc-id orderness and vice versa. Copied from LUCENE-1593, here is the list of changes: Deprecate Weight and create QueryWeight (abstract class) with a new scorer(reader, scoreDocsInOrder), replacing the current scorer(reader) method. QueryWeight implements Weight, while score(reader) calls score(reader, false /* out-of-order */) and scorer(reader, scoreDocsInOrder) is defined abstract. Also add QueryWeightWrapper to wrap a given Weight implementation. This one will also be deprecated, as well as package-private. Add to Query variants of createWeight and weight which return QueryWeight. For now, I prefer to add a default impl which wraps the Weight variant instead of overriding in all Query extensions, and in 3.0 when we remove the Weight variants - override in all extending classes. Add to Scorer isOutOfOrder with a default to false, and override in BS to true. Modify BooleanWeight to extend QueryWeight and implement the new scorer method to return BS2 or BS based on the number of required scorers and setAllowOutOfOrder. Add to Collector an abstract acceptsDocsOutOfOrder which returns true/false. Use it in IndexSearcher.search methods, that accept a Collector, in order to create the appropriate Scorer, using the new QueryWeight. Provide a static create method to TFC and TSDC which accept this as an argument and creates the proper instance. Wherever we create a Collector (TSDC or TFC), always ask for out-of-order Scorer and check on the resulting Scorer isOutOfOrder(), so that we can create the optimized Collector instance. Modify IndexSearcher to use all of the above logic. The only class I'm worried about, and would like to verify with you, is Searchable. If we want to deprecate all the search methods on IndexSearcher, Searcher and Searchable which accept Weight and add new ones which accept QueryWeight, we must do the following: Deprecate Searchable in favor of Searcher. Add to Searcher the new QueryWeight variants. Here we have two choices: (1) break back-compat and add them as abstract (like we've done with the new Collector method) or (2) add them with a default impl to call the Weight versions, documenting these will become abstract in 3.0. Have Searcher extend UnicastRemoteObject and have RemoteSearchable extend Searcher. That's the part I'm a little bit worried about - Searchable implements java.rmi.Remote, which means there could be an implementation out there which implements Searchable and extends something different than UnicastRemoteObject, like Activeable. I think there is very small chance this has actually happened, but would like to confirm with you guys first. Add a deprecated, package-private, SearchableWrapper which extends Searcher and delegates all calls to the Searchable member. Deprecate all uses of Searchable and add Searcher instead, defaulting the old ones to use SearchableWrapper. Make all the necessary changes to IndexSearcher, MultiSearcher etc. regarding overriding these new methods. One other optimization that was discussed in LUCENE-1593 is to expose a topScorer() API (on Weight) which returns a Scorer that its score(Collector) will be called, and additionally add a start() method to DISI. That will allow Scorers to initialize either on start() or score(Collector). This was proposed mainly because of BS and BS2 which check if they are initialized in every call to next(), skipTo() and score(). Personally I prefer to see that in a separate issue, following that one (as it might add methods to QueryWeight).</description>
      <attachments/>
    </issue>
    <issue>
      <id>1634</id>
      <title>LogMergePolicy should use the number of deleted docs when deciding which segments to merge</title>
      <description>I found that IndexWriter.optimize(int) method does not pick up large segments with a lot of deletes even when most of the docs are deleted. And the existence of such segments affected the query performance significantly. I created an index with 1 million docs, then went over all docs and updated a few thousand at a time. I ran optimize(20) occasionally. What saw were large segments with most of docs deleted. Although these segments did not have valid docs they remained in the directory for a very long time until more segments with comparable or bigger sizes were created. This is because LogMergePolicy.findMergeForOptimize uses the size of segments but does not take the number of deleted documents into consideration when it decides which segments to merge. So, a simple fix is to use the delete count to calibrate the segment size. I can create a patch for this.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1644</id>
      <title>Enable MultiTermQuery's constant score mode to also use BooleanQuery under the hood</title>
      <description>When MultiTermQuery is used (via one of its subclasses, eg WildcardQuery, PrefixQuery, FuzzyQuery, etc.), you can ask it to use "constant score mode", which pre-builds a filter and then wraps that filter as a ConstantScoreQuery. If you don't set that, it instead builds a [potentially massive] BooleanQuery with one SHOULD clause per term. There are some limitations of this approach: The scores returned by the BooleanQuery are often quite meaningless to the app, so, one should be able to use a BooleanQuery yet get constant scores back. (Though I vaguely remember at least one example someone raised where the scores were useful...). The resulting BooleanQuery can easily have too many clauses, throwing an extremely confusing exception to newish users. It'd be better to have the freedom to pick "build filter up front" vs "build massive BooleanQuery", when constant scoring is enabled, because they have different performance tradeoffs. In constant score mode, an OpenBitSet is always used, yet for sparse bit sets this does not give good performance. I think we could address these issues by giving BooleanQuery a constant score mode, then empower MultiTermQuery (when in constant score mode) to pick &amp; choose whether to use BooleanQuery vs up-front filter, and finally empower MultiTermQuery to pick the best (sparse vs dense) bit set impl.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1658</id>
      <title>Absorb NIOFSDirectory into FSDirectory</title>
      <description>I think whether one uses java.io.* vs java.nio.* or eventually java.nio2.*, or some other means, is an under-the-hood implementation detail of FSDirectory and doesn't merit a whole separate class. I think FSDirectory should be the core class one uses when one's index is in the filesystem. So, I'd like to deprecate NIOFSDirectory, absorbing it into FSDirectory, and add a setting "useNIO" to FSDirectory. It should default to "true" for non-Windows OSs, because it gives far better concurrent performance on all platforms but Windows (due to known Sun JRE issue http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734).</description>
      <attachments/>
    </issue>
    <issue>
      <id>1673</id>
      <title>Move TrieRange to core</title>
      <description>TrieRange was iterated many times and seems stable now (LUCENE-1470, LUCENE-1582, LUCENE-1602). There is lots of user interest, Solr added it to its default FieldTypes (SOLR-940) and if possible I want to move it to core before release of 2.9. Before this can be done, there are some things to think about: There are now classes called LongTrieRangeQuery, IntTrieRangeQuery, how should they be called in core? I would suggest to leave it as it is. On the other hand, if this keeps our only numeric query implementation, we could call it LongRangeQuery, IntRangeQuery or NumericRangeQuery (see below, here are problems). Same for the TokenStreams and Filters. Maybe the pairs of classes for indexing and searching should be moved into one class: NumericTokenStream, NumericRangeQuery, NumericRangeFilter. The problem here: ctors must be able to pass int, long, double, float as range parameters. For the end user, mixing these 4 types in one class is hard to handle. If somebody forgets to add a L to a long, it suddenly instantiates a int version of range query, hitting no results and so on. Same with other types. Maybe accept java.lang.Number as parameter (because nullable for half-open bounds) and one enum for the type. TrieUtils move into o.a.l.util? or document or? Move TokenStreams into o.a.l.analysis, ShiftAttribute into o.a.l.analysis.tokenattributes? Somewhere else? If we rename the classes, should Solr stay with Trie (because there are different impls)? Maybe add a subclass of AbstractField, that automatically creates these TokenStreams and omits norms/tf per default for easier addition to Document instances?</description>
      <attachments/>
    </issue>
    <issue>
      <id>1685</id>
      <title>Make the Highlighter use SpanScorer by default</title>
      <description>I've always thought this made sense, but frankly, it took me a year to get the SpanScorer included with Lucene at all, so I was pretty much ready to move on after I it got in, rather than push for it as a default. I think it makes sense as the default in Solr as well, and I mentioned that back when it was put in, but alas, its an option there as well. The Highlighter package has no back compat req, but custom has been conservative - one reason I havn't pushed for this change before. Might be best to actually make the switch in 3? I could go either way - as is, I know a bunch of people use it, but I'm betting its the large minority. It has never been listed in a changes entry and its not in LIA 1, so you pretty much have to stumble upon it, and figure out what its for. I'll point out again that its just as fast as the standard scorer for any clause of a query that is not position sensitive. Position sensitive query clauses will obviously be somewhat slower to highlight, but that is because they will be highlighted correctly rather than ignoring position.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1687</id>
      <title>Remove ExtendedFieldCache by rolling functionality into FieldCache</title>
      <description>It is silly that we have ExtendedFieldCache. It is a workaround to our supposed back compatibility problem. This patch will merge the ExtendedFieldCache interface into FieldCache, thereby breaking back compatibility, but creating a much simpler API for FieldCache.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1689</id>
      <title>supplementary character handling</title>
      <description>for Java 5. Java 5 is based on unicode 4, which means variable-width encoding. supplementary character support should be fixed for code that works with char/char[] For example: StandardAnalyzer, SimpleAnalyzer, StopAnalyzer, etc should at least be changed so they don't actually remove suppl characters, or modified to look for surrogates and behave correctly. LowercaseFilter should be modified to lowercase suppl. characters correctly. CharTokenizer should either be deprecated or changed so that isTokenChar() and normalize() use int. in all of these cases code should remain optimized for the BMP case, and suppl characters should be the exception, but still work.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1693</id>
      <title>AttributeSource/TokenStream API improvements</title>
      <description>This patch makes the following improvements to AttributeSource and TokenStream/Filter: introduces interfaces for all Attributes. The corresponding implementations have the postfix 'Impl', e.g. TermAttribute and TermAttributeImpl. AttributeSource now has a factory for creating the Attribute instances; the default implementation looks for implementing classes with the postfix 'Impl'. Token now implements all 6 TokenAttribute interfaces. new method added to AttributeSource: addAttributeImpl(AttributeImpl). Using reflection it walks up in the class hierarchy of the passed in object and finds all interfaces that the class or superclasses implement and that extend the Attribute interface. It then adds the interface-&gt;instance mappings to the attribute map for each of the found interfaces. removes the set/getUseNewAPI() methods (including the standard ones). Instead it is now enough to only implement the new API, if one old TokenStream implements still the old API (next()/next(Token)), it is wrapped automatically. The delegation path is determined via reflection (the patch determines, which of the three methods was overridden). Token is no longer deprecated, instead it implements all 6 standard token interfaces (see above). The wrapper for next() and next(Token) uses this, to automatically map all attribute interfaces to one TokenWrapper instance (implementing all 6 interfaces), that contains a Token instance. next() and next(Token) exchange the inner Token instance as needed. For the new incrementToken(), only one TokenWrapper instance is visible, delegating to the currect reusable Token. This API also preserves custom Token subclasses, that maybe created by very special token streams (see example in Backwards-Test). AttributeImpl now has a default implementation of toString that uses reflection to print out the values of the attributes in a default formatting. This makes it a bit easier to implement AttributeImpl, because toString() was declared abstract before. Cloning is now done much more efficiently in captureState. The method figures out which unique AttributeImpl instances are contained as values in the attributes map, because those are the ones that need to be cloned. It creates a single linked list that supports deep cloning (in the inner class AttributeSource.State). AttributeSource keeps track of when this state changes, i.e. whenever new attributes are added to the AttributeSource. Only in that case will captureState recompute the state, otherwise it will simply clone the precomputed state and return the clone. restoreState(AttributeSource.State) walks the linked list and uses the copyTo() method of AttributeImpl to copy all values over into the attribute that the source stream (e.g. SinkTokenizer) uses. Tee- and SinkTokenizer were deprecated, because they use Token instances for caching. This is not compatible to the new API using AttributeSource.State objects. You can still use the old deprecated ones, but new features provided by new Attribute types may get lost in the chain. A replacement is a new TeeSinkTokenFilter, which has a factory to create new Sink instances, that have compatible attributes. Sink instances created by one Tee can also be added to another Tee, as long as the attribute implementations are compatible (it is not possible to add a sink from a tee using one Token instance to a tee using the six separate attribute impls). In this case UOE is thrown. The cloning performance can be greatly improved if not multiple AttributeImpl instances are used in one TokenStream. A user can e.g. simply add a Token instance to the stream instead of the individual attributes. Or the user could implement a subclass of AttributeImpl that implements exactly the Attribute interfaces needed. I think this should be considered an expert API (addAttributeImpl), as this manual optimization is only needed if cloning performance is crucial. I ran some quick performance tests using Tee/Sink tokenizers (which do cloning) and the performance was roughly 20% faster with the new API. I'll run some more performance tests and post more numbers then. Note also that when we add serialization to the Attributes, e.g. for supporting storing serialized TokenStreams in the index, then the serialization should benefit even significantly more from the new API than cloning. This issue contains one backwards-compatibility break: TokenStreams/Filters/Tokenizers should normally be final (see LUCENE-1753 for the explaination). Some of these core classes are not final and so one could override the next() or next(Token) methods. In this case, the backwards-wrapper would automatically use incrementToken(), because it is implemented, so the overridden method is never called. To prevent users from errors not visible during compilation or testing (the streams just behave wrong), this patch makes all implementation methods final (next(), next(Token), incrementToken()), whenever the class itsself is not final. This is a BW break, but users will clearly see, that they have done something unsupoorted and should better create a custom TokenFilter with their additional implementation (instead of extending a core implementation). For further changing contrib token streams the following procedere should be used: rewrite and replace next(Token)/next() implementations by new API if the class is final, no next(Token)/next() methods needed (must be removed!!!) if the class is non-final add the following methods to the class: /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should * not be overridden. Delegates to the backwards compatibility layer. */ public final Token next(final Token reusableToken) throws java.io.IOException { return super.next(reusableToken); } /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should * not be overridden. Delegates to the backwards compatibility layer. */ public final Token next() throws java.io.IOException { return super.next(); } Also the incrementToken() method must be final in this case (and the new method end() of LUCENE-1448)</description>
      <attachments/>
    </issue>
    <issue>
      <id>1696</id>
      <title>Added New Token API impl for ASCIIFoldingFilter</title>
      <description>I added an implementation of incrementToken to ASCIIFoldingFilter.java and extended the existing testcase for it. I will attach the patch shortly. Beside this improvement I would like to start up a small discussion about this filter. ASCIIFoldingFitler is meant to be a replacement for ISOLatin1AccentFilter which is quite nice as it covers a superset of the latter. I have used this filter quite often but never on a as it is basis. In the most cases this filter does the correct thing (replace a special char with its ascii correspondent) but in some cases like for German umlaut it does not return the expected result. A german umlaut like 'ä' does not translate to a but rather to 'ae'. I would like to change this but I'n not 100% sure if that is expected by all users of that filter. Another way of doing it would be to make it configurable with a flag. This would not affect performance as we only check if such a umlaut char is found. Further it would be really helpful if that filter could "inject" the original/unmodified token with the same position increment into the token stream on demand. I think its a valid use-case to index the modified and unmodified token. For instance, the german word "süd" would be folded to "sud". In a query q:(süd) the filter would also fold to sud and therefore find sud which has a totally different meaning. Folding works quite well but for special cases would could add those options to make users life easier. The latter could be done in a subclass while the umlaut problem should be fixed in the base class. simon</description>
      <attachments/>
    </issue>
    <issue>
      <id>1701</id>
      <title>Add NumericField, make plain text numeric parsers public in FieldCache, move trie parsers to FieldCache</title>
      <description>In discussions about LUCENE-1673, Mike &amp; me wanted to add a new NumericField to o.a.l.document specific for easy indexing. An alternative would be to add a NumericUtils.newXxxField() factory, that creates a preconfigured Field instance with norms and tf off, optionally a stored text (LUCENE-1699) and the TokenStream already initialized. On the other hand NumericUtils.newXxxSortField could be moved to NumericSortField. I and Yonik tend to use the factory for both, Mike tends to create the new classes. Also the parsers for string-formatted numerics are not public in FieldCache. As the new SortField API (LUCENE-1478) makes it possible to support a parser in SortField instantiation, it would be good to have the static parsers in FieldCache public available. SortField would init its member variable to them (instead of NULL), so making code a lot easier (FieldComparator has this ugly null checks when retrieving values from the cache). Moving the Trie parsers also as static instances into FieldCache would make the code cleaner and we would be able to hide the "hack" StopFillCacheException by making it private to FieldCache (currently its public because NumericUtils is in o.a.l.util).</description>
      <attachments/>
    </issue>
    <issue>
      <id>1703</id>
      <title>Add a waitForMerges() method to IndexWriter</title>
      <description>It would be very useful to have a waitForMerges() method on the IndexWriter. Right now, the only way i can see to achieve this is to call IndexWriter.close() ideally, there would be a method on the IndexWriter to wait for merges without actually closing the index. This would make it so that background merges (or optimize) can be waited for without closing the IndexWriter, and then reopening a new IndexWriter the close() reopen IndexWriter method can be problematic if the close() fails as the write lock won't be released this could then result in the following sequence: close() - fails force unlock the write lock (per close() documentation) new IndexWriter() (acquires write lock) finalize() on old IndexWriter releases the write lock Index is now not locked, and another IndexWriter pointing to the same directory could be opened If you don't force unlock the write lock, opening a new IndexWriter will fail until garbage collection calls finalize() the old IndexWriter If the waitForMerges() method is available, i would likely never need to close() the IndexWriter until right before the process being shutdown, so this issue would not occur (worst case scenario, the waitForMerges() fails)</description>
      <attachments/>
    </issue>
    <issue>
      <id>1707</id>
      <title>Don't use ensureOpen() excessively in IndexReader and IndexWriter</title>
      <description>A spin off from here: http://www.nabble.com/Excessive-use-of-ensureOpen()-td24127806.html. We should stop calling this method when it's not necessary for any internal Lucene code. Currently, this code seems to hurt properly written apps, unnecessarily. Will post a patch soon</description>
      <attachments/>
    </issue>
    <issue>
      <id>1709</id>
      <title>Parallelize Tests</title>
      <description>The Lucene tests can be parallelized to make for a faster testing system. This task from ANT can be used: http://ant.apache.org/manual/CoreTasks/parallel.html Previous discussion: http://www.gossamer-threads.com/lists/lucene/java-dev/69669 Notes from Mike M.: I'd love to see a clean solution here (the tests are embarrassingly parallelizable, and we all have machines with good concurrency these days)... I have a rather hacked up solution now, that uses "-Dtestpackage=XXX" to split the tests up. Ideally I would be able to say "use N threads" and it'd do the right thing... like the -j flag to make.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1712</id>
      <title>Set default precisionStep for NumericField and NumericRangeFilter</title>
      <description>This is a spinoff from LUCENE-1701. A user using Numeric* should not need to understand what's "under the hood" in order to do their indexing &amp; searching. They should be able to simply: doc.add(new NumericField("price", 15.50); And have a decent default precisionStep selected for them. Actually, if we add ctors to NumericField for each of the supported types (so the above code works), we can set the default per-type. I think we should do that? 4 for int and 6 for long was proposed as good defaults. The default need not be "perfect", as advanced users can always optimize their precisionStep, and for users experiencing slow RangeQuery performance, NumericRangeQuery with any of the defaults we are discussing will be much faster.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1713</id>
      <title>Rename RangeQuery -&gt; TermRangeQuery</title>
      <description>Since we now have NumericRangeQuery (LUCENE-1701) we should rename RangeQuery to TextRangeQuery to make it clear that TextRangeQuery (TermRangeQuery? StringRangeQuery) is based entirely on text comparison. And, existing users on upgrading to 2.9 and using RangeQuery for [slow] numeric searching would realize they now have a good option for numeric range searching.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1720</id>
      <title>TimeLimitedIndexReader and associated utility class</title>
      <description>An alternative to TimeLimitedCollector that has the following advantages: 1) Any reader activity can be time-limited rather than just single searches e.g. the document retrieve phase. 2) Times out faster (i.e. runaway queries such as fuzzies detected quickly before last "collect" stage of query processing) Uses new utility timeout class that is independent of IndexReader. Initial contribution includes a performance test class but not had time as yet to work up a formal Junit test. TimeLimitedIndexReader is coded as JDK1.5 but can easily be undone.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1721</id>
      <title>IndexWriter to allow deletion by doc ids</title>
      <description>It would be great if IndexWriter would allow for deletion by doc ids as well. It makes sense for cases where a "query" has been executed beforehand, and later, that query needs to be applied in order to delete the matched documents. More information here: http://www.nabble.com/Delete-by-docId-in-IndexWriter-td24239930.html</description>
      <attachments/>
    </issue>
    <issue>
      <id>1726</id>
      <title>IndexWriter.readerPool create new segmentReader outside of sync block</title>
      <description>I think we will want to do something like what field cache does with CreationPlaceholder for IndexWriter.readerPool. Otherwise we have the (I think somewhat problematic) issue of all other readerPool.get* methods waiting for an SR to warm. It would be good to implement this for 2.9.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1728</id>
      <title>Move SmartChineseAnalyzer &amp; resources to own contrib project</title>
      <description>SmartChineseAnalyzer depends on a large dictionary that causes the analyzer jar to grow up to 3MB. The dictionary is quite big compared to all the other resouces / class files contained in that jar. Having a separate analyzer-cn contrib project enables footprint-sensitive users (e.g. using lucene on a mobile phone) to include analyzer.jar without getting into trouble with disk space. Moving SmartChineseAnalyzer to a separate project could also include a small refactoring as Robert mentioned in LUCENE-1722 several classes should be package protected, members and classes could be final, commented syserr and logging code should be removed etc. I set this issue target to 2.9 - if we can not make it until then feel free to move it to 3.0</description>
      <attachments/>
    </issue>
    <issue>
      <id>1749</id>
      <title>FieldCache introspection API</title>
      <description>FieldCache should expose an Expert level API for runtime introspection of the FieldCache to provide info about what is in the FieldCache at any given moment. We should also provide utility methods for sanity checking that the FieldCache doesn't contain anything "odd"... entries for the same reader/field with different types/parsers entries for the same field/type/parser in a reader and it's subreader(s) etc...</description>
      <attachments/>
    </issue>
    <issue>
      <id>1754</id>
      <title>Get rid of NonMatchingScorer from BooleanScorer2</title>
      <description>Over in LUCENE-1614 Mike has made a comment about removing NonMatchinScorer from BS2, and return null in BooleanWeight.scorer(). I've checked and this can be easily done, so I'm going to post a patch shortly. For reference: https://issues.apache.org/jira/browse/LUCENE-1614?focusedCommentId=12715064&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12715064. I've marked the issue as 2.9 just because it's small, and kind of related to all the search enhancements done for 2.9.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1768</id>
      <title>NumericRange support for new query parser</title>
      <description>It would be good to specify some type of "schema" for the query parser in future, to automatically create NumericRangeQuery for different numeric types? It would then be possible to index a numeric value (double,float,long,int) using NumericField and then the query parser knows, which type of field this is and so it correctly creates a NumericRangeQuery for strings like "[1.567..*]" or "(1.787..19.5]". There is currently no way to extract if a field is numeric from the index, so the user will have to configure the FieldConfig objects in the ConfigHandler. But if this is done, it will not be that difficult to implement the rest. The only difference between the current handling of RangeQuery is then the instantiation of the correct Query type and conversion of the entered numeric values (simple Number.valueOf(...) cast of the user entered numbers). Evenerything else is identical, NumericRangeQuery also supports the MTQ rewrite modes (as it is a MTQ). Another thing is a change in Date semantics. There are some strange flags in the current parser that tells it how to handle dates.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1782</id>
      <title>Rename OriginalQueryParserHelper</title>
      <description>We should rename the new QueryParser so it's clearer that it's Lucene's default QueryParser, going forward, and not just a temporary "bridge" to a future new QueryParser. How about we rename oal.queryParser.original --&gt; oal.queryParser.standard (can't use "default": it's a Java keyword)? Then, leave the OriginalQueryParserHelper under that package, but simply rename it to QueryParser? This way if we create other sub-packages in the future, eg ComplexPhraseQueryParser, they too can have a QueryParser class under them, to make it clear that's the "top" class you use to parse queries.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1790</id>
      <title>Add Boosting Function Term Query and Some Payload Query refactorings</title>
      <description>Similar to the BoostingTermQuery, the BoostingFunctionTermQuery is a SpanTermQuery, but the difference is the payload score for a doc is not the average of all the payloads, but applies a function to them instead. BoostingTermQuery becomes a BoostingFunctionTermQuery with an AveragePayloadFunction applied to it. Also add marker interface to indicate PayloadQuery types. Refactor Similarity.scorePayload to also take in the doc id.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1794</id>
      <title>implement reusableTokenStream for all contrib analyzers</title>
      <description>most contrib analyzers do not have an impl for reusableTokenStream regardless of how expensive the back compat reflection is for indexing speed, I think we should do this to mitigate any performance costs. hey, overall it might even be an improvement! the back compat code for non-final analyzers is already in place so this is easy money in my opinion.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1796</id>
      <title>Speed up repeated TokenStream init</title>
      <description>by caching isMethodOverridden results</description>
      <attachments/>
    </issue>
    <issue>
      <id>1799</id>
      <title>Unicode compression</title>
      <description>In lucene-1793, there is the off-topic suggestion to provide compression of Unicode data. The motivation was a custom encoding in a Russian analyzer. The original supposition was that it provided a more compact index. This led to the comment that a different or compressed encoding would be a generally useful feature. BOCU-1 was suggested as a possibility. This is a patented algorithm by IBM with an implementation in ICU. If Lucene provide it's own implementation a freely avIlable, royalty-free license would need to be obtained. SCSU is another Unicode compression algorithm that could be used. An advantage of these methods is that they work on the whole of Unicode. If that is not needed an encoding such as iso8859-1 (or whatever covers the input) could be used.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1808</id>
      <title>make Query.createWeight public (or add back Query.createQueryWeight())</title>
      <description>Now that the QueryWeight class has been removed, the public QueryWeight createQueryWeight() method on Query was also removed i have cases where i want to create a weight for a sub query (outside of the org.apache.lucene.search package) and i don't want the weight normalized (think BooleanQuery outside of the o.a.l.search package) in order to do this, i have to create a static Utils class inside o.a.l.search, pass in the Query and searcher, and have the static method call the protected createWeight method this should not be necessary This could be fixed in one of 2 ways: 1. make createWeight() public on Query (breaks back compat) 2. add the following method: public Weight createQueryWeight(Searcher searcher) throws IOException { return createWeight(searcher); } createWeight(Searcher) should then be deprectated in favor of the publicly accessible method</description>
      <attachments/>
    </issue>
    <issue>
      <id>1812</id>
      <title>Static index pruning by in-document term frequency (Carmel pruning)</title>
      <description>This module provides tools to produce a subset of input indexes by removing postings data for those terms where their in-document frequency is below a specified threshold. The net effect of this processing is a much smaller index that for common types of queries returns nearly identical top-N results as compared with the original index, but with increased performance. Optionally, stored values and term vectors can also be removed. This functionality is largely independent, so it can be used without term pruning (when term freq. threshold is set to 1). As the threshold value increases, the total size of the index decreases, search performance increases, and recall decreases (i.e. search quality deteriorates). NOTE: especially phrase recall deteriorates significantly at higher threshold values. Primary purpose of this class is to produce small first-tier indexes that fit completely in RAM, and store these indexes using IndexWriter.addIndexes(IndexReader[]). Usually the performance of this class will not be sufficient to use the resulting index view for on-the-fly pruning and searching. NOTE: If the input index is optimized (i.e. doesn't contain deletions) then the index produced via IndexWriter.addIndexes(IndexReader[]) will preserve internal document id-s so that they are in sync with the original index. This means that all other auxiliary information not necessary for first-tier processing, such as some stored fields, can also be removed, to be quickly retrieved on-demand from the original index using the same internal document id. Threshold values can be specified globally (for terms in all fields) using defaultThreshold parameter, and can be overriden using per-field or per-term values supplied in a thresholds map. Keys in this map are either field names, or terms in field:text format. The precedence of these values is the following: first a per-term threshold is used if present, then per-field threshold if present, and finally the default threshold. A command-line tool (PruningTool) is provided for convenience. At this moment it doesn't support all functionality available through API.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1813</id>
      <title>Add option to ReverseStringFilter to mark reversed tokens</title>
      <description>This patch implements additional functionality in the filter to "mark" reversed tokens with a special marker character (Unicode 0001). This is useful when indexing both straight and reversed tokens (e.g. to implement efficient leading wildcards search).</description>
      <attachments/>
    </issue>
    <issue>
      <id>1822</id>
      <title>FastVectorHighlighter: SimpleFragListBuilder hard-coded 6 char margin is too naive</title>
      <description>The new FastVectorHighlighter performs extremely well, however I've found in testing that the window of text chosen per fragment is often very poor, as it is hard coded in SimpleFragListBuilder to always select starting 6 characters to the left of the first phrase match in a fragment. When selecting long fragments, this often means that there is barely any context before the highlighted word, and lots after; even worse, when highlighting a phrase at the end of a short text the beginning is cut off, even though the entire phrase would fit in the specified fragCharSize. For example, highlighting "Punishment" in "Crime and Punishment" returns "e and &lt;b&gt;Punishment&lt;/b&gt;" no matter what fragCharSize is specified. I am going to attach a patch that improves the text window selection by recalculating the starting margin once all phrases in the fragment have been identified - this way if a single word is matched in a fragment, it will appear in the middle of the highlight, instead of 6 characters from the beginning. This way one can also guarantee that the entirety of short texts are represented in a fragment by specifying a large enough fragCharSize.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1823</id>
      <title>QueryParser with new features for Lucene 3</title>
      <description>I'd like to have a new QueryParser implementation in Lucene 3.1, ideally based on the new QP framework in contrib. It should share as much code as possible with the current StandardQueryParser implementation for easy maintainability. Wish list (feel free to extend): 1. Operator precedence: Support operator precedence for boolean operators 2. Opaque terms: Ability to plugin an external parser for certain syntax extensions, e.g. XML query terms 3. Improved RangeQuery syntax: Use more intuitive &lt;=, =, &gt;= instead of [] and {} 4. Support for trierange queries: See LUCENE-1768 5. Complex phrases: See LUCENE-1486 6. ANY operator: E.g. (a b c d) ANY 3 should match if 3 of the 4 terms occur in the same document 7. New syntax for Span queries: I think the surround parser supports this? 8. Escaped wildcards: See LUCENE-588</description>
      <attachments/>
    </issue>
    <issue>
      <id>1824</id>
      <title>FastVectorHighlighter truncates words at beginning and end of fragments</title>
      <description>FastVectorHighlighter does not take word boundaries into consideration when building fragments, so that in most cases the first and last word of a fragment are truncated. This makes the highlights less legible than they should be. I will attach a patch to BaseFragmentBuilder that resolves this by expanding the start and end boundaries of the fragment to the first whitespace character on either side of the fragment, or the beginning or end of the source text, whichever comes first. This significantly improves legibility, at the cost of returning a slightly larger number of characters than specified for the fragment size.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1844</id>
      <title>Speed up junit tests</title>
      <description>As Lucene grows, so does the number of JUnit tests. This is obviously a good thing, but it comes with longer and longer test times. Now that we also run back compat tests in a standard test run, this problem is essentially doubled. There are some ways this may get better, including running parallel tests. You will need the hardware to fully take advantage, but it should be a nice gain. There is already an issue for this, and Junit 4.6, 4.7 have the beginnings of something we might be able to count on soon. 4.6 was buggy, and 4.7 still doesn't come with nice ant integration. Parallel tests will come though. Beyond parallel testing, I think we also need to concentrate on keeping our tests lean. We don't want to sacrifice coverage or quality, but I'm sure there is plenty of fat to skim. I've started making a list of some of the longer tests - I think with some work we can make our tests much faster - and then with parallelization, I think we could see some really great gains.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1845</id>
      <title>if the build fails to download JARs for contrib/db, just skip its tests</title>
      <description>Every so often our nightly build fails because contrib/db is unable to download the necessary BDB JARs from http://downloads.osafoundation.org. I think in such cases we should simply skip contrib/db's tests, if it's the nightly build that's running, since it's a false positive failure.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1855</id>
      <title>Change AttributeSource API to use generics</title>
      <description>The AttributeSource API will be easier to use with JDK 1.5 generics. Uwe, if you started working on a patch for this already feel free to assign this to you.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1872</id>
      <title>Improve javadocs for Numeric*</title>
      <description>I'm working on improving Numeric* javadocs.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1877</id>
      <title>Use NativeFSLockFactory as default for new API (direct ctors &amp; FSDir.open)</title>
      <description>A user requested we add a note in IndexWriter alerting the availability of NativeFSLockFactory (allowing you to avoid retaining locks on abnormal jvm exit). Seems reasonable to me - we want users to be able to easily stumble upon this class. The below code looks like a good spot to add a note - could also improve whats there a bit - opening an IndexWriter does not necessarily create a lock file - that would depend on the LockFactory used. &lt;p&gt;Opening an &lt;code&gt;IndexWriter&lt;/code&gt; creates a lock file for the directory in use. Trying to open another &lt;code&gt;IndexWriter&lt;/code&gt; on the same directory will lead to a {@link LockObtainFailedException}. The {@link LockObtainFailedException} is also thrown if an IndexReader on the same directory is used to delete documents from the index.&lt;/p&gt; Anyone remember why NativeFSLockFactory is not the default over SimpleFSLockFactory?</description>
      <attachments/>
    </issue>
    <issue>
      <id>1879</id>
      <title>Parallel incremental indexing</title>
      <description>A new feature that allows building parallel indexes and keeping them in sync on a docID level, independent of the choice of the MergePolicy/MergeScheduler. Find details on the wiki page for this feature: http://wiki.apache.org/lucene-java/ParallelIncrementalIndexing Discussion on java-dev: http://markmail.org/thread/ql3oxzkob7aqf3jd</description>
      <attachments/>
    </issue>
    <issue>
      <id>1887</id>
      <title>o.a.l.messages should be moved to core</title>
      <description>contrib/queryParser contains an org.apache.lucene.messages package containing some generallized code that (claims in it's javadocs) is not specific to the queryParser. If this is truely general purpose code, it should probably be moved out of hte queryParser contrib – either into it's own contrib, or into the core (it's very small) EDIT: alternate suggestion to rename package to fall under the o.a.l.queryParser namespace retracted due to comments in favor of (eventually) promoting to it's own contrib</description>
      <attachments/>
    </issue>
    <issue>
      <id>1896</id>
      <title>Modify confusing javadoc for queryNorm</title>
      <description>See http://markmail.org/message/arai6silfiktwcer The javadoc confuses me as well.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1908</id>
      <title>Similarity javadocs for scoring function to relate more tightly to scoring models in effect</title>
      <description>See discussion in the related issue.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1959</id>
      <title>Index Splitter</title>
      <description>If an index has multiple segments, this tool allows splitting those segments into separate directories.</description>
      <attachments/>
    </issue>
    <issue>
      <id>1990</id>
      <title>Add unsigned packed int impls in oal.util</title>
      <description>There are various places in Lucene that could take advantage of an efficient packed unsigned int/long impl. EG the terms dict index in the standard codec in LUCENE-1458 could subsantially reduce it's RAM usage. FieldCache.StringIndex could as well. And I think "load into RAM" codecs like the one in TestExternalCodecs could use this too. I'm picturing something very basic like: interface PackedUnsignedLongs { long get(long index); void set(long index, long value); } Plus maybe an iterator for getting and maybe also for setting. If it helps, most of the usages of this inside Lucene will be "write once" so eg the set could make that an assumption/requirement. And a factory somewhere: PackedUnsignedLongs create(int count, long maxValue); I think we should simply autogen the code (we can start from the autogen code in LUCENE-1410), or, if there is an good existing impl that has a compatible license that'd be great. I don't have time near-term to do this... so if anyone has the itch, please jump!</description>
      <attachments/>
    </issue>
    <issue>
      <id>1997</id>
      <title>Explore performance of multi-PQ vs single-PQ sorting API</title>
      <description>Spinoff from recent "lucene 2.9 sorting algorithm" thread on java-dev, where a simpler (non-segment-based) comparator API is proposed that gathers results into multiple PQs (one per segment) and then merges them in the end. I started from John's multi-PQ code and worked it into contrib/benchmark so that we could run perf tests. Then I generified the Python script I use for running search benchmarks (in contrib/benchmark/sortBench.py). The script first creates indexes with 1M docs (based on SortableSingleDocSource, and based on wikipedia, if available). Then it runs various combinations: Index with 20 balanced segments vs index with the "normal" log segment size Queries with different numbers of hits (only for wikipedia index) Different top N Different sorts (by title, for wikipedia, and by random string, random int, and country for the random index) For each test, 7 search rounds are run and the best QPS is kept. The script runs singlePQ then multiPQ, and records the resulting best QPS for each and produces table (in Jira format) as output.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2015</id>
      <title>ASCIIFoldingFilter: expose folding logic + small improvements to ISOLatin1AccentFilter</title>
      <description>This patch adds a couple of non-ascii chars to ISOLatin1AccentFilter (namely: left &amp; right single quotation marks, en dash, em dash) which we very frequently encounter in our projects. I know that this class is now deprecated; this improvement is for legacy code that hasn't migrated yet. It also enables easy access to the ascii folding technique use in ASCIIFoldingFilter for potential re-use in non-Lucene-related code.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2019</id>
      <title>map unicode process-internal codepoints to replacement character</title>
      <description>A spinoff from LUCENE-2016. There are several process-internal codepoints in unicode, we should not store these in the index. Instead they should be mapped to replacement character (U+FFFD), so they can be used process-internally. An example of this is how Lucene Java currently uses U+FFFF process-internally, it can't be in the index or will cause problems.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2023</id>
      <title>Improve performance of SmartChineseAnalyzer</title>
      <description>I've noticed SmartChineseAnalyzer is a bit slow, compared to say CJKAnalyzer on chinese text. This patch improves the internal hhmm implementation. Time to index my chinese corpus is 75% of the previous time.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2026</id>
      <title>Refactoring of IndexWriter</title>
      <description>I've been thinking for a while about refactoring the IndexWriter into two main components. One could be called a SegmentWriter and as the name says its job would be to write one particular index segment. The default one just as today will provide methods to add documents and flushes when its buffer is full. Other SegmentWriter implementations would do things like e.g. appending or copying external segments [what addIndexes*() currently does]. The second component's job would it be to manage writing the segments file and merging/deleting segments. It would know about DeletionPolicy, MergePolicy and MergeScheduler. Ideally it would provide hooks that allow users to manage external data structures and keep them in sync with Lucene's data during segment merges. API wise there are things we have to figure out, such as where the updateDocument() method would fit in, because its deletion part affects all segments, whereas the new document is only being added to the new segment. Of course these should be lower level APIs for things like parallel indexing and related use cases. That's why we should still provide easy to use APIs like today for people who don't need to care about per-segment ops during indexing. So the current IndexWriter could probably keeps most of its APIs and delegate to the new classes.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2034</id>
      <title>Massive Code Duplication in Contrib Analyzers - unifly the analyzer ctors</title>
      <description>Due to the variouse tokenStream APIs we had in lucene analyzer subclasses need to implement at least one of the methodes returning a tokenStream. When you look at the code it appears to be almost identical if both are implemented in the same analyzer. Each analyzer defnes the same inner class (SavedStreams) which is unnecessary. In contrib almost every analyzer uses stopwords and each of them creates his own way of loading them or defines a large number of ctors to load stopwords from a file, set, arrays etc.. those ctors should be removed / deprecated and eventually removed.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2037</id>
      <title>Allow Junit4 tests in our environment.</title>
      <description>Now that we're dropping Java 1.4 compatibility for 3.0, we can incorporate Junit4 in testing. Junit3 and junit4 tests can coexist, so no tests should have to be rewritten. We should start this for the 3.1 release so we can get a clean 3.0 out smoothly. It's probably worthwhile to convert a small set of tests as an exemplar.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2039</id>
      <title>Regex support and beyond in JavaCC QueryParser</title>
      <description>Since the early days the standard query parser was limited to the queries living in core, adding other queries or extending the parser in any way always forced people to change the grammar file and regenerate. Even if you change the grammar you have to be extremely careful how you modify the parser so that other parts of the standard parser are affected by customisation changes. Eventually you had to live with all the limitation the current parser has like tokenizing on whitespaces before a tokenizer / analyzer has the chance to look at the tokens. I was thinking about how to overcome the limitation and add regex support to the query parser without introducing any dependency to core. I added a new special character that basically prevents the parser from interpreting any of the characters enclosed in the new special characters. I choose the forward slash '/' as the delimiter so that everything in between two forward slashes is basically escaped and ignored by the parser. All chars embedded within forward slashes are treated as one token even if it contains other special chars like * []?{} or whitespaces. This token is subsequently passed to a pluggable "parser extension" with builds a query from the embedded string. I do not interpret the embedded string in any way but leave all the subsequent work to the parser extension. Such an extension could be another full featured query parser itself or simply a ctor call for regex query. The interface remains quiet simple but makes the parser extendible in an easy way compared to modifying the javaCC sources. The downsides of this patch is clearly that I introduce a new special char into the syntax but I guess that would not be that much of a deal as it is reflected in the escape method though. It would truly be nice to have more than once extension an have this even more flexible so treat this patch as a kickoff though. Another way of solving the problem with RegexQuery would be to move the JDK version of regex into the core and simply have another method like: protected Query newRegexQuery(Term t) { ... } which I would like better as it would be more consistent with the idea of the query parser to be a very strict and defined parser. I will upload a patch in a second which implements the extension based approach I guess I will add a second patch with regex in core soon too.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2041</id>
      <title>Complete parallelizaton of ParallelMultiSearcher</title>
      <description>ParallelMultiSearcher is parallel only for the method signatures of 'search'. Part of a query process calls the method docFreq(). There was a TODO comment to parallelize this. Parallelizing this method actually increases the performance of a query on multiple indexes, especially remotely.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2047</id>
      <title>IndexWriter should immediately resolve deleted docs to docID in near-real-time mode</title>
      <description>Spinoff from LUCENE-1526. When deleteDocuments(Term) is called, we currently always buffer the Term and only later, when it's time to flush deletes, resolve to docIDs. This is necessary because we don't in general hold SegmentReaders open. But, when IndexWriter is in NRT mode, we pool the readers, and so deleting in the foreground is possible. It's also beneficial, in that in can reduce the turnaround time when reopening a new NRT reader by taking this resolution off the reopen path. And if multiple threads are used to do the deletion, then we gain concurrency, vs reopen which is not concurrent when flushing the deletes.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2051</id>
      <title>Contrib Analyzer Setters should be deprecated and replace with ctor arguments</title>
      <description>Some analyzers in contrib provide setters for stopword / stem exclusion sets / hashtables etc. Those setters should be deprecated as they yield unexpected behaviour. The way they work is they set the reusable token stream instance to null in a thread local cache which only affects the tokenstream in the current thread. Analyzers itself should be immutable except of the threadlocal. will attach a patch soon.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2064</id>
      <title>Highlighter should support all MultiTermQuery subclasses without casts</title>
      <description>In order to support MultiTermQuery subclasses the Highlighter component applies instanceof checks for concrete classes from the lucene core. This prevents classes like RegexQuery in contrib from being supported. Introducing dependencies on other contribs is not feasible just for being supported by the highlighter. While the instanceof checks and subsequent casts might hopefully go somehow away in the future but for supporting more multterm queries I have a alternative approach using a fake IndexReader that uses a RewriteMethod to force the MTQ to pass the field name to the given reader without doing any real work. It is easier to explain once you see the patch - I will upload shortly.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2069</id>
      <title>fix LowerCaseFilter for unicode 4.0</title>
      <description>lowercase suppl. characters correctly. this only fixes the filter, the LowerCaseTokenizer is part of a more complex issue (CharTokenizer)</description>
      <attachments/>
    </issue>
    <issue>
      <id>2075</id>
      <title>Share the Term -&gt; TermInfo cache across threads</title>
      <description>Right now each thread creates its own (thread private) SimpleLRUCache, holding up to 1024 terms. This is rather wasteful, since if there are a high number of threads that come through Lucene, you're multiplying the RAM usage. You're also cutting way back on likelihood of a cache hit (except the known multiple times we lookup a term within-query, which uses one thread). In NRT search we open new SegmentReaders (on tiny segments) often which each thread must then spend CPU/RAM creating &amp; populating. Now that we are on 1.5 we can use java.util.concurrent.*, eg ConcurrentHashMap. One simple approach could be a double-barrel LRU cache, using 2 maps (primary, secondary). You check the cache by first checking primary; if that's a miss, you check secondary and if you get a hit you promote it to primary. Once primary is full you clear secondary and swap them. Or... any other suggested approach?</description>
      <attachments/>
    </issue>
    <issue>
      <id>2084</id>
      <title>remove Byte/CharBuffer wrapping for collation key generation</title>
      <description>We can remove the overhead of ByteBuffer and CharBuffer wrapping in CollationKeyFilter and ICUCollationKeyFilter. this patch moves the logic in IndexableBinaryStringTools into char[],int,int and byte[],int,int based methods, with the previous Byte/CharBuffer methods delegating to these. Previously, the Byte/CharBuffer methods required a backing array anyway.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2086</id>
      <title>When resolving deletes, IW should resolve in term sort order</title>
      <description>See java-dev thread "IndexWriter.updateDocument performance improvement".</description>
      <attachments/>
    </issue>
    <issue>
      <id>2089</id>
      <title>explore using automaton for fuzzyquery</title>
      <description>we can optimize fuzzyquery by using AutomatonTermsEnum. The idea is to speed up the core FuzzyQuery in similar fashion to Wildcard and Regex speedups, maintaining all backwards compatibility. The advantages are: we can seek to terms that are useful, instead of brute-forcing the entire terms dict we can determine matches faster, as true/false from a DFA is array lookup, don't even need to run levenshtein. We build Levenshtein DFAs in linear time with respect to the length of the word: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652 To implement support for 'prefix' length, we simply concatenate two DFAs, which doesn't require us to do NFA-&gt;DFA conversion, as the prefix portion is a singleton. the concatenation is also constant time with respect to the size of the fuzzy DFA, it only need examine its start state. with this algorithm, parametric tables are precomputed so that DFAs can be constructed very quickly. if the required number of edits is too large (we don't have a table for it), we use "dumb mode" at first (no seeking, no DFA, just brute force like now). As the priority queue fills up during enumeration, the similarity score required to be a competitive term increases, so, the enum gets faster and faster as this happens. This is because terms in core FuzzyQuery are sorted by boost value, then by term (in lexicographic order). For a large term dictionary with a low minimal similarity, you will fill the pq very quickly since you will match many terms. This not only provides a mechanism to switch to more efficient DFAs (edit distance of 2 -&gt; edit distance of 1 -&gt; edit distance of 0) during enumeration, but also to switch from "dumb mode" to "smart mode". With this design, we can add more DFAs at any time by adding additional tables. The tradeoff is the tables get rather large, so for very high K, we would start to increase the size of Lucene's jar file. The idea is we don't have include large tables for very high K, by using the 'competitive boost' attribute of the priority queue. For more information, see http://en.wikipedia.org/wiki/Levenshtein_automaton</description>
      <attachments/>
    </issue>
    <issue>
      <id>2090</id>
      <title>convert automaton to char[] based processing and TermRef / TermsEnum api</title>
      <description>The automaton processing is currently done with String, mostly because TermEnum is based on String. it is easy to change the processing to work with char[], since behind the scenes this is used anyway. in general I think we should make sure char[] based processing is exposed in the automaton pkg anyway, for things like pattern-based tokenizers and such.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2091</id>
      <title>Add BM25 Scoring to Lucene</title>
      <description>http://nlp.uned.es/~jperezi/Lucene-BM25/ describes an implementation of Okapi-BM25 scoring in the Lucene framework, as an alternative to the standard Lucene scoring (which is a version of mixed boolean/TFIDF). I have refactored this a bit, added unit tests and improved the runtime somewhat. I would like to contribute the code to Lucene under contrib.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2098</id>
      <title>make BaseCharFilter more efficient in performance</title>
      <description>Performance degradation in Solr 1.4 was reported. See: http://www.lucidimagination.com/search/document/43c4bdaf5c9ec98d/html_stripping_slower_in_solr_1_4 The inefficiency has been pointed out in BaseCharFilter javadoc by Mike: NOTE: This class is not particularly efficient. For example, a new class instance is created for every call to addOffCorrectMap(int, int), which is then appended to a private list.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2100</id>
      <title>Make contrib analyzers final</title>
      <description>The analyzers in contrib/analyzers should all be marked final. None of the Analyzers should ever be subclassed - users should build their own analyzers if a different combination of filters and Tokenizers is desired.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2102</id>
      <title>LowerCaseFilter for Turkish language</title>
      <description>java.lang.Character.toLowerCase() converts 'I' to 'i' however in Turkish alphabet lowercase of 'I' is not 'i'. It is LATIN SMALL LETTER DOTLESS I.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2110</id>
      <title>Refactoring of FilteredTermsEnum and MultiTermQuery</title>
      <description>FilteredTermsEnum is confusing as it is initially positioned to the first term. It should instead work like an uninitialized TermsEnum for a field before the first call to next() or seek(). FilteredTermsEnums cannot implement seek() as eg. NRQ or Automaton are not able to support this. Seeking is also not needed for MTQ at all, so seek can just throw UOE. This issue changes some of the internal behaviour of MTQ and FilteredTermsEnum to allow also seeking in NRQ and Automaton (see comments below).</description>
      <attachments/>
    </issue>
    <issue>
      <id>2111</id>
      <title>Wrapup flexible indexing</title>
      <description>Spinoff from LUCENE-1458. The flex branch is in fairly good shape – all tests pass, initial search performance testing looks good, it survived several visits from the Unicode policeman But it still has a number of nocommits, could use some more scrutiny especially on the "emulate old API on flex index" and vice/versa code paths, and still needs some more performance testing. I'll do these under this issue, and we should open separate issues for other self contained fixes. The end is in sight!</description>
      <attachments/>
    </issue>
    <issue>
      <id>2122</id>
      <title>Use JUnit4 capabilites for more thorough Locale testing for classes deriving from LocalizedTestCase</title>
      <description>Use the @Parameterized capabilities of Junit4 to allow more extensive testing of Locales.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2126</id>
      <title>Split up IndexInput and IndexOutput into DataInput and DataOutput</title>
      <description>I'd like to introduce the two new classes DataInput and DataOutput that contain all methods from IndexInput and IndexOutput that actually decode or encode data, such as readByte()/writeByte(), readVInt()/writeVInt(). Methods like getFilePointer(), seek(), close(), etc., which are not related to data encoding, but to files as input/output source stay in IndexInput/IndexOutput. This patch also changes ByteSliceReader/ByteSliceWriter to extend DataInput/DataOutput. Previously ByteSliceReader implemented the methods that stay in IndexInput by throwing RuntimeExceptions. See also LUCENE-2125. All tests pass.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2127</id>
      <title>Improved large result handling</title>
      <description>Per http://search.lucidimagination.com/search/document/350c54fc90d257ed/lots_of_results#fbb84bd297d15dd5, it would be nice to offer some other Collectors that are better at handling really large number of results. This could be implemented in a variety of ways via Collectors. For instance, we could have a raw collector that does no sorting and just returns the ScoreDocs, or we could do as Mike suggests and have Collectors that have heuristics about memory tradeoffs and only heapify when appropriate.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2133</id>
      <title>[PATCH] IndexCache: Refactoring of FieldCache, FieldComparator, SortField</title>
      <description>Hi all, up to the current version Lucene contains a conceptual flaw, that is the FieldCache. The FieldCache is a singleton which is supposed to cache certain information for every IndexReader that is currently open The FieldCache is flawed because it is incorrect to assume that: 1. one IndexReader instance equals one index. In fact, there can be many clones (of SegmentReader) or decorators (FilterIndexReader) which all access the very same data. 2. the cache information remains valid for the lifetime of an IndexReader. In fact, some IndexReaders may be reopen()'ed and thus they may contain completely different information. 3. all IndexReaders need the same type of cache. In fact, because of the limitations imposed by the singleton construct there was no implementation other than FieldCacheImpl. Furthermore, FieldCacheImpl and FieldComparator are bloated by several static inner-classes that could be moved to package level. There have been a few attempts to improve FieldCache, namely LUCENE-831, LUCENE-1579 and LUCENE-1749, but the overall situation remains the same: There is a central registry for assigning Caches to IndexReader instances. I now propose the following: 1. Obsolete FieldCache and FieldCacheKey and provide index-specific, extensible cache instances ("IndexCache"). IndexCaches provide common caching functionality for all IndexReaders and may be extended (for example, SegmentReader would have a SegmentReaderIndexCache and store different data than a regular IndexCache) 2. Add the index-specific field cache (IndexFieldCache) to the IndexCache. IndexFieldCache is an interface just like FieldCache and may support different implementations. 3. The IndexCache instances may be flushed/closed by the associated IndexReaders whenever necessary. 4. Obsolete FieldCacheSanityChecker because no more "insanities" are expected (or at least, they do not impact the overall performance) 5. Refactor FieldCacheImpl and the related classes (FieldComparator, SortField) I have provided an patch which takes care of all these issues. It passes all JUnit tests. The patch is quite large, admittedly, but the change required several modifications and some more to preserve backwards-compatibility. Backwards-compatibility is preserved by moving some of the updated functionality in the package org.apache.lucene.search.fields (field comparators and parsers, SortField) while adding wrapper instances and keeping old code in org.apache.lucene.search. In detail and besides the above mentioned improvements, the following is provided: 1. An IndexCache specific for SegmentReaders. The two ThreadLocals are moved from SegmentReader to SegmentReaderIndexCache. 2. A housekeeping improvement to CloseableThreadLocal. Now delegates the close() method to all registered instances by calling an onClose() method with the threads' instances. 3. Analyzer.close now may throw an IOException (this already is covered by java.io.Closeable). 4. A change to Collector: allow IndexCache instead of IndexReader being passed to setNextReader() 5. SortField's numeric types have been replaced by direct assignments of FieldComparatorSource. This removes the "switch" statements and the possibility to throw IllegalArgumentExceptions because of unsupported type values. The following classes have been deprecated and replaced by new classes in org.apache.lucene.search.fields: FieldCacheRangeFilter (=&gt; IndexFieldCacheRangeFilter) FieldCacheTermsFilter (=&gt; IndexFieldCacheTermsFilter) FieldCache (=&gt; IndexFieldCache) FieldCacheImpl (=&gt; IndexFieldCacheImpl) all classes in FieldCacheImpl (=&gt; several package-level classes) all subclasses of FieldComparator (=&gt; several package-level classes) Final notes: The patch would be simpler if no backwards compatibility was necessary. The Lucene community has to decide which classes/methods can immediately be removed, which ones later, which not at all. Whenever new classes depend on the old ones, an appropriate notice exists in the javadocs. The patch introduces a new, deprecated class IndexFieldCacheSanityChecker.java which is just there for testing purposes, to show that no sanity checks are necessary any longer. This class may be removed at any time. I expect that the patch does not impact performance. On the contrary, as the patch removes a few unnecessary checks we might even see a slight speedup. No benchmarking has been done so far, though. I have tried to preserve the existing functionality wherever possible and to focus on the class/method structure only. We certainly may improve the caches' behavior, but this out of scope for this patch. The refactoring finally makes the high duplication of code visible: For all supported atomic types (byte, double, float, int, long, short) three classes each are required: *Cache, *Comparator and *Parser. I think that further simplification might be possible (maybe using Java generics?), but I guess the current patch is large enough for now. Cheers, Christian</description>
      <attachments/>
    </issue>
    <issue>
      <id>2139</id>
      <title>Cleanup and Improvement of Spatial Contrib</title>
      <description>The current spatial contrib can be improved by adding documentation, tests, removing unused classes and code, repackaging the classes and improving the performance of the distance filtering. The latter will incorporate the multi-threaded functionality introduced in LUCENE-1732. Other improvements involve adding better support for different distance units, different distance calculators and different data formats (whether it be lat/long fields, geohashes, or something else in the future). Patch to be added soon.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2167</id>
      <title>Implement StandardTokenizer with the UAX#29 Standard</title>
      <description>It would be really nice for StandardTokenizer to adhere straight to the standard as much as we can with jflex. Then its name would actually make sense. Such a transition would involve renaming the old StandardTokenizer to EuropeanTokenizer, as its javadoc claims: This should be a good tokenizer for most European-language documents The new StandardTokenizer could then say This should be a good tokenizer for most languages. All the english/euro-centric stuff like the acronym/company/apostrophe stuff can stay with that EuropeanTokenizer, and it could be used by the european analyzers.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2181</id>
      <title>benchmark for collation</title>
      <description>Steven Rowe attached a contrib/benchmark-based benchmark for collation (both jdk and icu) under LUCENE-2084, along with some instructions to run it... I think it would be a nice if we could turn this into a committable patch and add it to benchmark.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2183</id>
      <title>Supplementary Character Handling in CharTokenizer</title>
      <description>CharTokenizer is an abstract base class for all Tokenizers operating on a character level. Yet, those tokenizers still use char primitives instead of int codepoints. CharTokenizer should operate on codepoints and preserve bw compatibility.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2186</id>
      <title>First cut at column-stride fields (index values storage)</title>
      <description>I created an initial basic impl for storing "index values" (ie column-stride value storage). This is still a work in progress... but the approach looks compelling. I'm posting my current status/patch here to get feedback/iterate, etc. The code is standalone now, and lives under new package oal.index.values (plus some util changes, refactorings) – I have yet to integrate into Lucene so eg you can mark that a given Field's value should be stored into the index values, sorting will use these values instead of field cache, etc. It handles 3 types of values: Six variants of byte[] per doc, all combinations of fixed vs variable length, and stored either "straight" (good for eg a "title" field), "deref" (good when many docs share the same value, but you won't do any sorting) or "sorted". Integers (variable bit precision used as necessary, ie this can store byte/short/int/long, and all precisions in between) Floats (4 or 8 byte precision) String fields are stored as the UTF8 byte[]. This patch adds a BytesRef, which does the same thing as flex's TermRef (we should merge them). This patch also adds basic initial impl of PackedInts (LUCENE-1990); we can swap that out if/when we get a better impl. This storage is dense (like field cache), so it's appropriate when the field occurs in all/most docs. It's just like field cache, except the reading API is a get() method invocation, per document. Next step is to do basic integration with Lucene, and then compare sort performance of this vs field cache. For the "sort by String value" case, I think RAM usage &amp; GC load of this index values API should be much better than field caache, since it does not create object per document (instead shares big long[] and byte[] across all docs), and because the values are stored in RAM as their UTF8 bytes. There are abstract Writer/Reader classes. The current reader impls are entirely RAM resident (like field cache), but the API is (I think) agnostic, ie, one could make an MMAP impl instead. I think this is the first baby step towards LUCENE-1231. Ie, it cannot yet update values, and the reading API is fully random-access by docID (like field cache), not like a posting list, though I do think we should add an iterator() api (to return flex's DocsEnum) – eg I think this would be a good way to track avg doc/field length for BM25/lnu.ltc scoring.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2198</id>
      <title>support protected words in Stemming TokenFilters</title>
      <description>This is from LUCENE-1515 I propose that all stemming TokenFilters have an 'exclusion set' that bypasses any stemming for words in this set. Some stemming tokenfilters have this, some do not. This would be one way for Karl to implement his new swedish stemmer (as a text file of ignore words). Additionally, it would remove duplication between lucene and solr, as they reimplement snowballfilter since it does not have this functionality. Finally, I think this is a pretty common use case, where people want to ignore things like proper nouns in the stemming. As an alternative design I considered a case where we generalized this to CharArrayMap (and ignoring words would mean mapping them to themselves), which would also provide a mechanism to override the stemming algorithm. But I think this is too expert, could be its own filter, and the only example of this i can find is in the Dutch stemmer. So I think we should just provide ignore with CharArraySet, but if you feel otherwise please comment.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2205</id>
      <title>Rework of the TermInfosReader class to remove the Terms[], TermInfos[], and the index pointer long[] and create a more memory efficient data structure.</title>
      <description>Basically packing those three arrays into a byte array with an int array as an index offset. The performance benefits are stagering on my test index (of size 6.2 GB, with ~1,000,000 documents and ~175,000,000 terms), the memory needed to load the terminfos into memory were reduced to 17% of there original size. From 291.5 MB to 49.7 MB. The random access speed has been made better by 1-2%, load time of the segments are ~40% faster as well, and full GC's on my JVM were made 7 times faster. I have already performed the work and am offering this code as a patch. Currently all test in the trunk pass with this new code enabled. I did write a system property switch to allow for the original implementation to be used as well. -Dorg.apache.lucene.index.TermInfosReader=default or small I have also written a blog about this patch here is the link. http://www.nearinfinity.com/blogs/aaron_mccurry/my_first_lucene_patch.html</description>
      <attachments/>
    </issue>
    <issue>
      <id>2213</id>
      <title>Small improvements to ArrayUtil.getNextSize</title>
      <description>Spinoff from java-dev thread "Dynamic array reallocation algorithms" started on Jan 12, 2010. Here's what I did: Keep the +3 for small sizes Added 2nd arg = number of bytes per element. Round up to 4 or 8 byte boundary (if it's 32 or 64 bit JRE respectively) Still grow by 1/8th If 0 is passed in, return 0 back I also had to remove some asserts in tests that were checking the actual values returned by this method – I don't think we should test that (it's an impl. detail).</description>
      <attachments/>
    </issue>
    <issue>
      <id>2215</id>
      <title>paging collector</title>
      <description>http://issues.apache.org/jira/browse/LUCENE-2127?focusedCommentId=12796898&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12796898 Somebody assign this to Aaron McCurry and we'll see if we can get enough votes on this issue to convince him to upload his patch.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2217</id>
      <title>Remaining reallocation should use ArrayUtil.getNextSize()</title>
      <description>See recent discussion on ArrayUtils.getNextSize().</description>
      <attachments/>
    </issue>
    <issue>
      <id>2218</id>
      <title>ShingleFilter improvements</title>
      <description>ShingleFilter should allow configuration of minimum shingle size (in addition to maximum shingle size), so that it's possible to (e.g.) output only trigrams instead of bigrams mixed with trigrams. The token separator used in composing shingles should be configurable too.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2236</id>
      <title>Similarity can only be set per index, but I may want to adjust scoring behaviour at a field level</title>
      <description>Similarity can only be set per index, but I may want to adjust scoring behaviour at a field level, to faciliate this could we pass make field name available to all score methods. Currently it is only passed to some such as lengthNorm() but not others such as tf()</description>
      <attachments/>
    </issue>
    <issue>
      <id>2265</id>
      <title>improve automaton performance by running on byte[]</title>
      <description>Currently, when enumerating terms, automaton must convert entire terms from flex's native utf-8 byte[] to char[] first, then step each char thru the state machine. we can make this more efficient, by allowing the state machine to run on byte[], so it can return true/false faster.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2271</id>
      <title>Function queries producing scores of -inf or NaN (e.g. 1/x) return incorrect results with TopScoreDocCollector</title>
      <description>This is a foolowup to LUCENE-2270, where a part of this problem was fixed (boost = 0 leading to NaN scores, which is also un-intuitive), but in general, function queries in Solr can create these invalid scores easily. In previous version of Lucene these scores ordered correct (except NaN, which mixes up results), but never invalid document ids are returned (like Integer.MAX_VALUE). The problem is: TopScoreDocCollector pre-fills the HitQueue with sentinel ScoreDocs with a score of -inf and a doc id of Integer.MAX_VALUE. For the HQ to work, this sentinel must be smaller than all posible values, which is not the case: -inf is equal and the document is not inserted into the HQ, as not competitive, but the HQ is not yet full, so the sentinel values keep in the HQ and result is the Integer.MAX_VALUE docs. This problem is solveable (and only affects the Ordered collector) by chaning the exit condition to: if (score &lt;= pqTop.score &amp;&amp; pqTop.doc != Integer.MAX_VALUE) { // Since docs are returned in-order (i.e., increasing doc Id), a document // with equal score to pqTop.score cannot compete since HitQueue favors // documents with lower doc Ids. Therefore reject those docs too. return; } The NaN case can be fixed in the same way, but then has another problem: all comparisons with NaN result in false (none of these is true): x &lt; NaN, x &gt; NaN, NaN == NaN. This leads to the fact that HQ's lessThan always returns false, leading to unexspected ordering in the PQ and sometimes the sentinel values do not stay at the top of the queue. A later hit then overrides the top of the queue but leaves the incorrect sentinels unchanged -&gt; invalid results. This can be fixed in two ways in HQ: Force all sentinels to the top: protected final boolean lessThan(ScoreDoc hitA, ScoreDoc hitB) { if (hitA.doc == Integer.MAX_VALUE) return true; if (hitB.doc == Integer.MAX_VALUE) return false; if (hitA.score == hitB.score) return hitA.doc &gt; hitB.doc; else return hitA.score &lt; hitB.score; } or alternatively have a defined order for NaN (Float.compare sorts them after +inf): protected final boolean lessThan(ScoreDoc hitA, ScoreDoc hitB) { if (hitA.score == hitB.score) return hitA.doc &gt; hitB.doc; else return Float.compare(hitA.score, hitB.score) &lt; 0; } The problem with both solutions is, that we have now more comparisons per hit and the use of sentinels is questionable. I would like to remove the sentinels and use the old pre 2.9 code for comparing and using PQ.add() when a competitive hit arrives. The order of NaN would be unspecified. To fix the order of NaN, it would be better to replace all score comparisons by Float.compare() [also in FieldComparator]. I would like to delay 2.9.2 and 3.0.1 until this problem is discussed and solved.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2285</id>
      <title>Code cleanup from all sorts of (trivial) warnings</title>
      <description>I would like to do some code cleanup and remove all sorts of trivial warnings, like unnecessary casts, problems w/ javadocs, unused variables, redundant null checks, unnecessary semicolon etc. These are all very trivial and should not pose any problem. I'll create another issue for getting rid of deprecated code usage, like LuceneTestCase and all sorts of deprecated constructors. That's also trivial because it only affects Lucene code, but it's a different type of change. Another issue I'd like to create is about introducing more generics in the code, where it's missing today - not changing existing API. There are many places in the code like that. So, with you permission, I'll start with the trivial ones first, and then move on to the others.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2287</id>
      <title>Unexpected terms are highlighted within nested SpanQuery instances</title>
      <description>I haven't yet been able to resolve why I'm seeing spurious highlighting in nested SpanQuery instances. Briefly, the issue is illustrated by the second instance of "Lucene" being highlighted in the test below, when it doesn't satisfy the inner span. There's been some discussion about this on the java-dev list, and I'm opening this issue now because I have made some initial progress on this. This new test, added to the HighlighterTest class in lucene_2_9_1, illustrates this: /* Ref: http://www.lucidimagination.com/blog/2009/07/18/the-spanquery/ */ public void testHighlightingNestedSpans2() throws Exception { String theText = "The Lucene was made by Doug Cutting and Lucene great Hadoop was"; // Problem //String theText = "The Lucene was made by Doug Cutting and the great Hadoop was"; // Works okay String fieldName = "SOME_FIELD_NAME"; SpanNearQuery spanNear = new SpanNearQuery(new SpanQuery[] { new SpanTermQuery(new Term(fieldName, "lucene")), new SpanTermQuery(new Term(fieldName, "doug")) } , 5, true); Query query = new SpanNearQuery(new SpanQuery[] { spanNear, new SpanTermQuery(new Term(fieldName, "hadoop")) } , 4, true); String expected = "The &lt;B&gt;Lucene&lt;/B&gt; was made by &lt;B&gt;Doug&lt;/B&gt; Cutting and Lucene great &lt;B&gt;Hadoop&lt;/B&gt; was"; //String expected = "The &lt;B&gt;Lucene&lt;/B&gt; was made by &lt;B&gt;Doug&lt;/B&gt; Cutting and the great &lt;B&gt;Hadoop&lt;/B&gt; was"; String observed = highlightField(query, fieldName, theText); System.out.println("Expected: \"" + expected + "\n" + "Observed: \"" + observed); assertEquals("Why is that second instance of the term \"Lucene\" highlighted?", expected, observed); } Is this an issue that's arisen before? I've been reading through the source to QueryScorer, WeightedSpanTerm, WeightedSpanTermExtractor, Spans, and NearSpansOrdered, but haven't found the solution yet. Initially, I thought that the extractWeightedSpanTerms method in WeightedSpanTermExtractor should be called on each clause of a SpanNearQuery or SpanOrQuery, but that didn't get me too far.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2294</id>
      <title>Create IndexWriterConfiguration and store all of IW configuration there</title>
      <description>I would like to factor out of all IW configuration parameters into a single configuration class, which I propose to name IndexWriterConfiguration (or IndexWriterConfig). I want to store there almost everything besides the Directory, and to reduce all the ctors down to one: IndexWriter(Directory, IndexWriterConfiguration). What I was thinking of storing there are the following parameters: All of ctors parameters, except for Directory. The different setters where it makes sense. For example I still think infoStream should be set on IW directly. I'm thinking that IWC should expose everything in a setter/getter methods, and defaults to whatever IW defaults today. Except for Analyzer which will need to be defined in the ctor of IWC and won't have a setter. I am not sure why MaxFieldLength is required in all IW ctors, yet IW declares a DEFAULT (which is an int and not MaxFieldLength). Do we still think that 10000 should be the default? Why not default to UNLIMITED and otherwise let the application decide what LIMITED means for it? I would like to make MFL optional on IWC and default to something, and I hope that default will be UNLIMITED. We can document that on IWC, so that if anyone chooses to move to the new API, he should be aware of that ... I plan to deprecate all the ctors and getters/setters and replace them by: One ctor as described above getIndexWriterConfiguration, or simply getConfig, which can then be queried for the setting of interest. About the setters, I think maybe we can just introduce a setConfig method which will override everything that is overridable today, except for Analyzer. So someone could do iw.getConfig().setSomething(); iw.setConfig(newConfig); The setters on IWC can return an IWC to allow chaining set calls ... so the above will turn into iw.setConfig(iw.getConfig().setSomething1().setSomething2()); BTW, this is needed for Parallel Indexing (see LUCENE-1879), but I think it will greatly simplify IW's API. I'll start to work on a patch.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2295</id>
      <title>Create a MaxFieldLengthAnalyzer to wrap any other Analyzer and provide the same functionality as MaxFieldLength provided on IndexWriter</title>
      <description>A spinoff from LUCENE-2294. Instead of asking the user to specify on IndexWriter his requested MFL limit, we can get rid of this setting entirely by providing an Analyzer which will wrap any other Analyzer and its TokenStream with a TokenFilter that keeps track of the number of tokens produced and stop when the limit has reached. This will remove any count tracking in IW's indexing, which is done even if I specified UNLIMITED for MFL. Let's try to do it for 3.1.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2298</id>
      <title>Polish Analyzer</title>
      <description>Andrzej Bialecki has written a Polish stemmer and provided stemming tables for it under Apache License. You can read more about it here: http://www.getopt.org/stempel/ In reality, the stemmer is general code and we could use it for more languages too perhaps.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2302</id>
      <title>Replacement for TermAttribute+Impl with extended capabilities (byte[] support, CharSequence, Appendable)</title>
      <description>For flexible indexing terms can be simple byte[] arrays, while the current TermAttribute only supports char[]. This is fine for plain text, but e.g NumericTokenStream should directly work on the byte[] array. Also TermAttribute lacks of some interfaces that would make it simplier for users to work with them: Appendable and CharSequence I propose to create a new interface "CharTermAttribute" with a clean new API that concentrates on CharSequence and Appendable. The implementation class will simply support the old and new interface working on the same term buffer. DEFAULT_ATTRIBUTE_FACTORY will take care of this. So if somebody adds a TermAttribute, he will get an implementation class that can be also used as CharTermAttribute. As both attributes create the same impl instance both calls to addAttribute are equal. So a TokenFilter that adds CharTermAttribute to the source will work with the same instance as the Tokenizer that requested the (deprecated) TermAttribute. To also support byte[] only terms like Collation or NumericField needs, a separate getter-only interface will be added, that returns a reusable BytesRef, e.g. BytesRefGetterAttribute. The default implementation class will also support this interface. For backwards compatibility with old self-made-TermAttribute implementations, the indexer will check with hasAttribute(), if the BytesRef getter interface is there and if not will wrap a old-style TermAttribute (a deprecated wrapper class will be provided): new BytesRefGetterAttributeWrapper(TermAttribute), that is used by the indexer then.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2308</id>
      <title>Separately specify a field's type</title>
      <description>This came up from dicussions on IRC. I'm summarizing here... Today when you make a Field to add to a document you can set things index or not, stored or not, analyzed or not, details like omitTfAP, omitNorms, index term vectors (separately controlling offsets/positions), etc. I think we should factor these out into a new class (FieldType?). Then you could re-use this FieldType instance across multiple fields. The Field instance would still hold the actual value. We could then do per-field analyzers by adding a setAnalyzer on the FieldType, instead of the separate PerFieldAnalzyerWrapper (likewise for per-field codecs (with flex), where we now have PerFieldCodecWrapper). This would NOT be a schema! It's just refactoring what we already specify today. EG it's not serialized into the index. This has been discussed before, and I know Michael Busch opened a more ambitious (I think?) issue. I think this is a good first baby step. We could consider a hierarchy of FIeldType (NumericFieldType, etc.) but maybe hold off on that for starters...</description>
      <attachments/>
    </issue>
    <issue>
      <id>2309</id>
      <title>Fully decouple IndexWriter from analyzers</title>
      <description>IndexWriter only needs an AttributeSource to do indexing. Yet, today, it interacts with Field instances, holds a private analyzers, invokes analyzer.reusableTokenStream, has to deal with a wide variety (it's not analyzed; it is analyzed but it's a Reader, String; it's pre-analyzed). I'd like to have IW only interact with attr sources that already arrived with the fields. This would be a powerful decoupling – it means others are free to make their own attr sources. They need not even use any of Lucene's analysis impls; eg they can integrate to other things like OpenPipeline. Or make something completely custom. LUCENE-2302 is already a big step towards this: it makes IW agnostic about which attr is "the term", and only requires that it provide a BytesRef (for flex). Then I think LUCENE-2308 would get us most of the remaining way – ie, if the FieldType knows the analyzer to use, then we could simply create a getAttrSource() method (say) on it and move all the logic IW has today onto there. (We'd still need existing IW code for back-compat).</description>
      <attachments/>
    </issue>
    <issue>
      <id>2312</id>
      <title>Search on IndexWriter's RAM Buffer</title>
      <description>In order to offer user's near realtime search, without incurring an indexing performance penalty, we can implement search on IndexWriter's RAM buffer. This is the buffer that is filled in RAM as documents are indexed. Currently the RAM buffer is flushed to the underlying directory (usually disk) before being made searchable. Todays Lucene based NRT systems must incur the cost of merging segments, which can slow indexing. Michael Busch has good suggestions regarding how to handle deletes using max doc ids. https://issues.apache.org/jira/browse/LUCENE-2293?focusedCommentId=12841923&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12841923 The area that isn't fully fleshed out is the terms dictionary, which needs to be sorted prior to queries executing. Currently IW implements a specialized hash table. Michael B has a suggestion here: https://issues.apache.org/jira/browse/LUCENE-2293?focusedCommentId=12841915&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12841915</description>
      <attachments/>
    </issue>
    <issue>
      <id>2320</id>
      <title>Add MergePolicy to IndexWriterConfig</title>
      <description>Now that IndexWriterConfig is in place, I'd like to move MergePolicy to it as well. The change is not straightforward and so I've kept it for a separate issue. MergePolicy requires in its ctor an IndexWriter, however none can be passed to it before an IndexWriter actually exists. And today IW may create an MP just for it to be overridden by the application one line afterwards. I don't want to make iw member of MP non-final, or settable by extending classes, however it needs to remain protected so they can access it directly. So the proposed changes are: Add a SetOnce object (to o.a.l.util), or Immutable, which can only be set once (hence its name). It'll have the signature SetOnce&lt;T&gt; w/ synchronized set&lt;T&gt; and T get(). T will be declared volatile, so that get() won't be synchronized. MP will define a protected final SetOnce&lt;IndexWriter&gt; writer instead of the current writer. NOTE: this is a bw break. any suggestions are welcomed. MP will offer a public default ctor, together with a set(IndexWriter). IndexWriter will set itself on MP using set(this). Note that if set will be called more than once, it will throw an exception (AlreadySetException - or does someone have a better suggestion, preferably an already existing Java exception?). That's the core idea. I'd like to post a patch soon, so I'd appreciate your review and proposals.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2323</id>
      <title>reorganize contrib modules</title>
      <description>it would be nice to reorganize contrib modules, so that they are bundled together by functionality. For example: the wikipedia contrib is a tokenizer, i think really belongs in contrib/analyzers there are two highlighters, i think could be one highlighters package. there are many queryparsers and queries in different places in contrib</description>
      <attachments/>
    </issue>
    <issue>
      <id>2324</id>
      <title>Per thread DocumentsWriters that write their own private segments</title>
      <description>See LUCENE-2293 for motivation and more details. I'm copying here Mike's summary he posted on 2293: Change the approach for how we buffer in RAM to a more isolated approach, whereby IW has N fully independent RAM segments in-process and when a doc needs to be indexed it's added to one of them. Each segment would also write its own doc stores and "normal" segment merging (not the inefficient merge we now do on flush) would merge them. This should be a good simplification in the chain (eg maybe we can remove the *PerThread classes). The segments can flush independently, letting us make much better concurrent use of IO &amp; CPU.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2326</id>
      <title>Remove SVN.exe and revision numbers from build.xml by svn-copy the backwards branch and linking snowball tests by svn:externals</title>
      <description>As we often need to update backwards tests together with trunk and always have to update the branch first, record rev no, and update build xml, I would simply like to do a svn copy/move of the backwards branch. After a release, this is simply also done: svn rm backwards svn cp releasebranch backwards By this we can simply commit in one pass, create patches in one pass. The snowball tests are currently downloaded by svn.exe, too. These need a fixed version for checkout. I would like to change this to use svn:externals. Will provide patch, soon.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2329</id>
      <title>Use parallel arrays instead of PostingList objects</title>
      <description>This is Mike's idea that was discussed in LUCENE-2293 and LUCENE-2324. In order to avoid having very many long-living PostingList objects in TermsHashPerField we want to switch to parallel arrays. The termsHash will simply be a int[] which maps each term to dense termIDs. All data that the PostingList classes currently hold will then we placed in parallel arrays, where the termID is the index into the arrays. This will avoid the need for object pooling, will remove the overhead of object initialization and garbage collection. Especially garbage collection should benefit significantly when the JVM runs out of memory, because in such a situation the gc mark times can get very long if there is a big number of long-living objects in memory. Another benefit could be to build more efficient TermVectors. We could avoid the need of having to store the term string per document in the TermVector. Instead we could just store the segment-wide termIDs. This would reduce the size and also make it easier to implement efficient algorithms that use TermVectors, because no term mapping across documents in a segment would be necessary. Though this improvement we can make with a separate jira issue.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2339</id>
      <title>Allow Directory.copy() to accept a collection of file names to be copied</title>
      <description>Par example, I want to copy files pertaining to a certain commit, and not everything there is in a Directory.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2341</id>
      <title>explore morfologik integration</title>
      <description>Dawid Weiss mentioned on LUCENE-2298 that there is another Polish stemmer available: http://sourceforge.net/projects/morfologik/ This works differently than LUCENE-2298, and ideally would be another option for users.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2357</id>
      <title>Reduce transient RAM usage while merging by using packed ints array for docID re-mapping</title>
      <description>We allocate this int[] to remap docIDs due to compaction of deleted ones. This uses alot of RAM for large segment merges, and can fail to allocate due to fragmentation on 32 bit JREs. Now that we have packed ints, a simple fix would be to use a packed int array... and maybe instead of storing abs docID in the mapping, we could store the number of del docs seen so far (so the remap would do a lookup then a subtract). This may add some CPU cost to merging but should bring down transient RAM usage quite a bit.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2369</id>
      <title>Locale-based sort by field with low memory overhead</title>
      <description>The current implementation of locale-based sort in Lucene uses the FieldCache which keeps all sort terms in memory. Beside the huge memory overhead, searching requires comparison of terms with collator.compare every time, making searches with millions of hits fairly expensive. This proposed alternative implementation is to create a packed list of pre-sorted ordinals for the sort terms and a map from document-IDs to entries in the sorted ordinals list. This results in very low memory overhead and faster sorted searches, at the cost of increased startup-time. As the ordinals can be resolved to terms after the sorting has been performed, this approach supports fillFields=true. This issue is related to https://issues.apache.org/jira/browse/LUCENE-2335 which contain previous discussions on the subject.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2372</id>
      <title>Replace deprecated TermAttribute by new CharTermAttribute</title>
      <description>After LUCENE-2302 is merged to trunk with flex, we need to carry over all tokenizers and consumers of the TokenStreams to the new CharTermAttribute. We should also think about adding a AttributeFactory that creates a subclass of CharTermAttributeImpl that returns collation keys in toBytesRef() accessor. CollationKeyFilter is then obsolete, instead you can simply convert every TokenStream to indexing only CollationKeys by changing the attribute implementation.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2373</id>
      <title>Create a Codec to work with streaming and append-only filesystems</title>
      <description>Since early 2.x times Lucene used a skip/seek/write trick to patch the length of the terms dict into a place near the start of the output data file. This however made it impossible to use Lucene with append-only filesystems such as HDFS. In the post-flex trunk the following code in StandardTermsDictWriter initiates this: // Count indexed fields up front CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); out.writeLong(0); // leave space for end index pointer and completes this in close(): out.seek(CodecUtil.headerLength(CODEC_NAME)); out.writeLong(dirStart); I propose to change this layout so that this pointer is stored simply at the end of the file. It's always 8 bytes long, and we known the final length of the file from Directory, so it's a single additional seek(length - 8) to read it, which is not much considering the benefits.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2374</id>
      <title>Add reflection API to AttributeSource/AttributeImpl</title>
      <description>AttributeSource/TokenStream inspection in Solr needs to have some insight into the contents of AttributeImpls. As LUCENE-2302 has some problems with toString() [which is not structured and conflicts with CharSequence's definition for CharTermAttribute], I propose an simple API that get a default implementation in AttributeImpl (just like toString() current): Iterator&lt;Map.Entry&lt;String,?&gt;&gt; AttributeImpl.contentsIterator() returns an iterator (for most attributes its a singleton) of a key-value pair, e.g. "term"&gt;"foobar","startOffset"&gt;Integer.valueOf(0),... AttributeSource gets the same method, it just concat the iterators of each getAttributeImplsIterator() AttributeImpl No backwards problems occur, as the default toString() method will work like before (it just gets iterator and lists), but we simply remove the documentation for the format. (Char)TermAttribute gets a special impl fo toString() according to CharSequence and a corresponding iterator. I also want to remove the abstract hashCode() and equals() methods from AttributeImpl, as they are not needed and just create work for the implementor.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2378</id>
      <title>Cutover remaining usage of pre-flex APIs</title>
      <description>A number of places still use the pre-flex APIs. This is actually healthy, since it gives us ongoing testing of the back compat emulation layer. But we should at some point cut them all over to flex. Latest we can do this is 4.0, but I'm not sure we should do them all for 3.1... still marking this as 3.1 to "remind us"</description>
      <attachments/>
    </issue>
    <issue>
      <id>2380</id>
      <title>Add FieldCache.getTermBytes, to load term data as byte[]</title>
      <description>With flex, a term is now an opaque byte[] (typically, utf8 encoded unicode string, but not necessarily), so we need to push this up the search stack. FieldCache now has getStrings and getStringIndex; we need corresponding methods to load terms as native byte[], since in general they may not be representable as String. This should be quite a bit more RAM efficient too, for US ascii content since each character would then use 1 byte not 2.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2393</id>
      <title>Utility to output total term frequency and df from a lucene index</title>
      <description>This is a pair of command line utilities that provide information on the total number of occurrences of a term in a Lucene index. The first takes a field name, term, and index directory and outputs the document frequency for the term and the total number of occurrences of the term in the index (i.e. the sum of the tf of the term for each document). The second reads the index to determine the top N most frequent terms (by document frequency) and then outputs a list of those terms along with the document frequency and the total number of occurrences of the term. Both utilities are useful for estimating the size of the term's entry in the *prx files and consequent Disk I/O demands.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2400</id>
      <title>ShingleFilter: don't output all-filler shingles/unigrams; also, convert from TermAttribute to CharTermAttribute</title>
      <description>When the input token stream to ShingleFilter has position increments greater than one, filler tokens are inserted for each position for which there is no token in the input token stream. As a result, unigrams (if configured) and shingles can be filler-only. Filler-only output tokens make no sense - these should be removed. Also, because TermAttribute has been deprecated in favor of CharTermAttribute, the patch will also convert TermAttribute usages to CharTermAttribute in ShingleFilter.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2402</id>
      <title>Add an explicit method to invoke IndexDeletionPolicy</title>
      <description>Today, if one uses an IDP which holds onto segments, such as SnapshotDeletionPolicy, or any other IDP in the tests, those segments are left in the index even if the IDP no longer references them, until IW.commit() is called (and actually does something). I'd like to add a specific method to IW which will invoke the IDP's logic and get rid of the unused segments w/o forcing the user to call IW.commit(). There are a couple of reasons for that: Segments take up sometimes valuable HD space, and the application may wish to reclaim that space immediately. In some scenarios, the index is updated once in several hours (or even days), and waiting until then may not be acceptable. I think it's a cleaner solution than waiting for the next commit() to happen. One can still wait for it if one wants, but otherwise it will give you the ability to immediately get rid of those segments. TestSnapshotDeletionPolicy includes this code, which only strengthens (IMO) the need for such method: // Add one more document to force writer to commit a // final segment, so deletion policy has a chance to // delete again: Document doc = new Document(); doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS)); writer.addDocument(doc); If IW had an explicit method, that code would not need to exist there at all ... Here comes the fun part - naming the baby: invokeDeletionPolicy – describes exactly what is going to happen. However, if the user did not set IDP at all (relying on default, which I think many do), users won't understand what is it. deleteUnusedSegments - more user-friendly, assuming users understand what 'segments' are. BTW, IW already has deleteUnusedFiles() which only tries to delete unreferenced files that failed to delete before (such as on Windows, due to e.g. open readers). Perhaps instead of inventing a new name, we can change IW.deleteUnusedFiles to call IndexFileDeleter.checkpoint (instead of deletePendingFiles) which deletes those files + calls IDP.onCommit().</description>
      <attachments/>
    </issue>
    <issue>
      <id>2413</id>
      <title>Consolidate all (Solr's &amp; Lucene's) analyzers into modules/analysis</title>
      <description>We've been wanting to do this for quite some time now... I think, now that Solr/Lucene are merged, and we're looking at opening an unstable line of development for Solr/Lucene, now is the right time to do it. A standalone module for all analyzers also empowers apps to separately version the analyzers from which version of Solr/Lucene they use, possibly enabling us to remove Version entirely from the analyzers. We should also do LUCENE-2309 (decouple, as much as possible, indexer from the analysis API), but I don't think that issue needs to block this consolidation. Once we do this, there is one place where our users can find all the analyzers that Solr/Lucene provide.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2421</id>
      <title>Hardening of NativeFSLock</title>
      <description>NativeFSLock create a test lock file which its name might collide w/ another JVM that is running. Very unlikely, but still it happened a couple of times already, since the tests were parallelized. This may result in a false exception thrown from release(), when the lock file's delete() is called and returns false, because the file does not exist (deleted by another JVM already). In addition, release() should give a second attempt to delete() if it fails, since the file may be held temporarily by another process (like AntiVirus) before it fails. The proposed changes are: 1) Use ManagementFactory.getRuntimeMXBean().getName() as part of the test lock name (should include the process Id) 2) In release(), if delete() fails, check if the file indeed exists. If it is, let's attempt a re-delete() few ms later. 3) If (3) still fails, throw an exception. Alternatively, we can attempt a deleteOnExit. I'll post a patch later today.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2446</id>
      <title>Add checksums to Lucene segment files</title>
      <description>It would be useful for the different files in a Lucene index to include checksums. This would make it easy to spot corruption while copying index files around; the various cloud efforts assume many more data-copying operations than older single-index implementations. This feature might be much easier to implement if all index files are created in a sequential fashion. This issue therefore depends on LUCENE-2373.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2454</id>
      <title>Nested Document query support</title>
      <description>A facility for querying nested documents in a Lucene index as outlined in http://www.slideshare.net/MarkHarwood/proposal-for-nested-document-support-in-lucene</description>
      <attachments/>
    </issue>
    <issue>
      <id>2455</id>
      <title>Some house cleaning in addIndexes*</title>
      <description>Today, the use of addIndexes and addIndexesNoOptimize is confusing - especially on when to invoke each. Also, addIndexes calls optimize() in the beginning, but only on the target index. It also includes the following jdoc statement, which from how I understand the code, is wrong: After this completes, the index is optimized. – optimize() is called in the beginning and not in the end. On the other hand, addIndexesNoOptimize does not call optimize(), and relies on the MergeScheduler and MergePolicy to handle the merges. After a short discussion about that on the list (Thanks Mike for the clarifications!) I understand that there are really two core differences between the two: addIndexes supports IndexReader extensions addIndexesNoOptimize performs better This issue proposes the following: Clear up the documentation of each, spelling out the pros/cons of calling them clearly in the javadocs. Rename addIndexesNoOptimize to addIndexes Remove optimize() call from addIndexes(IndexReader...) Document that clearly in both, w/ a recommendation to call optimize() before on any of the Directories/Indexes if it's a concern. That way, we maintain all the flexibility in the API - addIndexes(IndexReader...) allows for using IR extensions, addIndexes(Directory...) is considered more efficient, by allowing the merges to happen concurrently (depending on MS) and also factors in the MP. So unless you have an IR extension, addDirectories is really the one you should be using. And you have the freedom to call optimize() before each if you care about it, or don't if you don't care. Either way, incurring the cost of optimize() is entirely in the user's hands. BTW, addIndexes(IndexReader...) does not use neither the MergeScheduler nor MergePolicy, but rather call SegmentMerger directly. This might be another place for improvement. I'll look into it, and if it's not too complicated, I may cover it by this issue as well. If you have any hints that can give me a good head start on that, please don't be shy .</description>
      <attachments/>
    </issue>
    <issue>
      <id>2471</id>
      <title>Supporting bulk copies in Directory</title>
      <description>A method can be added to IndexOutput that accepts IndexInput, and writes bytes using it as a source. This should be used for bulk-merge cases (offhand - norms, docstores?). Some Directories can then override default impl and skip intermediate buffers (NIO, MMap, RAM?).</description>
      <attachments/>
    </issue>
    <issue>
      <id>2474</id>
      <title>Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey)</title>
      <description>Allow to plug in a Cache Eviction Listener to IndexReader to eagerly clean custom caches that use the IndexReader (getFieldCacheKey). A spin of: https://issues.apache.org/jira/browse/LUCENE-2468. Basically, its make a lot of sense to cache things based on IndexReader#getFieldCacheKey, even Lucene itself uses it, for example, with the CachingWrapperFilter. FieldCache enjoys being called explicitly to purge its cache when possible (which is tricky to know from the "outside", especially when using NRT - reader attack of the clones). The provided patch allows to plug a CacheEvictionListener which will be called when the cache should be purged for an IndexReader.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2480</id>
      <title>Remove support for pre-3.0 indexes</title>
      <description>We should remove support for 2.x (and 1.9) indexes in 4.0. It seems that nothing can be done in 3x because there is no special code which handles 1.9, so we'll leave it there. This issue should cover: Remove the .zip indexes Remove the unnecessary code from SegmentInfo and SegmentInfos. Mike suggests we compare the version headers at the top of SegmentInfos, in 2.9.x vs 3.0.x, to see which ones can go. remove FORMAT_PRE from FieldInfos Remove old format from TermVectorsReader If you know of other places where code can be removed, then please post a comment here. I don't know when I'll have time to handle it, definitely not in the next few days. So if someone wants to take a stab at it, be my guest.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2482</id>
      <title>Index sorter</title>
      <description>A tool to sort index according to a float document weight. Documents with high weight are given low document numbers, which means that they will be first evaluated. When using a strategy of "early termination" of queries (see TimeLimitedCollector) such sorting significantly improves the quality of partial results. (Originally this tool was created by Doug Cutting in Nutch, and used norms as document weights - thus the ordering was limited by the limited resolution of norms. This is a pure Lucene version of the tool, and it uses arbitrary floats from a specified stored field).</description>
      <attachments/>
    </issue>
    <issue>
      <id>2507</id>
      <title>automaton spellchecker</title>
      <description>The current spellchecker makes an n-gram index of your terms, and queries this for spellchecking. The terms that come back from the n-gram query are then re-ranked by an algorithm such as Levenshtein. Alternatively, we could just do a levenshtein query directly against the index, then we wouldn't need a separate index to rebuild.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2548</id>
      <title>Remove all interning of field names from flex API</title>
      <description>In previous versions of Lucene, interning of fields was important to minimize string comparison cost when iterating TermEnums, to detect changes in field name. As we separated field names from terms in flex, no query compares field names anymore, so the whole performance problematic interning can be removed. I will start with doing this, but we need to carefully review some places e.g. in preflex codec. Maybe before this issue we should remove the Term class completely. Robert?</description>
      <attachments/>
    </issue>
    <issue>
      <id>2556</id>
      <title>(Char)TermAttribute cloning memory consumption</title>
      <description>The memory consumption problem with cloning a (Char)TermAttributeImpl object was raised on thread http://markmail.org/thread/bybuerugbk5w2u6z</description>
      <attachments/>
    </issue>
    <issue>
      <id>2573</id>
      <title>Tiered flushing of DWPTs by RAM with low/high water marks</title>
      <description>Now that we have DocumentsWriterPerThreads we need to track total consumed RAM across all DWPTs. A flushing strategy idea that was discussed in LUCENE-2324 was to use a tiered approach: Flush the first DWPT at a low water mark (e.g. at 90% of allowed RAM) Flush all DWPTs at a high water mark (e.g. at 110%) Use linear steps in between high and low watermark: E.g. when 5 DWPTs are used, flush at 90%, 95%, 100%, 105% and 110%. Should we allow the user to configure the low and high water mark values explicitly using total values (e.g. low water mark at 120MB, high water mark at 140MB)? Or shall we keep for simplicity the single setRAMBufferSizeMB() config method and use something like 90% and 110% for the water marks?</description>
      <attachments/>
    </issue>
    <issue>
      <id>2574</id>
      <title>Optimize copies between IndexInput and Output</title>
      <description>We've created an optimized copy of files from Directory to Directory. We've also optimized copyBytes recently. However, we're missing the opposite side of the copy - from IndexInput to Output. I'd like to mimic the FileChannel API by having copyTo on IndexInput and copyFrom on IndexOutput. That way, both sides can optimize the copy process, depending on the type of the IndexInput/Output that they need to copy to/from. FSIndexInput/Output can use FileChannel if the two are FS types. RAMInput/OutputStream can copy to/from the buffers directly, w/o going through intermediate ones. Actually, for RAMIn/Out this might be a big win, because it doesn't care about the type of IndexInput/Output given - it just needs to copy to its buffer directly. If we do this, I think we can consolidate all Dir.copy() impls down to one (in Directory), and rely on the In/Out ones to do the optimized copy. Plus, it will enable someone to do optimized copies between In/Out outside the scope of Directory. If this somehow turns out to be impossible, or won't make sense, then I'd like to optimize RAMDirectory.copy(Dir, src, dest) to not use an intermediate buffer.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2575</id>
      <title>Concurrent byte and int block implementations</title>
      <description>The current *BlockPool implementations aren't quite concurrent. We really need something that has a locking flush method, where flush is called at the end of adding a document. Once flushed, the newly written data would be available to all other reading threads (ie, postings etc). I'm not sure I understand the slices concept, it seems like it'd be easier to implement a seekable random access file like API. One'd seek to a given position, then read or write from there. The underlying management of byte arrays could then be hidden?</description>
      <attachments/>
    </issue>
    <issue>
      <id>2588</id>
      <title>terms index should not store useless suffixes</title>
      <description>This idea came up when discussing w/ Robert how to improve our terms index... The terms dict index today simply grabs whatever term was at a 0 mod 128 index (by default). But this is wasteful because you often don't need the suffix of the term at that point. EG if the 127th term is aa and the 128th (indexed) term is abcd123456789, instead of storing that full term you only need to store ab. The suffix is useless, and uses up RAM since we load the terms index into RAM. The patch is very simple. The optimization is particularly easy because terms are now byte[] and we sort in binary order. I tested on first 10M 1KB Wikipedia docs, and this reduces the terms index (tii) file from 3.9 MB -&gt; 3.3 MB = 16% smaller (using StandardAnalyzer, indexing body field tokenized but title / date fields untokenized). I expect on noisier terms dicts, especially ones w/ bad terms accidentally indexed, that the savings will be even more. In the future we could do crazier things. EG there's no real reason why the indexed terms must be regular (every N terms), so, we could instead pick terms more carefully, say "approximately" every N, but favor terms that have a smaller net prefix. We can also index more sparsely in regions where the net docFreq is lowish, since we can afford somewhat higher seek+scan time to these terms since enuming their docs will be much faster.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2590</id>
      <title>Enable access to the freq information in a Query's sub-scorers</title>
      <description>The ability to gather more details than just the score, of how a given doc matches the current query, has come up a number of times on the user's lists. (most recently in the thread "Query Match Count" by Ryan McV on java-user). EG if you have a simple TermQuery "foo", on each hit you'd like to know how many times "foo" occurred in that doc; or a BooleanQuery +foo +bar, being able to separately see the freq of foo and bar for the current hit. Lucene doesn't make this possible today, which is a shame because Lucene in fact does compute exactly this information; it's just not accessible from the Collector.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2599</id>
      <title>Deprecate Spatial Contrib</title>
      <description>The spatial contrib is blighted by bugs. The latest series, found by Grant and discussed here shows that we need to re-think the cartesian tier implementation. Given the need to create a spatial module containing code taken from both lucene and Solr, it makes sense to deprecate the spatial contrib, and start from scratch in the new module.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2604</id>
      <title>add regexpquery to queryparser</title>
      <description>patch that adds RegexpQuery if you /enter an expression between slashes like this/ i didnt do the contrib ones but could add it there too if it seems like a good idea.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2609</id>
      <title>Generate jar containing test classes.</title>
      <description>The test classes are useful for writing unit tests for code external to the Lucene project. It would be helpful to build a jar of these classes and publish them as a maven dependency.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2611</id>
      <title>IntelliJ IDEA and Eclipse setup</title>
      <description>Setting up Lucene/Solr in IntelliJ IDEA or Eclipse can be time-consuming. The attached patches add a new top level directory dev-tools/ with sub-dirs idea/ and eclipse/ containing basic setup files for trunk, as well as top-level ant targets named "idea" and "eclipse" that copy these files into the proper locations. This arrangement avoids the messiness attendant to in-place project configuration files directly checked into source control. The IDEA configuration includes modules for Lucene and Solr, each Lucene and Solr contrib, and each analysis module. A JUnit run configuration per module is included. The Eclipse configuration includes a source entry for each source/test/resource location and classpath setup: a library entry for each jar. For IDEA, once ant idea has been run, the only configuration that must be performed manually is configuring the project-level JDK. For Eclipse, once ant eclipse has been run, the user has to refresh the project (right-click on the project and choose Refresh). If these patches is committed, Subversion svn:ignore properties should be added/modified to ignore the destination IDEA and Eclipse configuration locations. Iam Jambour has written up on the Lucene wiki a detailed set of instructions for applying the 3.X branch patch for IDEA: http://wiki.apache.org/lucene-java/HowtoConfigureIntelliJ</description>
      <attachments/>
    </issue>
    <issue>
      <id>2621</id>
      <title>Extend Codec to handle also stored fields and term vectors</title>
      <description>Currently Codec API handles only writing/reading of term-related data, while stored fields data and term frequency vector data writing/reading is handled elsewhere. I propose to extend the Codec API to handle this data as well.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2632</id>
      <title>FilteringCodec, TeeCodec, TeeDirectory</title>
      <description>This issue adds two new Codec implementations: TeeCodec: there have been attempts in the past to implement parallel writing to multiple indexes so that they are all synchronized. This was however complicated due to the complexity of IndexWriter/SegmentMerger logic. The solution presented here offers a similar functionality but working on a different level - as the name suggests, the TeeCodec duplicates index data into multiple output Directories. TeeDirectory (used also in TeeCodec) is a simple abstraction to perform Directory operations on several directories in parallel (effectively mirroring their data). Optionally it's possible to specify a set of suffixes of files that should be mirrored so that non-matching files are skipped. FilteringCodec is related in a remote way to the ideas of index pruning presented in LUCENE-1812 and the concept of tiered search. Since we can use TeeCodec to write to multiple output Directories in a synchronized way, we could also filter out or modify some of the data that is being written. The FilteringCodec provides this functionality, so that you can use like this: IndexWriter --&gt; TeeCodec | | | +--&gt; StandardCodec --&gt; Directory1 +--&gt; FilteringCodec --&gt; StandardCodec --&gt; Directory2 The end result of this chain is two indexes that are kept in sync - one is the full regular index, and the other one is a filtered index.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2649</id>
      <title>FieldCache should include a BitSet for matching docs</title>
      <description>The FieldCache returns an array representing the values for each doc. However there is no way to know if the doc actually has a value. This should be changed to return an object representing the values and a BitSet for all valid docs.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2655</id>
      <title>Get deletes working in the realtime branch</title>
      <description>Deletes don't work anymore, a patch here will fix this.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2657</id>
      <title>Replace Maven POM templates with full POMs, and change documentation accordingly</title>
      <description>The current Maven POM templates only contain dependency information, the bare bones necessary for uploading artifacts to the Maven repository. The full Maven POMs in the attached patch include the information necessary to run a multi-module Maven build, in addition to serving the same purpose as the current POM templates. Several dependencies are not available through public maven repositories. A profile in the top-level POM can be activated to install these dependencies from the various lib/ directories into your local repository. From the top-level directory: mvn -N -Pbootstrap install Once these non-Maven dependencies have been installed, to run all Lucene/Solr tests via Maven's surefire plugin, and populate your local repository with all artifacts, from the top level directory, run: mvn install When one Lucene/Solr module depends on another, the dependency is declared on the artifact(s) produced by the other module and deposited in your local repository, rather than on the other module's un-jarred compiler output in the build/ directory, so you must run mvn install on the other module before its changes are visible to the module that depends on it. To create all the artifacts without running tests: mvn -DskipTests install I almost always include the clean phase when I do a build, e.g.: mvn -DskipTests clean install</description>
      <attachments/>
    </issue>
    <issue>
      <id>2662</id>
      <title>BytesHash</title>
      <description>This issue will have the BytesHash separated out from LUCENE-2186</description>
      <attachments/>
    </issue>
    <issue>
      <id>2665</id>
      <title>Rework FieldCache to be more flexible/general</title>
      <description>The existing FieldCache implementation is very rigid and does not allow much flexibility. In trying to implement simple features, it points to much larger structural problems. This patch aims to take a fresh approach to how we work with the FieldCache.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2671</id>
      <title>Add sort missing first/last ability to SortField and ValueComparator</title>
      <description>When SortField and ValueComparator use EntryCreators (from LUCENE-2649) they use a special sort value when the field is missing. This enables lucene to implement 'sort missing last' or 'sort missing first' for numeric values from the FieldCache.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2680</id>
      <title>Improve how IndexWriter flushes deletes against existing segments</title>
      <description>IndexWriter buffers up all deletes (by Term and Query) and only applies them if 1) commit or NRT getReader() is called, or 2) a merge is about to kickoff. We do this because, for a large index, it's very costly to open a SegmentReader for every segment in the index. So we defer as long as we can. We do it just before merge so that the merge can eliminate the deleted docs. But, most merges are small, yet in a big index we apply deletes to all of the segments, which is really very wasteful. Instead, we should only apply the buffered deletes to the segments that are about to be merged, and keep the buffer around for the remaining segments. I think it's not so hard to do; we'd have to have generations of pending deletions, because the newly merged segment doesn't need the same buffered deletions applied again. So every time a merge kicks off, we pinch off the current set of buffered deletions, open a new set (the next generation), and record which segment was created as of which generation. This should be a very sizable gain for large indices that mix deletes, though, less so in flex since opening the terms index is much faster.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2690</id>
      <title>Do MultiTermQuery boolean rewrites per segment</title>
      <description>MultiTermQuery currently rewrites FuzzyQuery (using TopTermsBooleanQueryRewrite), the auto constant rewrite method and the ScoringBQ rewrite methods using a MultiFields wrapper on the top-level reader. This is inefficient. This patch changes the rewrite modes to do the rewrites per segment and uses some additional datastructures (hashed sets/maps) to exclude duplicate terms. All tests currently pass, but FuzzyQuery's tests should not, because it depends for the minimum score handling, that the terms are collected in order.. Robert will fix FuzzyQuery in this issue, too. This patch is just a start.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2691</id>
      <title>Consolidate Near Real Time and Reopen API semantics</title>
      <description>We should consolidate the IndexWriter.getReader and the IndexReader.reopen semantics, since most people are already using the IR.reopen() method, we should simply add:: IR.reopen(IndexWriter) Initially, it could just call the IW.getReader(), but it probably should switch to just using package private methods for sharing the internals</description>
      <attachments/>
    </issue>
    <issue>
      <id>2694</id>
      <title>MTQ rewrite + weight/scorer init should be single pass</title>
      <description>Spinoff of LUCENE-2690 (see the hacked patch on that issue)... Once we fix MTQ rewrite to be per-segment, we should take it further and make weight/scorer init also run in the same single pass as rewrite.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2719</id>
      <title>Re-add SorterTemplate and use it to provide fast ArraySorting and replace BytesRefHash sorting</title>
      <description>This patch adds back an optimized and rewritten SorterTemplate back to Lucene (removed after release of 3.0). It is of use for several components: Automaton: Automaton needs to sort States and other things. Using Arrays.sort() is slow, because it clones internally to ensure stable search. This component is much faster. This patch adds Arrays.sort() replacements in ArrayUtil that work with natural order or using a Comparator&lt;?&gt;. You can choose between quickSort and mergeSort. BytesRefHash uses another QuickSort algorithm without insertionSort for very short ord arrays. This class uses SorterTemplate to provide the same with insertionSort fallback in a very elegant way. Ideally this class can be used everywhere, where the sort algorithm needs to be separated from the underlying data and you can implement a swap() and compare() function (that get slot numbers instead of real values). This also applies to Solr (Yonik?). SorterTemplate provides quickSort and mergeSort algorithms. Internally for short arrays, it automatically chooses insertionSort (like JDK's Arrays). The quickSort algorith was copied modified from old BytesRefHash. This new class only shares MergeSort with the original CGLIB SorterTemplate, which is no longer maintained.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2723</id>
      <title>Speed up Lucene's low level bulk postings read API</title>
      <description>Spinoff from LUCENE-1410. The flex DocsEnum has a simple bulk-read API that reads the next chunk of docs/freqs. But it's a poor fit for intblock codecs like FOR/PFOR (from LUCENE-1410). This is not unlike sucking coffee through those tiny plastic coffee stirrers they hand out airplanes that, surprisingly, also happen to function as a straw. As a result we see no perf gain from using FOR/PFOR. I had hacked up a fix for this, described at in my blog post at http://chbits.blogspot.com/2010/08/lucene-performance-with-pfordelta-codec.html I'm opening this issue to get that work to a committable point. So... I've worked out a new bulk-read API to address performance bottleneck. It has some big changes over the current bulk-read API: You can now also bulk-read positions (but not payloads), but, I have yet to cutover positional queries. The buffer contains doc deltas, not absolute values, for docIDs and positions (freqs are absolute). Deleted docs are not filtered out. The doc &amp; freq buffers need not be "aligned". For fixed intblock codecs (FOR/PFOR) they will be, but for varint codecs (Simple9/16, Group varint, etc.) they won't be. It's still a work in progress...</description>
      <attachments/>
    </issue>
    <issue>
      <id>2745</id>
      <title>ArabicAnalyzer - the ability to recognise email addresses host names and so on</title>
      <description>The ArabicAnalyzer does not recognise email addresses, hostnames and so on. For example, adam@hotmail.com will be tokenised to [adam] [hotmail] [com] It would be great if the ArabicAnalyzer can tokenises this to [adam@hotmail.com]. The same applies to hostnames and so on. Can this be resolved? I hope so Thanks MAA</description>
      <attachments/>
    </issue>
    <issue>
      <id>2747</id>
      <title>Deprecate/remove language-specific tokenizers in favor of StandardTokenizer</title>
      <description>As of Lucene 3.1, StandardTokenizer implements UAX#29 word boundary rules to provide language-neutral tokenization. Lucene contains several language-specific tokenizers that should be replaced by UAX#29-based StandardTokenizer (deprecated in 3.1 and removed in 4.0). The language-specific analyzers, by contrast, should remain, because they contain language-specific post-tokenization filters. The language-specific analyzers should switch to StandardTokenizer in 3.1. Some usages of language-specific tokenizers will need additional work beyond just replacing the tokenizer in the language-specific analyzer. For example, PersianAnalyzer currently uses ArabicLetterTokenizer, and depends on the fact that this tokenizer breaks tokens on the ZWNJ character (zero-width non-joiner; U+200C), but in the UAX#29 word boundary rules, ZWNJ is not a word boundary. Robert Muir has suggested using a char filter converting ZWNJ to spaces prior to StandardTokenizer in the converted PersianAnalyzer.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2755</id>
      <title>Some improvements to CMS</title>
      <description>While running optimize on a large index, I've noticed several things that got me to read CMS code more carefully, and find these issues: CMS may hold onto a merge if maxMergeCount is hit. That results in the MergeThreads taking merges from the IndexWriter until they are exhausted, and only then that blocked merge will run. I think it's unnecessary that that merge will be blocked. CMS sorts merges by segments size, doc-based and not bytes-based. Since the default MP is LogByteSizeMP, and I hardly believe people care about doc-based size segments anymore, I think we should switch the default impl. There are two ways to make it extensible, if we want: Have an overridable member/method in CMS that you can extend and override - easy. Have OneMerge be comparable and let the MP determine the order (e.g. by bytes, docs, calibrate deletes etc.). Better, but will need to tap into several places in the code, so more risky and complicated. On the go, I'd like to add some documentation to CMS - it's not very easy to read and follow. I'll work on a patch.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2768</id>
      <title>add infrastructure for longer running nightly test cases</title>
      <description>I'm spinning this out of LUCENE-2762... The patch there adds initial infrastructure for tests to pull documents from a line file, and adds a longish running test case using that line file to test NRT. I'd like to see some tests run on more substantial indices based on real data... so this is just a start.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2771</id>
      <title>Remove norms() support from non-atomic IndexReaders</title>
      <description>Spin-off from LUCENE-2769: Currently all IndexReaders support norms(), but the core of Lucene never uses it and its even dangerous because of memory usage. We should do the same like with MultiFields and factor it out and throw UOE on non-atomic readers. The SlowMultiReaderWrapper can then manage the norms. Also ParallelReader needs to be fixed.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2779</id>
      <title>Use ConcurrentHashMap in RAMDirectory</title>
      <description>RAMDirectory synchronizes on its instance in many places to protect access to map of RAMFiles, in addition to updating the sizeInBytes member. In many places the sync is done for 'read' purposes, while only in few places we need 'write' access. This looks like a perfect use case for ConcurrentHashMap Also, syncing around sizeInBytes is unnecessary IMO, since it's an AtomicLong ... I'll post a patch shortly.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2790</id>
      <title>IndexWriter should call MP.useCompoundFile and not LogMP.getUseCompoundFile</title>
      <description>Spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/112311. I will attach a patch shortly that addresses the issue on trunk.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2791</id>
      <title>WindowsDirectory</title>
      <description>We can use Windows' overlapped IO to do pread() and avoid the performance problems of SimpleFS/NIOFSDir.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2792</id>
      <title>Add a simple FST impl to Lucene</title>
      <description>I implemented the algo described at http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.3698 for incrementally building a finite state transducer (FST) from sorted inputs. This is not a fully general FST impl – it's only able to build up an FST incrementally from input/output pairs that are pre-sorted. Currently the inputs are BytesRefs, and the outputs are pluggable – NoOutputs gets you a simple FSA, PositiveIntOutputs maps to a long, ByteSequenceOutput maps to a BytesRef. The implementation has a low memory overhead, so that it can handle a fairly large set of terms. For example, it can build the FSA for the 9.8M terms from a 10M document wikipedia index in ~8 seconds (on beast), using ~256 MB peak RAM, resulting in an FSA that's ~60 MB. It packs the FST as-it-builds into a compact byte[], and then exposes the API to read nodes/arcs directly from the byte[]. The FST can be quickly saved/loaded to/from a Directory since it's just a big byte[]. The format is similar to what Morfologik uses (http://sourceforge.net/projects/morfologik/). I think there are a number of possible places we can use this in Lucene. For example, I think many apps could hold the entire terms dict in RAM, either at the multi-reader level or maybe per-segment (mapping to file offset or to something else custom to the app), which may possibly be a good speedup for certain MTQs (though, because the format is packed into a byte[], there is a decode cost when visiting arcs). The builder can also prune as it goes, so you get a prefix trie pruned according to how many terms run through the nodes, which makes it faster and even less memory consuming. This may be useful as a replacement for our current binary search terms index since it can achieve higher term density for the same RAM consumption of our current index. As an initial usage to make sure this is exercised, I cutover the SimpleText codec, which currently fully loads all terms into a TreeMap (and has caused intermittent OOME in some tests), to use an FST instead. SimpleText uses a PairOutputs which is able to "pair up" any two other outputs, since it needs to map each input term to an int docFreq and long filePosition. All tests pass w/ SimpleText forced codec, and I think this is committable except I'd love to get some help w/ the generics (confession to the policeman: I had to add @SuppressWarnings( {"unchecked"} )) all over!! Ideally an FST is parameterized by its output type (Integer, BytesRef, etc.). I even added a new @nightly test that makes a largeish set of random terms and tests the resulting FST on different outputs I think it would also be easy to make a variant that uses char[] instead of byte[] as its inputs, so we could eg use this during analysis (Robert's idea). It's already be easy to have a CharSequence output type since the outputs are pluggable. Dawid Weiss (author of HPPC – http://labs.carrotsearch.com/hppc.html – and Morfologik – http://sourceforge.net/projects/morfologik/) was very helpful iterating with me on this (thank you!).</description>
      <attachments/>
    </issue>
    <issue>
      <id>2793</id>
      <title>Directory createOutput and openInput should take an IOContext</title>
      <description>Today for merging we pass down a larger readBufferSize than for searching because we get better performance. I think we should generalize this to a class (IOContext), which would hold the buffer size, but then could hold other flags like DIRECT (bypass OS's buffer cache), SEQUENTIAL, etc. Then, we can make the DirectIOLinuxDirectory fully usable because we would only use DIRECT/SEQUENTIAL during merging. This will require fixing how IW pools readers, so that a reader opened for merging is not then used for searching, and vice/versa. Really, it's only all the open file handles that need to be different – we could in theory share del docs, norms, etc, if that were somehow possible.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2795</id>
      <title>Genericize DirectIOLinuxDir -&gt; UnixDir</title>
      <description>Today DirectIOLinuxDir is tricky/dangerous to use, because you only want to use it for indexWriter and not IndexReader (searching). It's a trap. But, once we do LUCENE-2793, we can make it fully general purpose because then a single native Dir impl can be used. I'd also like to make it generic to other Unices, if we can, so that it becomes UnixDirectory.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2810</id>
      <title>Explore Alternate Stored Field approaches for highly redundant data</title>
      <description>In some cases (logs, HTML pages w/ boilerplate, etc.), the stored fields for documents contain a lot of redundant information and end up wasting a lot of space across a large collection of documents. For instance, simply compressing a typical log file often results in &gt; 75% compression rates. We should explore mechanisms for applying compression across all the documents for a field (or fields) while still maintaining relatively fast lookup (that being said, in most logging applications, fast retrieval of a given event is not always critical.) For instance, perhaps it is possible to have a part of storage that contains the set of unique values for all the fields and the document field value simply contains a reference (could be as small as a few bits depending on the number of uniq. items) to that value instead of having a full copy. Extending this, perhaps we can leverage some existing compression capabilities in Java to provide this as well. It may make sense to implement this as a Directory, but it might also make sense as a Codec, if and when we have support for changing storage Codecs.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2814</id>
      <title>stop writing shared doc stores across segments</title>
      <description>Shared doc stores enables the files for stored fields and term vectors to be shared across multiple segments. We've had this optimization since 2.1 I think. It works best against a new index, where you open an IW, add lots of docs, and then close it. In that case all of the written segments will reference slices a single shared doc store segment. This was a good optimization because it means we never need to merge these files. But, when you open another IW on that index, it writes a new set of doc stores, and then whenever merges take place across doc stores, they must now be merged. However, since we switched to shared doc stores, there have been two optimizations for merging the stores. First, we now bulk-copy the bytes in these files if the field name/number assignment is "congruent". Second, we now force congruent field name/number mapping in IndexWriter. This means this optimization is much less potent than it used to be. Furthermore, the optimization adds a lot of hair to IndexWriter/DocumentsWriter; this has been the source of sneaky bugs over time, and causes odd behavior like a merge possibly forcing a flush when it starts. Finally, with DWPT (LUCENE-2324), which gets us truly concurrent flushing, we can no longer share doc stores. So, I think we should turn off the write-side of shared doc stores to pave the path for DWPT to land on trunk and simplify IW/DW. We still must support reading them (until 5.0), but the read side is far less hairy.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2824</id>
      <title>optimizations for bufferedindexinput</title>
      <description>along the same lines as LUCENE-2816: the readVInt/readVLong/readShort/readInt/readLong are not optimal here since they defer to readByte. for example this means checking the buffer's bounds per-byte in readVint instead of per-vint. its an easy win to speed this up, even for the vint case: its essentially always faster, the only slower case is 1024 single-byte vints in a row, in this case we would do a single extra bounds check (1025 instead of 1024)</description>
      <attachments/>
    </issue>
    <issue>
      <id>2829</id>
      <title>improve termquery "pk lookup" performance</title>
      <description>For things that are like primary keys and don't exist in some segments (worst case is primary/unique key that only exists in 1) we do wasted seeks. While LUCENE-2694 tries to solve some of this issue with TermState, I'm concerned we could every backport that to 3.1 for example. This is a simpler solution here just to solve this one problem in termquery... we could just revert it in trunk when we resolve LUCENE-2694, but I don't think we should leave things as they are in 3.x</description>
      <attachments/>
    </issue>
    <issue>
      <id>2831</id>
      <title>Revise Weight#scorer &amp; Filter#getDocIdSet API to pass Readers context</title>
      <description>Spinoff from LUCENE-2694 - instead of passing a reader into Weight#scorer(IR, boolean, boolean) we should / could revise the API and pass in a struct that has parent reader, sub reader, ord of that sub. The ord mapping plus the context with its parent would make several issues way easier. See LUCENE-2694, LUCENE-2348 and LUCENE-2829 to name some.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2837</id>
      <title>Collapse Searcher/Searchable/IndexSearcher; remove contrib/remote; merge PMS into IndexSearcher</title>
      <description>We've discussed cleaning up our *Searcher stack for some time... I think we should try to do this before releasing 4.0. So I'm attaching an initial patch which: Removes Searcher, Searchable, absorbing all their methods into IndexSearcher Removes contrib/remote Removes MultiSearcher Absorbs ParallelMultiSearcher into IndexSearcher (ie you can now pass useThreads=true, or a custom ES to the ctor) The patch is rough – I just ripped stuff out, did search/replace to IndexSearcher, etc. EG nothing is directly testing using threads with IndexSearcher, but before committing I think we should add a newSearcher to LuceneTestCase, which randomly chooses whether the searcher uses threads, and cutover tests to use this instead of making their own IndexSearcher. I think MultiSearcher has a useful purpose, but as it is today it's too low-level, eg it shouldn't be involved in rewriting queries: the Query.combine method is scary. Maybe in its place we make a higher level class, with limited API, that's able to federate search across multiple IndexSearchers? It'd also be able to optionally use thread per IndexSearcher.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2843</id>
      <title>Add variable-gap terms index impl.</title>
      <description>PrefixCodedTermsReader/Writer (used by all "real" core codecs) already supports pluggable terms index impls. The only impl we have now is FixedGapTermsIndexReader/Writer, which picks every Nth (default 32) term and holds it in efficient packed int/byte arrays in RAM. This is already an enormous improvement (RAM reduction, init time) over 3.x. This patch adds another impl, VariableGapTermsIndexReader/Writer, which lets you specify an arbitrary IndexTermSelector to pick which terms are indexed, and then uses an FST to hold the indexed terms. This is typically even more memory efficient than packed int/byte arrays, though, it does not support ord() so it's not quite a fair comparison. I had to relax the terms index plugin api for PrefixCodedTermsReader/Writer to not assume that the terms index impl supports ord. I also did some cleanup of the FST/FSTEnum APIs and impls, and broke out separate seekCeil and seekFloor in FSTEnum. Eg we need seekFloor when the FST is used as a terms index but seekCeil when it's holding all terms in the index (ie which SimpleText uses FSTs for).</description>
      <attachments/>
    </issue>
    <issue>
      <id>2856</id>
      <title>Create IndexWriter event listener, specifically for merges</title>
      <description>The issue will allow users to monitor merges occurring within IndexWriter using a callback notifier event listener. This can be used by external applications such as Solr to monitor large segment merges.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2868</id>
      <title>It should be easy to make use of TermState; rewritten queries should be shared automatically</title>
      <description>When you have the same query in a query hierarchy multiple times, tremendous savings can now be had if the user knows enough to share the rewritten queries in the hierarchy, due to the TermState addition. But this is clumsy and requires a lot of coding by the user to take advantage of. Lucene should be smart enough to share the rewritten queries automatically. This can be most readily (and powerfully) done by introducing a new method to Query.java: Query rewriteUsingCache(IndexReader indexReader) ... and including a caching implementation right in Query.java which would then work for all. Of course, all callers would want to use this new method rather than the current rewrite().</description>
      <attachments/>
    </issue>
    <issue>
      <id>2878</id>
      <title>Allow Scorer to expose positions and payloads aka. nuke spans</title>
      <description>Currently we have two somewhat separate types of queries, the one which can make use of positions (mainly spans) and payloads (spans). Yet Span*Query doesn't really do scoring comparable to what other queries do and at the end of the day they are duplicating lot of code all over lucene. Span*Queries are also limited to other Span*Query instances such that you can not use a TermQuery or a BooleanQuery with SpanNear or anthing like that. Beside of the Span*Query limitation other queries lacking a quiet interesting feature since they can not score based on term proximity since scores doesn't expose any positional information. All those problems bugged me for a while now so I stared working on that using the bulkpostings API. I would have done that first cut on trunk but TermScorer is working on BlockReader that do not expose positions while the one in this branch does. I started adding a new Positions class which users can pull from a scorer, to prevent unnecessary positions enums I added ScorerContext#needsPositions and eventually Scorere#needsPayloads to create the corresponding enum on demand. Yet, currently only TermQuery / TermScorer implements this API and other simply return null instead. To show that the API really works and our BulkPostings work fine too with positions I cut over TermSpanQuery to use a TermScorer under the hood and nuked TermSpans entirely. A nice sideeffect of this was that the Position BulkReading implementation got some exercise which now work all with positions while Payloads for bulkreading are kind of experimental in the patch and those only work with Standard codec. So all spans now work on top of TermScorer ( I truly hate spans since today ) including the ones that need Payloads (StandardCodec ONLY)!! I didn't bother to implement the other codecs yet since I want to get feedback on the API and on this first cut before I go one with it. I will upload the corresponding patch in a minute. I also had to cut over SpanQuery.getSpans(IR) to SpanQuery.getSpans(AtomicReaderContext) which I should probably do on trunk first but after that pain today I need a break first . The patch passes all core tests (org.apache.lucene.search.highlight.HighlighterTest still fails but I didn't look into the MemoryIndex BulkPostings API yet)</description>
      <attachments/>
    </issue>
    <issue>
      <id>2881</id>
      <title>Track FieldInfo per segment instead of per-IW-session</title>
      <description>Currently FieldInfo is tracked per IW session to guarantee consistent global field-naming / ordering. IW carries FI instances over from previous segments which also carries over field properties like isIndexed etc. While having consistent field ordering per IW session appears to be important due to bulk merging stored fields etc. carrying over other properties might become problematic with Lucene's Codec support. Codecs that rely on consistent properties in FI will fail if FI properties are carried over. The DocValuesCodec (DocValuesBranch) for instance writes files per segment and field (using the field id within the file name). Yet, if a segment has no DocValues indexed in a particular segment but a previous segment in the same IW session had DocValues, FieldInfo#docValues will be true since those values are reused from previous segments. We already work around this "limitation" in SegmentInfo with properties like hasVectors or hasProx which is really something we should manage per Codec &amp; Segment. Ideally FieldInfo would be managed per Segment and Codec such that its properties are valid per segment. It also seems to be necessary to bind FieldInfoS to SegmentInfo logically since its really just per segment metadata.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2886</id>
      <title>Adaptive Frame Of Reference</title>
      <description>We could test the implementation of the Adaptive Frame Of Reference [1] on the lucene-4.0 branch. I am providing the source code of its implementation. Some work needs to be done, as this implementation is working on the old lucene-1458 branch. I will attach a tarball containing a running version (with tests) of the AFOR implementation, as well as the implementations of PFOR and of Simple64 (simple family codec working on 64bits word) that has been used in the experiments in [1]. [1] http://www.deri.ie/fileadmin/documents/deri-tr-afor.pdf</description>
      <attachments/>
    </issue>
    <issue>
      <id>2894</id>
      <title>Use of google-code-prettify for Lucene/Solr Javadoc</title>
      <description>My company, RONDHUIT uses google-code-prettify (Apache License 2.0) in Javadoc for syntax highlighting: http://www.rondhuit-demo.com/RCSS/api/com/rondhuit/solr/analysis/JaReadingSynonymFilterFactory.html I think we can use it for Lucene javadoc (java sample code in overview.html etc) and Solr javadoc (Analyzer Factories etc) to improve or simplify our life.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2899</id>
      <title>Add OpenNLP Analysis capabilities as a module</title>
      <description>Now that OpenNLP is an ASF project and has a nice license, it would be nice to have a submodule (under analysis) that exposed capabilities for it. Drew Farris, Tom Morton and I have code that does: Sentence Detection as a Tokenizer (could also be a TokenFilter, although it would have to change slightly to buffer tokens) NamedEntity recognition as a TokenFilter We are also planning a Tokenizer/TokenFilter that can put parts of speech as either payloads (PartOfSpeechAttribute?) on a token or at the same position. I'd propose it go under: modules/analysis/opennlp</description>
      <attachments/>
    </issue>
    <issue>
      <id>2903</id>
      <title>Improvement of PForDelta Codec</title>
      <description>There are 3 versions of PForDelta implementations in the Bulk Branch: FrameOfRef, PatchedFrameOfRef, and PatchedFrameOfRef2. The FrameOfRef is a very basic one which is essentially a binary encoding (may result in huge index size). The PatchedFrameOfRef is the implmentation based on the original version of PForDelta in the literatures. The PatchedFrameOfRef2 is my previous implementation which are improved this time. (The Codec name is changed to NewPForDelta.). In particular, the changes are: 1. I fixed the bug of my previous version (in Lucene-1410.patch), where the old PForDelta does not support very large exceptions (since the Simple16 does not support very large numbers). Now this has been fixed in the new LCPForDelta. 2. I changed the PForDeltaFixedIntBlockCodec. Now it is faster than the other two PForDelta implementation in the bulk branch (FrameOfRef and PatchedFrameOfRef). The codec's name is "NewPForDelta", as you can see in the CodecProvider and PForDeltaFixedIntBlockCodec. 3. The performance test results are: 1) My "NewPForDelta" codec is faster then FrameOfRef and PatchedFrameOfRef for almost all kinds of queries, slightly worse then BulkVInt. 2) My "NewPForDelta" codec can result in the smallest index size among all 4 methods, including FrameOfRef, PatchedFrameOfRef, and BulkVInt, and itself) 3) All performance test results are achieved by running with "-server" instead of "-client"</description>
      <attachments/>
    </issue>
    <issue>
      <id>2906</id>
      <title>Filter to process output of ICUTokenizer and create overlapping bigrams for CJK</title>
      <description>The ICUTokenizer produces unigrams for CJK. We would like to use the ICUTokenizer but have overlapping bigrams created for CJK as in the CJK Analyzer. This filter would take the output of the ICUtokenizer, read the ScriptAttribute and for selected scripts (Han, Kana), would produce overlapping bigrams.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2919</id>
      <title>IndexSplitter that divides by primary key term</title>
      <description>Index splitter that divides by primary key term. The contrib MultiPassIndexSplitter we have divides by docid, however to guarantee external constraints it's sometimes necessary to split by a primary key term id. I think this implementation is a fairly trivial change.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2923</id>
      <title>cleanup contrib/demo</title>
      <description>I don't think we should include optimize in the demo; many people start from the demo and may think you must optimize to do searching, and that's clearly not the case. I think we should also use a buffered reader in FileDocument? And... I'm tempted to remove IndexHTML (and the html parser) entirely. It's ancient, and we now have Tika to extract text from many doc formats.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2952</id>
      <title>Make license checking/maintenance easier/automated</title>
      <description>Instead of waiting until release to check licenses are valid, we should make it a part of our build process to ensure that all dependencies have proper licenses, etc.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2957</id>
      <title>generate-maven-artifacts target should include all non-Mavenized Lucene &amp; Solr dependencies</title>
      <description>Currently, in addition to deploying artifacts for all of the Lucene and Solr modules to a repository (by default local), the generate-maven-artifacts target also deploys artifacts for the following non-Mavenized Solr dependencies (lucene_solr_3_1 version given here): solr/lib/commons-csv-1.0-SNAPSHOT-r966014.jar as org.apache.solr:solr-commons-csv:3.1 solr/lib/apache-solr-noggit-r944541.jar as org.apache.solr:solr-noggit:3.1 The following .jar's should be added to the above list (lucene_solr_3_1 version given here): lucene/contrib/icu/lib/icu4j-4_6.jar lucene/contrib/benchmark/lib/xercesImpl-2.9.1-patched-XERCESJ-1257.jar solr/contrib/clustering/lib/carrot2-core-3.4.2.jar** solr/contrib/uima/lib/uima-an-alchemy.jar solr/contrib/uima/lib/uima-an-calais.jar solr/contrib/uima/lib/uima-an-tagger.jar solr/contrib/uima/lib/uima-an-wst.jar solr/contrib/uima/lib/uima-core.jar I think it makes sense to follow the same model as the current non-Mavenized dependencies: groupId = org.apache.solr/.lucene artifactId = solr-/lucene-&lt;original-name&gt;, version = &lt;lucene-solr-release-version&gt;. **The carrot2-core jar doesn't need to be included in trunk's release artifacts, since there already is a Mavenized Java6-compiled jar. branch_3x and lucene_solr_3_1 will need this Solr-specific Java5-compiled maven artifact, though.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2958</id>
      <title>WriteLineDocTask improvements</title>
      <description>Make WriteLineDocTask and LineDocSource more flexible/extendable: allow to emit lines also for empty docs (keep current behavior as default) allow more/less/other fields</description>
      <attachments/>
    </issue>
    <issue>
      <id>2959</id>
      <title>[GSoC] Implementing State of the Art Ranking for Lucene</title>
      <description>Lucene employs the Vector Space Model (VSM) to rank documents, which compares unfavorably to state of the art algorithms, such as BM25. Moreover, the architecture is tailored specically to VSM, which makes the addition of new ranking functions a non- trivial task. This project aims to bring state of the art ranking methods to Lucene and to implement a query architecture with pluggable ranking functions. The wiki page for the project can be found at http://wiki.apache.org/lucene-java/SummerOfCode2011ProjectRanking.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2960</id>
      <title>Allow (or bring back) the ability to setRAMBufferSizeMB on an open IndexWriter</title>
      <description>In 3.1 the ability to setRAMBufferSizeMB is deprecated, and removed in trunk. It would be great to be able to control that on a live IndexWriter. Other possible two methods that would be great to bring back are setTermIndexInterval and setReaderTermsIndexDivisor. Most of the other setters can actually be set on the MergePolicy itself, so no need for setters for those (I think).</description>
      <attachments/>
    </issue>
    <issue>
      <id>2973</id>
      <title>Source distribution packaging targets should make a tarball from "svn export"</title>
      <description>Instead of picking and choosing which stuff to include from a local working copy, Lucene's dist-src/package-tgz-src target and Solr's package-src target should simply perform "svn export" with the same revision and URL as the local working copy.</description>
      <attachments/>
    </issue>
    <issue>
      <id>2979</id>
      <title>Simplify configuration API of contrib Query Parser</title>
      <description>The current configuration API is very complicated and inherit the concept used by Attribute API to store token information in token streams. However, the requirements for both (QP config and token stream) are not the same, so they shouldn't be using the same thing. I propose to simplify QP config and make it less scary for people intending to use contrib QP. The task is not difficult, it will just require a lot of code change and figure out the best way to do it. That's why it's a good candidate for a GSoC project. I would like to hear good proposals about how to make the API more friendly and less scaring</description>
      <attachments/>
    </issue>
    <issue>
      <id>2981</id>
      <title>Review and potentially remove unused/unsupported Contribs</title>
      <description>Some of our contribs appear to be lacking for development/support or are missing tests. We should review whether they are even pertinent these days and potentially deprecate and remove them. One of the things we did in Mahout when bringing in Colt code was to mark all code that didn't have tests as @deprecated and then we removed the deprecation once tests were added. Those that didn't get tests added over about a 6 mos. period of time were removed. I would suggest taking a hard look at: ant db lucli swing (spatial should be gutted to some extent and moved to modules)</description>
      <attachments/>
    </issue>
    <issue>
      <id>3001</id>
      <title>Add TrieFieldHelper lucene so we can write solr compatible Trie* fields w/o solr dependency</title>
      <description>The solr support for numeric fields writes the stored value as binary vs the lucene NumericField We should move this logic to a helper class in lucene core so that libraries that do not depend on solr can write TrieFields that solr can read.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3003</id>
      <title>Move UnInvertedField into Lucene core</title>
      <description>Solr's UnInvertedField lets you quickly lookup all terms ords for a given doc/field. Like, FieldCache, it inverts the index to produce this, and creates a RAM-resident data structure holding the bits; but, unlike FieldCache, it can handle multiple values per doc, and, it does not hold the term bytes in RAM. Rather, it holds only term ords, and then uses TermsEnum to resolve ord -&gt; term. This is great eg for faceting, where you want to use int ords for all of your counting, and then only at the end you need to resolve the "top N" ords to their text. I think this is a useful core functionality, and we should move most of it into Lucene's core. It's a good complement to FieldCache. For this first baby step, I just move it into core and refactor Solr's usage of it. After this, as separate issues, I think there are some things we could explore/improve: The first-pass that allocates lots of tiny byte[] looks like it could be inefficient. Maybe we could use the byte slices from the indexer for this... We can improve the RAM efficiency of the TermIndex: if the codec supports ords, and we are operating on one segment, we should just use it. If not, we can use a more RAM-efficient data structure, eg an FST mapping to the ord. We may be able to improve on the main byte[] representation by using packed ints instead of delta-vInt? Eventually we should fold this ability into docvalues, ie we'd write the byte[] image at indexing time, and then loading would be fast, instead of uninverting</description>
      <attachments/>
    </issue>
    <issue>
      <id>3030</id>
      <title>Block tree terms dict &amp; index</title>
      <description>Our default terms index today breaks terms into blocks of fixed size (ie, every 32 terms is a new block), and then we build an index on top of that (holding the start term for each block). But, it should be better to instead break terms according to how they share prefixes. This results in variable sized blocks, but means within each block we maximize the shared prefix and minimize the resulting terms index. It should also be a speedup for terms dict intensive queries because the terms index becomes a "true" prefix trie, and can be used to fast-fail on term lookup (ie returning NOT_FOUND without having to seek/scan a terms block). Having a true prefix trie should also enable much faster intersection with automaton (but this will be a new issue). I've made an initial impl for this (called BlockTreeTermsWriter/Reader). It's still a work in progress... lots of nocommits, and hairy code, but tests pass (at least once!). I made two new codecs, temporarily called StandardTree, PulsingTree, that are just like their counterparts but use this new terms dict. I added a new "exactOnly" boolean to TermsEnum.seek. If that's true and the term is NOT_FOUND, we will (quickly) return NOT_FOUND and the enum is unpositioned (ie you should not call next(), docs(), etc.). In this approach the index and dict are tightly connected, so it does not support a pluggable index impl like BlockTermsWriter/Reader. Blocks are stored on certain nodes of the prefix trie, and can contain both terms and pointers to sub-blocks (ie, if the block is not a leaf block). So there are two trees, tied to one another – the index trie, and the blocks. Only certain nodes in the trie map to a block in the block tree. I think this algorithm is similar to burst tries (http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3499), except it allows terms to be stored on inner blocks (not just leaf blocks). This is important for Lucene because an [accidental] "adversary" could produce a terms dict with way too many blocks (way too much RAM used by the terms index). Still, with my current patch, an adversary can produce too-big blocks... which we may need to fix, by letting the terms index not be a true prefix trie on it's leaf edges. Exactly how the blocks are picked can be factored out as its own policy (but I haven't done that yet). Then, burst trie is one policy, my current approach is another, etc. The policy can be tuned to the terms' expected distribution, eg if it's a primary key field and you only use base 10 for each character then you want block sizes of size 10. This can make a sizable difference on lookup cost. I modified the FST Builder to allow for a "plugin" that freezes the "tail" (changed suffix) of each added term, because I use this to find the blocks.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3041</id>
      <title>Support Query Visting / Walking</title>
      <description>Out of the discussion in LUCENE-2868, it could be useful to add a generic Query Visitor / Walker that could be used for more advanced rewriting, optimizations or anything that requires state to be stored as each Query is visited. We could keep the interface very simple: public interface QueryVisitor { Query visit(Query query); } and then use a reflection based visitor like Earwin suggested, which would allow implementators to provide visit methods for just Querys that they are interested in.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3065</id>
      <title>NumericField should be stored in binary format in index (matching Solr's format)</title>
      <description>(Spinoff of LUCENE-3001) Today when writing stored fields we don't record that the field was a NumericField, and so at IndexReader time you get back an "ordinary" Field and your number has turned into a string. See https://issues.apache.org/jira/browse/LUCENE-1701?focusedCommentId=12721972&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12721972 We have spare bits already in stored fields, so, we should use one to record that the field is numeric, and then encode the numeric field in Solr's more-compact binary format. A nice side-effect is we fix the long standing issue that you don't get a NumericField back when loading your document.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3069</id>
      <title>Lucene should have an entirely memory resident term dictionary</title>
      <description>FST based TermDictionary has been a great improvement yet it still uses a delta codec file for scanning to terms. Some environments have enough memory available to keep the entire FST based term dict in memory. We should add a TermDictionary implementation that encodes all needed information for each term into the FST (custom fst.Output) and builds a FST from the entire term not just the delta.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3071</id>
      <title>PathHierarchyTokenizer adaptation for urls: splits reversed</title>
      <description>PathHierarchyTokenizer should be usable to split urls the a "reversed" way (useful for faceted search against urls): www.site.com -&gt; www.site.com, site.com, com Moreover, it should be able to skip a given number of first (or last, if reversed) tokens: /usr/share/doc/somesoftware/INTERESTING/PART Should give with 4 tokens skipped: INTERESTING INTERESTING/PART</description>
      <attachments/>
    </issue>
    <issue>
      <id>3079</id>
      <title>Faceting module</title>
      <description>Faceting is a hugely important feature, available in Solr today but not [easily] usable by Lucene-only apps. We should fix this, by creating a shared faceting module. Ideally, we factor out Solr's faceting impl, and maybe poach/merge from other impls (eg Bobo browse). Hoss describes some important challenges we'll face in doing this (http://markmail.org/message/5w35c2fr4zkiwsz6), copied here: To look at "faceting" as a concrete example, there are big the reasons faceting works so well in Solr: Solr has total control over the index, knows exactly when the index has changed to rebuild caches, has a strict schema so it can make sense of field types and pick faceting algos accordingly, has multi-phase distributed search approach to get exact counts efficiently across multiple shards, etc... (and there are still a lot of additional enhancements and improvements that can be made to take even more advantage of knowledge solr has because it "owns" the index that we no one has had time to tackle) This is a great list of the things we face in refactoring. It's also important because, if Solr needed to be so deeply intertwined with caching, schema, etc., other apps that want to facet will have the same "needs" and so we really have to address them in creating the shared module. I think we should get a basic faceting module started, but should not cut Solr over at first. We should iterate on the module, fold in improvements, etc., and then, once we can fully verify that cutting over doesn't hurt Solr (ie lose functionality or performance) we can later cutover.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3080</id>
      <title>cutover highlighter to BytesRef</title>
      <description>Highlighter still uses char[] terms (consumes tokens from the analyzer as char[] not as BytesRef), which is causing problems for merging SOLR-2497 to trunk.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3082</id>
      <title>Add tool to upgrade all segments of an index to last recent supported index format without optimizing</title>
      <description>Currently if you want to upgrade an old index to the format of your current Lucene version, you have to optimize your index or use addIndexes(IndexReader...) [see LUCENE-2893] to copy to a new directory. The optimize() approach fails if your index is already optimized. I propose to add a custom MergePolicy to upgrade all segments to the last format. This MergePolicy could simply also ignore all segments already up-to-date. All segments in prior formats would be merged to a new segment using another MergePolicy's optimize strategy. This issue is different from LUCENE-2893, as it would only support upgrading indexes from previous Lucene versions in-place using the official path. Its a tool for the end user, not a developer tool. This addition should also go to Lucene 3.x, as we need to make users with pre-3.0 indexes go the step through 3.x, else they would not be able to open their index with 4.0. With this tool in 3.x the users could safely upgrade their index without relying on optimize to work on already-optimized indexes.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3084</id>
      <title>MergePolicy.OneMerge.segments should be List&lt;SegmentInfo&gt; not SegmentInfos, Remove Vector&lt;SI&gt; subclassing from SegmentInfos &amp; more refactoring</title>
      <description>SegmentInfos carries a bunch of fields beyond the list of SI, but for merging purposes these fields are unused. We should cutover to List&lt;SI&gt; instead. Also SegmentInfos subclasses Vector&lt;SI&gt;, this should be removed and the collections be hidden inside the class. We can add unmodifiable views on it (asList(), asSet()).</description>
      <attachments/>
    </issue>
    <issue>
      <id>3092</id>
      <title>NRTCachingDirectory, to buffer small segments in a RAMDir</title>
      <description>I created this simply Directory impl, whose goal is reduce IO contention in a frequent reopen NRT use case. The idea is, when reopening quickly, but not indexing that much content, you wind up with many small files created with time, that can possibly stress the IO system eg if merges, searching are also fighting for IO. So, NRTCachingDirectory puts these newly created files into a RAMDir, and only when they are merged into a too-large segment, does it then write-through to the real (delegate) directory. This lets you spend some RAM to reduce I0.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3097</id>
      <title>Post grouping faceting</title>
      <description>This issues focuses on implementing post grouping faceting. How to handle multivalued fields. What field value to show with the facet. Where the facet counts should be based on Facet counts can be based on the normal documents. Ungrouped counts. Facet counts can be based on the groups. Grouped counts. Facet counts can be based on the combination of group value and facet value. Matrix counts. And properly more implementation options. The first two methods are implemented in the SOLR-236 patch. For the first option it calculates a DocSet based on the individual documents from the query result. For the second option it calculates a DocSet for all the most relevant documents of a group. Once the DocSet is computed the FacetComponent and StatsComponent use one the DocSet to create facets and statistics. This last one is a bit more complex. I think it is best explained with an example. Lets say we search on travel offers: hotel departure_airport duration Hotel a AMS 5 Hotel a DUS 10 Hotel b AMS 5 Hotel b AMS 10 If we group by hotel and have a facet for airport. Most end users expect (according to my experience off course) the following airport facet: AMS: 2 DUS: 1 The above result can't be achieved by the first two methods. You either get counts AMS:3 and DUS:1 or 1 for both airports.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3098</id>
      <title>Grouped total count</title>
      <description>When grouping currently you can get two counts: Total hit count. Which counts all documents that matched the query. Total grouped hit count. Which counts all documents that have been grouped in the top N groups. Since the end user gets groups in his search result instead of plain documents with grouping. The total number of groups as total count makes more sense in many situations.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3099</id>
      <title>Grouping module should allow subclasses to set the group key per document</title>
      <description>The new grouping module can only group by a single-valued indexed field. But, if we make the 'getGroupKey' a method that a subclass could override, then I think we could refactor Solr over to the module, because it could do function queries and normal queries via subclass (I think). This also makes the impl more extensible to apps that might have their own interesting group values per document.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3130</id>
      <title>Use BoostAttribute in in TokenFilters to denote Terms that QueryParser should give lower boosts</title>
      <description>A recent thread asked if there was anyway to use QueryTime synonyms such that matches on the original term specified by the user would score higher then matches on the synonym. It occurred to me later that a float Attribute could be set by the SynonymFilter in such situations, and QueryParser could use that float as a boost in the resulting Query. IThis would be fairly straightforward for the simple "synonyms =&gt; BooleamQuery" case, but we'd have to decide how to handle the case of synonyms with multiple terms that produce MTPQ, possibly just punt for now) Likewise, there may be other TokenFilters that "inject" artificial tokens at query time where it also might make sense to have a reduced "boost" factor... SynonymFilter CommonGramsFilter WordDelimiterFilter etc... In all of these cases, the amount of the "boost" could me configured, and for back compact could default to "1.0" (or null to not set a boost at all) Furthermore: if we add a new BoostAttrToPayloadAttrFilter that just copied the boost attribute into the payload attribute, these same filters could give "penalizing" payloads to terms when used at index time) could give "penalizing" payloads to terms.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3151</id>
      <title>Make all of Analysis completely independent from Lucene Core</title>
      <description>Lucene's analysis package, including the definitions of Attribute, TokenStream, etc. are quite useful outside of Lucene (for instance, Mahout uses them) for text processing. I'd like to move the definitions, or at least their packaging, to a separate JAR file so that one can consume them w/o needing Lucene core. My draft idea is to have a definition area that Lucene core is dependent on and the rest of the analysis package can then be dependent on the definition area. (I'm open to other ideas as well)</description>
      <attachments/>
    </issue>
    <issue>
      <id>3167</id>
      <title>Make lucene/solr a OSGI bundle through Ant</title>
      <description>We need to make a bundle thriugh Ant, so the binary can be published and no more need the download of the sources. Actually to get a OSGI bundle we need to use maven tools and build the sources. Here the reference for the creation of the OSGI bundle through Maven: https://issues.apache.org/jira/browse/LUCENE-1344 Bndtools could be used inside Ant</description>
      <attachments/>
    </issue>
    <issue>
      <id>3178</id>
      <title>Native MMapDir</title>
      <description>Spinoff from LUCENE-2793. Just like we will create native Dir impl (UnixDirectory) to pass the right OS level IO flags depending on the IOContext, we could in theory do something similar with MMapDir. The problem is MMap is apparently quite hairy... and to pass the flags the native code would need to invoke mmap (I think?), unlike UnixDir where the code "only" has to open the file handle.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3179</id>
      <title>OpenBitSet.prevSetBit()</title>
      <description>Find a previous set bit in an OpenBitSet. Useful for parent testing in nested document query execution LUCENE-2454 .</description>
      <attachments/>
    </issue>
    <issue>
      <id>3191</id>
      <title>Add TopDocs.merge to merge multiple TopDocs</title>
      <description>It's not easy today to merge TopDocs, eg produced by multiple shards, supporting arbitrary Sort.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3200</id>
      <title>Cleanup MMapDirectory to use only one MMapIndexInput impl with mapping sized of powers of 2</title>
      <description>Robert and me discussed a little bit after Mike's investigations, that using SingleMMapIndexinput together with MultiMMapIndexInput leads to hotspot slowdowns sometimes. We had the following ideas: MultiMMapIndexInput is almost as fast as SingleMMapIndexInput, as the switching between buffer boundaries is done in exception catch blocks. So normal code path is always the same like for Single* Only the seek method uses strange calculations (the modulo is totally bogus, it could be simply: int bufOffset = (int) (pos % maxBufSize); - very strange way of calculating modulo in the original code) Because of speed we suggest to no longer use arbitrary buffer sizes. We should pass only the power of 2 to the indexinput as size. All calculations in seek and anywhere else would be simple bit shifts and AND operations (the and masks for the modulo can be calculated in the ctor like NumericUtils does when calculating precisionSteps). the maximum buffer size will now be 2^30, not 2^31-1. But thats not an issue at all. In my opinion, a buffer size of 2^31-1 is stupid in all cases, as it will no longer fit page boundaries and mmapping gets harder for the O/S. We will provide a patch with those cleanups.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3201</id>
      <title>improved compound file handling</title>
      <description>Currently CompoundFileReader could use some improvements, i see the following problems its CSIndexInput extends bufferedindexinput, which is stupid for directories like mmap. it seeks on every readInternal its not possible for a directory to override or improve the handling of compound files. for example: it seems if you were impl'ing this thing from scratch, you would just wrap the II directly (not extend BufferedIndexInput, and add compound file offset X to seek() calls, and override length(). But of course, then you couldnt throw read past EOF always when you should, as a user could read into the next file and be left unaware. however, some directories could handle this better. for example MMapDirectory could return an indexinput that simply mmaps the 'slice' of the CFS file. its underlying bytebuffer etc naturally does bounds checks already etc, so it wouldnt need to be buffered, not even needing to add any offsets to seek(), as its position would just work. So I think we should try to refactor this so that a Directory can customize how compound files are handled, the simplest case for the least code change would be to add this to Directory.java: public Directory openCompoundInput(String filename) { return new CompoundFileReader(this, filename); } Because most code depends upon the fact compound files are implemented as a Directory and transparent. at least then a subclass could override... but the 'recursion' is a little ugly... we could still label it expert+internal+experimental or whatever.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3206</id>
      <title>FST package API refactoring</title>
      <description>The current API is still marked @experimental, so I think there's still time to fiddle with it. I've been using the current API for some time and I do have some ideas for improvement. This is a placeholder for these – I'll post a patch once I have a working proof of concept.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3218</id>
      <title>Make CFS appendable</title>
      <description>Currently CFS is created once all files are written during a flush / merge. Once on disk the files are copied into the CFS format which is basically a unnecessary for some of the files. We can at any time write at least one file directly into the CFS which can save a reasonable amount of IO. For instance stored fields could be written directly during indexing and during a Codec Flush one of the written files can be appended directly. This optimization is a nice sideeffect for lucene indexing itself but more important for DocValues and LUCENE-3216 we could transparently pack per field files into a single file only for docvalues without changing any code once LUCENE-3216 is resolved.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3226</id>
      <title>rename SegmentInfos.FORMAT_3_1 and improve description in CheckIndex</title>
      <description>A 3.2 user recently asked if something was wrong because CheckIndex was reporting his (newly built) index version as... Segments file=segments_or numSegments=1 version=FORMAT_3_1 [Lucene 3.1] It seems like there are two very confusing pieces of information here... 1) the variable name of SegmentInfos.FORMAT_3_1 seems like poor choice. All other FORMAT_* constants in SegmentInfos are descriptive of the actual change made, and not specific to the version when they were introduced. 2) whatever the name of the FORMAT_* variable, CheckIndex is labeling it "Lucene 3.1", which is missleading since that format is alwasy used in 3.2 (and probably 3.3, etc...). I suggest: a) rename FORMAT_3_1 to something like "FORMAT_SEGMENT_RECORDS_VERSION" b) change CheckIndex so that the label for the "newest" format always ends with " and later" (ie: "Lucene 3.1 and later") so when we release versions w/o a format change we don't have to remember to manual list them in CheckIndex. when we do make format changes and update CheckIndex " and later" can be replaced with " to X.Y" and the new format can be added</description>
      <attachments/>
    </issue>
    <issue>
      <id>3233</id>
      <title>HuperDuperSynonymsFilter™</title>
      <description>The current synonymsfilter uses a lot of ram and cpu, especially at build time. I think yesterday I heard about "huge synonyms files" three times. So, I think we should use an FST-based structure, sharing the inputs and outputs. And we should be more efficient with the tokenStream api, e.g. using save/restoreState instead of cloneAttributes()</description>
      <attachments/>
    </issue>
    <issue>
      <id>3234</id>
      <title>Provide limit on phrase analysis in FastVectorHighlighter</title>
      <description>With larger documents, FVH can spend a lot of time trying to find the best-scoring snippet as it examines every possible phrase formed from matching terms in the document. If one is willing to accept less-than-perfect scoring by limiting the number of phrases that are examined, substantial speedups are possible. This is analogous to the Highlighter limit on the number of characters to analyze. The patch includes an artifical test case that shows &gt; 1000x speedup. In a more normal test environment, with English documents and random queries, I am seeing speedups of around 3-10x when setting phraseLimit=1, which has the effect of selecting the first possible snippet in the document. Most of our sites operate in this way (just show the first snippet), so this would be a big win for us. With phraseLimit = -1, you get the existing FVH behavior. At larger values of phraseLimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3262</id>
      <title>Facet benchmarking</title>
      <description>A spin off from LUCENE-3079. We should define few benchmarks for faceting scenarios, so we can evaluate the new faceting module as well as any improvement we'd like to consider in the future (such as cutting over to docvalues, implement FST-based caches etc.). Toke attached a preliminary test case to LUCENE-3079, so I'll attach it here as a starting point. We've also done some preliminary job for extending Benchmark for faceting, so I'll attach it here as well. We should perhaps create a Wiki page where we clearly describe the benchmark scenarios, then include results of 'default settings' and 'optimized settings', or something like that.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3271</id>
      <title>Move 'good' contrib/queries classes to Queries module</title>
      <description>With the Queries module now filled with the FunctionQuery stuff, we should look at closing down contrib/queries. While not a huge contrib, it contains a number of pretty useful classes and some that should go elsewhere. Heres my proposed plan: similar.* -&gt; suggest module regex.* -&gt; queries module BooleanFilter -&gt; queries module under .filters package BoostingQuery -&gt; queries module ChainedFilter -&gt; queries module under .filters package DuplicateFilter -&gt; queries module under .filters package FieldCacheRewriteMethod -&gt; This doesn't belong in this contrib or the queries module. I think we should push it to contrib/misc for the time being. It seems to have quite a few constraints on when its useful. If indeed CONSTANT_SCORE_AUTO rewrite is better, then I dont see a purpose for it. FilterClause -&gt; class inside BooleanFilter FuzzyLikeThisQuery -&gt; suggest module. This class seems a mess with its Similarity hardcoded. With all that said, it does seem to do what it claims and with some cleanup, it could be good. TermsFilter -&gt; queries module under .filters package SlowCollated* -&gt; They can stay in the module till we have a better place to nuke them. One of the implications of the above moves, is that the xml-query-parser, which supports many of the queries, will need to have a dependency on the queries module. But that seems unavoidable at this stage.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3298</id>
      <title>FST has hard limit max size of 2.1 GB</title>
      <description>The FST uses a single contiguous byte[] under the hood, which in java is indexed by int so we cannot grow this over Integer.MAX_VALUE. It also internally encodes references to this array as vInt. We could switch this to a paged byte[] and make the far larger. But I think this is low priority... I'm not going to work on it any time soon.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3305</id>
      <title>Kuromoji code donation - a new Japanese morphological analyzer</title>
      <description>Atilika Inc. (アティリカ株式会社) would like to donate the Kuromoji Japanese morphological analyzer to the Apache Software Foundation in the hope that it will be useful to Lucene and Solr users in Japan and elsewhere. The project was started in 2010 since we couldn't find any high-quality, actively maintained and easy-to-use Java-based Japanese morphological analyzers, and these become many of our design goals for Kuromoji. Kuromoji also has a segmentation mode that is particularly useful for search, which we hope will interest Lucene and Solr users. Compound-nouns, such as 関西国際空港 (Kansai International Airport) and 日本経済新聞 (Nikkei Newspaper), are segmented as one token with most analyzers. As a result, a search for 空港 (airport) or 新聞 (newspaper) will not give you a for in these words. Kuromoji can segment these words into 関西 国際 空港 and 日本 経済 新聞, which is generally what you would want for search and you'll get a hit. We also wanted to make sure the technology has a license that makes it compatible with other Apache Software Foundation software to maximize its usefulness. Kuromoji has an Apache License 2.0 and all code is currently owned by Atilika Inc. The software has been developed by my good friend and ex-colleague Masaru Hasegawa and myself. Kuromoji uses the so-called IPADIC for its dictionary/statistical model and its license terms are described in NOTICE.txt. I'll upload code distributions and their corresponding hashes and I'd very much like to start the code grant process. I'm also happy to provide patches to integrate Kuromoji into the codebase, if you prefer that. Please advise on how you'd like me to proceed with this. Thank you.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3312</id>
      <title>Break out StorableField from IndexableField</title>
      <description>In the field type branch we have strongly decoupled Document/Field/FieldType impl from the indexer, by having only a narrow API (IndexableField) passed to IndexWriter. This frees apps up use their own "documents" instead of the "user-space" impls we provide in oal.document. Similarly, with LUCENE-3309, we've done the same thing on the doc/field retrieval side (from IndexReader), with the StoredFieldsVisitor. But, maybe we should break out StorableField from IndexableField, such that when you index a doc you provide two Iterables – one for the IndexableFields and one for the StorableFields. Either can be null. One downside is possible perf hit for fields that are both indexed &amp; stored (ie, we visit them twice, lookup their name in a hash twice, etc.). But the upside is a cleaner separation of concerns in API....</description>
      <attachments/>
    </issue>
    <issue>
      <id>3328</id>
      <title>Specialize BooleanQuery if all clauses are TermQueries</title>
      <description>During work on LUCENE-3319 I ran into issues with BooleanQuery compared to PhraseQuery in the exact case. If I disable scoring on PhraseQuery and bypass the position matching, essentially doing a conjunction match, ExactPhraseScorer beats plain boolean scorer by 40% which is a sizeable gain. I converted a ConjunctionScorer to use DocsEnum directly but still didn't get all the 40% from PhraseQuery. Yet, it turned out with further optimizations this gets very close to PhraseQuery. The biggest gain here came from converting the hand crafted loop in ConjunctionScorer#doNext to a for loop which seems to be less confusing to hotspot. In this particular case I think code specialization makes lots of sense since BQ with TQ is by far one of the most common queries. I will upload a patch shortly</description>
      <attachments/>
    </issue>
    <issue>
      <id>3343</id>
      <title>Comparison operators &gt;,&gt;=,&lt;,&lt;= and = support as RangeQuery syntax in QueryParser</title>
      <description>To offer better interoperability with other search engines and to provide an easier and more straight forward syntax, the operators &gt;, &gt;=, &lt;, &lt;= and = should be available to express an open range query. They should at least work for numeric queries. '=' can be made a synonym for ':'.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3354</id>
      <title>Extend FieldCache architecture to multiple Values</title>
      <description>I would consider this a bug. It appears lots of people are working around this limitation, why don't we just change the underlying data structures to natively support multiValued fields in the FieldCache architecture? Then functions() will work properly, and we can do things like easily geodist() on a multiValued field. Thoughts?</description>
      <attachments/>
    </issue>
    <issue>
      <id>3360</id>
      <title>Move FieldCache to IndexReader</title>
      <description>Move the static FieldCache.DEFAULT field instance to atomic IndexReaders, so that FieldCache insanity caused by the WeakHashMap no longer occurs. Add a new method to IndexReader that by default throws an UOE: public FieldCache getFieldCache() The SegmentReader implements this method and returns its own internal FieldCache implementation. This implementation just uses a HashMap&lt;Entry&lt;T&gt;,Object&gt;&gt; to store entries. The SlowMultiReaderWrapper implements this method as well and basically behaves the same as the current FieldCacheImpl. This issue won't solve the insanity that comes from inconsistent usage of a single field (for example retrieve both int[] and DocTermIndex for the same field).</description>
      <attachments/>
    </issue>
    <issue>
      <id>3396</id>
      <title>Make TokenStream Reuse Mandatory for Analyzers</title>
      <description>In LUCENE-2309 it became clear that we'd benefit a lot from Analyzer having to return reusable TokenStreams. This is a big chunk of work, but its time to bite the bullet. I plan to attack this in the following way: Collapse the logic of ReusableAnalyzerBase into Analyzer Add a ReuseStrategy abstraction to Analyzer which controls whether the TokenStreamComponents are reused globally (as they are today) or per-field. Convert all Analyzers over to using TokenStreamComponents. I've already seen that some of the TokenStreams created in tests need some work to be reusable (even if they aren't reused). Remove Analyzer.reusableTokenStream and convert everything over to using .tokenStream (which will now be returning reusable TokenStreams).</description>
      <attachments/>
    </issue>
    <issue>
      <id>3410</id>
      <title>Make WordDelimiterFilter's instantiation more readable</title>
      <description>Currently WordDelimiterFilter's constructor is: public WordDelimiterFilter(TokenStream in, byte[] charTypeTable, int generateWordParts, int generateNumberParts, int catenateWords, int catenateNumbers, int catenateAll, int splitOnCaseChange, int preserveOriginal, int splitOnNumerics, int stemEnglishPossessive, CharArraySet protWords) { which means its instantiation is an unreadable combination of 1s and 0s. We should improve this by either using a Builder, 'int flags' or an EnumSet.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3413</id>
      <title>CombiningFilter to recombine tokens into a single token for sorting</title>
      <description>I whipped up this CombiningFilter for the following use case: I've got a bunch of titles of e.g., Books, such as: The Grapes of Wrath Tommy Tommerson saves the World Top of the World The Tales of Beedle the Bard Born Free etc. I want to sort these titles using a String field that includes stopword analysis (e.g., to remove "The"), and synonym filtering (e.g., for grouping), etc. I created an analysis chain in Solr for this that was based off of alphaOnlySort, which looks like this: &lt;fieldType name="alphaOnlySort" class="solr.TextField" sortMissingLast="true" omitNorms="true"&gt; &lt;analyzer&gt; &lt;!-- KeywordTokenizer does no actual tokenizing, so the entire input string is preserved as a single token --&gt; &lt;tokenizer class="solr.KeywordTokenizerFactory"/&gt; &lt;!-- The LowerCase TokenFilter does what you expect, which can be when you want your sorting to be case insensitive --&gt; &lt;filter class="solr.LowerCaseFilterFactory" /&gt; &lt;!-- The TrimFilter removes any leading or trailing whitespace --&gt; &lt;filter class="solr.TrimFilterFactory" /&gt; &lt;!-- The PatternReplaceFilter gives you the flexibility to use Java Regular expression to replace any sequence of characters matching a pattern with an arbitrary replacement string, which may include back references to portions of the original string matched by the pattern. See the Java Regular Expression documentation for more information on pattern and replacement string syntax. http://java.sun.com/j2se/1.5.0/docs/api/java/util/regex/package-summary.html --&gt; &lt;filter class="solr.PatternReplaceFilterFactory" pattern="([^a-z])" replacement="" replace="all" /&gt; &lt;/analyzer&gt; &lt;/fieldType&gt; The issue with alphaOnlySort is that it doesn't support stopword remove or synonyms because those are based on the original token level instead of the full strings produced by the KeywordTokenizer (which does not do tokenization). I needed a filter that would allow me to change alphaOnlySort and its analysis chain from using KeywordTokenizer to using WhitespaceTokenizer, and then a way to recombine the tokens at the end. So, take "The Grapes of Wrath". I needed a way for it to get turned into: grapes of wrath And then to combine those tokens into a single token: grapesofwrath The attached CombiningFilter takes care of that. It doesn't do it super efficiently I'm guessing (since I used a StringBuffer), but I'm open to suggestions on how to make it better. One other thing is that apparently this analyzer works fine for analysis (e.g., it produces the desired tokens), however, for sorting in Solr I'm getting null sort tokens. Need to figure out why. Here ya go!</description>
      <attachments/>
    </issue>
    <issue>
      <id>3414</id>
      <title>Bring Hunspell for Lucene into analysis module</title>
      <description>Some time ago I along with Robert and Uwe, wrote an Stemmer which uses the Hunspell algorithm. It has the benefit of supporting dictionaries for a wide array of languages. It seems to still be being used but has fallen out of date. I think it would benefit from being inside the analysis module where additional features such as decompounding support, could be added.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3416</id>
      <title>Allow to pass an instance of RateLimiter to FSDirectory allowing to rate limit merge IO across several directories / instances</title>
      <description>This can come in handy when running several Lucene indices in the same VM, and wishing to rate limit merge across all of them.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3426</id>
      <title>optimizer for n-gram PhraseQuery</title>
      <description>If 2-gram is used and the length of query string is 4, for example q="ABCD", QueryParser generates (when autoGeneratePhraseQueries is true) PhraseQuery("AB BC CD") with slop 0. But it can be optimized PhraseQuery("AB CD") with appropriate positions. The idea came from the Japanese paper "N.M-gram: Implementation of Inverted Index Using N-gram with Hash Values" by Mikio Hirabayashi, et al. (The main theme of the paper is different from the idea that I'm using here, though)</description>
      <attachments/>
    </issue>
    <issue>
      <id>3433</id>
      <title>Random access non RAM resident IndexDocValues (CSF)</title>
      <description>There should be a way to get specific IndexDocValues by going through the Directory rather than loading all of the values into memory.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3440</id>
      <title>FastVectorHighlighter: IDF-weighted terms for ordered fragments</title>
      <description>The FastVectorHighlighter uses for every term found in a fragment an equal weight, which causes a higher ranking for fragments with a high number of words or, in the worst case, a high number of very common words than fragments that contains all of the terms used in the original query. This patch provides ordered fragments with IDF-weighted terms: total weight = total weight + IDF for unique term per fragment * boost of query; The ranking-formula should be the same, or at least similar, to that one used in org.apache.lucene.search.highlight.QueryTermScorer. The patch is simple, but it works for us. Some ideas: A better approach would be moving the whole fragments-scoring into a separate class. Switch scoring via parameter Exact phrases should be given a even better score, regardless if a phrase-query was executed or not edismax/dismax-parameters pf, ps and pf^boost should be observed and corresponding fragments should be ranked higher</description>
      <attachments/>
    </issue>
    <issue>
      <id>3441</id>
      <title>Add NRT support to facets</title>
      <description>Currently LuceneTaxonomyReader does not support NRT - i.e., on changes to LuceneTaxonomyWriter, you cannot have the reader updated, like IndexReader/Writer. In order to do that we need to do the following: Add ctor to LuceneTaxonomyReader to allow you to instantiate it with LuceneTaxonomyWriter. Add API to LuceneTaxonomyWriter to expose its internal IndexReader Change LTR.refresh() to return an LTR, rather than void. This is actually not strictly related to that issue, but since we'll need to modify refresh() impl, I think it'll be good to change its API as well. Since all of facet API is @lucene.experimental, no backwards issues here (and the sooner we do it, the better).</description>
      <attachments/>
    </issue>
    <issue>
      <id>3445</id>
      <title>Add SearcherManager, to manage IndexSearcher usage across threads and reopens</title>
      <description>This is a simple helper class I wrote for Lucene in Action 2nd ed. I'd like to commit under Lucene (contrib/misc). It simplifies using &amp; reopening an IndexSearcher across multiple threads, by using IndexReader's ref counts to know when it's safe to close the reader. In the process I also factored out a test base class for tests that want to make lots of simultaneous indexing and searching threads, and fixed TestNRTThreads (core), TestNRTManager (contrib/misc) and the new TestSearcherManager (contrib/misc) to use this base class.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3449</id>
      <title>Fix FixedBitSet.nextSetBit/prevSetBit to support the common usage pattern in every programming book</title>
      <description>The usage pattern for nextSetBit/prevSetBit is the following: for(int i=bs.nextSetBit(0); i&gt;=0; i=bs.nextSetBit(i+1)) { // operate on index i here } The problem is that the i+1 at the end can be bs.length(), but the code in nextSetBit does not allow this (same applies to prevSetBit(0)). The above usage pattern is in every programming book, so it should really be supported. The check has to be done in all cases (with the current impl in the calling code). If the check is done inside xxxSetBit() it can also be optimized to be only called seldom and not all the time, like in the ugly looking replacement, thats currently needed: for(int i=bs.nextSetBit(0); i&gt;=0; i=(i&lt;bs.length()-1) ? bs.nextSetBit(i+1) : -1) { // operate on index i here } We should change this and allow out-of bounds indexes for those two methods (they already do some checks in that direction). Enforcing this with an assert is unuseable on the client side. The test code for FixedBitSet also uses this, horrible. Please support the common usage pattern for BitSets.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3451</id>
      <title>Remove special handling of pure negative Filters in BooleanFilter, disallow pure negative queries in BooleanQuery</title>
      <description>We should at least in Lucene 4.0 remove the hack in BooleanFilter that allows pure negative Filter clauses. This is not supported by BooleanQuery and confuses users (I think that's the problem in LUCENE-3450). The hack is buggy, as it does not respect deleted documents and returns them in its DocIdSet. Also we should think about disallowing pure-negative Queries at all and throw UOE.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3454</id>
      <title>rename optimize to a less cool-sounding name</title>
      <description>I think users see the name optimize and feel they must do this, because who wants a suboptimal system? but this probably just results in wasted time and resources. maybe rename to collapseSegments or something?</description>
      <attachments/>
    </issue>
    <issue>
      <id>3488</id>
      <title>Factor out SearcherManager from NRTManager</title>
      <description>Currently we have NRTManager and SearcherManager while NRTManager contains a big piece of the code that is already in SearcherManager. Users are kind of forced to use NRTManager if they want to have SearcherManager goodness with NRT. The integration into NRTManager also forces you to maintain two instances even if you know you always want deletes. To me NRTManager tries to do more than necessary and mixes lots of responsibilities ie. handling searchers and handling indexing generations. NRTManager should use a SearcherManager by aggregation rather than duplicate a lot of logic. SearcherManager should have a NRT and Directory based implementation users can simply choose from.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3490</id>
      <title>Restructure codec hierarchy</title>
      <description>Spinoff of LUCENE-2621. (Hoping we can do some of the renaming etc here in a rote way to make progress). Currently Codec.java only represents a portion of the index, but there are other parts of the index (stored fields, term vectors, fieldinfos, ...) that we want under codec control. There is also some inconsistency about what a Codec is currently, for example Memory and Pulsing are really just PostingsFormats, you might just apply them to a specific field. On the other hand, PreFlex actually is a Codec: it represents the Lucene 3.x index format (just not all parts yet). I imagine we would like SimpleText to be the same way. So, I propose restructuring the classes so that we have something like: CodecProvider &lt;-- dead, replaced by java ServiceProvider mechanism. All indexes are 'readable' if codecs are in classpath. Codec &lt;-- represents the index format (PostingsFormat + FieldsFormat + ...) PostingsFormat: this is what Codec controls today, and Codec will return one of these for a field. FieldsFormat: Stored Fields + Term Vectors + FieldInfos? I think for PreFlex, it doesnt make sense to expose its PostingsFormat as a 'public' class, because preflex can never be per-field so there is no use in allowing you to configure PreFlex for a specific field. Similarly, I think in the future we should do the same thing for SimpleText. Nobody needs SimpleText for production, it should just be a Codec where we try to make as much of the index as plain text and simple as possible for debugging/learning/etc. So we don't need to expose its PostingsFormat. On the other hand, I don't think we need Pulsing or Memory codecs, because its pretty silly to make your entire index use one of their PostingsFormats. To parallel with analysis: PostingsFormat is like Tokenizer and Codec is like Analyzer, and we don't need Analyzers to "show off" every Tokenizer. we can also move the baked in PerFieldCodecWrapper out (it would basically be PerFieldPostingsFormat). Privately it would write the ids to the file like it does today. in the future, all 3.x hairy backwards code would move to PreflexCodec. SimpleTextCodec would get a plain text fieldinfos impl, etc.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3496</id>
      <title>Support grouping by IndexDocValues</title>
      <description>Although IDV is not yet finalized (More particular the SortedSource). I think we already can discuss / investigate implementing grouping by IDV.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3509</id>
      <title>Add settings to IWC to optimize IDV indices for CPU or RAM respectivly</title>
      <description>spinnoff from LUCENE-3496 - we are seeing much better performance if required bits for PackedInts are rounded up to a 8/16/32/64. We should add this option to IWC and default to round up ie. more RAM &amp; faster lookups.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3531</id>
      <title>Improve CachingWrapperFilter to optionally also cache acceptDocs, if identical to liveDocs</title>
      <description>Spinoff from LUCENE-1536: This issue removed the different cache modes completely and always applies the acceptDocs using BitsFilteredDocIdSet.wrap(), the cache only contains raw DocIdSet without any deletions/acceptDocs. For IndexReaders that are seldom reopened, this might not be as performant as it could be. If the acceptDocs==IR.liveDocs, those DocIdSet could also be cached with liveDocs applied.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3550</id>
      <title>Create example code for core</title>
      <description>Trunk has gone under lots of API changes. Some of which are not trivial, and the migration path from 3.x to 4.0 seems hard. I'd like to propose some way to tackle this, by means of live example code. The facet module implements this approach. There is live Java code under src/examples that demonstrate some well documented scenarios. The code itself is documented, in addition to javadoc. Also, the code itself is being unit tested regularly. We found it very difficult to keep documentation up-to-date – javadocs always lag behind, Wiki pages get old etc. However, when you have live Java code, you're forced to keep it up-to-date. It doesn't compile if you break the API, it fails to run if you change internal impl behavior. If you keep it simple enough, its documentation stays simple to. And if we are successful at maintaining it (which we must be, otherwise the build should fail), then people should have an easy experience migrating between releases. So say you take the simple scenario "I'd like to index documents which have the fields ID, date and body". Then you create an example class/method that accomplishes that. And between releases, this code gets updated, and people can follow the changes required to implement that scenario. I'm not saying the examples code should always stay optimized. We can aim at that, but I don't try to fool myself thinking that we'll succeed. But at least we can get it compiled and regularly unit tested. I think that it would be good if we introduce the concept of examples such that if a module (core, contrib, modules) have an src/examples, we package it in a .jar and include it with the binary distribution. That's for a first step. We can also have meta examples, under their own module/contrib, that show how to combine several modules together (this might even uncover API problems), but that's definitely a second phase. At first, let's do the "unit examples" (ala unit tests) and better start with core. Whatever we succeed at writing for 4.0 will only help users. So let's use this issue to: List example scenarios that we want to demonstrate for core Building the infrastructure in our build system to package and distribute a module's examples. Please feel free to list here example scenarios that come to mind. We can then track what's been done and what's not. The more we do the better.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3564</id>
      <title>rename IndexWriter.rollback to .rollbackAndClose</title>
      <description>Spinoff from LUCENE-3454, where Shai noticed that rollback is trappy since it [unexpected] closes the IW. I think we should rename it to rollbackAndClose.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3569</id>
      <title>Consolidate IndexWriter's optimize, maybeMerge and expungeDeletes under one merge(MP) method</title>
      <description>Today, IndexWriter exposes 3 methods for 'cleaning up' / 'compacting' / 'optimizing' your index: optimize() – merges as much segments as possible (down to 1 segment), and is discouraged in many cases because of its performance implications. maybeMerge() – runs 'subtle' merges. Attempts to balance the index by not leaving too many segments, yet not merging large segments if unneeded. expungeDeletes() – cleans up deleted documents from segments and on the go merges them. a default MP that can be set on IndexWriterConfig, for ongoing merges IW performs (i.e. as a result of flushing a new segment). These methods are confusing in several levels: Their names are misleading, see LUCENE-3454. Why does expungeDeletes need to merge segments? Eventually, they really do what the MergePolicy decides that should be done. I.e., one could write an MP that always merges all segments, and therefore calling maybeMerge would not be so subtle anymore. On the other hand, one could write an MP that never merges large segments (we in fact have several of those), and therefore calling optimize(1) would not end up with one segment. So the proposal is to replace all these methods with a single one merge(MergePolicy) (more on the names later). MergePolicy will have only one method findSegmentsForMerge and the caller will be responsible to configure it in order to perform the needed merges. We will provide ready-to-use MPs: LightMergePolicy – for setting on IWC and doing the ongoing merges IW executes. This one will pick segments respecting various parameters such as mergeFactor, segmentSizes etc. HeavyMergePolicy – for doing the optimize()-style merges. ExpungeDeletesMergePolicy – for expunging deletes (my proposal is to drop segment merging from it, by default). Now about the names: I think that it will be good, API-backcompat wise and in general, if we name that method doMaintenance (as expungeDeletes does not have to merge anything). Instead of MergePolicy we call it MaintenancePolicy and similarly its single method findSegmentsForMaintenance, or getMaintenanceSpecification. I called the MPs Light and Heavy just for the text, I think a better name should be found, but nothing comes up to mind now. It will allow us to use this on 3.x, by deprecating MP and all related methods.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3586</id>
      <title>Choose a specific Directory implementation running the CheckIndex main</title>
      <description>It should be possible to choose a specific Directory implementation to use during the CheckIndex process when we run it from its main. What about an additional main parameter? In fact, I'm experiencing some problems with MMapDirectory working with a big segment, and after some failed attempts playing with maxChunkSize, I decided to switch to another FSDirectory implementation but I needed to do that on my own main. Should we also consider to use a FileSwitchDirectory? I'm willing to contribute, could you please let me know your thoughts about it?</description>
      <attachments/>
    </issue>
    <issue>
      <id>3598</id>
      <title>Improve InfoStream class in trunk to be more consistent with logging-frameworks like slf4j/log4j/commons-logging</title>
      <description>Followup on a thread by Shai Erea on java-dev@lao: I already discussed with Robert about that, that there is one thing missing. Currently the IW only checks if the infoStream!=null and then passes the message to the method, and that may ignore it. For your requirement it is the case that this is enabled or disabled dynamically. Unfortunately if the construction of the message is heavy, then this wastes resources. I would like to add another method to this class: abstract boolean isEnabled() that can also be implemented. I would then replace all null checks in IW by this method. The default config in IW would be changed to use a NoOutputInfoStream that returns false here and ignores the message. A simple logger wrapper for e.g. log4j / slf4j then could look like (ignoring component, could be enabled): Loger log = YourLoggingFramework.getLogger(IndexWriter.class); public void message(String component, String message) { log.debug(component + ": " + message); } public boolean isEnabled(String component) { return log.isDebugEnabled(); } Using this you could enable/disable logging live by e.g. the log4j management console of your app server by enabling/disabling IndexWriter.class logging. The changes are really simple: PrintStreamInfoStream returns true, always, mabye make it dynamically enable/disable to allow Shai's request infoStream.getDefault() is never null and can never be set to null. Instead the default is a singleton NoOutputInfoStream that returns false of isEnabled(component). All null checks on infoStream should be replaced by infoStream.isEanbled(component), this is possible as always != null. There are no slowdowns by this - it's like Collections.emptyList() instead stupid null checks.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3602</id>
      <title>Add join query to Lucene</title>
      <description>Solr has (psuedo) join query for a while now. I think this should also be available in Lucene.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3628</id>
      <title>Cut Norms over to DocValues</title>
      <description>since IR is now fully R/O and norms are inside codecs we can cut over to use a IDV impl for writing norms. LUCENE-3606 has some ideas about how this could be implemented</description>
      <attachments/>
    </issue>
    <issue>
      <id>3638</id>
      <title>IndexReader.document always return a doc with all the stored fields loaded. And this can be slow for the indexed document contain huge fields</title>
      <description>when generating digest for some documents with huge fields, it should be unnecessary to load the field but just interesting part of the field with the offset information. but indexreader always return the whole field content. afterward, the customized storedfieldsreader will got a repeated loading</description>
      <attachments/>
    </issue>
    <issue>
      <id>3653</id>
      <title>Lucene Search not scalling</title>
      <description>I've noticed that when doing thousands of searches in a single thread the average time is quite low i.e. a few milliseconds. When adding more concurrent searches doing exactly the same search the average time increases drastically. I've profiled the search classes and found that the whole of lucene blocks on org.apache.lucene.index.SegmentCoreReaders.getTermsReader org.apache.lucene.util.VirtualMethod public synchronized int getImplementationDistance org.apache.lucene.util.AttributeSourcew.getAttributeInterfaces These cause search times to increase from a few milliseconds to up to 2 seconds when doing 500 concurrent searches on the same in memory index. Note: That the index is not being updates at all, so not refresh methods are called at any stage. Some questions: Why do we need synchronization here? There must be a non-lockable solution for these, they basically cause lucene to be ok for single thread applications but disastrous for any concurrent implementation. I'll do some experiments by removing the synchronization from the methods of these classes.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3654</id>
      <title>Optimize BytesRef comparator to use Unsafe long based comparison (when possible)</title>
      <description>Inspire by Google Guava UnsignedBytes lexi comparator, that uses unsafe to do long based comparisons over the bytes instead of one by one (which yields 2-4x better perf), use similar logic in BytesRef comparator. The code was adapted to support offset/length.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3667</id>
      <title>Consider changing how we set the number of threads to use to run tests.</title>
      <description>The current way we set the number of threads to use is not expressive enough for some systems. My quad core with hyper threading is recognized as 8 CPUs - since I can only override the number of threads to use per core, 8 is as low as I can go. 8 threads can be problematic for me - just the amount of RAM used sometimes can toss me into heavy paging because I only have 8 GB of RAM - the heavy paging can cause my whole system to come to a crawl. Without hacking the build, I don't think I have a lot of workarounds. I'd like to propose that switch from using threadsPerProcessor to threadCount. In some ways, it's not as nice, because it does not try to scale automatically per system. But that auto scaling is often not ideal (hyper threading, wanting to be able to do other work at the same time), so perhaps we just default to 1 or 2 threads and devs can override individually?</description>
      <attachments/>
    </issue>
    <issue>
      <id>3687</id>
      <title>Allow similarity to encode norms other than a single byte</title>
      <description>LUCENE-3628 cut over norms to docvalues. This removes the long standing limitation that norms are a single byte. Yet, we still need to expose this functionality to Similarity to write / encode norms in a different format.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3690</id>
      <title>JFlex-based HTMLStripCharFilter replacement</title>
      <description>A JFlex-based HTMLStripCharFilter replacement would be more performant and easier to understand and maintain.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3714</id>
      <title>add suggester that uses shortest path/wFST instead of buckets</title>
      <description>Currently the FST suggester (really an FSA) quantizes weights into buckets (e.g. single byte) and puts them in front of the word. This makes it fast, but you lose granularity in your suggestions. Lately the question was raised, if you build lucene's FST with positiveintoutputs, does it behave the same as a tropical semiring wFST? In other words, after completing the word, we instead traverse min(output) at each node to find the 'shortest path' to the best suggestion (with the highest score). This means we wouldnt need to quantize weights at all and it might make some operations (e.g. adding fuzzy matching etc) a lot easier.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3725</id>
      <title>Add optional packing to FST building</title>
      <description>The FSTs produced by Builder can be further shrunk if you are willing to spend highish transient RAM to do so... our Builder today tries hard not to use much RAM (and has options to tweak down the RAM usage, in exchange for somewhat lager FST), even when building immense FSTs. But for apps that can afford highish transient RAM to get a smaller net FST, I think we should offer packing.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3731</id>
      <title>Create a analysis/uima module for UIMA based tokenizers/analyzers</title>
      <description>As discussed in SOLR-3013 the UIMA Tokenizers/Analyzer should be refactored out in a separate module (modules/analysis/uima) as they can be used in plain Lucene. Then the solr/contrib/uima will contain only the related factories.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3747</id>
      <title>Support Unicode 6.1.0</title>
      <description>Now that Unicode 6.1.0 has been released, Lucene/Solr should support it. JFlex trunk now supports Unicode 6.1.0. Tasks include: Upgrade ICU4J to v49 (after it's released, on 2012-03-21, according to http://icu-project.org). Use icu module tools to regenerate the supplementary character additions to JFlex grammars. Version the JFlex grammars: copy the current implementations to *Impl3&lt;X&gt;; cause the versioning tokenizer wrappers to instantiate this version when the Version c-tor param is in the range 3.1 to the version in which these changes are released (excluding the range endpoints); then change the specified Unicode version in the non-versioned JFlex grammars from 6.0 to 6.1. Regenerate JFlex scanners, including StandardTokenizerImpl, UAX29URLEmailTokenizerImpl, and HTMLStripCharFilter. Using generateJavaUnicodeWordBreakTest.pl, generate and then run WordBreakTestUnicode_6_1_0.java under modules/analysis/common/src/test/org/apache/lucene/analysis/core/</description>
      <attachments/>
    </issue>
    <issue>
      <id>3753</id>
      <title>Restructure the Lucene build system</title>
      <description>Split out separate core/, test-framework/, and tools/ modules, each with its own build.xml, under the lucene/ directory, similar to the Solr restructuring done in SOLR-2452.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3758</id>
      <title>Allow the ComplexPhraseQueryParser to search order or un-order proximity queries.</title>
      <description>The ComplexPhraseQueryParser use SpanNearQuery, but always set the "inOrder" value hardcoded to "true". This could be configurable.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3759</id>
      <title>Support joining in a distributed environment.</title>
      <description>Add two more methods in JoinUtil to support joining in a distributed manner. Method to retrieve all from values. Method to create a TermsQuery based on a set of from terms. With these two methods distributed joining can be supported following these steps: Retrieve from values from each shard Merge the retrieved from values. Create a TermsQuery based on the merged from terms and send this query to all shards.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3760</id>
      <title>Cleanup DR.getCurrentVersion/DR.getUserData/DR.getIndexCommit().getUserData()</title>
      <description>Spinoff from Ryan's dev thread "DR.getCommitUserData() vs DR.getIndexCommit().getUserData()"... these methods are confusing/dups right now.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3761</id>
      <title>Generalize SearcherManager</title>
      <description>I'd like to generalize SearcherManager to a class which can manage instances of a certain type of interfaces. The reason is that today SearcherManager knows how to handle IndexSearcher instances. I have a SearcherManager which manages a pair of IndexSearcher and TaxonomyReader pair. Recently, few concurrency bugs were fixed in SearcherManager, and I realized that I need to apply them to my version as well. Which led me to think why can't we have an SM version which is generic enough so that both my version and Lucene's can benefit from? The way I see SearcherManager, it can be divided into two parts: (1) the part that manages the logic of acquire/release/maybeReopen (i.e., ensureOpen, protect from concurrency stuff etc.), and (2) the part which handles IndexSearcher, or my SearcherTaxoPair. I'm thinking that if we'll have an interface with incRef/decRef/tryIncRef/maybeRefresh, we can make SearcherManager a generic class which handles this interface. I will post a patch with the initial idea, and we can continue from there.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3762</id>
      <title>Upgrade JUnit to 4.10, refactor state-machine of detecting setUp/tearDown call chaining.</title>
      <description>Both Lucene and Solr use JUnit 4.7. I suggest we move forward and upgrade to JUnit 4.10 which provides several infrastructural changes (serializable Description objects, class-level rules, various tweaks). JUnit 4.10 also changes (or fixes, depends how you look at it) the order in which @Before/@After hooks and @Rules are applied. This makes the old state-machine in LuceneTestCase fail (because the order is changed). I rewrote the state machine and used a different, I think simpler, although Uwe may disagree , mechanism in which the hook methods setUp/ tearDown are still there, but they are empty at the top level and serve only to detect whether subclasses chain super.setUp/tearDown properly (if they override anything). In the long term, I would love to just get rid of public setup/teardown methods and make them private (so that they cannot be overriden or even seen by subclasses) but this will require changes to the runner itself.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3767</id>
      <title>Explore streaming Viterbi search in Kuromoji</title>
      <description>I've been playing with the idea of changing the Kuromoji viterbi search to be 2 passes (intersect, backtrace) instead of 4 passes (break into sentences, intersect, score, backtrace)... this is very much a work in progress, so I'm just getting my current state up. It's got tons of nocommits, doesn't properly handle the user dict nor extended modes yet, etc. One thing I'm playing with is to add a double backtrace for the long compound tokens, ie, instead of penalizing these tokens so that shorter tokens are picked, leave the scores unchanged but on backtrace take that penalty and use it as a threshold for a 2nd best segmentation...</description>
      <attachments/>
    </issue>
    <issue>
      <id>3774</id>
      <title>check-legal isn't doing its job</title>
      <description>In trunk, the check-legal-lucene ant target is not checking any lucene/contrib/**/lib/ directories; the modules/**/lib/ directories are not being checked; and check-legal-solr can't be checking solr/example/lib/**/*.jar, because there are currently .jar files in there that don't have a license. These targets are set up to take in a full list of lib/ directories in which to check, but modules move around, and these lists are not being kept up-to-date. Instead, check-legal-* should run for each module, if the module has a lib/ directory, and it should be specialized for modules that have more than one (solr/core/) or that have a lib/ directory in a non-standard place (lucene/core/).</description>
      <attachments/>
    </issue>
    <issue>
      <id>3778</id>
      <title>Create a grouping convenience class</title>
      <description>Currently the grouping module has many collector classes with a lot of different options per class. I think it would be a good idea to have a GroupUtil (Or another name?) convenience class. I think this could be a builder, because of the many options (sort,sortWithinGroup,groupOffset,groupCount and more) and implementations (term/dv/function) grouping has.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3786</id>
      <title>Create SearcherTaxoManager</title>
      <description>If an application wants to use an IndexSearcher and TaxonomyReader in a SearcherManager-like fashion, it cannot use a separate SearcherManager, and say a TaxonomyReaderManager, because the IndexSearcher and TaxoReader instances need to be in sync. That is, the IS-TR pair must match, or otherwise the category ordinals that are encoded in the search index might not match the ones in the taxonomy index. This can happen if someone reopens the IndexSearcher's IndexReader, but does not refresh the TaxonomyReader, and the category ordinals that exist in the reopened IndexReader are not yet visible to the TaxonomyReader instance. I'd like to create a SearcherTaxoManager (which is a ReferenceManager) which manages an IndexSearcher and TaxonomyReader pair. Then an application will call: SearcherTaxoPair pair = manager.acquire(); try { IndexSearcher searcher = pair.searcher; TaxonomyReader taxoReader = pair.taxoReader; // do something with them } finally { manager.release(pair); pair = null; }</description>
      <attachments/>
    </issue>
    <issue>
      <id>3795</id>
      <title>Replace spatial contrib module with LSP's spatial-lucene module</title>
      <description>I propose that Lucene's spatial contrib module be replaced with the spatial-lucene module within Lucene Spatial Playground (LSP). LSP has been in development for approximately 1 year by David Smiley, Ryan McKinley, and Chris Male and we feel it is ready. LSP is here: http://code.google.com/p/lucene-spatial-playground/ and the spatial-lucene module is intuitively in svn/trunk/spatial-lucene/. I'll add more comments to prevent the issue description from being too long.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3807</id>
      <title>Cleanup suggester API</title>
      <description>Currently the suggester api and especially TermFreqIterator don't play that nice with BytesRef and other paradigms we use in lucene, further the java iterator pattern isn't that useful when it gets to work with TermsEnum, BytesRef etc. We should try to clean up this api step by step moving over to BytesRef including the Lookup class and its interface...</description>
      <attachments/>
    </issue>
    <issue>
      <id>3825</id>
      <title>Please push maven snapshots to repositories.apache.org again</title>
      <description>Once upon a time, snapshots of the lucene trunk went into the snapshot repo at repositories.apache.org. No longer. Instead, they just sit at: https://builds.apache.org//job/Lucene-Solr-Maven-trunk/lastSuccessfulBuild/artifact/maven_artifacts/ Unfortunately, Jenkins makes a rather mediocre maven repo. the maven-wagon-plugin can't copy it and Nexus can't proxy it.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3830</id>
      <title>MappingCharFilter could be improved by switching to an FST.</title>
      <description>MappingCharFilter stores an overly complex tree-like structure for matching input patterns. The input is a union of fixed strings mapped to a set of fixed strings; an fst matcher would be ideal here and provide both memory and speed improvement I bet.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3832</id>
      <title>Port BasicAutomata.stringUnion from Brics to Lucene</title>
      <description>Brics has my code to build Automaton from a set of sorted strings in one step (Daciuk/Mihov's algorithm again). This should be easily portable to Lucene and is quite useful.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3842</id>
      <title>Analyzing Suggester</title>
      <description>Since we added shortest-path wFSA search in LUCENE-3714, and generified the comparator in LUCENE-3801, I think we should look at implementing suggesters that have more capabilities than just basic prefix matching. In particular I think the most flexible approach is to integrate with Analyzer at both build and query time, such that we build a wFST with: input: analyzed text such as ghost0christmas0past &lt;-- byte 0 here is an optional token separator output: surface form such as "the ghost of christmas past" weight: the weight of the suggestion we make an FST with PairOutputs&lt;weight,output&gt;, but only do the shortest path operation on the weight side (like the test in LUCENE-3801), at the same time accumulating the output (surface form), which will be the actual suggestion. This allows a lot of flexibility: Using even standardanalyzer means you can offer suggestions that ignore stopwords, e.g. if you type in "ghost of chr...", it will suggest "the ghost of christmas past" we can add support for synonyms/wdf/etc at both index and query time (there are tradeoffs here, and this is not implemented!) this is a basis for more complicated suggesters such as Japanese suggesters, where the analyzed form is in fact the reading, so we would add a TokenFilter that copies ReadingAttribute into term text to support that... other general things like offering suggestions that are more "fuzzy" like using a plural stemmer or ignoring accents or whatever. According to my benchmarks, suggestions are still very fast with the prototype (e.g. ~ 100,000 QPS), and the FST size does not explode (its short of twice that of a regular wFST, but this is still far smaller than TST or JaSpell, etc).</description>
      <attachments/>
    </issue>
    <issue>
      <id>3846</id>
      <title>Fuzzy suggester</title>
      <description>Would be nice to have a suggester that can handle some fuzziness (like spell correction) so that it's able to suggest completions that are "near" what you typed. As a first go at this, I implemented 1T (ie up to 1 edit, including a transposition), except the first letter must be correct. But there is a penalty, ie, the "corrected" suggestion needs to have a much higher freq than the "exact match" suggestion before it can compete. Still tons of nocommits, and somehow we should merge this / make it work with analyzing suggester too (LUCENE-3842).</description>
      <attachments/>
    </issue>
    <issue>
      <id>3847</id>
      <title>LuceneTestCase should check for modifications on System properties</title>
      <description>fail the test if changes have been detected. revert the state of system properties before the suite. cleanup after the suite.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3883</id>
      <title>Analysis for Irish</title>
      <description>Adds analysis for Irish. The stemmer is generated from a snowball stemmer. I've sent it to Martin Porter, who says it will be added during the week.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3888</id>
      <title>split off the spell check word and surface form in spell check dictionary</title>
      <description>The "did you mean?" feature by using Lucene's spell checker cannot work well for Japanese environment unfortunately and is the longstanding problem, because the logic needs comparatively long text to check spells, but for some languages (e.g. Japanese), most words are too short to use the spell checker. I think, for at least Japanese, the things can be improved if we split off the spell check word and surface form in the spell check dictionary. Then we can use ReadingAttribute for spell checking but CharTermAttribute for suggesting, for example.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3892</id>
      <title>Add a useful intblock postings format (eg, FOR, PFOR, PFORDelta, Simple9/16/64, etc.)</title>
      <description>On the flex branch we explored a number of possible intblock encodings, but for whatever reason never brought them to completion. There are still a number of issues opened with patches in different states. Initial results (based on prototype) were excellent (see http://blog.mikemccandless.com/2010/08/lucene-performance-with-pfordelta-codec.html ). I think this would make a good GSoC project.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3907</id>
      <title>Improve the Edge/NGramTokenizer/Filters</title>
      <description>Our ngram tokenizers/filters could use some love. EG, they output ngrams in multiple passes, instead of "stacked", which messes up offsets/positions and requires too much buffering (can hit OOME for long tokens). They clip at 1024 chars (tokenizers) but don't (token filters). The split up surrogate pairs incorrectly.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3922</id>
      <title>Add Japanese Kanji number normalization to Kuromoji</title>
      <description>Japanese people use Kanji numerals instead of Arabic numerals for writing price, address and so on. i.e 12万4800円(124,800JPY), 二番町三ノ二(3-2 Nibancho) and 十二月(December). So, we would like to normalize those Kanji numerals to Arabic numerals (I don't think we need to have a capability to normalize to Kanji numerals).</description>
      <attachments/>
    </issue>
    <issue>
      <id>3932</id>
      <title>Improve load time of .tii files</title>
      <description>We have a large 50 gig index which is optimized as one segment, with a 66 MEG .tii file. This index has no norms, and no field cache. It takes about 5 seconds to load this index, profiling reveals that 60% of the time is spent in GrowableWriter.set(index, value), and most of time in set(...) is spent resizing PackedInts.Mutatable current. In the constructor for TermInfosReaderIndex, you initialize the writer with the line, GrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false); For our index using four as the bit estimate results in 27 resizes. The last value in indexToTerms is going to be ~ tiiFileLength, and if instead you use, int bitEstimate = (int) Math.ceil(Math.log10(tiiFileLength) / Math.log10(2)); GrowableWriter indexToTerms = new GrowableWriter(bitEstimate, indexSize, false); Load time improves to ~ 2 seconds.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3943</id>
      <title>Use ivy cachepath and cachefileset instead of ivy retrieve</title>
      <description>In LUCENE-3930 we moved to resolving all external dependencies using ivy:retrieve. This process places the dependencies into the lib/ folder of the respective modules which was ideal since it replicated the existing build process and limited the number of changes to be made to the build. However it can lead to multiple jars for the same dependency in the lib folder when the dependency is upgraded, and just isn't the most efficient way to use Ivy. Uwe pointed out that when working from svn or in using src releases we can remove the ivy:retrieve calls and make use of ivy:cachepath and ivy:cachefileset to build our classpaths and packages respectively, which will go some way to addressing these limitations – however we still need the build system capable of putting the actual jars into specific lib folders when assembling the binary artifacts</description>
      <attachments/>
    </issue>
    <issue>
      <id>3972</id>
      <title>Improve AllGroupsCollector implementations</title>
      <description>I think that the performance of TermAllGroupsCollectorm, DVAllGroupsCollector.BR and DVAllGroupsCollector.SortedBR can be improved by using BytesRefHash to store the groups instead of an ArrayList.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3973</id>
      <title>Incorporate PMD / FindBugs</title>
      <description>This has been touched on a few times over the years. Having static analysis as part of our build seems like a big win. For example, we could use PMD to look at System.out.println statements like discussed in LUCENE-3877 and we could possibly incorporate the nocommit / @author checks as well. There are a few things to work out as part of this: Should we use both PMD and FindBugs or just one of them? They look at code from different perspectives (bytecode vs source code) and target different issues. At the moment I'm in favour of trying both but that might be too heavy handed for our needs. What checks should we use? There's no point having the analysis if it's going to raise too many false-positives or problems we don't deem problematic. How should the analysis be integrated in our build? Need to work out when the analysis should run, how it should be incorporated in Ant and/or Maven, what impact errors should have.</description>
      <attachments/>
    </issue>
    <issue>
      <id>3985</id>
      <title>Refactor support for thread leaks</title>
      <description>This will be duplicated in the runner and in LuceneTestCase; try to consolidate.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4004</id>
      <title>Add syntax for DisjunctionMaxQuery to the Query Parser</title>
      <description>I've come up with a use for DisjunctionMaxQuery, but not the dismax parser. I note that the toString() method on that item proposes a syntax with vertical bars. Is there any sympathy for a patch that added this to the standard parser, or some other syntax?</description>
      <attachments/>
    </issue>
    <issue>
      <id>4043</id>
      <title>Add scoring support for query time join</title>
      <description>Have similar scoring for query time joining just like the index time block join (with the score mode).</description>
      <attachments/>
    </issue>
    <issue>
      <id>4055</id>
      <title>Refactor SegmentInfo / FieldInfo to make them extensible</title>
      <description>After LUCENE-4050 is done the resulting SegmentInfo / FieldInfo classes should be made abstract so that they can be extended by Codec-s.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4062</id>
      <title>More fine-grained control over the packed integer implementation that is chosen</title>
      <description>In order to save space, Lucene has two main PackedInts.Mutable implentations, one that is very fast and is based on a byte/short/integer/long array (Direct*) and another one which packs bits in a memory-efficient manner (Packed*). The packed implementation tends to be much slower than the direct one, which discourages some Lucene components to use it. On the other hand, if you store 21 bits integers in a Direct32, this is a space loss of (32-21)/32=35%. If you accept to trade some space for speed, you could store 3 of these 21 bits integers in a long, resulting in an overhead of 1/3 bit per value. One advantage of this approach is that you never need to read more than one block to read or write a value, so this can be significantly faster than Packed32 and Packed64 which always need to read/write two blocks in order to avoid costly branches. I ran some tests, and for 10000000 21 bits values, this implementation takes less than 2% more space and has 44% faster writes and 30% faster reads. The 12 bits version (5 values per block) has the same performance improvement and a 6% memory overhead compared to the packed implementation. In order to select the best implementation for a given integer size, I wrote the PackedInts.getMutable(valueCount, bitsPerValue, acceptableOverheadPerValue) method. This method select the fastest implementation that has less than acceptableOverheadPerValue wasted bits per value. For example, if you accept an overhead of 20% (acceptableOverheadPerValue = 0.2f * bitsPerValue), which is pretty reasonable, here is what implementations would be selected: 1: Packed64SingleBlock1 2: Packed64SingleBlock2 3: Packed64SingleBlock3 4: Packed64SingleBlock4 5: Packed64SingleBlock5 6: Packed64SingleBlock6 7: Direct8 8: Direct8 9: Packed64SingleBlock9 10: Packed64SingleBlock10 11: Packed64SingleBlock12 12: Packed64SingleBlock12 13: Packed64 14: Direct16 15: Direct16 16: Direct16 17: Packed64 18: Packed64SingleBlock21 19: Packed64SingleBlock21 20: Packed64SingleBlock21 21: Packed64SingleBlock21 22: Packed64 23: Packed64 24: Packed64 25: Packed64 26: Packed64 27: Direct32 28: Direct32 29: Direct32 30: Direct32 31: Direct32 32: Direct32 33: Packed64 34: Packed64 35: Packed64 36: Packed64 37: Packed64 38: Packed64 39: Packed64 40: Packed64 41: Packed64 42: Packed64 43: Packed64 44: Packed64 45: Packed64 46: Packed64 47: Packed64 48: Packed64 49: Packed64 50: Packed64 51: Packed64 52: Packed64 53: Packed64 54: Direct64 55: Direct64 56: Direct64 57: Direct64 58: Direct64 59: Direct64 60: Direct64 61: Direct64 62: Direct64 Under 32 bits per value, only 13, 17 and 22-26 bits per value would still choose the slower Packed64 implementation. Allowing a 50% overhead would prevent the packed implementation to be selected for bits per value under 32. Allowing an overhead of 32 bits per value would make sure that a Direct* implementation is always selected. Next steps would be to: make lucene components use this getMutable method and let users decide what trade-off better suits them, write a Packed32SingleBlock implementation if necessary (I didn't do it because I have no 32-bits computer to test the performance improvements). I think this would allow more fine-grained control over the speed/space trade-off, what do you think?</description>
      <attachments/>
    </issue>
    <issue>
      <id>4069</id>
      <title>Segment-level Bloom filters</title>
      <description>An addition to each segment which stores a Bloom filter for selected fields in order to give fast-fail to term searches, helping avoid wasted disk access. Best suited for low-frequency fields e.g. primary keys on big indexes with many segments but also speeds up general searching in my tests. Overview slideshow here: http://www.slideshare.net/MarkHarwood/lucene-bloomfilteredsegments Benchmarks based on Wikipedia content here: http://goo.gl/X7QqU Patch based on 3.6 codebase attached. There are no 3.6 API changes currently - to play just add a field with "_blm" on the end of the name to invoke special indexing/querying capability. Clearly a new Field or schema declaration would need adding to APIs to configure the service properly. Also, a patch for Lucene4.0 codebase introducing a new PostingsFormat</description>
      <attachments/>
    </issue>
    <issue>
      <id>4072</id>
      <title>CharFilter that Unicode-normalizes input</title>
      <description>I'd like to contribute a CharFilter that Unicode-normalizes input with ICU4J. The benefit of having this process as CharFilter is that tokenizer can work on normalised text while offset-correction ensuring fast vector highlighter and other offset-dependent features do not break. The implementation is available at following repository: https://github.com/ippeiukai/ICUNormalizer2CharFilter Unfortunately this is my unpaid side-project and cannot spend much time to merge my work to Lucene to make appropriate patch. I'd appreciate it if anyone could give it a go. I'm happy to relicense it to whatever that meets your needs.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4079</id>
      <title>The hunspell filter should support compressed Hunspell dictionaries</title>
      <description>OpenOffice dictionaries are often compressed via some aliases on the beginning of the affixe file. The french one for instance. Currently the hunspell filter does not read the aliases.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4100</id>
      <title>Maxscore - Efficient Scoring</title>
      <description>At Berlin Buzzwords 2012, I will be presenting 'maxscore', an efficient algorithm first published in the IR domain in 1995 by H. Turtle &amp; J. Flood, that I find deserves more attention among Lucene users (and developers). I implemented a proof of concept and did some performance measurements with example queries and lucenebench, the package of Mike McCandless, resulting in very significant speedups. This ticket is to get started the discussion on including the implementation into Lucene's codebase. Because the technique requires awareness about it from the Lucene user/developer, it seems best to become a contrib/module package so that it consciously can be chosen to be used.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4120</id>
      <title>FST should use packed integer arrays</title>
      <description>There are some places where an int[] could be advantageously replaced with a packed integer array. I am thinking (at least) of: FST.nodeAddress (GrowableWriter) FST.inCounts (GrowableWriter) FST.nodeRefToAddress (read-only Reader) The serialization/deserialization methods should be modified too in order to take advantage of PackedInts.get {Reader,Writer} .</description>
      <attachments/>
    </issue>
    <issue>
      <id>4132</id>
      <title>IndexWriterConfig live settings</title>
      <description>A while ago there was a discussion about making some IW settings "live" and I remember that RAM buffer size was one of them. Judging from IW code, I see that RAM buffer can be changed "live" as IW never caches it. However, I don't remember which other settings were decided to be "live" and I don't see any documentation in IW nor IWC for that. IW.getConfig mentions: * &lt;b&gt;NOTE:&lt;/b&gt; some settings may be changed on the * returned {@link IndexWriterConfig}, and will take * effect in the current IndexWriter instance. See the * javadocs for the specific setters in {@link * IndexWriterConfig} for details. But there's no text on e.g. IWC.setRAMBuffer mentioning that. I think that it'd be good if we make it easier for users to tell which of the settings are "live" ones. There are few possible ways to do it: Introduce a custom @live.setting tag on the relevant IWC.set methods, and add special text for them in build.xml Or, drop the tag and just document it clearly. Separate IWC to two interfaces, LiveConfig and OneTimeConfig (name proposals are welcome !), have IWC impl both, and introduce another IW.getLiveConfig which will return that interface, thereby clearly letting the user know which of the settings are "live". It'd be good if IWC itself could only expose setXYZ methods for the "live" settings though. So perhaps, off the top of my head, we can do something like this: Introduce a Config object, which is essentially what IWC is today, and pass it to IW. IW will create a different object, IWC from that Config and IW.getConfig will return IWC. IWC itself will only have setXYZ methods for the "live" settings. It adds another object, but user code doesn't change - it still creates a Config object when initializing IW, and need to handle a different type if it ever calls IW.getConfig. Maybe that's not such a bad idea?</description>
      <attachments/>
    </issue>
    <issue>
      <id>4199</id>
      <title>Add ANT tool to track/disallow "forbidden" method invocations</title>
      <description>In LUCENE-3877 Greg Bowyer has some asm.jar-based code to inspe ct class files for System.out/err class. I wanted to modify this code to run it in a jar-linter task on ant, so all compiled class files are parsed and method/ctor calls to e.g. new String(byte[]) without charset are forbidden. We would add a list of method signatures that we dont want to have (new FileReader(File), commons.IOUtils.loadFileToString()) and this linter will throw BuildException after static inspection, if any class file in Lucene/Solr (including line numbers) uses any method call. Greg's code would be changed to use visitMethodInsn visitor, very easy.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4201</id>
      <title>Add Japanese character filter to normalize iteration marks</title>
      <description>For some applications it might be useful to normalize kanji and kana iteration marks such as 々, ゞ, ゝ, ヽ and ヾ to make sure they are treated uniformly.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4208</id>
      <title>Spatial distance relevancy should use score of 1/distance</title>
      <description>The SpatialStrategy.makeQuery() at the moment uses the distance as the score (although some strategies – TwoDoubles if I recall might not do anything which would be a bug). The distance is a poor value to use as the score because the score should be related to relevancy, and the distance itself is inversely related to that. A score of 1/distance would be nice. Another alternative is earthCircumference/2 - distance, although I like 1/distance better. Maybe use a different constant than 1. Credit: this is Chris Male's idea.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4226</id>
      <title>Efficient compression of small to medium stored fields</title>
      <description>I've been doing some experiments with stored fields lately. It is very common for an index with stored fields enabled to have most of its space used by the .fdt index file. To prevent this .fdt file from growing too much, one option is to compress stored fields. Although compression works rather well for large fields, this is not the case for small fields and the compression ratio can be very close to 100%, even with efficient compression algorithms. In order to improve the compression ratio for small fields, I've written a StoredFieldsFormat that compresses several documents in a single chunk of data. To see how it behaves in terms of document deserialization speed and compression ratio, I've run several tests with different index compression strategies on 100,000 docs from Mike's 1K Wikipedia articles (title and text were indexed and stored): no compression, docs compressed with deflate (compression level = 1), docs compressed with deflate (compression level = 9), docs compressed with Snappy, using the compressing StoredFieldsFormat with deflate (level = 1) and chunks of 6 docs, using the compressing StoredFieldsFormat with deflate (level = 9) and chunks of 6 docs, using the compressing StoredFieldsFormat with Snappy and chunks of 6 docs. For those who don't know Snappy, it is compression algorithm from Google which has very high compression ratios, but compresses and decompresses data very quickly. Format Compression ratio IndexReader.document time ———————————————————————————————————————————————————————————————— uncompressed 100% 100% doc/deflate 1 59% 616% doc/deflate 9 58% 595% doc/snappy 80% 129% index/deflate 1 49% 966% index/deflate 9 46% 938% index/snappy 65% 264% (doc = doc-level compression, index = index-level compression) I find it interesting because it allows to trade speed for space (with deflate, the .fdt file shrinks by a factor of 2, much better than with doc-level compression). One other interesting thing is that index/snappy is almost as compact as doc/deflate while it is more than 2x faster at retrieving documents from disk. These tests have been done on a hot OS cache, which is the worst case for compressed fields (one can expect better results for formats that have a high compression ratio since they probably require fewer read/write operations from disk).</description>
      <attachments/>
    </issue>
    <issue>
      <id>4236</id>
      <title>clean up booleanquery conjunction optimizations a bit</title>
      <description>After LUCENE-3505, I want to do a slight cleanup: compute the term conjunctions optimization in scorer(), so its applied even if we have optional and prohibited clauses that dont exist in the segment (e.g. return null) use the term conjunctions optimization when optional.size() == minShouldMatch, as that means they are all mandatory, too. don't return booleanscorer1 when optional.size() == minShouldMatch, because it means we have required clauses and in general BS2 should do a much better job (e.g. use advance).</description>
      <attachments/>
    </issue>
    <issue>
      <id>4258</id>
      <title>Incremental Field Updates through Stacked Segments</title>
      <description>Shai and I would like to start working on the proposal to Incremental Field Updates outlined here (http://markmail.org/message/zhrdxxpfk6qvdaex).</description>
      <attachments/>
    </issue>
    <issue>
      <id>4283</id>
      <title>Support more frequent skip with Block Postings Format</title>
      <description>This change works on the new bulk branch. Currently, our BlockPostingsFormat only supports skipInterval==blockSize. Every time the skipper reaches the last level 0 skip point, we'll have to decode a whole block to read doc/freq data. Also, a higher level skip list will be created only for those df&gt;blockSize^k, which means for most terms, skipping will just be a linear scan. If we increase current blockSize for better bulk i/o performance, current skip setting will be a bottleneck. For ForPF, the encoded block can be easily splitted if we set skipInterval=32*k.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4290</id>
      <title>basic highlighter that uses postings offsets</title>
      <description>We added IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS so you can efficiently compress character offsets in the postings list, but nothing yet makes use of this. Here is a simple highlighter that uses them: it doesn't have many tests or fancy features, but I think its ok for the sandbox/ (maybe with a couple more tests) Additionally I didnt do any benchmarking.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4332</id>
      <title>Integrate PiTest mutation coverage tool into build</title>
      <description>As discussed briefly on the mailing list, this patch is an attempt to integrate the PiTest mutation coverage tool into the lucene build</description>
      <attachments/>
    </issue>
    <issue>
      <id>4335</id>
      <title>Builds should regenerate all generated sources</title>
      <description>We have more and more sources that are generated programmatically (query parsers, fuzzy levN tables from Moman, packed ints specialized decoders, etc.), and it's dangerous because developers may directly edit the generated sources and forget to edit the meta-source. It's happened to me several times ... most recently just after landing the BlockPostingsFormat branch. I think we should re-gen all of these in our builds and fail the build if this creates a difference. I know some generators (eg JavaCC) embed timestamps and so always create mods ... we can leave them out of this for starters (or maybe post-process the sources to remove the timestamps) ...</description>
      <attachments/>
    </issue>
    <issue>
      <id>4345</id>
      <title>Create a Classification module</title>
      <description>Lucene/Solr can host huge sets of documents containing lots of information in fields so that these can be used as training examples (w/ features) in order to very quickly create classifiers algorithms to use on new documents and / or to provide an additional service. So the idea is to create a contrib module (called 'classification') to host a ClassificationComponent that will use already seen data (the indexed documents / fields) to classify new documents / text fragments. The first version will contain a (simplistic) Lucene based Naive Bayes classifier but more implementations should be added in the future.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4378</id>
      <title>QueryParsers do not support negative boosts</title>
      <description>Negative query boosts have been supported at the "Query" object level for a long time (resulting in negative scores for matching documents), but evidently we never updated the QueryParsers to know about this - attempting to specify a negative boost in the query string results in a parse error. we should probably add this to the parser grammer(s)</description>
      <attachments/>
    </issue>
    <issue>
      <id>4396</id>
      <title>BooleanScorer should sometimes be used for MUST clauses</title>
      <description>Today we only use BooleanScorer if the query consists of SHOULD and MUST_NOT. If there is one or more MUST clauses we always use BooleanScorer2. But I suspect that unless the MUST clauses have very low hit count compared to the other clauses, that BooleanScorer would perform better than BooleanScorer2. BooleanScorer still has some vestiges from when it used to handle MUST so it shouldn't be hard to bring back this capability ... I think the challenging part might be the heuristics on when to use which (likely we would have to use firstDocID as proxy for total hit count). Likely we should also have BooleanScorer sometimes use .advance() on the subs in this case, eg if suddenly the MUST clause skips 1000000 docs then you want to .advance() all the SHOULD clauses. I won't have near term time to work on this so feel free to take it if you are inspired!</description>
      <attachments/>
    </issue>
    <issue>
      <id>4399</id>
      <title>Rename AppendingCodec to Appending40Codec</title>
      <description>In order AppendingCodec to follow Lucene codecs version, I think its name should include a version number (so that, for example, if we get to releave Lucene 4.3 with a new Lucene43Codec, there will also be a new Appending43Codec).</description>
      <attachments/>
    </issue>
    <issue>
      <id>4410</id>
      <title>Make FilteredQuery more flexible with regards to how filters are applied</title>
      <description>Currently FilteredQuery uses either the "old" lucene 3 leap frog approach or pushes the filter down together with accepted docs. Yet there might be more strategies required to fit common usecases like geo-filtering where a rather costly function is applied to each document. Using leap frog this might result in a very slow query if the filter is advanced since it might have linear running time to find the next valid document. We should be more flexible with regards to those usecases and make it possible to either tell FQ what to do or plug in a strategy that applied a filter in a different way. The current FQ impl also uses an heuristic to decide if RA or LeapFrog should be used. This is really an implementation detail of the strategy and not of FQ and should be moved out.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4440</id>
      <title>FilterCodec should take a delegate Codec in its ctor</title>
      <description>FilterCodec has a delegate() method through which an extension can return its delegate Codec. This method is called on every Codec method. Adrien, on LUCENE-4391, failed to pass a Codec in the ctor, since he couldn't called Codec.forName(). Instead, we should just pass e.g. new Lucene40Codec(). I'll post a patch shortly.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4472</id>
      <title>Add setting that prevents merging on updateDocument</title>
      <description>Currently we always call maybeMerge if a segment was flushed after updateDocument. Some apps and in particular ElasticSearch uses some hacky workarounds to disable that ie for merge throttling. It should be easier to enable this kind of behavior.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4515</id>
      <title>Make MemoryIndex more memory efficient</title>
      <description>Currently MemoryIndex uses BytesRef objects to represent terms and holds an int[] per term per field to represent postings. For highlighting this creates a ton of objects for each search that 1. need to be GCed and 2. can't be reused.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4524</id>
      <title>Merge DocsEnum and DocsAndPositionsEnum into PostingsEnum</title>
      <description>spinnoff from http://www.gossamer-threads.com/lists/lucene/java-dev/172261 hey folks, I have spend a hell lot of time on the positions branch to make positions and offsets working on all queries if needed. The one thing that bugged me the most is the distinction between DocsEnum and DocsAndPositionsEnum. Really when you look at it closer DocsEnum is a DocsAndFreqsEnum and if we omit Freqs we should return a DocIdSetIter. Same is true for DocsAndPostionsAndPayloadsAndOffsets*YourFancyFeatureHere*Enum. I don't really see the benefits from this. We should rather make the interface simple and call it something like PostingsEnum where you have to specify flags on the TermsIterator and if we can't provide the sufficient enum we throw an exception? I just want to bring up the idea here since it might simplify a lot for users as well for us when improving our positions / offset etc. support. thoughts? Ideas? simon</description>
      <attachments/>
    </issue>
    <issue>
      <id>4537</id>
      <title>Move RateLimiter up to Directory and make it IOContext aware</title>
      <description>Currently the RateLimiter only applies to FSDirectory which is fine in general but always requires casts and other dir. impls (custom ones could benefit from this too.) We are also only able to rate limit merge operations which limits the functionality here a lot. Since we have the context information what the IndexOutput is used for we can use that for rate limiting.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4542</id>
      <title>Make RECURSION_CAP in HunspellStemmer configurable</title>
      <description>Currently there is private static final int RECURSION_CAP = 2; in the code of the class HunspellStemmer. It makes using hunspell with several dictionaries almost unusable, due to bad performance (f.ex. it costs 36ms to stem long sentence in latvian for recursion_cap=2 and 5 ms for recursion_cap=1). It would be nice to be able to tune this number as needed. AFAIK this number (2) was chosen arbitrary. (it's a first issue in my life, so please forgive me any mistakes done).</description>
      <attachments/>
    </issue>
    <issue>
      <id>4560</id>
      <title>Support Filtering Segments During Merge</title>
      <description>Spun off from LUCENE-4557 It is desirable to be able to filter segments during merge. Most often, full reindex of content is not possible. Merging segments can sometimes have negative consequences when fields are have different options (most restrictive option is forced during merge) Being able to filter segments during merges will allow gradually migrating indexed data to new index settings, support pruning/enhancing existing data gradually Use Cases: Migrate IndexOptions for fields (See LUCENE-4557) Gradually Remove index fields no longer used Migrate indexed sort fields to DocValues Support converting data types for indexed data and so on patch will be forthcoming</description>
      <attachments/>
    </issue>
    <issue>
      <id>4570</id>
      <title>Release ForbiddenAPI checker on Google Code</title>
      <description>Currently there is source code in lucene/tools/src (e.g. Forbidden APIs checker ant task). It would be convenient if you could download this thing in your ant build from ivy (especially if maybe it included our definitions .txt files as resources). In general checking for locale/charset violations in this way is a pretty general useful thing for a server-side app. Can we either release lucene-tools.jar as an artifact, or maybe alternatively move this somewhere else as a standalone project and suck it in ourselves?</description>
      <attachments/>
    </issue>
    <issue>
      <id>4571</id>
      <title>speedup disjunction with minShouldMatch</title>
      <description>even minShouldMatch is supplied to DisjunctionSumScorer it enumerates whole disjunction, and verifies minShouldMatch condition on every doc: public int nextDoc() throws IOException { assert doc != NO_MORE_DOCS; while(true) { while (subScorers[0].docID() == doc) { if (subScorers[0].nextDoc() != NO_MORE_DOCS) { heapAdjust(0); } else { heapRemoveRoot(); if (numScorers &lt; minimumNrMatchers) { return doc = NO_MORE_DOCS; } } } afterNext(); if (nrMatchers &gt;= minimumNrMatchers) { break; } } return doc; } Stefan Pohl proposes (as well as I get it) to pop nrMatchers-1 scorers from the heap first, and then push them back advancing behind that top doc. For me the question no.1 is there a performance test for minShouldMatch constrained disjunction.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4575</id>
      <title>Allow IndexWriter to commit, even just commitData</title>
      <description>Spinoff from here http://lucene.472066.n3.nabble.com/commit-with-only-commitData-td4022155.html. In some cases, it is valuable to be able to commit changes to the index, even if the changes are just commitData. Such data is sometimes used by applications to register in the index some global application information/state. The proposal is: Add a setCommitData() API and separate it from commit() and prepareCommit() (simplify their API) When that API is called, flip on the dirty/changes bit, so that this gets committed even if no other changes were made to the index. I will work on a patch a post.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4578</id>
      <title>ICUTokenizerFactory - per-script RBBI customization</title>
      <description>Initially this started out as an idea for a configuration knob on ICUTokenizer that would allow me to tell it not to tokenize on punctuation. Through IRC discussion on #lucene, it sorta ballooned. The committers had a long discussion about it that I don't really understand, so I'll be including it in the comments. I am a Solr user, so I would also need the ability to access the configuration from there, likely either in schema.xml or solrconfig.xml.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4580</id>
      <title>Facet DrillDown should return a ConstantScoreQuery</title>
      <description>DrillDown is a helper class which the user can use to convert a facet value that a user selected into a Query for performing drill-down or narrowing the results. The API has several static methods that create e.g. a Term or Query. Rather than creating a Query, it would make more sense to create a Filter I think. In most cases, the clicked facets should not affect the scoring of documents. Anyway, even if it turns out that it must return a Query (which I doubt), we should at least modify the impl to return a ConstantScoreQuery.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4591</id>
      <title>Make StoredFieldsFormat more configurable</title>
      <description>The current StoredFieldsFormat are implemented with the assumption that only one type of StoredfieldsFormat is used by the index. We would like to be able to configure a StoredFieldsFormat per field, similarly to the PostingsFormat. There is a few issues that need to be solved for allowing that: 1) allowing to configure a segment suffix to the StoredFieldsFormat 2) implement SPI interface in StoredFieldsFormat 3) create a PerFieldStoredFieldsFormat We are proposing to start first with 1) by modifying the signature of StoredFieldsFormat#fieldsReader and StoredFieldsFormat#fieldsWriter so that they use SegmentReadState and SegmentWriteState instead of the current set of parameters. Let us know what you think about this idea. If this is of interest, we can contribute with a first path for 1).</description>
      <attachments/>
    </issue>
    <issue>
      <id>4598</id>
      <title>Change PayloadIterator to not use top-level reader API</title>
      <description>Currently the facet module uses MultiFields.* to pull the D&amp;PEnum in PayloadIterator, to access the payloads that store the facet ords. It then makes heavy use of .advance and .getPayload to visit all docIDs in the result set. I think we should get some speedup if we go segment by segment instead ...</description>
      <attachments/>
    </issue>
    <issue>
      <id>4600</id>
      <title>Explore facets aggregation during documents collection</title>
      <description>Today the facet module simply gathers all hits (as a bitset, optionally with a float[] to hold scores as well, if you will aggregate them) during collection, and then at the end when you call getFacetsResults(), it makes a 2nd pass over all those hits doing the actual aggregation. We should investigate just aggregating as we collect instead, so we don't have to tie up transient RAM (fairly small for the bit set but possibly big for the float[]).</description>
      <attachments/>
    </issue>
    <issue>
      <id>4602</id>
      <title>Use DocValues to store per-doc facet ord</title>
      <description>Spinoff from LUCENE-4600 DocValues can be used to hold the byte[] encoding all facet ords for the document, instead of payloads. I made a hacked up approximation of in-RAM DV (see CachedCountingFacetsCollector in the patch) and the gains were somewhat surprisingly large: Task QPS base StdDev QPS comp StdDev Pct diff HighTerm 0.53 (0.9%) 1.00 (2.5%) 87.3% ( 83% - 91%) LowTerm 7.59 (0.6%) 26.75 (12.9%) 252.6% ( 237% - 267%) MedTerm 3.35 (0.7%) 12.71 (9.0%) 279.8% ( 268% - 291%) I didn't think payloads were THAT slow; I think it must be the advance implementation? We need to separately test on-disk DV to make sure it's at least on-par with payloads (but hopefully faster) and if so ... we should cutover facets to using DV.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4609</id>
      <title>Write a PackedIntsEncoder/Decoder for facets</title>
      <description>Today the facets API lets you write IntEncoder/Decoder to encode/decode the category ordinals. We have several such encoders, including VInt (default), and block encoders. It would be interesting to implement and benchmark a PackedIntsEncoder/Decoder, with potentially two variants: (1) receives bitsPerValue up front, when you e.g. know that you have a small taxonomy and the max value you can see and (2) one that decides for each doc on the optimal bitsPerValue, writes it as a header in the byte[] or something.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4614</id>
      <title>Create dev-tools/eclipse/dot.classpath automatically</title>
      <description>It is a pain to keep the file up-to-date. As it is pure XML we can use a template to produce it automatically. The same trikc like for creating index.html in the docs is used. The patch will produce it automatically from filesets/dirsets in ant. It is still a pain with the duplicate JARs, but maybe we can fix that later.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4620</id>
      <title>Explore IntEncoder/Decoder bulk API</title>
      <description>Today, IntEncoder/Decoder offer a streaming API, where you can encode(int) and decode(int). Originally, we believed that this layer can be useful for other scenarios, but in practice it's used only for writing/reading the category ordinals from payload/DV. Therefore, Mike and I would like to explore a bulk API, something like encode(IntsRef, BytesRef) and decode(BytesRef, IntsRef). Perhaps the Encoder can still be streaming (as we don't know in advance how many ints will be written), dunno. Will figure this out as we go. One thing to check is whether the bulk API can work w/ e.g. facet associations, which can write arbitrary byte[], and so may decoding to an IntsRef won't make sense. This too we'll figure out as we go. I don't rule out that associations will use a different bulk API. At the end of the day, the requirement is for someone to be able to configure how ordinals are written (i.e. different encoding schemes: VInt, PackedInts etc.) and later read, with as little overhead as possible.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4642</id>
      <title>Add create(AttributeFactory) to TokenizerFactory and subclasses with ctors taking AttributeFactory, and remove Tokenizer's and subclasses' ctors taking AttributeSource</title>
      <description>All tokenizer implementations have a constructor that takes a given AttributeSource as parameter (LUCENE-1826). These should be removed. TokenizerFactory does not provide an API to create tokenizers with a given AttributeFactory, but quite a few tokenizers have constructors that take an AttributeFactory. TokenizerFactory should add a create(AttributeFactory) method, as should subclasses for tokenizers with AttributeFactory accepting ctors.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4670</id>
      <title>Add TermVectorsWriter.finish{Doc,Field,Term} to make development of new formats easier</title>
      <description>This is especially useful to LUCENE-4599 where actions have to be taken after a doc/field/term has been added.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4682</id>
      <title>Reduce wasted bytes in FST due to array arcs</title>
      <description>When a node is close to the root, or it has many outgoing arcs, the FST writes the arcs as an array (each arc gets N bytes), so we can e.g. bin search on lookup. The problem is N is set to the max(numBytesPerArc), so if you have an outlier arc e.g. with a big output, you can waste many bytes for all the other arcs that didn't need so many bytes. I generated Kuromoji's FST and found it has 271187 wasted bytes vs total size 1535612 = ~18% wasted. It would be nice to reduce this. One thing we could do without packing is: in addNode, if we detect that number of wasted bytes is above some threshold, then don't do the expansion. Another thing, if we are packing: we could record stats in the first pass about which nodes wasted the most, and then in the second pass (paack) we could set the threshold based on the top X% nodes that waste ... Another idea is maybe to deref large outputs, so that the numBytesPerArc is more uniform ...</description>
      <attachments/>
    </issue>
    <issue>
      <id>4713</id>
      <title>SPI: Allow fallback to default ClassLoader if Thread#getContextClassLoader fails</title>
      <description>NOTE: This issue has been renamed from: "Replace calls to Thread#getContextClassLoader with the ClassLoader of the current class" because the revised patch provides a clean fallback path. I am not sure whether it is a design decision or if we can indeed consider this a bug: In core and analysis-common some classes provide on-demand class loading using SPI. In NamedSPILoader, SPIClassIterator, ClasspathResourceLoader and AnalysisSPILoader there are constructors that use the Thread's context ClassLoader by default whenever no particular other ClassLoader was specified. Unfortunately this does not work as expected when the Thread's ClassLoader can't see the required classes that are instantiated downstream with the help of Class.forName (e.g., Codecs, Analyzers, etc.). That's what happened to us here. We currently experiment with running Lucene 2.9 and 4.x in one JVM, both being separated by custom ClassLoaders, each seeing only the corresponding Lucene version and the upstream classpath. While NamedSPILoader and company get successfully loaded by our custom ClassLoader, their instantiation fails because our Thread's Context-ClassLoader cannot find the additionally required classes. We could probably work-around this by using Thread#setContextClassLoader at construction time (and quickly reverting back afterwards), but I have the impression this might just hide the actual problem and cause further trouble when lazy-loading classes later on, and potentially from another Thread. Removing the call to Thread#getContextClassLoader would also align with the behavior of AttributeSource.DEFAULT_ATTRIBUTE_FACTORY, which in fact uses Attribute#getClass().getClassLoader() instead. A simple patch is attached. All tests pass.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4715</id>
      <title>Add OrdinalPolicy.ALL_BUT_DIMENSION</title>
      <description>With the move of OrdinalPolicy to CategoryListParams, NonTopLevelOrdinalPolicy was nuked. It might be good to restore it, as another enum value of OrdinalPolicy. It's the same like ALL_PARENTS, only doesn't add the dimension ordinal, which could save space as well as computation time. It's good for when you don't care about the count of Date/, but only about its children counts.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4728</id>
      <title>Allow CommonTermsQuery to be highlighted</title>
      <description>Add support for CommonTermsQuery to all highlighter impls. This might add a dependency (query-jar) to the highlighter so we might think about adding it to core?</description>
      <attachments/>
    </issue>
    <issue>
      <id>4733</id>
      <title>Make CompressingTermVectorsFormat the new default term vectors format?</title>
      <description>In LUCENE-4599, I wrote an alternate term vectors format which has a more compact format, and I think it could replace the current Lucene40TermVectorsFormat for the next (4.2) release?</description>
      <attachments/>
    </issue>
    <issue>
      <id>4746</id>
      <title>Create a move method in Directory.</title>
      <description>I'd like to make a move method for directory. We already have a move for Solr in DirectoryFactory, but it seems it belongs at the directory level really. The default impl can do a copy and delete, but most implementations will be able to optimize to a rename. Besides the move we do for Solr (to move a replicated index into place), it would also be useful for another feature I'd like to add - the ability to merge an index with moves rather than copies. In some cases, you don't need/want to copy all the files and could just rename/move them.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4748</id>
      <title>Add DrillSideways helper class to Lucene facets module</title>
      <description>This came out of a discussion on the java-user list with subject "Faceted search in OR": http://markmail.org/thread/jmnq6z2x7ayzci5k The basic idea is to count "near misses" during collection, ie documents that matched the main query and also all except one of the drill down filters. Drill sideways makes for a very nice faceted search UI because you don't "lose" the facet counts after drilling in. Eg maybe you do a search for "cameras", and you see facets for the manufacturer, so you drill into "Nikon". With drill sideways, even after drilling down, you'll still get the counts for all the other brands, where each count tells you how many hits you'd get if you changed to a different manufacturer. This becomes more fun if you add further drill-downs, eg maybe I next drill down into Resolution=10 megapixels", and then I can see how many 10 megapixel cameras all other manufacturers, and what other resolutions Nikon cameras offer.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4750</id>
      <title>Convert DrillDown to DrillDownQuery</title>
      <description>DrillDown is a utility class for creating drill-down queries over a base query and a bunch of categories. We've been asked to support AND, OR and AND of ORs. The latter is not so simple as a static utility method though, so instead we have some sample code ... Rather, I think that we can just create a DrillDownQuery (extends Query) which takes a baseQuery in its ctor and exposes add(CategoryPath...), such that every such group of categories is AND'ed with other groups, and internally they are OR'ed. It's very similar to how you would construct a BooleanQuery, only simpler and specific to facets. Internally, it would build a BooleanQuery and delegate rewrite, createWeight etc to it. That will remove the need for the static utility methods .. or we can keep static term() for convenience.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4752</id>
      <title>Merge segments to sort them</title>
      <description>It would be awesome if Lucene could write the documents out in a segment based on a configurable order. This of course applies to merging segments to. The benefit is increased locality on disk of documents that are likely to be accessed together. This often applies to documents near each other in time, but also spatially.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4757</id>
      <title>Cleanup FacetsAccumulator API path</title>
      <description>FacetsAccumulator and FacetRequest expose too many things to users, even when they are not needed, e.g. complements and partitions. Also, Aggregator is created per-FacetRequest, while in fact applied per category list. This is confusing, because if you want to do two aggregations, e.g. count and sum-score, you need to separate the two dimensions into two different category lists at indexing time. It's not so easy to refactor everything in one go, since there's a lot of code involved. So in this issue I will: Remove complements from FacetRequest. It is only relevant to CountFacetRequest anyway. In the future, it should be a special Accumulator. Make FacetsAccumulator concrete class, and StandardFacetsAccumulator extend it and handles all the stuff that's relevant to sampling, complements and partitions. Gradually, these things will be migrated to the new API, and hopefully StandardFacetsAccumulator will go away. Aggregator is per-document. I could not break its API b/c some features (e.g. complement) depend on it. So rather I created a new FacetsAggregator, with a bulk, per-segment, API. So far migrated Counting and SumScore to that API. In the new API, you need to override FacetsAccumulator to define an Aggregator for use, the default is CountingFacetsAggregator. Started to refactor FacetResultsHandler, which its API was guided by the use of partitions. I added a simple compute(FacetArrays) to it, which by default delegates to the nasty API, but overridden by specific classes. This will get cleaned further along too. FacetRequest has a .getValueOf() which resolves an ordinal to its value (i.e. which of the two arrays to use). I added FacetRequest.FacetArraysSource and specialize when they are INT or FLOAT, creating a special FacetResultsHandler which does not go back to FR.getValueOf for every ordinal. I think that we can migrate other FacetResultsHandlers to behave like that ... at the expense of code duplication. I also added a TODO to get rid of getValueOf entirely .. will be done separately. Got rid of CountingFacetsCollector and StandardFacetsCollector in favor of a single FacetsCollector which collects matching documents, and optionally scores, per-segment. I wrote a migration class from these per-segment MatchingDocs to ScoredDocIDs (which is global), so that the rest of the code works, but the new code works w/ the optimized per-segment API. I hope performance is still roughly the same w/ these changes too. There will be follow-on issues to migrate more features to the new API, and more cleanups ...</description>
      <attachments/>
    </issue>
    <issue>
      <id>4764</id>
      <title>Faster but more RAM/Disk consuming DocValuesFormat for facets</title>
      <description>The new default DV format for binary fields has much more RAM-efficient encoding of the address for each document ... but it's also a bit slower at decode time, which affects facets because we decode for every collected docID.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4766</id>
      <title>Pattern token filter which emits a token for every capturing group</title>
      <description>The PatternTokenizer either functions by splitting on matches, or allows you to specify a single capture group. This is insufficient for my needs. Quite often I want to capture multiple overlapping tokens in the same position. I've written a pattern token filter which accepts multiple patterns and emits tokens for every capturing group that is matched in any pattern. Patterns are not anchored to the beginning and end of the string, so each pattern can produce multiple matches. For instance a pattern like : "(([a-z]+)(\d*))" when matched against: "abc123def456" would produce the tokens: abc123, abc, 123, def456, def, 456 Multiple patterns can be applied, eg these patterns could be used for camelCase analysis: "([A-Z]{2,})", "(?&lt;![A-Z])([A-Z][a-z]+)", "(?:^|\\b|(?&lt;=[0-9_])|(?&lt;=[A-Z]{2}))([a-z]+)", "([0-9]+)" When matched against the string "letsPartyLIKEits1999_dude", they would produce the tokens: lets, Party, LIKE, its, 1999, dude If no token is emitted, the original token is preserved. If the preserveOriginal flag is true, it will output the full original token (ie "letsPartyLIKEits1999_dude") in addition to any matching tokens (but in this case, if a matching token is identical to the original, it will only emit one copy of the full token). Multiple patterns are required to allow overlapping captures, but also means that patterns are less dense and easier to understand. This is my first Java code, so apologies if I'm doing something stupid.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4769</id>
      <title>Add a CountingFacetsAggregator which reads ordinals from a cache</title>
      <description>Mike wrote a prototype of a FacetsCollector which reads ordinals from a CachedInts structure on LUCENE-4609. I ported it to the new facets API, as a FacetsAggregator. I think we should offer users the means to use such a cache, even if it consumes more RAM. Mike tests show that this cache consumed x2 more RAM than if the DocValues were loaded into memory in their raw form. Also, a PackedInts version of such cache took almost the same amount of RAM as straight int[], but the gains were minor. I will post the patch shortly.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4795</id>
      <title>Add FacetsCollector based on SortedSetDocValues</title>
      <description>Recently (LUCENE-4765) we added multi-valued DocValues field (SortedSetDocValuesField), and this can be used for faceting in Solr (SOLR-4490). I think we should also add support in the facet module? It'd be an option with different tradeoffs. Eg, it wouldn't require the taxonomy index, since the main index handles label/ord resolving. There are at least two possible approaches: On every reopen, build the seg -&gt; global ord map, and then on every collect, get the seg ord, map it to the global ord space, and increment counts. This adds cost during reopen in proportion to number of unique terms ... On every collect, increment counts based on the seg ords, and then do a "merge" in the end just like distributed faceting does. The first approach is much easier so I built a quick prototype using that. The prototype does the counting, but it does NOT do the top K facets gathering in the end, and it doesn't "know" parent/child ord relationships, so there's tons more to do before this is real. I also was unsure how to properly integrate it since the existing classes seem to expect that you use a taxonomy index to resolve ords. I ran a quick performance test. base = trunk except I disabled the "compute top-K" in FacetsAccumulator to make the comparison fair; comp = using the prototype collector in the patch: Task QPS base StdDev QPS comp StdDev Pct diff OrHighLow 18.79 (2.5%) 14.36 (3.3%) -23.6% ( -28% - -18%) HighTerm 21.58 (2.4%) 16.53 (3.7%) -23.4% ( -28% - -17%) OrHighMed 18.20 (2.5%) 13.99 (3.3%) -23.2% ( -28% - -17%) Prefix3 14.37 (1.5%) 11.62 (3.5%) -19.1% ( -23% - -14%) LowTerm 130.80 (1.6%) 106.95 (2.4%) -18.2% ( -21% - -14%) OrHighHigh 9.60 (2.6%) 7.88 (3.5%) -17.9% ( -23% - -12%) AndHighHigh 24.61 (0.7%) 20.74 (1.9%) -15.7% ( -18% - -13%) Fuzzy1 49.40 (2.5%) 43.48 (1.9%) -12.0% ( -15% - -7%) MedSloppyPhrase 27.06 (1.6%) 23.95 (2.3%) -11.5% ( -15% - -7%) MedTerm 51.43 (2.0%) 46.21 (2.7%) -10.2% ( -14% - -5%) IntNRQ 4.02 (1.6%) 3.63 (4.0%) -9.7% ( -15% - -4%) Wildcard 29.14 (1.5%) 26.46 (2.5%) -9.2% ( -13% - -5%) HighSloppyPhrase 0.92 (4.5%) 0.87 (5.8%) -5.4% ( -15% - 5%) MedSpanNear 29.51 (2.5%) 27.94 (2.2%) -5.3% ( -9% - 0%) HighSpanNear 3.55 (2.4%) 3.38 (2.0%) -4.9% ( -9% - 0%) AndHighMed 108.34 (0.9%) 104.55 (1.1%) -3.5% ( -5% - -1%) LowSloppyPhrase 20.50 (2.0%) 20.09 (4.2%) -2.0% ( -8% - 4%) LowPhrase 21.60 (6.0%) 21.26 (5.1%) -1.6% ( -11% - 10%) Fuzzy2 53.16 (3.9%) 52.40 (2.7%) -1.4% ( -7% - 5%) LowSpanNear 8.42 (3.2%) 8.45 (3.0%) 0.3% ( -5% - 6%) Respell 45.17 (4.3%) 45.38 (4.4%) 0.5% ( -7% - 9%) MedPhrase 113.93 (5.8%) 115.02 (4.9%) 1.0% ( -9% - 12%) AndHighLow 596.42 (2.5%) 617.12 (2.8%) 3.5% ( -1% - 8%) HighPhrase 17.30 (10.5%) 18.36 (9.1%) 6.2% ( -12% - 28%) I'm impressed that this approach is only ~24% slower in the worst case! I think this means it's a good option to make available? Yes it has downsides (NRT reopen more costly, small added RAM usage, slightly slower faceting), but it's also simpler (no taxo index to manage).</description>
      <attachments/>
    </issue>
    <issue>
      <id>4817</id>
      <title>Add KeywordRepeaterFilter to emit tokens twice once as keyword and once not as keyword</title>
      <description>if you want to have a stemmed and an unstemmed version of a token one for recall and one for precision you have to do two fields today in most of the cases. Yet, most of the stemmers respect the keyword attribute so we could add a token filter that emits the same token twice once as keyword and once plain. Folks would most likely need to combine this RemoveDuplicatesTokenFilter but that way we can have stemmed and unstemmed version in the same field.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4819</id>
      <title>move Sorted[Set]DocValuesTermsEnum to codec</title>
      <description>Currently a user can instantiate a SortedDocValuesTermsEnum(SortedDocValues). This is a generic termsenum, implementing all operations by lookupOrd(). I think instead this should be the default implementation, and we should have e.g. SortedDocValues.termsEnum() that returns it (codec can implement something fancier). For example the default codec implements lookupOrd as an FST binary search, which means next() on this termsenum is much slower than it needs to be for the places where this enum is actually used (segment merging, OrdinalMap used for faceting in SOLR-4490 and LUCENE-4795) So instead, it can override this method and use an FSTEnum, and these operations are significantly faster (3x faster for me with a simple benchmark with 10M terms).</description>
      <attachments/>
    </issue>
    <issue>
      <id>4822</id>
      <title>Add PatternKeywordTokenFilter to marks keywords based on regular expressions</title>
      <description>today we need to pass in an explicit set of terms that we want to marks as keywords. It might make sense to allow patterns as well to prevent certain suffixes etc. to be keyworded.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4845</id>
      <title>Add AnalyzingInfixSuggester</title>
      <description>Our current suggester impls do prefix matching of the incoming text against all compiled suggestions, but in some cases it's useful to allow infix matching. E.g, Netflix does infix suggestions in their search box. I did a straightforward impl, just using a normal Lucene index, and using PostingsHighlighter to highlight matching tokens in the suggestions. I think this likely only works well when your suggestions have a strong prior ranking (weight input to build), eg Netflix knows the popularity of movies.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4858</id>
      <title>Early termination with SortingMergePolicy</title>
      <description>Spin-off of LUCENE-4752, see https://issues.apache.org/jira/browse/LUCENE-4752?focusedCommentId=13606565&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13606565 and https://issues.apache.org/jira/browse/LUCENE-4752?focusedCommentId=13607282&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13607282 When an index is sorted per-segment, queries that sort according to the index sort order could be early terminated.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4864</id>
      <title>Add AsyncFSDirectory to work around Windows issues with NIOFS (Lucene 5.0 only)</title>
      <description>On LUCENE-4848 a new directory implementation was proposed that uses AsyncFileChannel to make a sync-less directory implementation (only needed for IndexInput). The problem on Windows is that positional reads are impossible without overlapping (async) I/O, so FileChannel in the JDK has to syncronize all reads, because they consist of an atomic seek and atomic read. AsyncFSDirectoty would not have this issue, but has to take care of thread management, because you need a separate thread to get notified when the read is done. This involves overhead, but might still be better than the synchronization.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4901</id>
      <title>TestIndexWriterOnJRECrash should work on any JRE vendor via Runtime.halt()</title>
      <description>I successfully compiled Lucene 4.2 with IBM. Then ran unit tests with the nightly option set to "true" The test case TestIndexWriterOnJRECrash was skipped returning "IBM Corporation JRE not supported": [junit4:junit4] Suite: org.apache.lucene.index.TestIndexWriterOnJRECrash [junit4:junit4] IGNOR/A 0.28s | TestIndexWriterOnJRECrash.testNRTThreads [junit4:junit4] &gt; Assumption #1: IBM Corporation JRE not supported. [junit4:junit4] Completed in 0.68s, 1 test, 1 skipped</description>
      <attachments/>
    </issue>
    <issue>
      <id>4903</id>
      <title>Add AssertingScorer</title>
      <description>I think we would benefit from having an AssertingScorer that would assert that scorers are advanced correctly, return valid scores (eg. not NaN), ...</description>
      <attachments/>
    </issue>
    <issue>
      <id>4906</id>
      <title>PostingsHighlighter's PassageFormatter should allow for rendering to arbitrary objects</title>
      <description>For example, in a server, I may want to render the highlight result to JsonObject to send back to the front-end. Today since we render to string, I have to render to JSON string and then re-parse to JsonObject, which is inefficient... Or, if (Rob's idea we make a query that's like MoreLikeThis but it pulls terms from snippets instead, so you get proximity-influenced salient/expanded terms, then perhaps that renders to just an array of tokens or fragments or something from each snippet.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4922</id>
      <title>A SpatialPrefixTree based on the Hilbert Curve and variable grid sizes</title>
      <description>My wish-list for an ideal SpatialPrefixTree has these properties: Hilbert Curve ordering Variable grid size per level (ex: 256 at the top, 64 at the bottom, 16 for all in-between) Compact binary encoding (so-called "Morton number") Works for geodetic (i.e. lat &amp; lon) and non-geodetic Some bonus wishes for use in geospatial: Use an equal-area projection such that each cell has an equal area to all others at the same level. When advancing a grid level, if a cell's width is less than half its height. then divide it as 4 vertically stacked instead of 2 by 2. The point is to avoid super-skinny cells which occurs towards the poles and degrades performance. All of this requires some basic performance benchmarks to measure the effects of these characteristics.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4930</id>
      <title>Lucene's use of WeakHashMap at index time prevents full use of cores on some multi-core machines, due to contention</title>
      <description>Our project is not optimally using full processing power during under indexing load on Lucene 4.2.0. The reason is the AttributeSource.addAttribute() method, which goes through a WeakHashMap synchronizer, which is apparently single-threaded for a significant amount of time. Have a look at the following trace: "pool-1-thread-28" prio=10 tid=0x00007f47fc104800 nid=0x672b waiting for monitor entry [0x00007f47d19ed000] java.lang.Thread.State: BLOCKED (on object monitor) at java.lang.ref.ReferenceQueue.poll(ReferenceQueue.java:98) waiting to lock &lt;0x00000005c5cd9988&gt; (a java.lang.ref.ReferenceQueue$Lock) at org.apache.lucene.util.WeakIdentityMap.reap(WeakIdentityMap.java:189) at org.apache.lucene.util.WeakIdentityMap.get(WeakIdentityMap.java:82) at org.apache.lucene.util.AttributeSource$AttributeFactory$DefaultAttributeFactory.getClassForInterface(AttributeSource.java:74) at org.apache.lucene.util.AttributeSource$AttributeFactory$DefaultAttributeFactory.createAttributeInstance(AttributeSource.java:65) at org.apache.lucene.util.AttributeSource.addAttribute(AttributeSource.java:271) at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:107) at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:254) at org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:256) at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:376) at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1473) at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1148) at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1129) … We’ve had to make significant changes to the way we were indexing in order to not hit this issue as much, such as indexing using TokenStreams which we reuse, when it would have been more convenient to index with just tokens. (The reason is that Lucene internally creates TokenStream objects when you pass a token array to IndexableField, and doesn’t reuse them, and the addAttribute() causes massive contention as a result.) However, as you can see from the trace above, we’re still running into contention due to other addAttribute() method calls that are buried deep inside Lucene. I can see two ways forward. Either not use WeakHashMap or use it in a more efficient way, or make darned sure no addAttribute() calls are done in the main code indexing execution path. (I think it would be easy to fix DocInverterPerField in that way, FWIW. I just don’t know what we’ll run into next.)</description>
      <attachments/>
    </issue>
    <issue>
      <id>4936</id>
      <title>docvalues date compression</title>
      <description>DocValues fields can be very wasteful if you are storing dates (like solr's TrieDateField does if you enable docvalues) and don't actually need all the precision: e.g. "date-only" fields like date of birth with no time component, time fields without milliseconds precision, and so on. Ideally we'd compute GCD of all the values to save space (numberOfTrailingZeros is not really enough here), but i think we should at least look for values like 86400000, 3600000, and 1000 to be practical.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4942</id>
      <title>Indexed non-point shapes index excessive terms</title>
      <description>Indexed non-point shapes are comprised of a set of terms that represent grid cells. Cells completely within the shape or cells on the intersecting edge that are at the maximum detail depth being indexed for the shape are denoted as "leaf" cells. Such cells have a trailing '+' at the end. Such tokens are actually indexed twice, one with the leaf byte and one without. The TermQuery based PrefixTree Strategy doesn't consider the notion of 'leaf' cells and so the tokens with '+' are completely redundant. The Recursive [algorithm] based PrefixTree Strategy better supports correct search of indexed non-point shapes than TermQuery does and the distinction is relevant. However, the foundational search algorithms used by this strategy (Intersects &amp; Contains; the other 2 are based on these) could each be upgraded to deal with this correctly. Not trivial but very doable. In the end, spatial non-point indexes can probably be trimmed my ~40% by doing this.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4946</id>
      <title>Refactor SorterTemplate</title>
      <description>When working on TimSort (LUCENE-4839), I was a little frustrated of not being able to add galloping support because it would have required to add new primitive operations in addition to compare and swap. I started working on a prototype that uses inheritance to allow some sorting algorithms to rely on additional primitive operations. You can have a look at https://github.com/jpountz/sorts/tree/master/src/java/net/jpountz/sorts (but beware it is a prototype and still misses proper documentation and good tests). I think it would offer several advantages: no more need to implement setPivot and comparePivot when using in-place merge sort or insertion sort, the ability to use faster stable sorting algorithms at the cost of some memory overhead (our in-place merge sort is very slow), the ability to implement properly algorithms that are useful on specific datasets but require different primitive operations (such as TimSort for partially-sorted data). If you are interested in comparing these implementations with Arrays.sort, there is a Benchmark class in src/examples. What do you think?</description>
      <attachments/>
    </issue>
    <issue>
      <id>4947</id>
      <title>Java implementation (and improvement) of Levenshtein &amp; associated lexicon automata</title>
      <description>I was encouraged by Mike McCandless to open an issue concerning this after I contacted him privately about it. Thanks Mike! I'd like to submit my Java implementation of the Levenshtein Automaton as a homogenous replacement for the current heterogenous, multi-component implementation in Lucene. Benefits of upgrading include Reduced code complexity Better performance from components that were previously implemented in Python Support for on-the-fly dictionary-automaton manipulation (if you wish to use my dictionary-automaton implementation) The code for all the components is well structured, easy to follow, and extensively commented. It has also been fully tested for correct functionality and performance. The levenshtein automaton implementation (along with the required MDAG reference) can be found in my LevenshteinAutomaton Java library here: https://github.com/klawson88/LevenshteinAutomaton. The minimalistic directed acyclic graph (MDAG) which the automaton code uses to store and step through word sets can be found here: https://github.com/klawson88/MDAG *Transpositions aren't currently implemented. I hope the comment filled, editing-friendly code combined with the fact that the section in the Mihov paper detailing transpositions is only 2 pages makes adding the functionality trivial. Update introduces transposition inclusion in edit distance calculations! *As a result of support for on-the-fly manipulation, the MDAG (dictionary-automaton) creation process incurs a slight speed penalty. In order to have the best of both worlds, i'd recommend the addition of a constructor which only takes sorted input. The complete, easy to follow pseudo-code for the simple procedure can be found in the first article I linked under the references section in the MDAG repository)</description>
      <attachments/>
    </issue>
    <issue>
      <id>4956</id>
      <title>the korean analyzer that has a korean morphological analyzer and dictionaries</title>
      <description>Korean language has specific characteristic. When developing search service with lucene &amp; solr in korean, there are some problems in searching and indexing. The korean analyer solved the problems with a korean morphological anlyzer. It consists of a korean morphological analyzer, dictionaries, a korean tokenizer and a korean filter. The korean anlyzer is made for lucene and solr. If you develop a search service with lucene in korean, It is the best idea to choose the korean analyzer.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4963</id>
      <title>Deprecate broken TokenFilter constructors</title>
      <description>We have some TokenFilters which are only broken with specific options. This includes: TrimFilter when updateOffsets=true StopFilter, JapanesePartOfSpeechStopFilter, KeepWordFilter, LengthFilter, TypeTokenFilter when enablePositionIncrements=false I think we should deprecate these behaviors in 4.4 and remove them in trunk.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4965</id>
      <title>Add dynamic numeric range faceting</title>
      <description>The facet module today requires the app to compute the hierarchy at index time, eg a timestamp field might use a year/month/day hierarchy. While this gives great performance, since it minimizes the search-time computation, sometimes it's unfortunately useful/necessary to do things entirely at search time, like Solr does. E.g. I'm playing with a prototype Lucene search for Jira issues and I'd like to add a drill down+sideways for "Updated in past day, 2 days, week, month" etc. But because time is constantly advancing, doing this at index time is a not easy ...</description>
      <attachments/>
    </issue>
    <issue>
      <id>4975</id>
      <title>Add Replication module to Lucene</title>
      <description>I wrote a replication module which I think will be useful to Lucene users who want to replicate their indexes for e.g high-availability, taking hot backups etc. I will upload a patch soon where I'll describe in general how it works.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4981</id>
      <title>Deprecate PositionFilter</title>
      <description>According to the documentation (http://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters#solr.PositionFilterFactory), PositionFilter is mainly useful to make query parsers generate boolean queries instead of phrase queries although this problem can be solved at query parsing level instead of analysis level (eg. using QueryParser.setAutoGeneratePhraseQueries). So given that PositionFilter corrupts token graphs (see TestRandomChains), I propose to deprecate it.</description>
      <attachments/>
    </issue>
    <issue>
      <id>4985</id>
      <title>Make it easier to mix different kinds of FacetRequests</title>
      <description>Spinoff from LUCENE-4980, where we added a strange class called RangeFacetsAccumulatorWrapper, which takes an incoming FSP, splits out the FacetRequests into range and non-range, delegates to two accumulators for each set, and then zips the results back together in order. Somehow we should generalize this class and make it work with SortedSetDocValuesAccumulator as well.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5012</id>
      <title>Make graph-based TokenFilters easier</title>
      <description>SynonymFilter has two limitations today: It cannot create positions, so eg dns -&gt; domain name service creates blatantly wrong highlights (SOLR-3390, LUCENE-4499 and others). It cannot consume a graph, so e.g. if you try to apply synonyms after Kuromoji tokenizer I'm not sure what will happen. I've thought about how to fix these issues but it's really quite difficult with the current PosInc/PosLen graph representation, so I'd like to explore an alternative approach.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5013</id>
      <title>ScandinavianFoldingFilterFactory and ScandinavianNormalizationFilterFactory</title>
      <description>This filter is an augmentation of output from ASCIIFoldingFilter, it discriminate against double vowels aa, ae, ao, oe and oo, leaving just the first one. blåbærsyltetøj == blåbärsyltetöj == blaabaarsyltetoej == blabarsyltetoj räksmörgås == ræksmørgås == ræksmörgaos == raeksmoergaas == raksmorgas Caveats: Since this is a filtering on top of ASCIIFoldingFilter äöåøæ already has been folded down to aoaoae when handled by this filter it will cause effects such as: bøen -&gt; boen -&gt; bon åene -&gt; aene -&gt; ane I find this to be a trivial problem compared to not finding anything at all. Background: Swedish åäö is in fact the same letters as Norwegian and Danish åæø and thus interchangeable in when used between these languages. They are however folded differently when people type them on a keyboard lacking these characters and ASCIIFoldingFilter handle ä and æ differently. When a Swedish person is lacking umlauted characters on the keyboard they consistently type a, a, o instead of å, ä, ö. Foreigners also tend to use a, a, o. In Norway people tend to type aa, ae and oe instead of å, æ and ø. Some use a, a, o. I've also seen oo, ao, etc. And permutations. Not sure about Denmark but the pattern is probably the same. This filter solves that problem, but might also cause new.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5038</id>
      <title>Don't call MergePolicy / IndexWriter during DWPT Flush</title>
      <description>We currently consult the indexwriter -&gt; merge policy to decide if we need to write CFS or not which is bad in many ways. we should call mergepolicy only during merges we should never sync on IW during DWPT flush we should be able to make the decision if we need to write CFS or not before flush, ie. we could write parts of the flush directly to CFS or even start writing stored fields directly. in the NRT case it might make sense to write all flushes to CFS to minimize filedescriptors independent of the index size. I wonder if we can use a simple boolean for this in the IWC and get away with not consulting merge policy. This would simplify concurrency a lot here already.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5052</id>
      <title>bitset codec for off heap filters</title>
      <description>Colleagues, When we filter we don’t care any of scoring factors i.e. norms, positions, tf, but it should be fast. The obvious way to handle this is to decode postings list and cache it in heap (CachingWrappingFilter, Solr’s DocSet). Both of consuming a heap and decoding as well are expensive. Let’s write a posting list as a bitset, if df is greater than segment's maxdocs/8 (what about skiplists? and overall performance?). Beside of the codec implementation, the trickiest part to me is to design API for this. How we can let the app know that a term query don’t need to be cached in heap, but can be held as an mmaped bitset? WDYT?</description>
      <attachments/>
    </issue>
    <issue>
      <id>5081</id>
      <title>WAH8DocIdSet</title>
      <description>Our filters use bit sets a lot to store document IDs. However, it is likely that most of them are sparse hence easily compressible. Having efficient compressed sets would allow for caching more data.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5084</id>
      <title>EliasFanoDocIdSet</title>
      <description>DocIdSet in Elias-Fano encoding</description>
      <attachments/>
    </issue>
    <issue>
      <id>5086</id>
      <title>RamUsageEstimator causes AWT classes to be loaded by calling ManagementFactory#getPlatformMBeanServer</title>
      <description>Yea, that type of day and that type of title . Since the last update of Java 6 on OS X, I started to see an annoying icon pop up at the doc whenever running elasticsearch. By default, all of our scripts add headless AWT flag so people will probably not encounter it, but, it was strange that I saw it when before I didn't. I started to dig around, and saw that when RamUsageEstimator was being loaded, it was causing AWT classes to be loaded. Further investigation showed that actually for some reason, calling ManagementFactory#getPlatformMBeanServer now with the new Java version causes AWT classes to be loaded (at least on the mac, haven't tested on other platforms yet). There are several ways to try and solve it, for example, by identifying the bug in the JVM itself, but I think that there should be a fix for it in Lucene itself, specifically since there is no need to call #getPlatformMBeanServer to get the hotspot diagnostics one (its a heavy call...). Here is a simple call that will allow to get the hotspot mxbean without using the #getPlatformMBeanServer method, and not causing it to be loaded and loading all those nasty AWT classes: Object getHotSpotMXBean() { try { // Java 6 Class sunMF = Class.forName("sun.management.ManagementFactory"); return sunMF.getMethod("getDiagnosticMXBean").invoke(null); } catch (Throwable t) { // ignore } // potentially Java 7 try { return ManagementFactory.class.getMethod("getPlatformMXBean", Class.class).invoke(null, Class.forName("com.sun.management.HotSpotDiagnosticMXBean")); } catch (Throwable t) { // ignore } return null; }</description>
      <attachments/>
    </issue>
    <issue>
      <id>5092</id>
      <title>join: don't expect all filters to be FixedBitSet instances</title>
      <description>The join module throws exceptions when the parents filter isn't a FixedBitSet. The reason is that the join module relies on prevSetBit to find the first child document given a parent ID. As suggested by Uwe and Paul Elschot on LUCENE-5081, we could fix it by exposing methods in the iterators to iterate backwards. When the join modules gets an iterator which isn't able to iterate backwards, it would just need to dump its content into another DocIdSet that supports backward iteration, FixedBitSet for example.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5101</id>
      <title>make it easier to plugin different bitset implementations to CachingWrapperFilter</title>
      <description>Currently this is possible, but its not so friendly: protected DocIdSet docIdSetToCache(DocIdSet docIdSet, AtomicReader reader) throws IOException { if (docIdSet == null) { // this is better than returning null, as the nonnull result can be cached return EMPTY_DOCIDSET; } else if (docIdSet.isCacheable()) { return docIdSet; } else { final DocIdSetIterator it = docIdSet.iterator(); // null is allowed to be returned by iterator(), // in this case we wrap with the sentinel set, // which is cacheable. if (it == null) { return EMPTY_DOCIDSET; } else { /* INTERESTING PART */ final FixedBitSet bits = new FixedBitSet(reader.maxDoc()); bits.or(it); return bits; /* END INTERESTING PART */ } } } Is there any value to having all this other logic in the protected API? It seems like something thats not useful for a subclass... Maybe this stuff can become final, and "INTERESTING PART" calls a simpler method, something like: protected DocIdSet cacheImpl(DocIdSetIterator iterator, AtomicReader reader) { final FixedBitSet bits = new FixedBitSet(reader.maxDoc()); bits.or(iterator); return bits; }</description>
      <attachments/>
    </issue>
    <issue>
      <id>5127</id>
      <title>FixedGapTermsIndex should use monotonic compression</title>
      <description>for the addresses in the big in-memory byte[] and disk blocks, we could save a good deal of RAM here. I think this codec just never got upgraded when we added these new packed improvements, but it might be interesting to try to use for the terms data of sorted/sortedset DV implementations. patch works, but has nocommits and currently ignores the divisor. The annoying problem there being that we have the shared interface with "get(int)" for PackedInts.Mutable/Reader, but no equivalent base class for monotonics get(long)... Still its enough that we could benchmark/compare for now.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5153</id>
      <title>Allow wrapping Reader from AnalyzerWrapper</title>
      <description>It can be useful to allow AnalyzerWrapper extensions to wrap the Reader given to initReader, e.g. with a CharFilter.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5157</id>
      <title>Refactoring MultiDocValues.OrdinalMap to clarify API and internal structure.</title>
      <description>I refactored MultiDocValues.OrdinalMap, removing one unused parameter and renaming some methods to more clearly communicate what they do. Also I renamed subIndex references to segmentIndex.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5161</id>
      <title>review FSDirectory chunking defaults and test the chunking</title>
      <description>Today there is a loop in SimpleFS/NIOFS: try { do { final int readLength; if (total + chunkSize &gt; len) { readLength = len - total; } else { // LUCENE-1566 - work around JVM Bug by breaking very large reads into chunks readLength = chunkSize; } final int i = file.read(b, offset + total, readLength); total += i; } while (total &lt; len); } catch (OutOfMemoryError e) { I bet if you look at the clover report its untested, because its fixed at 100MB for 32-bit users and 2GB for 64-bit users (are these defaults even good?!). Also if you call the setter on a 64-bit machine to change the size, it just totally ignores it. We should remove that, the setter should always work. And we should set it to small values in tests so this loop is actually executed.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5173</id>
      <title>Add checkindex piece of LUCENE-5116</title>
      <description>LUCENE-5116 fixes addIndexes(Reader) to never write a 0-document segment (in the case you merge in empty or all-deleted stuff). I considered it just an inconsistency, but it could cause confusing exceptions to real users too if there was a "regression" here. (see solr users list:Split Shard Error - maxValue must be non-negative).</description>
      <attachments/>
    </issue>
    <issue>
      <id>5178</id>
      <title>doc values should expose missing values (or allow configurable defaults)</title>
      <description>DocValues should somehow allow a configurable default per-field. Possible implementations include setting it on the field in the document or registration of an IndexWriter callback. If we don't make the default configurable, then another option is to have DocValues fields keep track of whether a value was indexed for that document or not.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5189</id>
      <title>Numeric DocValues Updates</title>
      <description>In LUCENE-4258 we started to work on incremental field updates, however the amount of changes are immense and hard to follow/consume. The reason is that we targeted postings, stored fields, DV etc., all from the get go. I'd like to start afresh here, with numeric-dv-field updates only. There are a couple of reasons to that: NumericDV fields should be easier to update, if e.g. we write all the values of all the documents in a segment for the updated field (similar to how livedocs work, and previously norms). It's a fairly contained issue, attempting to handle just one data type to update, yet requires many changes to core code which will also be useful for updating other data types. It has value in and on itself, and we don't need to allow updating all the data types in Lucene at once ... we can do that gradually. I have some working patch already which I'll upload next, explaining the changes.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5197</id>
      <title>Add a method to SegmentReader to get the current index heap memory size</title>
      <description>It would be useful to at least estimate the index heap size being used by Lucene. Ideally a method exposing this information at the SegmentReader level.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5199</id>
      <title>Improve LuceneTestCase.defaultCodecSupportsDocsWithField to check the actual DocValuesFormat used per-field</title>
      <description>On LUCENE-5178 Han reported the following test failure: [junit4] FAILURE 0.27s | TestRangeAccumulator.testMissingValues &lt;&lt;&lt; [junit4] &gt; Throwable #1: org.junit.ComparisonFailure: expected:&lt;...(0) [junit4] &gt; less than 10 ([8) [junit4] &gt; less than or equal to 10 (]8) [junit4] &gt; over 90 (8) [junit4] &gt; 9...&gt; but was:&lt;...(0) [junit4] &gt; less than 10 ([28) [junit4] &gt; less than or equal to 10 (2]8) [junit4] &gt; over 90 (8) [junit4] &gt; 9...&gt; [junit4] &gt; at __randomizedtesting.SeedInfo.seed([815B6AA86D05329C:EBC638EE498F066D]:0) [junit4] &gt; at org.apache.lucene.facet.range.TestRangeAccumulator.testMissingValues(TestRangeAccumulator.java:670) [junit4] &gt; at java.lang.Thread.run(Thread.java:722) which can be reproduced with tcase=TestRangeAccumulator -Dtests.method=testMissingValues -Dtests.seed=815B6AA86D05329C -Dtests.slow=true -Dtests.postingsformat=Lucene41 -Dtests.locale=ca -Dtests.timezone=Australia/Currie -Dtests.file.encoding=UTF-8 It seems that the Codec that is picked is a Lucene45Codec with Lucene42DVFormat, which does not support docsWithFields for numericDV. We should improve LTC.defaultCodecSupportsDocsWithField to take a list of fields and check that the actual DVF used for each field supports it.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5205</id>
      <title>SpanQueryParser with recursion, analysis and syntax very similar to classic QueryParser</title>
      <description>This parser extends QueryParserBase and includes functionality from: Classic QueryParser: most of its syntax SurroundQueryParser: recursive parsing for "near" and "not" clauses. ComplexPhraseQueryParser: can handle "near" queries that include multiterms (wildcard, fuzzy, regex, prefix), AnalyzingQueryParser: has an option to analyze multiterms. At a high level, there's a first pass BooleanQuery/field parser and then a span query parser handles all terminal nodes and phrases. Same as classic syntax: term: test fuzzy: roam~0.8, roam~2 wildcard: te?t, test*, t*st regex: /[mb]oat/ phrase: "jakarta apache" phrase with slop: "jakarta apache"~3 default "or" clause: jakarta apache grouping "or" clause: (jakarta apache) boolean and +/-: (lucene OR apache) NOT jakarta; +lucene +apache -jakarta multiple fields: title:lucene author:hatcher Main additions in SpanQueryParser syntax vs. classic syntax: Can require "in order" for phrases with slop with the ~&gt; operator: "jakarta apache"~&gt;3 Can specify "not near": "fever bieber"!~3,10 :: find "fever" but not if "bieber" appears within 3 words before or 10 words after it. Fully recursive phrasal queries with [ and ]; as in: [[jakarta apache]~3 lucene]~&gt;4 :: find "jakarta" within 3 words of "apache", and that hit has to be within four words before "lucene" Can also use [] for single level phrasal queries instead of " as in: [jakarta apache] Can use "or grouping" clauses in phrasal queries: "apache (lucene solr)"~3 :: find "apache" and then either "lucene" or "solr" within three words. Can use multiterms in phrasal queries: "jakarta~1 ap*che"~2 Did I mention full recursion: [[jakarta~1 ap*che]~2 (solr~ /l[ou]+[cs][en]+/)]~10 :: Find something like "jakarta" within two words of "ap*che" and that hit has to be within ten words of something like "solr" or that "lucene" regex. Can require at least x number of hits at boolean level: "apache AND (lucene solr tika)~2 Can use negative only query: -jakarta :: Find all docs that don't contain "jakarta" Can use an edit distance &gt; 2 for fuzzy query via SlowFuzzyQuery (beware of potential performance issues!). Trivial additions: Can specify prefix length in fuzzy queries: jakarta~1,2 (edit distance =1, prefix =2) Can specifiy Optimal String Alignment (OSA) vs Levenshtein for distance &lt;=2: (jakarta~1 (OSA) vs jakarta~&gt;1(Levenshtein) This parser can be very useful for concordance tasks (see also LUCENE-5317 and LUCENE-5318) and for analytical search. Until LUCENE-2878 is closed, this might have a use for fans of SpanQuery. Most of the documentation is in the javadoc for SpanQueryParser. Any and all feedback is welcome. Thank you. Until this is added to the Lucene project, I've added a standalone lucene-addons repo (with jars compiled for the latest stable build of Lucene) on github.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5207</id>
      <title>lucene expressions module</title>
      <description>Expressions are geared at defining an alternative ranking function (e.g. incorporating the text relevance score and other field values/ranking signals). So they are conceptually much more like ElasticSearch's scripting support (http://www.elasticsearch.org/guide/reference/modules/scripting/) than solr's function queries. Some additional notes: In addition to referring to other fields, they can also refer to other expressions, so they can be used as "computed fields". You can rank documents easily by multiple expressions (its a SortField at the end), e.g. Sort by year descending, then some function of score price and time ascending. The provided javascript expression syntax is much more efficient than using a scripting engine, because it does not have dynamic typing (compiles to .class files that work on doubles). Performance is similar to writing a custom FieldComparator yourself, but much easier to do. We have solr integration to contribute in the future, but this is just the standalone lucene part as a start. Since lucene has no schema, it includes an implementation of Bindings (SimpleBindings) that maps variable names to SortField's or other expressions.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5215</id>
      <title>Add support for FieldInfos generation</title>
      <description>In LUCENE-5189 we've identified few reasons to do that: If you want to update docs' values of field 'foo', where 'foo' exists in the index, but not in a specific segment (sparse DV), we cannot allow that and have to throw a late UOE. If we could rewrite FieldInfos (with generation), this would be possible since we'd also write a new generation of FIS. When we apply NDV updates, we call DVF.fieldsConsumer. Currently the consumer isn't allowed to change FI.attributes because we cannot modify the existing FIS. This is implicit however, and we silently ignore any modified attributes. FieldInfos.gen will allow that too. The idea is to add to SIPC fieldInfosGen, add to each FieldInfo a dvGen and add support for FIS generation in FieldInfosFormat, SegReader etc., like we now do for DocValues. I'll work on a patch. Also on LUCENE-5189, Rob raised a concern about SegmentInfo.attributes that have same limitation – if a Codec modifies them, they are silently being ignored, since we don't gen the .si files. I think we can easily solve that by recording SI.attributes in SegmentInfos, so they are recorded per-commit. But I think it should be handled in a separate issue.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5235</id>
      <title>throw illegalstate from Tokenizer (instead of NPE/IIOBE) if reset not called</title>
      <description>We added these best effort checks, but it would be much better if we somehow gave a clear exception... this comes up often</description>
      <attachments/>
    </issue>
    <issue>
      <id>5236</id>
      <title>Use broadword bit selection in EliasFanoDecoder</title>
      <description>Try and speed up decoding</description>
      <attachments/>
    </issue>
    <issue>
      <id>5248</id>
      <title>Improve the data structure used in ReaderAndLiveDocs to hold the updates</title>
      <description>Currently ReaderAndLiveDocs holds the updates in two structures: Map&lt;String,Map&lt;Integer,Long&gt;&gt; Holds a mapping from each field, to all docs that were updated and their values. This structure is updated when applyDeletes is called, and needs to satisfy several requirements: Un-ordered writes: if a field "f" is updated by two terms, termA and termB, in that order, and termA affects doc=100 and termB doc=2, then the updates are applied in that order, meaning we cannot rely on updates coming in order. Same document may be updated multiple times, either by same term (e.g. several calls to IW.updateNDV) or by different terms. Last update wins. Sequential read: when writing the updates to the Directory (fieldsConsumer), we iterate on the docs in-order and for each one check if it's updated and if not, pull its value from the current DV. A single update may affect several million documents, therefore need to be efficient w.r.t. memory consumption. Map&lt;Integer,Map&lt;String,Long&gt;&gt; Holds a mapping from a document, to all the fields that it was updated in and the updated value for each field. This is used by IW.commitMergedDeletes to apply the updates that came in while the segment was merging. The requirements this structure needs to satisfy are: Access in doc order: this is how commitMergedDeletes works. One-pass: we visit a document once (currently) and so if we can, it's better if we know all the fields in which it was updated. The updates are applied to the merged ReaderAndLiveDocs (where they are stored in the first structure mentioned above). Comments with proposals will follow next.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5258</id>
      <title>add distance function to expressions/</title>
      <description>Adding this static function makes it really easy to incorporate distance with the score or other signals in arbitrary ways, e.g. score / (1 + log(distance)) or whatever.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5261</id>
      <title>add simple API to build queries from analysis chain</title>
      <description>Currently this is pretty crazy stuff. Additionally its duplicated in like 3 or 4 places in our codebase (i noticed it doing LUCENE-5259) We can solve that duplication, and make it easy to simply create queries from an analyzer (its been asked on the user list), as well as make it easier to build new queryparsers.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5274</id>
      <title>Teach fast FastVectorHighlighter to highlight "child fields" with parent fields</title>
      <description>I've been messing around with the FastVectorHighlighter and it looks like I can teach it to highlight matches on "child fields". Like this query: foo:scissors foo_exact:running would highlight foo like this: &lt;em&gt;running&lt;/em&gt; with &lt;em&gt;scissors&lt;/em&gt; Where foo is stored WITH_POSITIONS_OFFSETS and foo_plain is an unstored copy of foo a different analyzer and its own WITH_POSITIONS_OFFSETS. This would make queries that perform weighted matches against different analyzers much more convenient to highlight. I have working code and test cases but they are hacked into Elasticsearch. I'd love to Lucene-ify if you'll take them.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5299</id>
      <title>Refactor Collector API for parallelism</title>
      <description>Motivation We should be able to scale-up better with Solr/Lucene by utilizing multiple CPU cores, and not have to resort to scaling-out by sharding (with all the associated distributed system pitfalls) when the index size does not warrant it. Presently, IndexSearcher has an optional constructor arg for an ExecutorService, which gets used for searching in parallel for call paths where one of the TopDocCollector's is created internally. The per-atomic-reader search happens in parallel and then the TopDocs/TopFieldDocs results are merged with locking around the merge bit. However there are some problems with this approach: If arbitary Collector args come into play, we can't parallelize. Note that even if ultimately results are going to a TopDocCollector it may be wrapped inside e.g. a EarlyTerminatingCollector or TimeLimitingCollector or both. The special-casing with parallelism baked on top does not scale, there are many Collector's that could potentially lend themselves to parallelism, and special-casing means the parallelization has to be re-implemented if a different permutation of collectors is to be used. Proposal A refactoring of collectors that allows for parallelization at the level of the collection protocol. Some requirements that should guide the implementation: easy migration path for collectors that need to remain serial the parallelization should be composable (when collectors wrap other collectors) allow collectors to pick the optimal solution (e.g. there might be memory tradeoffs to be made) by advising the collector about whether a search will be parallelized, so that the serial use-case is not penalized. encourage use of non-blocking constructs and lock-free parallelism, blocking is not advisable for the hot-spot of a search, besides wasting pooled threads.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5310</id>
      <title>Merge Threads unnecessarily block on SerialMergeScheduler</title>
      <description>I have been working on a high level merge multiplexer that shares threads across different IW instances and I came across the fact that SerialMergeScheduler actually blocks incoming thread is a merge in going on. Yet this blocks threads unnecessarily since we pull the merges in a loop anyway. We should use a tryLock operation instead of syncing the entire method?</description>
      <attachments/>
    </issue>
    <issue>
      <id>5316</id>
      <title>Taxonomy tree traversing improvement</title>
      <description>The taxonomy traversing is done today utilizing the ParallelTaxonomyArrays. In particular, two taxonomy-size int arrays which hold for each ordinal it's (array #1) youngest child and (array #2) older sibling. This is a compact way of holding the tree information in memory, but it's not perfect: Large (8 bytes per ordinal in memory) Exposes internal implementation Utilizing these arrays for tree traversing is not straight forward Lose reference locality while traversing (the array is accessed in increasing only entries, but they may be distant from one another) In NRT, a reopen is always (not worst case) done at O(Taxonomy-size) This issue is about making the traversing more easy, the code more readable, and open it for future improvements (i.e memory footprint and NRT cost) - without changing any of the internals. A later issue(s?) could be opened to address the gaps once this one is done.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5317</id>
      <title>Concordance/Key Word In Context (KWIC) capability</title>
      <description>This patch enables a Lucene-powered concordance search capability. Concordances are extremely useful for linguists, lawyers and other analysts performing analytic search vs. traditional snippeting/document retrieval tasks. By "analytic search," I mean that the user wants to browse every time a term appears (or at least the topn) in a subset of documents and see the words before and after. Concordance technology is far simpler and less interesting than IR relevance models/methods, but it can be extremely useful for some use cases. Traditional concordance sort orders are available (sort on words before the target, words after, target then words before and target then words after). Under the hood, this is running SpanQuery's getSpans() and reanalyzing to obtain character offsets. There is plenty of room for optimizations and refactoring. Many thanks to my colleague, Jason Robinson, for input on the design of this patch.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5325</id>
      <title>Move ValueSource and FunctionValues under core/</title>
      <description>Spinoff from LUCENE-5298: ValueSource and FunctionValues are abstract APIs which exist under the queries/ module. That causes any module which wants to depend on these APIs (but not necessarily on any of their actual implementations!), to depend on the queries/ module. If we move these APIs under core/, we can eliminate these dependencies and add some mock impls for testing purposes. Quoting Robert from LUCENE-5298: we should eliminate the suggest/ dependencies on expressions and queries, the expressions/ on queries, the grouping/ dependency on queries, the spatial/ dependency on queries, its a mess. To add to that list, facet/ should not depend on queries too.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5339</id>
      <title>Simplify the facet module APIs</title>
      <description>I'd like to explore simplifications to the facet module's APIs: I think the current APIs are complex, and the addition of a new feature (sparse faceting, LUCENE-5333) threatens to add even more classes (e.g., FacetRequestBuilder). I think we can do better. So, I've been prototyping some drastic changes; this is very early/exploratory and I'm not sure where it'll wind up but I think the new approach shows promise. The big changes are: Instead of *FacetRequest/Params/Result, you directly instantiate the classes that do facet counting (currently TaxonomyFacetCounts, RangeFacetCounts or SortedSetDVFacetCounts), passing in the SimpleFacetsCollector, and then you interact with those classes to pull labels + values (topN under a path, sparse, specific labels). At index time, no more FacetIndexingParams/CategoryListParams; instead, you make a new SimpleFacetFields and pass it the field it should store facets + drill downs under. If you want more than one CLI you create more than one instance of SimpleFacetFields. I added a simple schema, where you state which dimensions are hierarchical or multi-valued. From this we decide how to index the ordinals (no more OrdinalPolicy). Sparse faceting is just another method (getAllDims), on both taxonomy &amp; ssdv facet classes. I haven't created a common base class / interface for all of the search-time facet classes, but I think this may be possible/clean, and perhaps useful for drill sideways. All the new classes are under oal.facet.simple.*. Lots of things that don't work yet: drill sideways, complements, associations, sampling, partitions, etc. This is just a start ...</description>
      <attachments/>
    </issue>
    <issue>
      <id>5350</id>
      <title>Add Context Aware Suggester</title>
      <description>It would be nice to have a Context Aware Suggester (i.e. a suggester that could return suggestions depending on some specified context(s)). Use-cases: location-based suggestions: returns suggestions which 'match' the context of a particular area suggest restaurants names which are in Palo Alto (context -&gt; Palo Alto) category-based suggestions: returns suggestions for items that are only in certain categories/genres (contexts) suggest movies that are of the genre sci-fi and adventure (context -&gt; [sci-fi, adventure])</description>
      <attachments/>
    </issue>
    <issue>
      <id>5353</id>
      <title>ShingleFilter should have a way to specify FILLER_TOKEN</title>
      <description>Today we have no choice that if pos_inc is &gt; 1 there will be a `_` inserted in between the tokens. We should have the ability to change this character and the char[] that holds it should not be public static since it's mutable.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5354</id>
      <title>Blended score in AnalyzingInfixSuggester</title>
      <description>I'm working on a custom suggester derived from the AnalyzingInfix. I require what is called a "blended score" (//TODO ln.399 in AnalyzingInfixSuggester) to transform the suggestion weights depending on the position of the searched term(s) in the text. Right now, I'm using an easy solution : If I want 10 suggestions, then I search against the current ordered index for the 100 first results and transform the weight : a) by using the term position in the text (found with TermVector and DocsAndPositionsEnum) or b) by multiplying the weight by the score of a SpanQuery that I add when searching and return the updated 10 most weighted suggestions. Since we usually don't need to suggest so many things, the bigger search + rescoring overhead is not so significant but I agree that this is not the most elegant solution. We could include this factor (here the position of the term) directly into the index. So, I can contribute to this if you think it's worth adding it. Do you think I should tweak AnalyzingInfixSuggester, subclass it or create a dedicated class ?</description>
      <attachments/>
    </issue>
    <issue>
      <id>5356</id>
      <title>Morfologik filter can accept custom dictionary resources</title>
      <description>I have little proposal for morfologik lucene module. Current module is tightly coupled with polish DICTIONARY enumeration. But other people (like me) can build own dictionaries to FSA and use it with lucene. You can find proposal in attachment and also example usage in analyzer (SlovakLemmaAnalyzer). It uses dictionary property as String resource from classpath, not enumeration. One change is, that dictionary variable must be set in MofologikFilterFactory (no default value).</description>
      <attachments/>
    </issue>
    <issue>
      <id>5360</id>
      <title>Add support for developing in netbeans IDE</title>
      <description>It will be nice to have ant target for building netbeans IDE project definition.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5376</id>
      <title>Add a demo search server</title>
      <description>I think it'd be useful to have a "demo" search server for Lucene. Rather than being fully featured, like Solr, it would be minimal, just wrapping the existing Lucene modules to show how you can make use of these features in a server setting. The purpose is to demonstrate how one can build a minimal search server on top of APIs like SearchManager, SearcherLifetimeManager, etc. This is also useful for finding rough edges / issues in Lucene's APIs that make building a server unnecessarily hard. I don't think it should have back compatibility promises (except Lucene's index back compatibility), so it's free to improve as Lucene's APIs change. As a starting point, I'll post what I built for the "eating your own dog food" search app for Lucene's &amp; Solr's jira issues http://jirasearch.mikemccandless.com (blog: http://blog.mikemccandless.com/2013/05/eating-dog-food-with-lucene.html ). It uses Netty to expose basic indexing &amp; searching APIs via JSON, but it's very rough (lots nocommits).</description>
      <attachments/>
    </issue>
    <issue>
      <id>5378</id>
      <title>Enable using extended field types with prefix queries for non-default encoded strings</title>
      <description>Enable users to be able to use prefix query with custom field types with non-default encoding/decoding for queries more easily. e.g. having a custom field work with base64 encoded query strings. Currently, the workaround for it is to have the override at getRewriteMethod level. Perhaps having the prefixQuery also use the calling FieldType's readableToIndexed method would work better.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5383</id>
      <title>fix changes2html to link pull requests</title>
      <description>If someone submits a pull request, i think we should put it in changes.txt in some way similar to the jira issues: e.g. for a JIRA issue we do: * LUCENE-XXXX: Add FooBar. (Joe Contributor via John Committer) changes2html recognizes and expands these to jira issue links. so I think we should be able to do something like: * pull request #xxx: Add FooBar. (Joe Contributor via John Committer) and have it link to the request, too.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5388</id>
      <title>Eliminate construction over readers for Tokenizer</title>
      <description>In the modern world, Tokenizers are intended to be reusable, with input supplied via #setReader. The constructors that take Reader are a vestige. Worse yet, they invite people to make mistakes in handling the reader that tangle them up with the state machine in Tokenizer. The sensible thing is to eliminate these ctors, and force setReader usage.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5404</id>
      <title>Add support to get number of entries a Suggester Lookup was built with and minor refactorings</title>
      <description>It would be nice to be able to tell the number of entries a suggester lookup was built with. This would let components using lookups to keep some stats regarding how many entries were used to build a lookup. Additionally, Dictionary could use InputIterator rather than the BytesRefIteratator, as most of the implmentations now use it.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5405</id>
      <title>Exception strategy for analysis improved</title>
      <description>SOLR-5623 included some conversation about the dilemmas of exception management and reporting in the analysis chain. I've belatedly become educated about the infostream, and this situation is a job for it. The DocInverterPerField can note exceptions in the analysis chain, log out to the infostream, and then rethrow them as before. No wrapping, no muss, no fuss. There are comments on this JIRA from a more complex prior idea that readers might want to ignore.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5410</id>
      <title>Add fuzziness support to SimpleQueryParser</title>
      <description>It would be nice to add fuzzy query support to the SimpleQueryParser so that: foo~2 generates a FuzzyQuery with an max edit distance of 2 and: "foo bar"~2 generates a PhraseQuery with a slop of 2.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5418</id>
      <title>Don't use .advance on costly (e.g. distance range facets) filters</title>
      <description>If you use a distance filter today (see http://blog.mikemccandless.com/2014/01/geospatial-distance-faceting-using.html ), then drill down on one of those ranges, under the hood Lucene is using .advance on the Filter, which is very costly because we end up computing distance on (possibly many) hits that don't match the query. It's better performance to find the hits matching the Query first, and then check the filter. FilteredQuery can already do this today, when you use its QUERY_FIRST_FILTER_STRATEGY. This essentially accomplishes the same thing as Solr's "post filters" (I think?) but with a far simpler/better/less code approach. E.g., I believe ElasticSearch uses this API when it applies costly filters. Longish term, I think Query/Filter ought to know itself that it's expensive, and cases where such a Query/Filter is MUST'd onto a BooleanQuery (e.g. ConstantScoreQuery), or the Filter is a clause in BooleanFilter, or it's passed to IndexSearcher.search, we should also be "smart" here and not call .advance on such clauses. But that'd be a biggish change ... so for today the "workaround" is the user must carefully construct the FilteredQuery themselves. In the mean time, as another workaround, I want to fix DrillSideways so that when you drill down on such filters it doesn't use .advance; this should give a good speedup for the "normal path" API usage with a costly filter. I'm iterating on the lucene server branch (LUCENE-5376) but once it's working I plan to merge this back to trunk / 4.7.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5425</id>
      <title>Make creation of FixedBitSet in FacetsCollector overridable</title>
      <description>In FacetsCollector, creation of bits in MatchingDocs are allocated per query. For large indexes where maxDocs are large creating a bitset of maxDoc bits will be expensive and would great a lot of garbage. Attached patch is to allow for this allocation customizable while maintaining current behavior.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5434</id>
      <title>NRT support for file systems that do no have delete on last close or cannot delete while referenced semantics.</title>
      <description>See SOLR-5693 and our HDFS support - for something like HDFS to work with NRT, we need an ability for near realtime readers to hold references to their files to prevent deletes.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5437</id>
      <title>ASCIIFoldingFilter that emits both unfolded and folded tokens</title>
      <description>I've found myself wanting an ASCIIFoldingFilter that emits both the folded tokens and the original, unfolded tokens.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5438</id>
      <title>add near-real-time replication</title>
      <description>Lucene's replication module makes it easy to incrementally sync index changes from a master index to any number of replicas, and it handles/abstracts all the underlying complexity of holding a time-expiring snapshot, finding which files need copying, syncing more than one index (e.g., taxo + index), etc. But today you must first commit on the master, and then again the replica's copied files are fsync'd, because the code operates on commit points. But this isn't "technically" necessary, and it mixes up durability and fast turnaround time. Long ago we added near-real-time readers to Lucene, for the same reason: you shouldn't have to commit just to see the new index changes. I think we should do the same for replication: allow the new segments to be copied out to replica(s), and new NRT readers to be opened, to fully decouple committing from visibility. This way apps can then separately choose when to replicate (for freshness), and when to commit (for durability). I think for some apps this could be a compelling alternative to the "re-index all documents on each shard" approach that Solr Cloud / ElasticSearch implement today, and it may also mean that the transaction log can remain external to / above the cluster.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5439</id>
      <title>Add Jacoco option for Test Coverage</title>
      <description>Jacoco (http://www.jacoco.org/) is a much cleaner and simpler to use code coverage tool than clover and additionally doesn't require having a third party license since it is open source. It also has nice Jenkins integration tools that make it incredibly easy to see what is and isn't tested. We should convert the Lucene and Solr builds to use Jacoco instead of Clover.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5440</id>
      <title>Add LongFixedBitSet and replace usage of OpenBitSet</title>
      <description>Spinoff from here: http://lucene.markmail.org/thread/35gw3amo53dsqsqj. I wrote a LongFixedBitSet which behaves like FixedBitSet, only allows managing more than 2.1B bits. It overcome some issues I've encountered with OpenBitSet, such as the use of set/fastSet as well the implementation of DocIdSet. I'll post a patch shortly and describe it in more detail.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5449</id>
      <title>Two ancient classes renamed to be less peculiar: _TestHelper and _TestUtil</title>
      <description>_TestUtil and _TestHelper begin with _ for historical reasons that don't apply any longer. Lets eliminate those _'s.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5460</id>
      <title>Allow driving a query by sparse filters</title>
      <description>Today if a filter is very sparse we execute the query in sort of a leap-frog manner between the query and filter. If the query is very expensive to compute, and/or matching few docs only too, calling scorer.advance(doc) just to discover the doc it landed on isn't accepted by the filter, is a waste of time. Since Filter is always the "final ruler", I wonder if we had something like boolean DISI.advanceExact(doc) we could use it instead, in some cases. There are many combinations in which I think we'd want to use/not-use this API, and they depend on: Filter's complexity, Filter.cost(), Scorer.cost(), query complexity (span-near, many clauses) etc. I open an issue so we can discuss. DISI.advanceExact(doc) is just a preliminary proposal, to get an API we could experiment with. The default implementation should be fairly easy and straightforward, and we could override where we can offer a more optimized imp.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5472</id>
      <title>Long terms should generate a RuntimeException, not just infoStream</title>
      <description>As reported on the solr-user list, when a term is greater then 2^15 bytes it is silently ignored at indexing time – a message is logged in to infoStream if enabled, but no error is thrown. seems like we should change this behavior (if nothing else starting in 5.0) to throw an exception.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5476</id>
      <title>Facet sampling</title>
      <description>With LUCENE-5339 facet sampling disappeared. When trying to display facet counts on large datasets (&gt;10M documents) counting facets is rather expensive, as all the hits are collected and processed. Sampling greatly reduced this and thus provided a nice speedup. Could it be brought back?</description>
      <attachments/>
    </issue>
    <issue>
      <id>5482</id>
      <title>improve default TurkishAnalyzer</title>
      <description>Add a TokenFilter that strips characters after an apostrophe (including the apostrophe itself).</description>
      <attachments/>
    </issue>
    <issue>
      <id>5487</id>
      <title>Can we separate "top scorer" from "sub scorer"?</title>
      <description>This is just an exploratory patch ... still many nocommits, but I think it may be promising. I find the two booleans we pass to Weight.scorer confusing, because they really only apply to whoever will call score(Collector) (just IndexSearcher and BooleanScorer). The params are pointless for the vast majority of scorers, because very, very few query scorers really need to change how top-scoring is done, and those scorers can only score top-level (throw throw UOE from nextDoc/advance). It seems like these two types of scorers should be separately typed.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5489</id>
      <title>Add query rescoring API</title>
      <description>When costly scoring factors are used during searching, a common approach is to do a cheaper / basic query first, collect the top few hundred hits, and then rescore those hits using the more costly query. It's not clear/simple to do this with Lucene today; I think we should make it easier.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5527</id>
      <title>Make the Collector API work per-segment</title>
      <description>Spin-off of LUCENE-5299. LUCENE-5229 proposes different changes, some of them being controversial, but there is one of them that I really really like that consists in refactoring the Collector API in order to have a different Collector per segment. The idea is, instead of having a single Collector object that needs to be able to take care of all segments, to have a top-level Collector: public interface Collector { AtomicCollector setNextReader(AtomicReaderContext context) throws IOException; } and a per-AtomicReaderContext collector: public interface AtomicCollector { void setScorer(Scorer scorer) throws IOException; void collect(int doc) throws IOException; boolean acceptsDocsOutOfOrder(); } I think it makes the API clearer since it is now obious setScorer and acceptDocsOutOfOrder need to be called after setNextReader which is otherwise unclear. It also makes things more flexible. For example, a collector could much more easily decide to use different strategies on different segments. In particular, it makes the early-termination collector much cleaner since it can return different atomic collectors implementations depending on whether the current segment is sorted or not. Even if we have lots of collectors all over the place, we could make it easier to migrate by having a Collector that would implement both Collector and AtomicCollector, return this in setNextReader and make current concrete Collector implementations extend this class instead of directly extending Collector.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5528</id>
      <title>Add context to AnalyzingInfixSuggester</title>
      <description>Spinoff from LUCENE-5350.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5545</id>
      <title>Add ExpressionRescorer</title>
      <description>In LUCENE-5489 we added QueryRescorer, to rescore first-pass hits using scores from a (usually) more expensive second-pass query. I think we should also add ExpressionRescorer, to compute the second pass score using an arbitrary JS expression.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5554</id>
      <title>Add TermBulkScorer</title>
      <description>Hotspot was unhappy with the changes in LUCENE-5487, e.g.: http://people.apache.org/~mikemccand/lucenebench/OrHighHigh.html But it looks like we can get the performance back by making a dedicated BulkScorer for TermQuery.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5558</id>
      <title>Add TruncateTokenFilter</title>
      <description>I am using this filter as a stemmer for Turkish language. In many academic research (classification, retrieval) it is used and called as Fixed Prefix Stemmer or Simple Truncation Method or F5 in short. Among F3 TO F7, F5 stemmer (length=5) is found to work well for Turkish language in Information Retrieval on Turkish Texts. It is the same work where most of stopwords_tr.txt are acquired. ElasticSearch has truncate filter but it does not respect keyword attribute. And it has a use case similar to TruncateFieldUpdateProcessorFactory Main advantage of F5 stemming is : it does not effected by the meaning loss caused by ascii folding. It is a diacritics-insensitive stemmer and works well with ascii folding. Effects of diacritics on Turkish information retrieval Here is the full field type I use for "diacritics-insensitive search" for Turkish &lt;fieldType name="text_tr_ascii_f5" class="solr.TextField" positionIncrementGap="100"&gt; &lt;analyzer&gt; &lt;tokenizer class="solr.StandardTokenizerFactory"/&gt; &lt;filter class="solr.ApostropheFilterFactory"/&gt; &lt;filter class="solr.TurkishLowerCaseFilterFactory"/&gt; &lt;filter class="solr.ASCIIFoldingFilterFactory"/&gt; &lt;filter class="solr.KeywordRepeatFilterFactory"/&gt; &lt;filter class="solr.TruncateTokenFilterFactory" prefixLength="5"/&gt; &lt;filter class="solr.RemoveDuplicatesTokenFilterFactory"/&gt; &lt;/analyzer&gt; I would like to get community opinions : 1) Any interest in this? 2) keyword attribute should be respected? 3) package name analysis.misc versus analyis.tr 4) name of the class TruncateTokenFilter versus FixedPrefixStemFilter</description>
      <attachments/>
    </issue>
    <issue>
      <id>5559</id>
      <title>Argument validation for TokenFilters having numeric constructor parameter(s)</title>
      <description>Some TokenFilters have numeric arguments in their constructors. They should throw IllegalArgumentException for negative or meaningless values. Here is some examples that demonstrates invalid/meaningless arguments : &lt;filter class="solr.LimitTokenCountFilterFactory" maxTokenCount="-10" /&gt; &lt;filter class="solr.LengthFilterFactory" min="-5" max="-1" /&gt; &lt;filter class="solr.LimitTokenPositionFilterFactory" maxTokenPosition="-3" /&gt;</description>
      <attachments/>
    </issue>
    <issue>
      <id>5569</id>
      <title>Rename AtomicReader to LeafReader</title>
      <description>See LUCENE-5527 for more context: several of us seem to prefer Leaf to Atomic. Talking from my experience, I was a bit confused in the beginning that this thing is named AtomicReader, since Atomic is otherwise used in Java in the context of concurrency. So maybe renaming it to Leaf would help remove this confusion and also carry the information that these readers are used as leaves of top-level readers?</description>
      <attachments/>
    </issue>
    <issue>
      <id>5584</id>
      <title>Allow FST read method to also recycle the output value when traversing FST</title>
      <description>The FST class heavily reuses Arc instances when traversing the FST. The output of an Arc however is not reused. This can especially be important when traversing large portions of a FST and using the ByteSequenceOutputs and CharSequenceOutputs. Those classes create a new byte[] or char[] for every node read (which has an output). In our use case we intersect a lucene Automaton with a FST&lt;BytesRef&gt; much like it is done in org.apache.lucene.search.suggest.analyzing.FSTUtil.intersectPrefixPaths() and since the Automaton and the FST are both rather large tens or even hundreds of thousands of temporary byte array objects are created. One possible solution to the problem would be to change the org.apache.lucene.util.fst.Outputs class to have two additional methods (if you don't want to change the existing methods for compatibility): /** Decode an output value previously written with {@link * #write(Object, DataOutput)} reusing the object passed in if possible */ public abstract T read(DataInput in, T reuse) throws IOException; /** Decode an output value previously written with {@link * #writeFinalOutput(Object, DataOutput)}. By default this * just calls {@link #read(DataInput)}. This tries to reuse the object * passed in if possible */ public T readFinalOutput(DataInput in, T reuse) throws IOException { return read(in, reuse); } The new methods could then be used in the FST in the readNextRealArc() method passing in the output of the reused Arc. For most inputs they could even just invoke the original read(in) method. If you should decide to make that change I'd be happy to supply a patch and/or tests for the feature.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5588</id>
      <title>We should also fsync the directory when committing</title>
      <description>Since we are on Java 7 now and we already fixed FSDir.sync to use FileChannel (LUCENE-5570), we can also fsync the directory (at least try to do it). Unlike RandomAccessFile, which must be a regular file, FileChannel.open() can also open a directory: http://stackoverflow.com/questions/7694307/using-filechannel-to-fsync-a-directory-with-nio-2</description>
      <attachments/>
    </issue>
    <issue>
      <id>5596</id>
      <title>Support for index/search large numeric field</title>
      <description>Currently if an number is larger than Long.MAX_VALUE, we can't index/search that in lucene as a number. For example, IPv6 address is an 128 bit number, so we can't index that as a numeric field and do numeric range query etc. It would be good to support BigInteger / BigDecimal I've tried use BigInteger for IPv6 in Elasticsearch and that works fine, but there are still lots of things to do https://github.com/elasticsearch/elasticsearch/pull/5758</description>
      <attachments/>
    </issue>
    <issue>
      <id>5604</id>
      <title>Should we switch BytesRefHash to MurmurHash3?</title>
      <description>MurmurHash3 has better hashing distribution than the current hash function we use for BytesRefHash which is a simple multiplicative function with 31 multiplier (same as Java's String.hashCode, but applied to bytes not chars). Maybe we should switch ...</description>
      <attachments/>
    </issue>
    <issue>
      <id>5606</id>
      <title>Add @Monster annotation and run these tests periodically</title>
      <description>We have some awesome very heavy tests, Test2B*, but they are @Ignored and we never run them. I think we should add a new @Monster annotation and set up a Jenkins job with huge test timeout, big Java heap, etc.?</description>
      <attachments/>
    </issue>
    <issue>
      <id>5609</id>
      <title>Should we revisit the default numeric precision step?</title>
      <description>Right now it's 4, for both 8 (long/double) and 4 byte (int/float) numeric fields, but this is a pretty big hit on indexing speed and disk usage, especially for tiny documents, because it creates many (8 or 16) terms for each value. Since we originally set these defaults, a lot has changed... e.g. we now rewrite MTQs per-segment, we have a faster (BlockTree) terms dict, a faster postings format, etc. Index size is important because it limits how much of the index will be hot (fit in the OS's IO cache). And more apps are using Lucene for tiny docs where the overhead of individual fields is sizable. I used the Geonames corpus to run a simple benchmark (all sources are committed to luceneutil). It has 8.6 M tiny docs, each with 23 fields, with these numeric fields: lat/lng (double) modified time, elevation, population (long) dem (int) I tested 4, 8 and 16 precision steps: indexing: PrecStep Size IndexTime 4 1812.7 MB 651.4 sec 8 1203.0 MB 443.2 sec 16 894.3 MB 361.6 sec searching: Field PrecStep QueryTime TermCount geoNameID 4 2872.5 ms 20306 geoNameID 8 2903.3 ms 104856 geoNameID 16 3371.9 ms 5871427 latitude 4 2160.1 ms 36805 latitude 8 2249.0 ms 240655 latitude 16 2725.9 ms 4649273 modified 4 2038.3 ms 13311 modified 8 2029.6 ms 58344 modified 16 2060.5 ms 77763 longitude 4 3468.5 ms 33818 longitude 8 3629.9 ms 214863 longitude 16 4060.9 ms 4532032 Index time is with 1 thread (for identical index structure). The query time is time to run 100 random ranges for that field, averaged over 20 iterations. TermCount is the total number of terms the MTQ rewrote to across all 100 queries / segments, and it gets higher as expected as precStep gets higher, but the search time is not that heavily impacted ... negligible going from 4 to 8, and then some impact from 8 to 16. Maybe we should increase the int/float default precision step to 8 and long/double to 16? Or both to 16?</description>
      <attachments/>
    </issue>
    <issue>
      <id>5611</id>
      <title>Simplify the default indexing chain</title>
      <description>I think Lucene's current indexing chain has too many classes / hierarchy / abstractions, making it look much more complex than it really should be, and discouraging users from experimenting/innovating with their own indexing chains. Also, if it were easier to understand/approach, then new developers would more likely try to improve it ... it really should be simpler. So I'm exploring a pared back indexing chain, and have a starting patch that I think is looking ok: it seems more approachable than the current indexing chain, or at least has fewer strange classes. I also thought this could give some speedup for tiny documents (a more common use of Lucene lately), and it looks like, with the evil optimizations, this is a ~25% speedup for Geonames docs. Even without those evil optos it's a bit faster. This is very much a work in progress / nocommits, and there are some behavior changes e.g. the new chain requires all fields to have the same TV options (rather than auto-upgrading all fields by the same name that the current chain does)...</description>
      <attachments/>
    </issue>
    <issue>
      <id>5620</id>
      <title>LowerCaseFilter.preserveOriginal</title>
      <description>Following closely the model of LUCENE-5437 (which worked on ASCIIFoldingFilter), this patch adds the ability to preserve the original token to LowerCaseFilter. This is useful if you want an all-lowercase search term to match without regard to case, while search terms with uppercase letters match in a case-sensitive manner.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5627</id>
      <title>Positional joins</title>
      <description>Prototype of analysis and search for labeled fragments</description>
      <attachments/>
    </issue>
    <issue>
      <id>5634</id>
      <title>Reuse TokenStream instances in Field</title>
      <description>If you don't reuse your Doc/Field instances (which is very expert: I suspect few apps do) then there's a lot of garbage created to index each StringField because we make a new StringTokenStream or NumericTokenStream (and their Attributes). We should be able to re-use these instances via a static ThreadLocal...</description>
      <attachments/>
    </issue>
    <issue>
      <id>5648</id>
      <title>Index/search multi-valued time durations</title>
      <description>If you need to index a date/time duration, then the way to do that is to have a pair of date fields; one for the start and one for the end – pretty straight-forward. But if you need to index a variable number of durations per document, then the options aren't pretty, ranging from denormalization, to joins, to using Lucene spatial with 2D as described here. Ideally it would be easier to index durations, and work in a more optimal way. This issue implements the aforementioned feature using Lucene-spatial with a new single-dimensional SpatialPrefixTree implementation. Unlike the other two SPT implementations, it's not based on floating point numbers. It will have a Date based customization that indexes levels at meaningful quantities like seconds, minutes, hours, etc. The point of that alignment is to make it faster to query across meaningful ranges (i.e. [2000 TO 2014]) and to enable a follow-on issue to facet on the data in a really fast way. I'll expect to have a working patch up this week.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5650</id>
      <title>Enforce read-only access to any path outside the temporary folder via security manager</title>
      <description>The recent refactoring to all the create temp file/dir functions (which is great!) has a minor regression from what existed before. With the old LuceneTestCase.TEMP_DIR, the directory was created if it did not exist. So, if you set java.io.tmpdir to "./temp", then it would create that dir within the per jvm working dir. However, getBaseTempDirForClass() now does asserts that check the dir exists, is a dir, and is writeable. Lucene uses "." as java.io.tmpdir. Then in the test security manager, the per jvm cwd has read/write/execute permissions. However, this allows tests to write to their cwd, which I'm trying to protect against (by setting cwd to read/execute in my test security manager).</description>
      <attachments/>
    </issue>
    <issue>
      <id>5666</id>
      <title>Add UninvertingReader</title>
      <description>Currently the fieldcache is not pluggable at all. It would be better if everything used the docvalues apis. This would allow people to customize the implementation, extend the classes with custom subclasses with additional stuff, etc etc. FieldCache can be accessed via the docvalues apis, using the FilterReader api.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5674</id>
      <title>A new token filter: SubSequence</title>
      <description>A new configurable token filter which, given a token breaks it into sub-parts and outputs consecutive sub-sequences of those sub-parts. Useful for, for example, using during indexing to generate variations on domain names, so that "www.google.com" can be found by searching for "google.com", or "www.google.com". Parameters: sepRegexp: A regular expression used split incoming tokens into sub-parts. glue: A string used to concatenate sub-parts together when creating sub-sequences. minLen: Minimum length (in sub-parts) of output sub-sequences maxLen: Maximum length (in sub-parts) of output sub-sequences (0 for unlimited; negative numbers for token length in sub-parts minus specified length) anchor: Anchor.START to output only prefixes, or Anchor.END to output only suffixes, or Anchor.NONE to output any sub-sequence withOriginal: whether to output also the original token EDIT: now includes tests for filter and for factory.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5675</id>
      <title>"ID postings format"</title>
      <description>Today the primary key lookup in lucene is not that great for systems like solr and elasticsearch that have versioning in front of IndexWriter. To some extend BlockTree can "sometimes" help avoid seeks by telling you the term does not exist for a segment. But this technique (based on FST prefix) is fragile. The only other choice today is bloom filters, which use up huge amounts of memory. I don't think we are using everything we know: particularly the version semantics. Instead, if the FST for the terms index used an algebra that represents the max version for any subtree, we might be able to answer that there is no term T with version &lt; V in that segment very efficiently. Also ID fields dont need postings lists, they dont need stats like docfreq/totaltermfreq, etc this stuff is all implicit. As far as API, i think for users to provide "IDs with versions" to such a PF, a start would to set a payload or whatever on the term field to get it thru indexwriter to the codec. And a "consumer" of the codec can just cast the Terms to a subclass that exposes the FST to do this version check efficiently.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5680</id>
      <title>Allow updating multiple DocValues fields atomically</title>
      <description>This has come up on the list (http://markmail.org/message/2wmpvksuwc5t57pg) – it would be good if we can allow updating several doc-values fields, atomically. It will also improve/simplify our tests, where today we index two fields, e.g. the field itself and a control field. In some multi-threaded tests, since we cannot be sure which updates came through first, we limit the test such that each thread updates a different set of fields, otherwise they will collide and it will be hard to verify the index in the end. I was working on a patch and it looks pretty simple to do, will post a patch shortly.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5693</id>
      <title>don't write deleted documents on flush</title>
      <description>When we flush a new segment, sometimes some documents are "born deleted", e.g. if the app did a IW.deleteDocuments that matched some not-yet-flushed documents. We already compute the liveDocs on flush, but then we continue (wastefully) to send those known-deleted documents to all Codec parts. I started to implement this on LUCENE-5675 but it was too controversial. Also, I expect typically the number of deleted docs is 0, or small, so not writing "born deleted" docs won't be much of a win for most apps. Still it seems silly to write them, consuming IO/CPU in the process, only to consume more IO/CPU later for merging to re-delete them.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5703</id>
      <title>Don't allocate/copy bytes all the time in binary DV producers</title>
      <description>Our binary doc values producers keep on creating new byte[] arrays and copying bytes when a value is requested, which likely doesn't help performance. This has been done because of the way fieldcache consumers used the API, but we should try to fix it in 5.0.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5714</id>
      <title>Improve tests for BBoxStrategy then port to 4x.</title>
      <description>BBoxStrategy needs better tests before I'm comfortable seeing it in 4x. Specifically it should use random rectangles based validation (ones that may cross the dateline), akin to the other tests. And I think I see an equals/hashcode bug to be fixed in there too. One particular thing I'd like to see added is how to handle a zero-area case for AreaSimilarity. I think an additional feature in which you declare a minimum % area (relative to the query shape) would be good. It should be possible for the user to combine rectangle center-point to query shape center-point distance sorting as well. I think it is but I need to make sure it's possible without having to index a separate center point field. Another possibility (probably not to be addressed here) is a minimum ratio between width/height, perhaps 10%. A long but nearly no height line should not be massively disadvantaged relevancy-wise to an equivalently long diagonal road that has a square bbox.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5735</id>
      <title>Faceting for DateRangePrefixTree</title>
      <description>The newly added DateRangePrefixTree (DRPT) encodes terms in a fashion amenable to faceting by meaningful time buckets. The motivation for this feature is to efficiently populate a calendar bar chart or heat-map. It's not hard if you have date instances like many do but it's challenging for date ranges. Internally this is going to iterate over the terms using seek/next with TermsEnum as appropriate. It should be quite efficient; it won't need any special caches. I should be able to re-use SPT traversal code in AbstractVisitingPrefixTreeFilter. If this goes especially well; the underlying implementation will be re-usable for geospatial heat-map faceting.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5752</id>
      <title>Explore light weight Automaton replacement</title>
      <description>This effort started with the patch on LUCENE-4556, to create a "light weight" replacement for the current object-heavy Automaton class (which creates separate State and Transition objects). I took that initial patch much further, and cutover most places in Lucene that use Automaton to LightAutomaton. Tests pass. The core idea of LightAutomaton is all states are ints, and you build up the automaton under the restriction that you add all outgoing transitions one state at a time. This worked well for most operations, but for some (e.g. UTF32ToUTF8!!) it was harder, so I also added a separate builder to add transitions in any order and then in the end they are sorted and added to the real automaton. If this is successful I think we should just replace the current Automaton with LightAutomaton; right now they both exist in my current patch... This is very much a work in progress, and I'm not sure the restrictions the API imposes are "reasonable" (some algos got uglier). But I think it's at least worth exploring/iterating... I'll make a branch and commit my current state.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5803</id>
      <title>Add another AnalyzerWrapper class that does not have its own cache, so delegate-only wrappers don't create thread local resources several times</title>
      <description>This is a followup issue for the following Elasticsearch issue: https://github.com/elasticsearch/elasticsearch/pull/6714 Basically the problem is the following: Elasticsearch has a pool of Analyzers that are used for analysis in several indexes Each index uses a different PerFieldAnalyzerWrapper PerFieldAnalyzerWrapper uses PER_FIELD_REUSE_STRATEGY. Because of this it caches the tokenstreams for every field. If there are many fields, this are a lot. In addition, the underlying analyzers may also cache tokenstreams and other PerFieldAnalyzerWrappers do the same, although the delegate Analyzer can always return the same components. We should add similar code to Elasticsearch's directly to Lucene: If the delegating Analyzer just delegates per Field or just wraps CharFilters around the Reader, there is no need to cache the TokenStreamComponents a second time in the delegating Analyzers. This is only needed, if the delegating Analyzers adds additional TokenFilters (like ShingleAnalyzerWrapper). We should name this new class DelegatingAnalyzerWrapper extends AnalyzerWrapper. The wrapComponents method must be final, because we are not allowed to add additional TokenFilters, but unlike ES, we don't need to disallow wrapping with CharFilters. Internally this class uses a private ReuseStrategy that just delegates to the underlying analyzer. It does not matter here if the strategy of the delegate is global or per field, this is private to the delegate.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5868</id>
      <title>JoinUtil support for NUMERIC docValues fields</title>
      <description>while polishing SOLR-6234 I found that JoinUtil can't join int dv fields at least. I plan to provide test/patch. It might be important, because Solr's join can do that. Please vote if you care!</description>
      <attachments/>
    </issue>
    <issue>
      <id>5879</id>
      <title>Add auto-prefix terms to block tree terms dict</title>
      <description>This cool idea to generalize numeric/trie fields came from Adrien: Today, when we index a numeric field (LongField, etc.) we pre-compute (via NumericTokenStream) outside of indexer/codec which prefix terms should be indexed. But this can be inefficient: you set a static precisionStep, and always add those prefix terms regardless of how the terms in the field are actually distributed. Yet typically in real world applications the terms have a non-random distribution. So, it should be better if instead the terms dict decides where it makes sense to insert prefix terms, based on how dense the terms are in each region of term space. This way we can speed up query time for both term (e.g. infix suggester) and numeric ranges, and it should let us use less index space and get faster range queries. This would also mean that min/maxTerm for a numeric field would now be correct, vs today where the externally computed prefix terms are placed after the full precision terms, causing hairy code like NumericUtils.getMaxInt/Long. So optos like LUCENE-5860 become feasible. The terms dict can also do tricks not possible if you must live on top of its APIs, e.g. to handle the adversary/over-constrained case when a given prefix has too many terms following it but finer prefixes have too few (what block tree calls "floor term blocks").</description>
      <attachments/>
    </issue>
    <issue>
      <id>5889</id>
      <title>AnalyzingInfixSuggester should expose commit()</title>
      <description>There is no way short of close() for a user of AnalyzingInfixSuggester to cause it to commit() its underlying index: only refresh() is provided. But callers might want to ensure the index is flushed to disk without closing.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5909</id>
      <title>Run smoketester on Java 8</title>
      <description>In the past, when we were on Java 6, we ran the Smoketester on Java 6 and Java 7. As Java 8 is now officially released and supported, smoketester should now use and require JAVA8_HOME. For the nightly-smoke tests I have to install the openjdk8 FreeBSD package, but that should not be a problem.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5911</id>
      <title>Make MemoryIndex thread-safe for queries</title>
      <description>We want to be able to run multiple queries at once over a MemoryIndex in luwak (see https://github.com/flaxsearch/luwak/commit/49a8fba5764020c2f0e4dc29d80d93abb0231191), but this isn't possible with the current implementation. However, looking at the code, it seems that it would be relatively simple to make MemoryIndex thread-safe for reads/queries.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5914</id>
      <title>More options for stored fields compression</title>
      <description>Since we added codec-level compression in Lucene 4.1 I think I got about the same amount of users complaining that compression was too aggressive and that compression was too light. I think it is due to the fact that we have users that are doing very different things with Lucene. For example if you have a small index that fits in the filesystem cache (or is close to), then you might never pay for actual disk seeks and in such a case the fact that the current stored fields format needs to over-decompress data can sensibly slow search down on cheap queries. On the other hand, it is more and more common to use Lucene for things like log analytics, and in that case you have huge amounts of data for which you don't care much about stored fields performance. However it is very frustrating to notice that the data that you store takes several times less space when you gzip it compared to your index although Lucene claims to compress stored fields. For that reason, I think it would be nice to have some kind of options that would allow to trade speed for compression in the default codec.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5925</id>
      <title>Use rename instead of segments_N fallback / segments.gen etc</title>
      <description>Our commit logic is strange, we write corrupted commit points and only on the last phase of commit do we "correct them". This means the logic to get the latest commit is always scary and awkward, since it must deal with partial commits, and try to determine if it should fall back to segments_N-1 or actually relay an exception. This logic is incomplete/sheisty as we, e.g. i think we only fall back so far (at most one). If we somehow screw up in all this logic do the wrong thing, then we lose data (e.g. LUCENE-4870 wiped entire index because of TooManyOpenFiles). We now require java 7, i think we should expore instead writing pending_segments_N and then in finishCommit() doing an atomic rename to segments_N. We could then remove all the complex fallback logic completely, since we no longer have to deal with "ignoring partial commits", instead simply delivering any exception we get when trying to read the commit and sleep better at night. In java 7, we have the apis for this (ATOMIC_MOVE).</description>
      <attachments/>
    </issue>
    <issue>
      <id>5930</id>
      <title>IntelliJ config: drop resource-only modules, add module groups, and add module for lucene/backward-codecs</title>
      <description>The number of intellij modules is getting out of hand. Intellij supports marking subdirectories within a module as source/resources/tests/test-resources. I think we should consolidate these modules so we have just one per lucene module. Is there some reason I'm missing that this was not done in the first place?</description>
      <attachments/>
    </issue>
    <issue>
      <id>5938</id>
      <title>New DocIdSet implementation with random write access</title>
      <description>We have a great cost API that is supposed to help make decisions about how to best execute queries. However, due to the fact that several of our filter implementations (eg. TermsFilter and BooleanFilter) return FixedBitSets, either we use the cost API and make bad decisions, or need to fall back to heuristics which are not as good such as RandomAccessFilterStrategy.useRandomAccess which decides that random access should be used if the first doc in the set is less than 100. On the other hand, we also have some nice compressed and cacheable DocIdSet implementation but we cannot make use of them because TermsFilter requires a DocIdSet that has random write access, and FixedBitSet is the only DocIdSet that we have that supports random access. I think it would be nice to replace FixedBitSet in those filters with another DocIdSet that would also support random write access but would have a better cost?</description>
      <attachments/>
    </issue>
    <issue>
      <id>5941</id>
      <title>IndexWriter.forceMerge documentation error</title>
      <description>IndexWriter.forceMerge documents that it requires up to 3X FREE space in order to run successfully. We even go further with it and test it in TestIWForceMerge.testForceMergeTempSpaceUsage(). But I think that's wrong. I cannot think of a situation where we consume 3X additional space during merge: 1X - that's the source segments to be merged 2X - that's the result non-CFS merged segment 3X - that's the CFS creation At no point do we publish the non-CFS merged segment, therefore the merge, as I understand it, only consumes up to 2X additional space during that merge. And anyway, we only require 2X of additional space of the largest merge (or total batch of running merges, depends on your MergeScheduler), not the whole index size. This is an important observation, since if you e.g. have a 500GB index, users shouldn't think they need to reserve an additional 1TB for merging, since most of their big segments won't be merged by default anyway (TieredMP defaults to 5GB largest segment). I'll post a patch which fixes the documentation and the test. If anyone can think of a scenario where we consume up to 3X additional space, please chime, and I'll only modify IW.forceMerge documentation to explain that.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5944</id>
      <title>move trunk to 6.x, create branch_5x</title>
      <description>In order to actually add real features (as opposed to just spending 24/7 fixing bugs and back compat), I need a trunk that doesn't have the back compat handcuffs. In the meantime, we should rename the current trunk (which is totally tied down in back compat already, without even a single release!) to branch_5x while you guys (i wont be doing any back compat anymore) figure out what you want to do with the back compat policly. Here is the proposal what to do in this issue: http://mail-archives.apache.org/mod_mbox/lucene-dev/201409.mbox/%3CCAOdYfZUpAbYp-omdw=ngJSdzBKVHn2ZYdoBZvj1gDxK+LRT1SQ@mail.gmail.com%3E</description>
      <attachments/>
    </issue>
    <issue>
      <id>5951</id>
      <title>Detect when index is on SSD and set dynamic defaults</title>
      <description>E.g. ConcurrentMergeScheduler should default maxMergeThreads to 3 if it's on SSD and 1 if it's on spinning disks. I think the new NIO2 APIs can let us figure out which device we are mounted on, and from there maybe we can do os-specific stuff e.g. look at /sys/block/dev/queue/rotational to see if it's spinning storage or not ...</description>
      <attachments/>
    </issue>
    <issue>
      <id>5953</id>
      <title>Refactor LockFactory usage in Directory</title>
      <description>We should remove the setters for the LockFactory from Directory and make the field final. It is a bug to change the LockFactory after creating a directory, because you may break locking (if locks are currently held). The LockFactory should be passed on ctor only. The other suggestion: Should LockFactory have a directory at all? We moved away from having the lock separately from the index directory. This is no longer a supported configuration (since approx Lucene 2.9 or 3.0). I would like to remove the directory from LockFactory and make it part of the Directory only.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5969</id>
      <title>Add Lucene50Codec</title>
      <description>Spinoff from LUCENE-5952: Fix .si to write Version as 3 ints, not a String that requires parsing at read time. Lucene42TermVectorsFormat should not use the same codecName as Lucene41StoredFieldsFormat It would also be nice if we had a "bumpCodecVersion" script so rolling a new codec is not so daunting.</description>
      <attachments/>
    </issue>
    <issue>
      <id>5989</id>
      <title>Allow StringField to take BytesRef value, to index a single binary token</title>
      <description>5 years ago (LUCENE-1458) we "enabled" fully binary terms in the lowest levels of Lucene (the codec APIs) yet today, actually adding an arbitrary byte[] binary term during indexing is far from simple: you must make a custom Field with a custom TokenStream and a custom TermToBytesRefAttribute, as far as I know. This is supremely expert, I wonder if anyone out there has succeeded in doing so? I think we should make indexing a single byte[] as simple as indexing a single String. This is a pre-cursor for issues like LUCENE-5596 (encoding IPv6 address as byte[16]) and LUCENE-5879 (encoding native numeric values in their simple binary form).</description>
      <attachments/>
    </issue>
    <issue>
      <id>5992</id>
      <title>Version should not be encoded as a String in the index</title>
      <description>The version is really "just" 3 (maybe 4) ints under-the-hood, but today we write it as a String which then requires spooky string tokenization/parsing when we open the index. I think it should be encoded directly as ints. In LUCENE-5952 I had tried to make this change, but it was controversial, and got booted. Then in LUCENE-5969, I tried again, but that issue has morphed (nicely!) into fixing all sorts of things except these three ints. Maybe 3rd time's a charm</description>
      <attachments/>
    </issue>
    <issue>
      <id>6006</id>
      <title>Replace FieldInfo.normsType with FieldInfo.hasNorms boolean</title>
      <description>I came across this precursor while working on LUCENE-6005: I think FieldInfo.normsType can only be null (field did not index norms) or DocValuesType.NUMERIC (it did). I'd like to simplify to just boolean hasNorms. This is a strange boolean, though: in theory it should be derived from indexed &amp;&amp; omitNorms == false, but we have it for the exceptions case where every document in a segment hit an exception and never added norms. I think this is the only reason it exists? (In theory, such cases should result in 100% deleted segments, which IW should then drop ... but seems dangerous to "rely" on that). So I changed the indexing chain to just fill in the default (0) norms for all documents in such exceptional cases; this way going forward (starting with 5.0 indices) we really don't need this hasNorms. But we still need it for pre-5.0 indices...</description>
      <attachments/>
    </issue>
    <issue>
      <id>6025</id>
      <title>Add BitSet.prevSetBit</title>
      <description>This would allow the join module to work with any BitSet as opposed to only FixedBitSet.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6031</id>
      <title>TokenSources optimization, avoid sort</title>
      <description>TokenSources.java, in the highlight module, is a facade that returns a TokenStream for a field by either un-inverting &amp; converting the TermVector Terms, or by text re-analysis if TermVectors are unavailable or don't have the right options. TokenSources is used by the default highlighter, which is the most accurate highlighter we've got. When documents are large (say hundreds of kilobytes on up), I found that most of the highlighter's activity was up-front spent un-inverting &amp; converting the term vector to a TokenStream, not on the actual/real highlighting that follows. Much of that time was on a huge sort of hundreds of thousands of Tokens. Time was also spent doing lots of String conversion and char copying, and it used a lot of memory, too. In this patch, I overhauled TokenStreamFromTermPositionVector.java, and I removed similar logic in TokenSources that was used in circumstances when positions weren't available but offsets were. This class can un-invert term vectors that have positions and/or offsets (at least one). It doesn't sort. It places Tokens directly into an array of tokens directly indexed by position. When positions aren't available, the startOffset/8 is a substitute. I've got a more light-weight Token inner class used in place of the former and deprecated Token that ultimately forms a linked-list when the process is done. There is no string conversion; character copying is minimized. The Token array is GC'ed after initialization, it's only needed during construction. Misc: It implements reset() efficiently so it need not be wrapped in CachingTokenFilter (I'll supply a patch later on this). It only fetches payloads if you ask for them by adding the attribute (the default highlighter won't add the attribute). It exposes the underlying TermVector terms via a getter too, which is needed by another patch to follow later. A key assumption is that the position increment gap or first position isn't gigantic, as that will create wasted space and the linked-list formation ultimately has to visit all the slots. We also assume that there aren't a ton of tokens at the same position, since inserting new tokens in sorted order is O(N^2) where 'N' is the average co-occurring token length. My performance testing using Lucene's benchmark module on a megabyte document showed &gt;5x speedup, in conjunction with some other patches to be posted separately. This patch made the most difference.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6034</id>
      <title>MemoryIndex should be able to wrap TermVector Terms</title>
      <description>The default highlighter has a "WeightedSpanTermExtractor" that uses MemoryIndex for certain queries – basically phrases, SpanQueries, and the like. For lots of text, this aspect of highlighting is time consuming and consumes a fair amount of memory. What also consumes memory is that it wraps the tokenStream in CachingTokenFilter in this case. But if the underlying TokenStream is actually from TokenSources (wrapping TermVector Terms), this is all needless! Furthermore, MemoryIndex doesn't support payloads. The patch here has 3 aspects to it: Internal refactoring to MemoryIndex to simplify it by maintaining the fields in a sorted state using a TreeMap. The ramifications of this led to reduced LOC for this file, even with the other features I added. It also puts the FieldInfo on the Info, and thus there's one less data structure to keep around. I suppose if there are a huge variety of fields in MemoryIndex, the aggregated N*Log(N) field lookup could add up, but that seems very unlikely. I also brought in the MemoryIndexNormDocValues as a simple anonymous inner class - it's super-simple after all, not worth having in a separate file. New MemoryIndex.addField(String fieldName, Terms) method. In this case, MemoryIndex is providing the supporting wrappers around the underlying Terms so that it appears as an Index. In so doing, MemoryIndex supports payloads for such fields. WeightedSpanTermExtractor now detects TokenSources' wrapping of Terms and it supplies this to MemoryIndex.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6039</id>
      <title>Add IndexOptions.NO and DocValuesType.NO, instead of null</title>
      <description>Idea from Simon: it seems dangerous for IndexOptions and DocValuesType via Indexable/FieldType and FieldInfo that we use null to mean it's not indexed or has no doc values. We should instead have an explicit choice (IndexOptions.NO, DocValuesType.NO) in the enum?</description>
      <attachments/>
    </issue>
    <issue>
      <id>6057</id>
      <title>Clarify the Sort(SortField...) constructor)</title>
      <description>I don't really know which version this affects, but I clarified the documentation of the Sort(SortField...) constructor to ease the understanding for new users. Pull Request: https://github.com/apache/lucene-solr/pull/20</description>
      <attachments/>
    </issue>
    <issue>
      <id>6066</id>
      <title>Collector that manages diversity in search results</title>
      <description>This issue provides a new collector for situations where a client doesn't want more than N matches for any given key (e.g. no more than 5 products from any one retailer in a marketplace). In these circumstances a document that was previously thought of as competitive during collection has to be removed from the final PQ and replaced with another doc (eg a retailer who already has 5 matches in the PQ receives a 6th match which is better than his previous ones). This requires a new remove method on the existing PriorityQueue class.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6083</id>
      <title>Span containing/contained queries</title>
      <description>SpanContainingQuery reducing a spans to where it is containing another spans. SpanContainedQuery reducing a spans to where it is contained in another spans.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6119</id>
      <title>Add auto-io-throttle to ConcurrentMergeScheduler</title>
      <description>This method returns number of "incoming" bytes IW has written since it was opened, excluding merging. It tracks flushed segments, new commits (segments_N), incoming files/segments by addIndexes, newly written live docs / doc values updates files. It's an easy statistic for IW to track and should be useful to help applications more intelligently set defaults for IO throttling (RateLimiter). For example, an application that does hardly any indexing but finally triggered a large merge can afford to heavily throttle that large merge so it won't interfere with ongoing searches. But an application that's causing IW to write new bytes at 50 MB/sec must set a correspondingly higher IO throttling otherwise merges will clearly fall behind.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6121</id>
      <title>Fix CachingTokenFilter to propagate reset() the first time</title>
      <description>CachingTokenFilter should have been propagating reset() but only the first time and thus you would then use CachingTokenFilter in a more normal way – wrap it and call reset() then increment in a loop, etc., instead of knowing you need to reset() on what it wraps but not this token filter itself. That's weird. It's ab-normal for a TokenFilter to never propagate reset, so every user of CachingTokenFilter to date has worked around this by calling reset() on the underlying input instead of the final wrapping token filter (CachingTokenFilter in this case).</description>
      <attachments/>
    </issue>
    <issue>
      <id>6149</id>
      <title>Infix suggesters' highlighting, allTermsRequired options are hardwired and not configurable for non-contextual lookup</title>
      <description>Highlighting and allTermsRequired are hardwired in AnalyzingInfixSuggester for non-contextual lookup (via Lookup) see true, true below: AnalyzingInfixSuggester.java (extends Lookup.java) public List&lt;LookupResult&gt; lookup(CharSequence key, Set&lt;BytesRef&gt; contexts, boolean onlyMorePopular, int num) throws IOException { return lookup(key, contexts, num, true, true); } /** Lookup, without any context. */ public List&lt;LookupResult&gt; lookup(CharSequence key, int num, boolean allTermsRequired, boolean doHighlight) throws IOException { return lookup(key, null, num, allTermsRequired, doHighlight); } Lookup.java public List&lt;LookupResult&gt; lookup(CharSequence key, boolean onlyMorePopular, int num) throws IOException { return lookup(key, null, onlyMorePopular, num); } The above means the majority of the current infix suggester lookup always return highlighted results with allTermsRequired in effect. There is no way to change this despite the options and improvement of LUCENE-6050, made to incorporate Boolean lookup clauses (MUST/SHOULD). This shortcoming has also been reported in SOLR-6648. The suggesters (AnalyzingInfixSuggester, BlendedInfixSuggester) should provide a proper mechanism to set defaults for highlighting and "allTermsRequired", e.g. in constructors (and in Solr factories, thus configurable via solrconfig.xml).</description>
      <attachments/>
    </issue>
    <issue>
      <id>6179</id>
      <title>Remove out-of-order scoring</title>
      <description>Out-of-order currently adds complexity that I would like to remove. Here is a selection of issues that come from out-of-order scoring. lots of specializations with collectors: we have two versions of every top score/field collector depending on whether it should support out-of-order collection or not it feels like it should be an implementation detail of our bulk scorers but it also makes our APIs more complicated, eg. LeafCollector.acceptsDocsOutOfOrder if you create a TopFieldCollector, how do you know if you should pass docsScoredInOrder=true or false? To make the decision, you actually need to know whether your query supports out-of-order scoring while the API is on Weight. I initially wanted to keep it and improve the decision process in LUCENE-6172 but I'm not sure it's the right approach as it would require to make the API even more complicated... hence the suggestion to remove out-of-order scoring completely.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6188</id>
      <title>Remove HTML verification from checkJavaDocs.py</title>
      <description>Currently, the broken HTML verification in checkJavaDocs.py has issues in some cases (see SOLR-6902). On looking further to fix it with the html.parser package instead, noticed that there is broken HTML verification already present (using html.parser!)in checkJavadocLinks.py anyway which takes care of validation, and probably jTidy does it as well, going by the output (haven't verified it). Given this, the validation in checkJavaDocs.py doesn't seem to add any further value, so here's a patch to just nuke it instead.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6191</id>
      <title>Spatial 2D faceting (heatmaps)</title>
      <description>Lucene spatial's PrefixTree (grid) based strategies index data in a way highly amenable to faceting on grids cells to compute a so-called heatmap. The underlying code in this patch uses the PrefixTreeFacetCounter utility class which was recently refactored out of faceting for NumberRangePrefixTree LUCENE-5735. At a low level, the terms (== grid cells) are navigated per-segment, forward only with TermsEnum.seek, so it's pretty quick and furthermore requires no extra caches &amp; no docvalues. Ideally you should use QuadPrefixTree (or Flex once it comes out) to maximize the number grid levels which in turn maximizes the fidelity of choices when you ask for a grid covering a region. Conveniently, the provided capability returns the data in a 2-D grid of counts, so the caller needn't know a thing about how the data is encoded in the prefix tree. Well almost... at this point they need to provide a grid level, but I'll soon provide a means of deriving the grid level based on a min/max cell count. I recommend QuadPrefixTree with geo=false so that you can provide a square world-bounds (360x360 degrees), which means square grid cells which are more desirable to display than rectangular cells.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6196</id>
      <title>Include geo3d package, along with Lucene integration to make it useful</title>
      <description>I would like to explore contributing a geo3d package to Lucene. This can be used in conjunction with Lucene search, both for generating geohashes (via spatial4j) for complex geographic shapes, as well as limiting results resulting from those queries to those results within the exact shape in highly performant ways. The package uses 3d planar geometry to do its magic, which basically limits computation necessary to determine membership (once a shape has been initialized, of course) to only multiplications and additions, which makes it feasible to construct a performant BoostSource-based filter for geographic shapes. The math is somewhat more involved when generating geohashes, but is still more than fast enough to do a good job.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6198</id>
      <title>two phase intersection</title>
      <description>Currently some scorers have to do a lot of per-document work to determine if a document is a match. The simplest example is a phrase scorer, but there are others (spans, sloppy phrase, geospatial, etc). Imagine a conjunction with two MUST clauses, one that is a term that matches all odd documents, another that is a phrase matching all even documents. Today this conjunction will be very expensive, because the zig-zag intersection is reading a ton of useless positions. The same problem happens with filteredQuery and anything else that acts like a conjunction.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6199</id>
      <title>Reduce per-field heap usage for indexed fields</title>
      <description>Lucene uses a non-trivial baseline bytes of heap for each indexed field, and I know it's abusive for an app to create 100K indexed fields but I still think we can and should make some effort to reduce heap usage per unique field? E.g. in block tree we store 3 BytesRefs per field, when 3 byte[]s would do...</description>
      <attachments/>
    </issue>
    <issue>
      <id>6212</id>
      <title>Remove IndexWriter's per-document analyzer add/updateDocument APIs</title>
      <description>IndexWriter already takes an analyzer up-front (via IndexWriterConfig), but it also allows you to specify a different one for each add/updateDocument. I think this is quite dangerous/trappy since it means you can easily index tokens for that document that don't match at search-time based on the search-time analyzer. I think we should remove this trap in 5.0.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6220</id>
      <title>Move needsScores from Weight.scorer to Query.createWeight</title>
      <description>Whether scores are needed is currently a Scorer-level property while it should actually be a Weight thing I think?</description>
      <attachments/>
    </issue>
    <issue>
      <id>6226</id>
      <title>Add interval iterators to Scorer</title>
      <description>This change will allow Scorers to expose which positions within a document they have matched, via a new IntervalIterator interface. Consumers get the iterator by calling intervals() on the Scorer, then call reset(docId) whenever the scorer has advanced and nextInterval() to iterate through positions. Once all matching intervals on the current document have been exhausted, nextInterval() returns false.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6227</id>
      <title>Add BooleanClause.Occur.FILTER</title>
      <description>Now that we have weight-level control of whether scoring is needed or not, we could add a new clause type to BooleanQuery. It would behave like MUST exept that it would not participate in scoring. Why do we need it given that we already have FilteredQuery? The idea is that by having a single query that performs conjunctions, we could potentially take better decisions. It's not ready to replace FilteredQuery yet as FilteredQuery has handling of random-access filters that BooleanQuery doesn't, but it's a first step towards that direction and eventually FilteredQuery would just rewrite to a BooleanQuery. I've been calling this new clause type FILTER so far, but feel free to propose a better name.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6229</id>
      <title>Remove Scorer.getChildren?</title>
      <description>This API is used in a single place in our code base: ToParentBlockJoinCollector. In addition, the usage is a bit buggy given that using this API from a collector only works if setScorer is called with an actual Scorer (and not eg. FakeScorer or BooleanScorer like you would get in disjunctions) so it needs a custom IndexSearcher that does not use the BulkScorer API.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6257</id>
      <title>Remove javadocs from releases (except for publishing)</title>
      <description>In LUCENE-6247, one idea discussed to decrease the size of release artifacts was to remove javadocs from the binary release. Anyone needing javadocs offline can download the source distribution and generate the javadocs. I also think we should investigate removing javadocs jars from maven. I did a quick test, and getting the source in intellij seemed sufficient to show javadocs. However, this test was far from scientific, so if someone knows for sure whether a separate javadocs jar is truly necessary, please say so. Regardless of the outcome of the two ideas above, we would continue building, validating and making the javadocs available online.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6259</id>
      <title>Remove dependencies from binary releases</title>
      <description>In LUCENE-6247, one idea discussed to decrease the size of release artifacts was to remove the inclusion of dependencies from binary releases. These jar files increase the size of the binary releases a lot, and the size is mostly in a couple modules (eg benchmark and spatial). I think most people consume lucene through maven. For those that do use the binary release, we can still make pulling the dependencies for these modules easy. We can add a generated README file in each module that has dependencies, with instructions indicating they need to download the dependencies, and then give the list of jar files they need to download, with exact links to maven (which we can extract from ivy?).</description>
      <attachments/>
    </issue>
    <issue>
      <id>6272</id>
      <title>Scorer should not extend PostingsEnum</title>
      <description>Scorer currently has to implement a whole bunch of methods that are never called. The only method that Scorer uses in addition to the methods on DocIdSetIterator is freq(), and as currently implemented this means different things on different Scorers: TermScorer returns its underlying termfreq MinShouldMatchScorer returns how many of its subscorers are matching {Exact|Sloppy} PhraseScorer returns how many phrases it has found on a document In addition, freq() is never actually called on TermScorer, and it's only used in explain() on the phrase scorers. We should make Scorer extend DocIdSetIterator instead. In place of freq(), Scorer would have a coord() method that by default returns 1, and for boolean scorers returns how many subscorers are matching.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6276</id>
      <title>Add matchCost() api to TwoPhaseDocIdSetIterator</title>
      <description>We could add a method like TwoPhaseDISI.matchCost() defined as something like estimate of nanoseconds or similar. ConjunctionScorer could use this method to sort its 'twoPhaseIterators' array so that cheaper ones are called first. Today it has no idea if one scorer is a simple phrase scorer on a short field vs another that might do some geo calculation or more expensive stuff. PhraseScorers could implement this based on index statistics (e.g. totalTermFreq/maxDoc)</description>
      <attachments/>
    </issue>
    <issue>
      <id>6277</id>
      <title>Allow Ivy resolutionCacheDir to be overriden by system property.</title>
      <description>This makes it simpler to run tests in parallel (https://gist.github.com/markrmiller/dbdb792216dc98b018ad) without making any tweaks.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6296</id>
      <title>BBoxFieldType should not store values for subfields (by default)</title>
      <description>When the bbox field creates the subfields, it uses the schema for 'double' and 'boolean' types. As is, we can specify these field types as indexed, not stored – but that is a bit trappy. Lets add a property to the field definition: storeSubFields="false" and register the subfields appropriatly</description>
      <attachments/>
    </issue>
    <issue>
      <id>6304</id>
      <title>Add MatchNoDocsQuery that matches no documents</title>
      <description>As a followup to LUCENE-6298, it would be nice to have an explicit MatchNoDocsQuery to indicate that no documents should be matched. This would hopefully be a better indicator than a BooleanQuery with no clauses or (even worse) null.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6339</id>
      <title>[suggest] Near real time Document Suggester</title>
      <description>The idea is to index documents with one or more SuggestField(s) and be able to suggest documents with a SuggestField value that matches a given key. A SuggestField can be assigned a numeric weight to be used to score the suggestion at query time. Document suggestion can be done on an indexed SuggestField. The document suggester can filter out deleted documents in near real-time. The suggester can filter out documents based on a Filter (note: may change to a non-scoring query?) at query time. A custom postings format (CompletionPostingsFormat) is used to index SuggestField(s) and perform document suggestions. Usage // hook up custom postings format // indexAnalyzer for SuggestField Analyzer analyzer = ... IndexWriterConfig config = new IndexWriterConfig(analyzer); Codec codec = new Lucene50Codec() { PostingsFormat completionPostingsFormat = new Completion50PostingsFormat(); @Override public PostingsFormat getPostingsFormatForField(String field) { if (isSuggestField(field)) { return completionPostingsFormat; } return super.getPostingsFormatForField(field); } }; config.setCodec(codec); IndexWriter writer = new IndexWriter(dir, config); // index some documents with suggestions Document doc = new Document(); doc.add(new SuggestField("suggest_title", "title1", 2)); doc.add(new SuggestField("suggest_name", "name1", 3)); writer.addDocument(doc) ... // open an nrt reader for the directory DirectoryReader reader = DirectoryReader.open(writer, false); // SuggestIndexSearcher is a thin wrapper over IndexSearcher // queryAnalyzer will be used to analyze the query string SuggestIndexSearcher indexSearcher = new SuggestIndexSearcher(reader, queryAnalyzer); // suggest 10 documents for "titl" on "suggest_title" field TopSuggestDocs suggest = indexSearcher.suggest("suggest_title", "titl", 10); Indexing Index analyzer set through IndexWriterConfig SuggestField(String name, String value, long weight) Query Query analyzer set through SuggestIndexSearcher. Hits are collected in descending order of the suggestion's weight // full options for TopSuggestDocs (TopDocs) TopSuggestDocs suggest(String field, CharSequence key, int num, Filter filter) // full options for Collector // note: only collects does not score void suggest(String field, CharSequence key, int num, Filter filter, TopSuggestDocsCollector collector) Analyzer CompletionAnalyzer can be used instead to wrap another analyzer to tune suggest field only parameters. CompletionAnalyzer(Analyzer analyzer, boolean preserveSep, boolean preservePositionIncrements, int maxGraphExpansions)</description>
      <attachments/>
    </issue>
    <issue>
      <id>6341</id>
      <title>add CheckIndex -fast option</title>
      <description>CheckIndex is great for testing and when tracking down lucene bugs. But in cases where users just want to verify their index files are OK, it is very slow and expensive. I think we should add a -fast option, that only opens the reader and calls checkIntegrity(). This means all files are the correct files (identifiers match) and have the correct CRC32 checksums. For our 10M doc wikipedia index, this is the difference between a 2 second check and a 2 minute check.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6360</id>
      <title>TermsQuery should rewrite to a ConstantScoreQuery over a BooleanQuery when there are few terms</title>
      <description>TermsQuery helps when there are lot of terms from which you would like to compute the union, but it is a bit harmful when you have few terms since it cannot really skip: it always consumes all documents matching the underlying terms. It would certainly help to rewrite this query to a ConstantScoreQuery over a BooleanQuery when there are few terms in order to have actual skip support. As usual the hard part is probably to figure out the threshold.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6365</id>
      <title>Optimized iteration of finite strings</title>
      <description>Replaced Operations.getFiniteStrings() by an optimized FiniteStringIterator. Benefits: Avoid huge hash set of finite strings. Avoid massive object/array creation during processing. "Downside": Iteration order changed, so when iterating with a limit, the result may differ slightly. Old: emit current node, if accept / recurse. New: recurse / emit current node, if accept. The old method Operations.getFiniteStrings() still exists, because it eases the tests. It is now implemented by use of the new FiniteStringIterator.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6371</id>
      <title>Improve Spans payload collection</title>
      <description>Spin off from LUCENE-6308, see the comments there from around 23 March 2015.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6373</id>
      <title>Complete two phase doc id iteration support for Spans</title>
      <description>Spin off from LUCENE-6308, see comments there from about 23 March 2015.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6417</id>
      <title>Upgrade ANTLR to version 4.5</title>
      <description>I would like to upgrade ANTLR from 3.5 to 4.5. This version adds several features that will improve the existing grammars. The main improvement would be the allowance of left-hand recursion in grammar rules which will reduce the number of rules significantly for expressions. This change will require some code refactoring to the existing expressions work.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6420</id>
      <title>Update forbiddenapis to 1.8</title>
      <description>Update forbidden-apis plugin to 1.8: Initial support for Java 9 including JIGSAW Errors are now reported sorted by line numbers and correctly grouped (synthetic methods/lambdas) Package-level forbids: Deny all classes from a package: org.hatedpkg.** (also other globs work) In addition to file-level excludes, forbiddenapis now supports fine granular excludes using Java annotations. You can use the one shipped, but define your own, e.g. inside Lucene and pass its name to forbidden (e.g. using a glob: **.SuppressForbidden would any annotation in any package to suppress errors). Annotation need to be on class level, no runtime annotation required. This will for now only update the dependency and remove the additional forbid by Shalin Shekhar Mangar for MessageFormat (which is now shipped with forbidden). But we should review and for example suppress forbidden failures in command line tools using @SuppressForbidden (or similar annotation). The discussion is open, I can make a patch.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6422</id>
      <title>Add PackedQuadPrefixTree</title>
      <description>This task introduces a PackedQuadPrefixTree that includes two things: 1. A packed 8 byte representation for a QuadCell, including more efficient implementations of the SPT API than the existing QuadPrefixTree or GeoHashPrefixTree. 2. An alternative implementation to RPT's "pruneLeafyBranches" that streams the cells without buffering them all, which is way more memory efficient. However pruning is limited to the target detail level, where it accomplishes the most good. Future improvements over this approach may include the generation of the packed cells using an AutoPrefixAutomaton</description>
      <attachments/>
    </issue>
    <issue>
      <id>6450</id>
      <title>Add simple encoded GeoPointField type to core</title>
      <description>At the moment all spatial capabilities, including basic point based indexing and querying, require the lucene-spatial module. The spatial module, designed to handle all things geo, requires dependency overhead (s4j, jts) to provide spatial rigor for even the most simplistic spatial search use-cases (e.g., lat/lon bounding box, point in poly, distance search). This feature trims the overhead by adding a new GeoPointField type to core along with GeoBoundingBoxQuery and GeoPolygonQuery classes to the .search package. This field is intended as a straightforward lightweight type for the most basic geo point use-cases without the overhead. The field uses simple bit twiddling operations (currently morton hashing) to encode lat/lon into a single long term. The queries leverage simple multi-phase filtering that starts by leveraging NumericRangeQuery to reduce candidate terms deferring the more expensive mathematics to the smaller candidate sets.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6458</id>
      <title>MultiTermQuery's FILTER rewrite method should support skipping whenever possible</title>
      <description>Today MultiTermQuery's FILTER rewrite always builds a bit set fom all matching terms. This means that we need to consume the entire postings lists of all matching terms. Instead we should try to execute like regular disjunctions when there are few terms.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6459</id>
      <title>[suggest] Query Interface for suggest API</title>
      <description>This patch factors out common indexing/search API used by the recently introduced NRTSuggester. The motivation is to provide a query interface for FST-based fields (SuggestField and ContextSuggestField) to enable suggestion scoring and more powerful automaton queries. Previously, only prefix ‘queries’ with index-time weights were supported but we can also support: Prefix queries expressed as regular expressions: get suggestions that match multiple prefixes Example: star[wa|tr] matches starwars and startrek Fuzzy Prefix queries supporting scoring: get typo tolerant suggestions scored by how close they are to the query prefix Example: querying for seper will score separate higher then superstitious Context Queries: get suggestions boosted and/or filtered based on their indexed contexts (meta data) Boost example: get typo tolerant suggestions on song names with prefix like a roling boosting songs with genre rock and indie Filter example: get suggestion on all file names starting with finan only for user1 and user2 Suggest API SuggestIndexSearcher searcher = new SuggestIndexSearcher(reader); CompletionQuery query = ... TopSuggestDocs suggest = searcher.suggest(query, num); CompletionQuery CompletionQuery is used to query SuggestField and ContextSuggestField. A CompletionQuery produces a CompletionWeight, which allows CompletionQuery implementations to pass in an automaton that will be intersected with a FST and allows boosting and meta data extraction from the intersected partial paths. A CompletionWeight produces a CompletionScorer. A CompletionScorer executes a Top N search against the FST with the provided automaton, scoring and filtering all matched paths. PrefixCompletionQuery Return documents with values that match the prefix of an analyzed term text Documents are sorted according to their suggest field weight. PrefixCompletionQuery(Analyzer analyzer, Term term) RegexCompletionQuery Return documents with values that match the prefix of a regular expression Documents are sorted according to their suggest field weight. RegexCompletionQuery(Term term) FuzzyCompletionQuery Return documents with values that has prefixes within a specified edit distance of an analyzed term text. Documents are ‘boosted’ by the number of matching prefix letters of the suggestion with respect to the original term text. FuzzyCompletionQuery(Analyzer analyzer, Term term) Scoring suggestion_weight * boost where suggestion_weight and boost are all integers. boost = # of prefix characters matched ContextQuery Return documents that match a CompletionQuery filtered and/or boosted by provided context(s). ContextQuery(CompletionQuery query) contextQuery.addContext(CharSequence context, int boost, boolean exact) NOTE: ContextQuery should be used with ContextSuggestField to query suggestions boosted and/or filtered by contexts. Running ContextQuery against a SuggestField will error out. Scoring suggestion_weight * context_boost where suggestion_weight and context_boost are all integers When used with FuzzyCompletionQuery, suggestion_weight * (context_boost + fuzzy_boost) Context Suggest Field To use ContextQuery, use ContextSuggestField instead of SuggestField. Any CompletionQuery can be used with ContextSuggestField, the default behaviour is to return suggestions from all contexts. Context for every completion hit can be accessed through SuggestScoreDoc#context. ContextSuggestField(String name, Collection&lt;CharSequence&gt; contexts, String value, int weight)</description>
      <attachments/>
    </issue>
    <issue>
      <id>6464</id>
      <title>Allow possibility to group contexts in AnalyzingInfixSuggester.loockup()</title>
      <description>This is an enhancement to LUCENE-6050 LUCENE-6050 added lookup(CharSequence key, Map&lt;BytesRef, BooleanClause.Occur&gt; contextInfo, int num, boolean allTermsRequired, boolean doHighlight) which allowed to do something like (A OR B AND C OR D ...) In our use-case, we realise that we need grouping i.e (A OR B) AND (C OR D) AND (...) In other words, we need the intersection of multiple contexts. The attached patch allows to pass in a varargs of map, each one representing the each group. Looks a bit heavy IMHO. This is an initial patch. The question to Michael McCandless and jane chang is: is it better to expose a FilteredQuery/Query and let the user build their own query instead of passing a map? Exposing a filteredQuery will probably give the best flexibility to the end-users.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6466</id>
      <title>Move SpanQuery.getSpans() to SpanWeight</title>
      <description>SpanQuery.getSpans() should only be called on rewritten queries, so it seems to make more sense to have this being called from SpanWeight</description>
      <attachments/>
    </issue>
    <issue>
      <id>6477</id>
      <title>Add BKD tree for spatial shape query intersecting indexed points</title>
      <description>I'd like to explore using dedicated spatial trees for faster shape intersection filters than postings-based implementations. I implemented the tree data structure from https://www.cs.duke.edu/~pankaj/publications/papers/bkd-sstd.pdf The idea is simple: it builds a full binary tree, partitioning 2D space, alternately on lat and then lon, into smaller and smaller rectangles until a leaf has &lt;= N (default 1024) points. It cannot index shapes (just points), and can then do fast shape intersection queries. Multi-valued fields are supported. I only implemented the "point is contained in this bounding box" query for now, but I think polygon shape querying should be easy to implement using the same approach from LUCENE-6450. For indexing, you add BKDPointField (takes lat, lon) to your doc, and must set up your Codec use BKDTreeDocValuesFormat for that field. This DV format wraps Lucene50DVFormat, but then builds the disk-based BKD tree structure on the side. BKDPointInBBoxQuery then requires this DVFormat, and casts it to gain access to the tree. I quantize each incoming double lat/lon to 32 bits precision (so 64 bits per point) = ~9 milli-meter lon precision at the equator, I think.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6480</id>
      <title>Extend Simple GeoPointField Type to 3d</title>
      <description>LUCENE-6450 proposes a simple GeoPointField type to lucene core. This field uses 64bit encoding of 2 dimensional points to construct sorted term representations of GeoPoints (aka: GeoHashing). This feature investigates adding support for encoding 3 dimensional GeoPoints, either by extending GeoPointField to a Geo3DPointField or adding an additional 3d constructor.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6481</id>
      <title>Improve GeoPointField type to only visit high precision boundary terms</title>
      <description>Current GeoPointField LUCENE-6450 computes a set of ranges along the space-filling curve that represent a provided bounding box. This determines which terms to visit in the terms dictionary and which to skip. This is suboptimal for large bounding boxes as we may end up visiting all terms (which could be quite large). This incremental improvement is to improve GeoPointField to only visit high precision terms in boundary ranges and use the postings list for ranges that are completely within the target bounding box. A separate improvement is to switch over to auto-prefix and build an Automaton representing the bounding box. That can be tracked in a separate issue.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6487</id>
      <title>Add WGS84 capability to geo3d support</title>
      <description>WGS84 compatibility has been requested for geo3d. This involves working with an ellipsoid rather than a unit sphere. The general formula for an ellipsoid is: x^2/a^2 + y^2/b^2 + z^2/c^2 = 1</description>
      <attachments/>
    </issue>
    <issue>
      <id>6494</id>
      <title>Make PayloadSpanUtil apply to other postings information</title>
      <description>With the addition of SpanCollectors, we can now get arbitrary postings information from SpanQueries. PayloadSpanUtil does some rewriting to convert non-span queries into SpanQueries so that it can collect payloads. It would be good to make this more generic, so that we can collect any postings information from any query (without having to make invasive changes to already optimized Scorers, etc).</description>
      <attachments/>
    </issue>
    <issue>
      <id>6524</id>
      <title>Create an IndexWriter from an already opened NRT or non-NRT reader</title>
      <description>I'd like to add a new ctor to IndexWriter, letting you start from an already opened NRT or non-NRT DirectoryReader. I think this is a long missing API in Lucene today, and we've talked in the past about different ways to fix it e.g. factoring out a shared reader pool between writer and reader. One use-case, which I hit in LUCENE-5376: if you have a read-only index, so you've opened a non-NRT DirectoryReader to search it, and then you want to "upgrade" to a read/write index, we don't handle that very gracefully now because you are forced to open 2X the SegmentReaders. But with this API, IW populates its reader pool with the incoming SegmentReaders so they are shared on any subsequent NRT reopens / segment merging / deletes applying, etc. Another (more expert) use case is allowing rollback to an NRT-point. Today, you can only rollback to a commit point (segments_N). But an NRT reader also reflects a valid "point in time" view of the index (it just doesn't have a segments_N file, and its ref'd files are not fsync'd), so with this change you can close your old writer, open a new one from this NRT point, and revert all changes that had been done after the NRT reader was opened from the old writer.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6530</id>
      <title>Use Java 7 ProcessBuilder.inheritIO() instead of own ThreadPumper</title>
      <description>In some tests wie spawn separate processes (TestIndexWriterOnJRECrash and Solr's IPTables). To capture stdin/stdout/stderr we spawn several threads that pump those to stdout/stderr. Since Java 7 there is ProcessBuilder.inheritIO() that does this for us without any additional threads. We should use this instead. Fix is easy, just remove some stuff I did the same already for my Codec classloader deadlock test, so this is just a followup for the other tests. Patch is attached and can be committed to trunk and 5.x.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6531</id>
      <title>Make PhraseQuery immutable</title>
      <description>Mutable queries are an issue for automatic filter caching since modifying a query after it has been put into the cache will corrupt the cache. We should make all queries immutable (up to the boost) to avoid this issue.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6544</id>
      <title>Geo3d cleanup: Regularize path and polygon construction, plus consider adding ellipsoid surface distance method</title>
      <description>Geo3d's way of constructing polygons and paths differs in that in one case you construct points and the other you feed lat/lon values directly to the builder. Probably both should be supported for both kinds of entity. Also it may be useful to have an accurate point-point ellipsoidal distance function. This is expensive and would be an addition to the arc distance we currently compute. It would probably be called "surface distance".</description>
      <attachments/>
    </issue>
    <issue>
      <id>6547</id>
      <title>Add dateline crossing support to GeoPointInBBox and GeoPointDistance Queries</title>
      <description>The current GeoPointInBBoxQuery only supports bounding boxes that are within the standard -180:180 longitudinal bounds. While its perfectly fine to require users to split dateline crossing bounding boxes in two, GeoPointDistanceQuery should support distance queries that cross the dateline. Since morton encoding doesn't support unwinding this issue will add dateline crossing to GeoPointInBBoxQuery and GeoPointDistanceQuery classes.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6553</id>
      <title>Simplify how we handle deleted docs in read APIs</title>
      <description>Today, all scorers and postings formats need to be able to handle deleted documents. I suspect that the reason is that we want to be able to make sure to not perform costly operations on documents that are deleted. For instance if you run a phrase query, reading positions on a document which is deleted is useless. I suspect this is also a source of inefficiencies since in some cases we apply deleted documents several times: for instance conjunctions apply deleted docs to every sub scorer. However, with the new two-phase iteration API, we have a way to make sure that we never run expensive operations on deleted documents: we could first iterate over the approximation, then check that the document is not deleted, and finally confirm the match. Since approximations are cheap, applying deleted docs after them would not be an issue. I would like to explore removing the "Bits acceptDocs" parameter from TermsEnum.postings, Weight.scorer, SpanWeight.getSpans and Weight.BulkScorer, and add it to BulkScorer.score. This way, bulk scorers would be the only API which would need to know how to apply deleted docs, which I think would be more manageable since we only have 3 or 4 impls. And DefaultBulkScorer would be implemented the way described above: first advance the approximation, then check deleted docs, then confirm the match, then collect. Of course that's only in the case the scorer supports approximations, if it does not, it means it is cheap so we can directly iterate the scorer and check deleted docs on top.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6575</id>
      <title>Improve API of PhraseQuery.Builder</title>
      <description>From LUCENE-6531 In current PhraseQuery.Builder API. User must retype field again and again : PhraseQuery.Builder builder = new PhraseQuery.Builder(); builder.add(new Term("lyrics", "when"), 1); builder.add(new Term("lyrics", "believe"), 3); Cleaner API : PhraseQuery.Builder builder = new PhraseQuery.Builder("lyrics"); builder.add("when", 1); builder.add("believe", 3);</description>
      <attachments/>
    </issue>
    <issue>
      <id>6585</id>
      <title>Make ConjunctionDISI flatten sub ConjunctionDISI instances</title>
      <description>Today ConjunctionDISI wraps some sub (two-phase) iterators. I would like to improve it by flattening sub iterators when they implement ConjunctionDISI. In practice, this would make "+A +(+B +C)" be executed more like "+A +B +C" (only in terms of matching, scoring would not change). My motivation for this is that if we don't flatten and are unlucky, we can sometimes hit some worst cases. For instance consider the 3 following postings lists (sorted by increasing cost): A: 1, 1001, 2001, 3001, ... C: 0, 2, 4, 6, 8, 10, 12, 14, ... B: 1, 3, 5, 7, 9, 11, 13, 15, ... If we run "+A +B +C", then everything works fine, we use A as a lead, and advance B 1000 by 1000 to find the next match (if any). However if we run "+A +(+B +C)", then we would iterate B and C 2 by 2 over the entire doc ID space when trying to find the first match which occurs on or after A:1. This is an extreme example which is unlikely to happen in practice, but flattening would also help a bit on some more common cases. For instance imagine that A, B and C have respective costs of 100, 10 and 1000. If you search for "+A +(+B +C)", then we will use the most costly iterator (C) to confirm matches of B (the least costly iterator, used as a lead) while it would have been more efficient to confirm matches of B with A first, since A is less costly than C.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6607</id>
      <title>Move geo3d to Lucene's sandbox module</title>
      <description>Geo3d is a powerful low-level geo API, recording places on the earth's surface in the index in three dimensions (as 3 separate numbers) and offering fast shape intersection/distance testing at search time. Karl Wright originally contributed this in LUCENE-6196, and we put it in spatial module, but I think a more natural place for it, for now anyway, is Lucene's sandbox module: it's very new, its APIs/abstractions are very much in flux (and the higher standards for abstractions in the spatial module cause disagreements: LUCENE-6578), Karl Wright and others could iterate faster on changes in sandbox, etc. This would also un-block issues like LUCENE-6480, allowing GeoPointField and BKD trees to also use geo3d.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6631</id>
      <title>Lucene Document Classification</title>
      <description>Currently the Lucene Classification module supports the classification for an input text using the Lucene index as a trained model. This improvement is adding to the module a set of components to provide Document classification ( where the Document is a Lucene document ). All selected fields from the Document will have their part in the classification ( including the use of the proper Analyzer per field).</description>
      <attachments/>
    </issue>
    <issue>
      <id>6645</id>
      <title>BKD tree queries should use BitDocIdSet.Builder</title>
      <description>When I was iterating on BKD tree originally I remember trying to use this builder (which makes a sparse bit set at first and then upgrades to dense if enough bits get set) and being disappointed with its performance. I wound up just making a FixedBitSet every time, but this is obviously wasteful for small queries. It could be the perf was poor because I was always .or'ing in DISIs that had 512 - 1024 hits each time (the size of each leaf cell in the BKD tree)? I also had to make my own DISI wrapper around each leaf cell... maybe that was the source of the slowness, not sure. I also sort of wondered whether the SmallDocSet in spatial module (backed by a SentinelIntSet) might be faster ... though it'd need to be sorted in the and after building before returning to Lucene.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6651</id>
      <title>Remove private field reflection (setAccessible) in AttributeImpl#reflectWith</title>
      <description>In AttributeImpl we currently have a "default" implementation of reflectWith (which is used by toString() and other methods) that uses reflection to list all private fields of the implementation class and reports them to the AttributeReflector (used by Solr and Elasticsearch to show analysis output). Unfortunately this default implementation needs to access private fields of a subclass, which does not work without doing Field#setAccessible(true). And this is done without AccessController#doPrivileged()! There are 2 solutions to solve this: Reimplement the whole thing with MethodHandles. MethodHandles allow to access private fields, if you have a MethodHandles.Lookup object created from inside the subclass. The idea is to add a protected constructor taking a Lookup object (must come from same class). This Lookup object is then used to build methodHandles that can be executed to report the fields. Backside: We have to require subclasses that want this "automatic" reflection to pass a Lookup object in ctor's super(MethodHandles.lookup()) call. This breaks backwards for implementors of AttributeImpls The second idea is to remove the whole reflectWith default impl and make the method abstract. This would require a bit more work in tons of AttributeImpl classes, but you already have to implement something like this for equals/hashCode, so its just listing all fields. This would of couse break backwards, too. So my plan would be to implement the missing methods everywhere (as if it were abstract), but keep the default implementation in 5.x. We just would do AccessController.doPrivileged().</description>
      <attachments/>
    </issue>
    <issue>
      <id>6664</id>
      <title>Replace SynonymFilter with SynonymGraphFilter</title>
      <description>Spinoff from LUCENE-6582. I created a new SynonymGraphFilter (to replace the current buggy SynonymFilter), that produces correct graphs (does no "graph flattening" itself). I think this makes it simpler. This means you must add the FlattenGraphFilter yourself, if you are applying synonyms during indexing. Index-time syn expansion is a necessarily "lossy" graph transformation when multi-token (input or output) synonyms are applied, because the index does not store posLength, so there will always be phrase queries that should match but do not, and then phrase queries that should not match but do. http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html goes into detail about this. However, with this new SynonymGraphFilter, if instead you do synonym expansion at query time (and don't do the flattening), and you use TermAutomatonQuery (future: somehow integrated into a query parser), or maybe just "enumerate all paths and make union of PhraseQuery", you should get 100% correct matches (not sure about "proper" scoring though...). This new syn filter still cannot consume an arbitrary graph.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6697</id>
      <title>Use 1D KD tree for alternative to postings based numeric range filters</title>
      <description>Today Lucene uses postings to index a numeric value at multiple precision levels for fast range searching. It's somewhat costly: each numeric value is indexed with multiple terms (4 terms by default) ... I think a dedicated 1D BKD tree should be more compact and perform better. It should also easily generalize beyond 64 bits to arbitrary byte[], e.g. for LUCENE-5596, but I haven't explored that here. A 1D BKD tree just sorts all values, and then indexes adjacent leaf blocks of size 512-1024 (by default) values per block, and their docIDs, into a fully balanced binary tree. Building the range filter is then just a recursive walk through this tree. It's the same structure we use for 2D lat/lon BKD tree, just with 1D instead. I implemented it as a DocValuesFormat that also writes the numeric tree on the side.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6699</id>
      <title>Integrate lat/lon BKD and spatial3d</title>
      <description>I'm opening this for discussion, because I'm not yet sure how to do this integration, because of my ignorance about spatial in general and spatial3d in particular Our BKD tree impl is very fast at doing lat/lon shape intersection (bbox, polygon, soon distance: LUCENE-6698) against previously indexed points. I think to integrate with spatial3d, we would first need to record lat/lon/z into doc values. Somewhere I saw discussion about how we could stuff all 3 into a single long value with acceptable precision loss? Or, we could use BinaryDocValues? We need all 3 dims available to do the fast per-hit query time filtering. But, second: what do we index into the BKD tree? Can we "just" index earth surface lat/lon, and then at query time is spatial3d able to give me an enclosing "surface lat/lon" bbox for a 3d shape? Or ... must we index all 3 dimensions into the BKD tree (seems like this could be somewhat wasteful)?</description>
      <attachments/>
    </issue>
    <issue>
      <id>6706</id>
      <title>Support Payload scoring for all SpanQueries</title>
      <description>I need a way to have payloads influence the score of SpanOrQuery's.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6732</id>
      <title>Improve validate-source-patterns in build.xml (e.g., detect invalid license headers!!)</title>
      <description>Today I enabled warnings analysis on Policeman Jenkins. This scans the build log for warnings by javac and reports them in statistics, together with source file dumps. When doing that I found out that someone added again a lot of "invalid" license headers using /** instead a simple comment. This causes javadocs warnings under some circumstances, because /** is start of javadocs and not a license comment. I then tried to fix the validate-source-patterns to detect this, but due to a bug in ANT, the &lt;containsregexp/&gt; filter is applied per line (although it has multiline matching capabilities!!!). So I rewrote our checker to run with groovy. This also has some good parts: it tells you wwhat was broken, otherwise you just know there is an error, but not whats wrong (tab, nocommit,...) its much faster (multiple &lt;containsregexp/&gt; read file over and over, this one reads file one time into a string and then applies all regular expressions).</description>
      <attachments/>
    </issue>
    <issue>
      <id>6740</id>
      <title>Reduce warnings emitted by javac in Java 7/Java 8</title>
      <description>This is a overview issue about improvements to reduce warnings during the build. Since we moved trunk to Java 8 and branch_5x to Java 7, there were introduced some additional warnings, mostly in generated code. ANTLR4 does automatically add the needed @SuppressWarnings, but jflex does not. There are also some rawtypes warnings, which changed in Java 7 (before Java 7 there were only "unsafe" warnings, now you need both. I enabled the Warnings tracker in Policeman Jenkins, we can therefore very easily check the warnings: e.g. on trunk: http://jenkins.thetaphi.de/job/Lucene-Solr-trunk-Linux/ (see diagram on the right), you can click on it, e.g. http://jenkins.thetaphi.de/job/Lucene-Solr-trunk-Linux/warnings4</description>
      <attachments/>
    </issue>
    <issue>
      <id>6759</id>
      <title>Integrate lat/long BKD and spatial 3d, part 2</title>
      <description>This is just a continuation of LUCENE-6699, which became too big.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6766</id>
      <title>Make index sorting a first-class citizen</title>
      <description>Today index sorting is a very expert feature. You need to use a custom merge policy, custom collectors, etc. I would like to explore making it a first-class citizen so that: the sort order could be configured on IndexWriterConfig segments would record the sort order that was used to write them IndexSearcher could automatically early terminate when computing top docs on a sort order that is a prefix of the sort order of a segment (and if the user is not interested in totalHits).</description>
      <attachments/>
    </issue>
    <issue>
      <id>6770</id>
      <title>FSDirectory ctor should use getAbsolutePath instead of getRealPath for directory</title>
      <description>After upgrade from 4.1 to 5.2.1 I found that one of our test failed. Appeared the guilty was FSDirectory that converts given Path to Path.getRealPath. As result the test will fail: Path p = Paths.get("/var/lucene_store"); FSDirectory d = new FSDirectory(p); assertEquals(p.toString(), d.getDirectory().toString()); It because /var/lucene_store is a symlink and Path directory =path.getRealPath(); resolves it to /private/var/lucene_store I think this is bad design decision because "direcrory" isn't just internal state but is exposed in a public interface and "getDirectory()" is widely used to initialize other components. It should use paths.getAbsolutePath() instead. build and "ant test" were successful after fix.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6779</id>
      <title>Reduce memory allocated by CompressingStoredFieldsWriter to write large strings</title>
      <description>In SOLR-7927, I am trying to reduce the memory required to index very large documents (between 10 to 100MB) and one of the places which allocate a lot of heap is the UTF8 encoding in CompressingStoredFieldsWriter. The same problem existed in JavaBinCodec and we reduced its memory allocation by falling back to a double pass approach in SOLR-7971 when the utf8 size of the string is greater than 64KB. I propose to make the same changes to CompressingStoredFieldsWriter as we made to JavaBinCodec in SOLR-7971.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6785</id>
      <title>Consider merging Query.rewrite() into Query.createWeight()</title>
      <description>Prompted by the discussion on LUCENE-6590. Query.rewrite() is a bit of an oddity. You call it to create a query for a specific IndexSearcher, and to ensure that you get a query implementation that has a working createWeight() method. However, Weight itself already encapsulates the notion of a per-searcher query. You also need to repeatedly call rewrite() until the query has stopped rewriting itself, which is a bit trappy - there are a few places (in highlighting code for example) that just call rewrite() once, rather than looping round as IndexSearcher.rewrite() does. Most queries don't need to be called multiple times, however, so this seems a bit redundant. And the ones that do currently return un-rewritten queries can be changed simply enough to rewrite them. Finally, in pretty much every case I can find in the codebase, rewrite() is called purely as a prelude to createWeight(). This means, in the case of for example large BooleanQueries, we end up cloning the whole query structure, only to throw it away immediately. I'd like to try removing rewrite() entirely, and merging the logic into createWeight(), simplifying the API and removing the trap where code only calls rewrite once. What do people think?</description>
      <attachments/>
    </issue>
    <issue>
      <id>6825</id>
      <title>Add multidimensional byte[] indexing support to Lucene</title>
      <description>I think we should graduate the low-level block KD-tree data structure from sandbox into Lucene's core? This can be used for very fast 1D range filtering for numerics, removing the 8 byte (long/double) limit we have today, so e.g. we could efficiently support BigInteger, BigDecimal, IPv6 addresses, etc. It can also be used for &gt; 1D use cases, like 2D (lat/lon) and 3D (x/y/z with geo3d) geo shape intersection searches. The idea here is to add a new part of the Codec API (DimensionalFormat maybe?) that can do low-level N-dim point indexing and at runtime exposes only an "intersect" method. It should give sizable performance gains (smaller index, faster searching) over what we have today, and even over what auto-prefix with efficient numeric terms would do. There are many steps here ... and I think adding this is analogous to how we added FSTs, where we first added low level data structure support and then gradually cutover the places that benefit from an FST. So for the first step, I'd like to just add the low-level block KD-tree impl into oal.util.bkd, but make a couple improvements over what we have now in sandbox: Use byte[] as the value not int (@rjernst's good idea!) Generalize it to arbitrary dimensions vs. specialized/forked 1D, 2D, 3D cases we have now This is already hard enough After that we can build the DimensionalFormat on top, then cutover existing specialized block KD-trees. We also need to fix OfflineSorter to use Directory API so we don't fill up /tmp when building a block KD-tree. A block KD-tree is at heart an inverted data structure, like postings, but is also similar to auto-prefix in that it "picks" proper N-dimensional "terms" (leaf blocks) to index based on how the specific data being indexed is distributed. I think this is a big part of why it's so fast, i.e. in contrast to today where we statically slice up the space into the same terms regardless of the data (trie shifting, morton codes, geohash, hilbert curves, etc.) I'm marking this as trunk only for now... as we iterate we can see if it could maybe go back to 5.x...</description>
      <attachments/>
    </issue>
    <issue>
      <id>6829</id>
      <title>OfflineSorter should use Directory API</title>
      <description>I think this is a blocker for LUCENE-6825, because the block KD-tree makes heavy use of OfflineSorter and we don't want to fill up tmp space ... This should be a straightforward cutover, but there are some challenges, e.g. the test was failing because virus checker blocked deleting of files.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6833</id>
      <title>Upgrade morfologik to version 2.0.1, simplify MorfologikFilter's dictionary lookup</title>
      <description>This is a follow-up to Uwe's work on LUCENE-6774. This patch updates the code to use Morfologik stemming version 2.0.1, which removes the "automatic" lookup of classpath-relative dictionary resources in favor of an explicit InputStream or URL. So the user code is explicitly responsible to provide these resources, reacting to missing files, etc. There were no other "default" dictionaries in Morfologik other than the Polish dictionary so I also cleaned up the filter code from a number of attributes that were, to me, confusing. MorfologikFilterFactory now accepts an (optional) dictionary attribute which contains an explicit name of the dictionary resource to load. The resource is loaded with a ResourceLoader passed to the inform(..) method, so the final location depends on the resource loader. There is no way to load the dictionary and metadata separately (this isn't at all useful). If the dictionary attribute is missing, the filter loads the Polish dictionary by default (since most people would be using Morfologik for stemming Polish anyway). This patch is not backward compatible, but it attempts to provide useful feedback on initialization: if the removed attributes were used, it points at this JIRA issue, so it should be clear what to change and how.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6835</id>
      <title>Directory.deleteFile should "own" retrying deletions on Windows</title>
      <description>Rob's idea: Today, we have hairy logic in IndexFileDeleter to deal with Windows file systems that cannot delete still open files. And with LUCENE-6829, where OfflineSorter now must deal with the situation too ... I worked around it by fixing all tests to disable the virus checker. I think it makes more sense to push this "platform specific problem" lower in the stack, into Directory? I.e., its deleteFile method would catch the access denied, and then retry the deletion later. Then we could re-enable virus checker on all these tests, simplify IndexFileDeleter, etc. Maybe in the future we could further push this down, into WindowsDirectory, and fix FSDirectory.open to return WindowsDirectory on windows ...</description>
      <attachments/>
    </issue>
    <issue>
      <id>6837</id>
      <title>Add N-best output capability to JapaneseTokenizer</title>
      <description>Japanese morphological analyzers often generate mis-segmented tokens. N-best output reduces the impact of mis-segmentation on search result. N-best output is more meaningful than character N-gram, and it increases hit count too. If you use N-best output, you can get decompounded tokens (ex: "シニアソフトウェアエンジニア" =&gt; {"シニア", "シニアソフトウェアエンジニア", "ソフトウェア", "エンジニア"} ) and overwrapped tokens (ex: "数学部長谷川" =&gt; {"数学", "部", "部長", "長谷川", "谷川"} ), depending on the dictionary and N-best parameter settings.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6863</id>
      <title>Store sparse doc values more efficiently</title>
      <description>For both NUMERIC fields and ordinals of SORTED fields, we store data in a dense way. As a consequence, if you have only 1000 documents out of 1B that have a value, and 8 bits are required to store those 1000 numbers, we will not require 1KB of storage, but 1GB. I suspect this mostly happens in abuse cases, but still it's a pity that we explode storage requirements. We could try to detect sparsity and compress accordingly.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6874</id>
      <title>WhitespaceTokenizer should tokenize on NBSP</title>
      <description>WhitespaceTokenizer uses Character.isWhitespace to decide what is whitespace. Here's a pertinent excerpt: It is a Unicode space character (SPACE_SEPARATOR, LINE_SEPARATOR, or PARAGRAPH_SEPARATOR) but is not also a non-breaking space ('\u00A0', '\u2007', '\u202F') Perhaps Character.isWhitespace should have been called isLineBreakableWhitespace? I think WhitespaceTokenizer should tokenize on this. I am aware it's easy to work around but why leave this trap in by default?</description>
      <attachments/>
    </issue>
    <issue>
      <id>6875</id>
      <title>New Serbian Filter</title>
      <description>This is a new Serbian filter that works with regular Latin text (the current filter works with "bald" Latin). I described in detail what does it do and why is it necessary at the wiki.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6889</id>
      <title>BooleanQuery.rewrite could easily optimize some simple cases</title>
      <description>Follow-up of SOLR-8251: APIs and user interfaces sometimes encourage to write BooleanQuery instances that are not optimal, for instance a typical case that happens often with Solr/Elasticsearch is to send a request that has a MatchAllDocsQuery as a query and some filter, which could be executed more efficiently by directly wrapping the filter into a ConstantScoreQuery. Here are some ideas of rewrite operations that BooleanQuery could perform: remove FILTER clauses when they are also a MUST clause rewrite queries of the form "+: #filter" to a ConstantScoreQuery(filter) rewrite to a MatchNoDocsQuery when a clause that is a MUST or FILTER clause is also a MUST_NOT clause</description>
      <attachments/>
    </issue>
    <issue>
      <id>6917</id>
      <title>Deprecate and rename NumericField/RangeQuery to LegacyNumeric</title>
      <description>DimensionalValues seems to be better across the board (indexing time, indexing size, search-speed, search-time heap required) than NumericField, at least in my testing so far. I think for 6.0 we should move IntField, LongField, FloatField, DoubleField and NumericRangeQuery to backward-codecs, and rename with Legacy prefix?</description>
      <attachments/>
    </issue>
    <issue>
      <id>6919</id>
      <title>Change the Scorer API to expose an iterator instead of extending DocIdSetIterator</title>
      <description>I was working on trying to address the performance regression on LUCENE-6815 but this is hard to do without introducing specialization of DisjunctionScorer which I'd like to avoid at all costs. I think the performance regression would be easy to address without specialization if Scorers were changed to return an iterator instead of extending DocIdSetIterator. So conceptually the API would move from class Scorer extends DocIdSetIterator { } to class Scorer { DocIdSetIterator iterator(); } This would help me because then if none of the sub clauses support two-phase iteration, DisjunctionScorer could directly return the approximation as an iterator instead of having to check if twoPhase == null at every iteration. Such an approach could also help remove some method calls. For instance TermScorer.nextDoc calls PostingsEnum.nextDoc but with this change TermScorer.iterator() could return the PostingsEnum and TermScorer would not even appear in stack traces when scoring. I hacked a patch to see how much that would help and luceneutil seems to like the change: TaskQPS baseline StdDev QPS patch StdDev Pct diff Fuzzy1 88.54 (15.7%) 86.73 (16.6%) -2.0% ( -29% - 35%) AndHighLow 698.98 (4.1%) 691.11 (5.1%) -1.1% ( -9% - 8%) Fuzzy2 26.47 (11.2%) 26.28 (10.3%) -0.7% ( -19% - 23%) MedSpanNear 141.03 (3.3%) 140.51 (3.2%) -0.4% ( -6% - 6%) HighPhrase 60.66 (2.6%) 60.48 (3.3%) -0.3% ( -5% - 5%) LowSpanNear 29.25 (2.4%) 29.21 (2.1%) -0.1% ( -4% - 4%) MedPhrase 28.32 (1.9%) 28.28 (2.0%) -0.1% ( -3% - 3%) LowPhrase 17.31 (2.1%) 17.29 (2.6%) -0.1% ( -4% - 4%) HighSloppyPhrase 10.93 (6.0%) 10.92 (6.0%) -0.1% ( -11% - 12%) MedSloppyPhrase 72.21 (2.2%) 72.27 (1.8%) 0.1% ( -3% - 4%) Respell 57.35 (3.2%) 57.41 (3.4%) 0.1% ( -6% - 6%) HighSpanNear 26.71 (3.0%) 26.75 (2.5%) 0.1% ( -5% - 5%) OrNotHighLow 803.46 (3.4%) 807.03 (4.2%) 0.4% ( -6% - 8%) LowSloppyPhrase 88.02 (3.4%) 88.77 (2.5%) 0.8% ( -4% - 7%) OrNotHighMed 200.45 (2.7%) 203.83 (2.5%) 1.7% ( -3% - 7%) OrHighHigh 38.98 (7.9%) 40.30 (6.6%) 3.4% ( -10% - 19%) HighTerm 92.53 (5.3%) 95.94 (5.8%) 3.7% ( -7% - 15%) OrHighMed 53.80 (7.7%) 55.79 (6.6%) 3.7% ( -9% - 19%) AndHighMed 266.69 (1.7%) 277.15 (2.5%) 3.9% ( 0% - 8%) Prefix3 44.68 (5.4%) 46.60 (7.0%) 4.3% ( -7% - 17%) MedTerm 261.52 (4.9%) 273.52 (5.4%) 4.6% ( -5% - 15%) Wildcard 42.39 (6.1%) 44.35 (7.8%) 4.6% ( -8% - 19%) IntNRQ 10.46 (7.0%) 10.99 (9.5%) 5.0% ( -10% - 23%) OrNotHighHigh 67.15 (4.6%) 70.65 (4.5%) 5.2% ( -3% - 15%) OrHighNotHigh 43.07 (5.1%) 45.36 (5.4%) 5.3% ( -4% - 16%) OrHighLow 64.19 (6.4%) 67.72 (5.5%) 5.5% ( -6% - 18%) AndHighHigh 64.17 (2.3%) 67.87 (2.1%) 5.8% ( 1% - 10%) LowTerm 642.94 (10.9%) 681.48 (8.5%) 6.0% ( -12% - 28%) OrHighNotMed 12.68 (6.9%) 13.51 (6.6%) 6.5% ( -6% - 21%) OrHighNotLow 54.69 (6.8%) 58.25 (7.0%) 6.5% ( -6% - 21%)</description>
      <attachments/>
    </issue>
    <issue>
      <id>6922</id>
      <title>Improve svn to git workaround script</title>
      <description>As the git-svn mirror for Lucene/Solr will be turned off near the end of 2015, try and improve the workaround script to become more usable.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6930</id>
      <title>Decouple GeoPointField from NumericType</title>
      <description>GeoPointField currently relies on NumericTokenStream to create prefix terms for a GeoPoint using the precision step defined in GeoPointField. At search time GeoPointTermsEnum recurses to a max precision that is computed by the Query parameters. This max precision is never the full precision, so creating and indexing the full precision terms is useless and wasteful (it was always a side effect of just using indexing logic from the Numeric type). Furthermore, since the numerical logic always stored high precision terms first, the recursion in GeoPointTermsEnum required transient memory for storing ranges. By moving the trie logic to its own GeoPointTokenStream and reversing the term order (such that lower resolution terms are first), the GeoPointTermsEnum can naturally traverse, enabling on-demand creation of PrefixTerms. This will be done in a separate issue.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6965</id>
      <title>Expression's JavascriptCompiler to throw ParseExceptions with bad function names or arity</title>
      <description>Currently JavascriptCompiler will throw IllegalArgumentException for bad function names (or functions that don't exist) and for bad arity. I can see why this was done this way, but I believe ParseException would also be correct and it would be better since that's the exception clients will be prepared to receive.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6966</id>
      <title>Contribution: Codec for index-level encryption</title>
      <description>We would like to contribute a codec that enables the encryption of sensitive data in the index that has been developed as part of an engagement with a customer. We think that this could be of interest for the community. Below is a description of the project. Introduction In comparison with approaches where all data is encrypted (e.g., file system encryption, index output / directory encryption), encryption at a codec level enables more fine-grained control on which block of data is encrypted. This is more efficient since less data has to be encrypted. This also gives more flexibility such as the ability to select which field to encrypt. Some of the requirements for this project were: The performance impact of the encryption should be reasonable. The user can choose which field to encrypt. Key management: During the life cycle of the index, the user can provide a new version of his encryption key. Multiple key versions should co-exist in one index. What is supported ? Block tree terms index and dictionary Compressed stored fields format Compressed term vectors format Doc values format (prototype based on an encrypted index output) - this will be submitted as a separated patch Index upgrader: command to upgrade all the index segments with the latest key version available. How it is implemented ? Key Management One index segment is encrypted with a single key version. An index can have multiple segments, each one encrypted using a different key version. The key version for a segment is stored in the segment info. The provided codec is abstract, and a subclass is responsible in providing an implementation of the cipher factory. The cipher factory is responsible of the creation of a cipher instance based on a given key version. Encryption Model The encryption model is based on AES/CBC with padding. Initialisation vector (IV) is reused for performance reason, but only on a per format and per segment basis. While IV reuse is usually considered a bad practice, the CBC mode is somehow resilient to IV reuse. The only "leak" of information that this could lead to is being able to know that two encrypted blocks of data starts with the same prefix. However, it is unlikely that two data blocks in an index segment will start with the same data: Stored Fields Format: Each encrypted data block is a compressed block (~4kb) of one or more documents. It is unlikely that two compressed blocks start with the same data prefix. Term Vectors: Each encrypted data block is a compressed block (~4kb) of terms and payloads from one or more documents. It is unlikely that two compressed blocks start with the same data prefix. Term Dictionary Index: The term dictionary index is encoded and encrypted in one single data block. Term Dictionary Data: Each data block of the term dictionary encodes a set of suffixes. It is unlikely to have two dictionary data blocks sharing the same prefix within the same segment. DocValues: A DocValues file will be composed of multiple encrypted data blocks. It is unlikely to have two data blocks sharing the same prefix within the same segment (each one will encodes a list of values associated to a field). To the best of our knowledge, this model should be safe. However, it would be good if someone with security expertise in the community could review and validate it. Performance We report here a performance benchmark we did on an early prototype based on Lucene 4.x. The benchmark was performed on the Wikipedia dataset where all the fields (id, title, body, date) were encrypted. Only the block tree terms and compressed stored fields format were tested at that time. Indexing The indexing throughput slightly decreased and is roughly 15% less than with the base Lucene. The merge time slightly increased by 35%. There was no significant difference in term of index size. Query Throughput With respect to query throughput, we observed no significant impact on the following queries: Term query, boolean query, phrase query, numeric range query. We observed the following performance impact for queries that needs to scan a larger portion of the term dictionary: prefix query: decrease of ~25% wildcard query (e.g., “fu*r”): decrease of ~60% fuzzy query (distance 1): decrease of ~40% fuzzy query (distance 2): decrease of ~80% We can see that the decrease of performance is relative to the size of the dictionary scan. Document Retrieval We observed a decrease of performance that is relative to the size of the set of documents to be retrieved: ~20% when retrieving a medium set of documents (100) ~30/40% when retrieving a large set of documents (1000) Known Limitations compressed stored field do not keep order of fields since non-encrypted and encrypted fields are stored in separated blocks. the current implementation of the cipher factory does not enforce the use of AES/CBC. We are planning to add this to the final version of the patch. the current implementation does not change the IV per segment. We are planning to add this to the final version of the patch. the current implementation of compressed stored fields decrypts a full compressed block even if a small portion is decompressed (high impact when storing very small documents). We are planning to add this optimisation to the final version of the patch. The overall document retrieval performance might increase with this optimisation. The codec has been implemented as a contrib. Given that most of the classes were final, we had to copy most of the original code from the extended formats. At a later stage, we could think of opening some of these classes to extend them properly in order to reduce code duplication and simplify code maintenance.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6968</id>
      <title>LSH Filter</title>
      <description>I'm planning to implement LSH. Which support query like this Find similar documents that have 0.8 or higher similar score with a given document. Similarity measurement can be cosine, jaccard, euclid.. For example. Given following corpus 1. Solr is an open source search engine based on Lucene 2. Solr is an open source enterprise search engine based on Lucene 3. Solr is an popular open source enterprise search engine based on Lucene 4. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java We wanna find documents that have 0.6 score in jaccard measurement with this doc Solr is an open source search engine It will return only docs 1,2 and 3 (MoreLikeThis will also return doc 4)</description>
      <attachments/>
    </issue>
    <issue>
      <id>6973</id>
      <title>Improve TeeSinkTokenFilter</title>
      <description>TeeSinkTokenFilter can be improved in several ways, as it's written today: The most major one is removing SinkFilter which just doesn't work and is confusing. E.g., if you set a SinkFilter which filters tokens, the attributes on the stream such as PositionIncrementAttribute are not updated. Also, if you update any attribute on the stream, you affect other SinkStreams ... It's best if we remove this confusing class, and let consumers reuse existing TokenFilters by chaining them to the sink stream. After we do that, we can make all the cached states a single (immutable) list, which is shared between all the sink streams, so we don't need to keep many references around, and also deal with WeakReference. Besides that there are some other minor improvements to the code that will come after we clean up this class. From a backwards-compatibility standpoint, I don't think that SinkFilter is actually used anywhere (since it just ... confusing and doesn't work as expected), and therefore I believe it won't affect anyone. If however someone did implement a SinkFilter, it should be trivial to convert it to a TokenFilter and chain it to the SinkStream.</description>
      <attachments/>
    </issue>
    <issue>
      <id>6978</id>
      <title>Make LuceneTestCase use language tags instead of parsing locales by hand</title>
      <description>Since we are on Java 7, the JDK supports standardized language tags as identifiers for Locales. Previous versions of JDK were missing a constructor from Locale#toString() back to a locale, so we had our own, which was broken several times after the JDK changed their Locale internals. This patch will do the following: When printing the reproduce line, it will use Locale#getLanguageTag(), so you can identify the locale in standardized form. Most notable change is (next to more flexibility around asian languages) the change away from undescores. So it prints "en-US", not "en_US". The code that parses a locale uses Locale's Builder and sets the language tag. This will fail if the tag is invalid! A trap is Locale#forLanguageTag, because this one silently returns root locale if unparseable... The random locale is choosen from all language tags, which are extracted from the JDK as a String[] array. I would also like to place Locale#forLanguageTag on the forbidden list and disallow directly calling Locale#toString(), the latter is legacy API (according to Java 7 Javadocs). This would fail code that calls toString() directly, e.g. when formatting stuff like "my Locale: " + locale. Of course we cannot catch all bad uses.</description>
      <attachments/>
    </issue>
  </issues>
</root>
